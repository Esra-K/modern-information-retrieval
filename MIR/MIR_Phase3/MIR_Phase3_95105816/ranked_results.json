[{"id": "f90720ed12e045ac84beb94c27271d6fb8ad48cf", "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "authors": ["Jonathan Frankle", "Michael Carbin"], "date": 2018, "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. \nThis paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. \nThe lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.", "references": ["34f25a8704614163c4095b3ee2fc969b60de4698", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "cc46229a7c47f485e090857cbab6e6bf68c09811", "642d0f49b7826adcf986616f4af77e736229990f", "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "397de65a9a815ec39b3704a79341d687205bc80a", "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724"], "page_rank": 0.0}, {"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "date": 2017, "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "references": ["b60abe57bc195616063be10638c6437358c81d1e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "98445f4172659ec5e891e031d8202c102135c644", "032274e57f7d8b456bd255fe76b909b2c1d7458e", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "13d9323a8716131911bfda048a40e2cde1a76a46", "d76c07211479e233f7c6a6f32d5346c983c5598f", "43428880d75b3a14257c3ee9bda054e61eb869c0", "510e26733aaff585d65701b9f1be7ca9d5afc586"], "page_rank": 0.00029556650246305416}, {"id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "date": 2019, "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "references": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "8c1b00128e74f1cd92aede3959690615695d5101", "93b8da28d006415866bf48f9a6e06b5242129195", "687bac2d3320083eb4530bf18bb8f8f721477600", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "27e98e09cf09bc13c913d01676e5f32624011050", "6e795c6e9916174ae12349f5dc3f516570c17ce8"], "page_rank": 0.0}, {"id": "cc46229a7c47f485e090857cbab6e6bf68c09811", "title": "Understanding Dropout", "authors": ["Pierre Baldi", "Peter Sadowski"], "date": 2013, "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically \"dropping out\" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.", "references": ["327d3df8ea2020882827d6bace1e26c9d24309c2", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "7b0db6135b8dd3e2a9efa86163e91c0cd0fdf660", "0688fbcbfb08d7b91238bc90589209b31f97290f", "7ab5ceb40c0e267ea6fcdbcaedd327d7b263bb8e", "ba15f09796d53adfbe9e78cf79182e59b6045543", "fc6b1ff29f2da985cccfa644652bb320d7720d59"], "page_rank": 0.000541871921182266}, {"id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "date": 2016, "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "references": ["7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "d559dd84fc473fca7e91b9075675750823935afa", "3ed94217fbf29b86d5f1baec90dc33adacb40b58", "d5b4721c8188269b120d3d06149a04435753e755", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "8ad35df17ae4064dd174690efb04d347428f1117", "751c8884c1e857e675d85d8594c5f9b608005ed5", "397de65a9a815ec39b3704a79341d687205bc80a", "b64601d509711468f5d085261d463846f36785b2", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff"], "page_rank": 0.0002134646962233169}, {"id": "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "title": "Data-free Parameter Pruning for Deep Neural Networks", "authors": ["Suraj Srinivas", "R. Venkatesh Babu"], "date": 2015, "abstract": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\\% of the total parameters in an MNIST-trained network, and about 35\\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.", "references": ["2a4117849c88d4728c33b1becaa9fb6ed7030725", "34f25a8704614163c4095b3ee2fc969b60de4698", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "cd85a549add0c7c7def36aca29837efd24b24080", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "0c908739fbff75f03469d13d4a1a07de3414ee19", "e8650503ab80ad7299f0845b1843abf3a97f313a"], "page_rank": 0.00041871921182266007}, {"id": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning", "authors": ["Babak Hassibi", "David G. Stork", "Gregory J. Wolff"], "date": 1993, "abstract": "The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. OBS deletes the correct weights from a trained XOR network in every case.>", "references": ["a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "5887de8eed53c444b2ef93d8ab9c8cc685cd7ac5", "de996c32045df6f7b404dda2a753b6a9becf3c08", "1b29884885401d12299a01b0eae099f425dd32e1", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "57dc98cfb48247b400cc8decb93380e022864905"], "page_rank": 4.926108374384236e-05}, {"id": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": 2016, "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.", "references": ["1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "a6373454105df0c5511ca5f6cae4d20c48214272", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "efb5032e6199c80f83309fd866b25be9545831fd", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "27a99c21a1324f087b2f144adc119f04137dfd87", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6bdb186ec4726e00a8051119636d4df3b94043b5"], "page_rank": 0.0003366174055829228}, {"id": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression", "authors": ["Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin"], "date": 2017, "abstract": "We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63\u00d7 compression on VGG-16, with only 0.52% top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.", "references": ["c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "60ae4f18cb53efff0174e3fea7064049737e1e67", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "642d0f49b7826adcf986616f4af77e736229990f", "7601b995303f953955004db7b9b8b206c0e02ff8", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "eb42cf88027de515750f230b23b1a057dc782108"], "page_rank": 4.926108374384236e-05}, {"id": "397de65a9a815ec39b3704a79341d687205bc80a", "title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": 2015, "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, We introduce a three stage pipeline: pruning, quantization and Huffman encoding, that work together to reduce the storage requirement of neural networks by 35\u00d7 to 49\u00d7 without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman encoding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\u00d7 to 13\u00d7; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\u00d7, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG16 by 49\u00d7 from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory, which has 180\u00d7 less access energy.", "references": ["1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "efb5032e6199c80f83309fd866b25be9545831fd", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "27a99c21a1324f087b2f144adc119f04137dfd87"], "page_rank": 0.00014778325123152708}, {"id": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "date": 2014, "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "references": ["5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "5d90f06bb70a0a3dced62413346235c02b1aa086", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "db869fa192a3222ae4f2d766674a378e47013b1b", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "3c20df69865df6a627cc45c524869ccc0297048f", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "d124a098cdc6f99b9a152fcf8afa9327dac583be", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd"], "page_rank": 0.0006650246305418719}, {"id": "b60abe57bc195616063be10638c6437358c81d1e", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "authors": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "date": 2016, "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT\u201914 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT\u201914 English-to-German task.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "1956c239b3552e030db1b78951f64781101125ed", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "c34e41312b47f60986458759d5cc546c2b53f748", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d"], "page_rank": 0.0005829228243021346}, {"id": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann Dauphin"], "date": 2017, "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "f958d4921951e394057a1c4ec33bad9a34e5dad1", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "b60abe57bc195616063be10638c6437358c81d1e", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "0b544dfe355a5070b60986319a3f51fb45d1348e", "510e26733aaff585d65701b9f1be7ca9d5afc586", "0936352b78a52bc5d2b5e3f04233efc56664af51"], "page_rank": 4.926108374384236e-05}, {"id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "date": 2015, "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "references": ["642d0f49b7826adcf986616f4af77e736229990f", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "34f25a8704614163c4095b3ee2fc969b60de4698", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "081651b38ff7533550a3adfc1c00da333a8fe86c", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916"], "page_rank": 0.0008866995073891625}, {"id": "d76c07211479e233f7c6a6f32d5346c983c5598f", "title": "Multi-task Sequence to Sequence Learning", "authors": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "date": 2016, "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "83cf4b2f39bcc802b09fd59b69e23068447b26b7", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "93499a7c7f699b6630a86fad964536f9423bb6d0", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "a41b826d23957d6ad4e9e794d20a583a9b567c5d", "c3b8367a80181e28c95630b9b63060d895de08ff"], "page_rank": 0.00017241379310344826}, {"id": "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "authors": ["Zelda Mariet", "Suvrit Sra"], "date": 2015, "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "references": ["b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "d7bfd8a283a7ed8b43255cfd04909484cd3ade28", "87d810fcea61068e8b29f2b75fa1cbb00c190bea", "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "31f88db95eb5c66b95cd7335b0cd4f27f0f271f2", "efb5032e6199c80f83309fd866b25be9545831fd", "8ba555d9587688bd3225d71ef9d686dad288e1f1"], "page_rank": 4.926108374384236e-05}, {"id": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks", "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "date": 2017, "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "references": ["04d1a26c2516dc14a765112a63ec60dc3cb3de72", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "e06a68b26bde368883761c9dceb547914b2ecca8", "bc82b4f9f202062857958f0336fc28327a75563b", "705dcc8eadba137834e4b0359e2d696d4b209f5b", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2cd8e8f510c89c7c18268e8ad51c061e459ad321"], "page_rank": 4.926108374384236e-05}, {"id": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc V. Le", "Geoffrey E. Hinton", "Jeff Dean"], "date": 2017, "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "references": ["c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "44ddac48353ead135eef4096859956eaa31be2a5", "cf3229e74f912ef365d67d1954441b32ce2573ee", "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "66b8d34477cf1736f91fd22b27e37ce0b703c86e", "5a5d48986b855b83a7d9df5005bbd155024ce756", "4d376d6978dad0374edfa6709c9556b42d3594d3", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "b60abe57bc195616063be10638c6437358c81d1e", "cea967b59209c6be22829699f05b8b1ac4dc092d"], "page_rank": 0.00017241379310344826}, {"id": "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "title": "Can Active Memory Replace Attention?", "authors": ["Lukasz Kaiser", "Samy Bengio"], "date": 2016, "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.", "references": ["eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "cea967b59209c6be22829699f05b8b1ac4dc092d", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "0811597b0851b7ebe21aadce7cb4daac4664b44f", "33108287fbc8d94160787d7b2c7ef249d3ad6437", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "2c03df8b48bf3fa39054345bafabfeff15bfd11d"], "page_rank": 4.926108374384236e-05}, {"id": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "date": 2015, "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "71ae756c75ac89e2d731c9c79649562b5768ff39", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "30110856f45fde473f1903f686aa365cf70ed4c7"], "page_rank": 0.0010755336617405582}, {"id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "date": 2019, "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "references": ["88caa4a0253a8b0076176745ebc072864eab66e1", "58c6f890a1ae372958b7decf56132fe258152722", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "f9a1b3850dfd837793743565a8af95973d395a4e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "84ca430856a92000e90cd728445ca2241c10ddc3", "27981998aaef92952eabef2c1490b926f9150c4f"], "page_rank": 4.926108374384236e-05}, {"id": "98445f4172659ec5e891e031d8202c102135c644", "title": "Neural Machine Translation in Linear Time", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A{\\\"a}ron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "date": 2016, "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "b60abe57bc195616063be10638c6437358c81d1e", "acec46ffd3f6046af97529127d98f1d623816ea4", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "733b821faeebe49b6efcf5369e3b9902b476529e", "93499a7c7f699b6630a86fad964536f9423bb6d0", "944a1cfd79dbfb6fef460360a0765ba790f4027a"], "page_rank": 4.926108374384236e-05}, {"id": "032274e57f7d8b456bd255fe76b909b2c1d7458e", "title": "A Deep Reinforced Model for Abstractive Summarization", "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "date": 2018, "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "references": ["7a67159fc7bc76d0b37930b55005a69b51241635", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "5082a1a13daea5c7026706738f8528391a1e6d59", "5ab72d44237533534de8402e30f3ccce25ce30de", "668db48c6a79826456341680ee1175dfc4cced71", "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d", "f77a604410d88307ec5c6331c8b6133272fbaa10", "cea967b59209c6be22829699f05b8b1ac4dc092d", "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "489955574c435169abd72285cfe2f055f538a401"], "page_rank": 4.926108374384236e-05}, {"id": "27e98e09cf09bc13c913d01676e5f32624011050", "title": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "authors": ["Fu Sun", "Linyang Li", "Xipeng Qiu", "Yang P. Liu"], "date": 2018, "abstract": "Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.", "references": ["9a5ba9aee44ab873f3d60b05e2773c693707da88", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "05dd7254b632376973f3a1b4d39485da17814df5", "bb6daf3bd95668ea9775d7e06d8b3f6994306cb7", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "fa025e5d117929361bcf798437957762eb5bb6d4", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "page_rank": 4.926108374384236e-05}, {"id": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "date": 2014, "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "96494e722f58705fa20302fe6179d483f52705b4", "6658bbf68995731b2083195054ff45b4eca38b3a", "167ad306d84cca2455bc50eb833454de9f2dcd02", "0b544dfe355a5070b60986319a3f51fb45d1348e", "d0be39ee052d246ae99c082a565aba25b811be2d", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "9819b600a828a57e1cde047bbe710d3446b30da5", "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6"], "page_rank": 0.004047619047619047}, {"id": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "date": 2013, "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "references": ["27e38351e48fe4b7da2775bf94341738bc4da07e", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "cfa2646776405d50533055ceb1b7f050e9014dcb", "2063745d08868c928455f422202b72146a1960fb", "553fb89d5858826c02f26e94262e8958debc777e", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "d9b0190b06ac7270e9052895f8592beb4959ccfd", "57458bc1cffe5caa45a885af986d70f723f406b4", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "6af58c061f2e4f130c3b795c21ff0c7e3903278f"], "page_rank": 0.000270935960591133}, {"id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "date": 2015, "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.", "references": ["27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "cea967b59209c6be22829699f05b8b1ac4dc092d", "687bac2d3320083eb4530bf18bb8f8f721477600", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "0b544dfe355a5070b60986319a3f51fb45d1348e", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "944a1cfd79dbfb6fef460360a0765ba790f4027a"], "page_rank": 0.00029556650246305416}, {"id": "93b8da28d006415866bf48f9a6e06b5242129195", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "date": 2018, "abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.", "references": ["ade0c116120b54b57a91da51235108b75c28375a", "e242ba1a62eb2595d89afbec2657f33d9ab4abe3", "93b4cc549a1bc4bc112189da36c318193d05d806", "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "afc2850945a871e72c245818f9bc141bd659b453", "8472e999f723a9ccaffc6089b7be1865d8a1b863", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "8f1c9b656157b1d851563fb42129245701d83175", "2997b26ffb8c291ce478bd8a6e47979d5a55c466"], "page_rank": 4.926108374384236e-05}, {"id": "8c1b00128e74f1cd92aede3959690615695d5101", "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le"], "date": 2018, "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "references": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "e94697b98b707f557436e025bdc8498fa261d3bc", "93499a7c7f699b6630a86fad964536f9423bb6d0", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "adc276e6eae7051a027a4c269fb21dae43cadfed", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "de0c30321b22c56d637e7c29cb59180f157272a8", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "c25a67ad7e8629a9d12b9e2fc356cd73af99a060"], "page_rank": 4.926108374384236e-05}, {"id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "title": "Semi-Supervised Sequence Modeling with Cross-View Training", "authors": ["Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le"], "date": 2018, "abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "references": ["7647a06965d868a4f6451bef0818994100a142e8", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "85f94d8098322f8130512b4c6c4627548ce4a6cc", "afc2850945a871e72c245818f9bc141bd659b453", "ac17cfa150d802750b46220084d850cfdb64d1c1", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "cea967b59209c6be22829699f05b8b1ac4dc092d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "d76c07211479e233f7c6a6f32d5346c983c5598f"], "page_rank": 4.926108374384236e-05}, {"id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models", "authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "date": 2017, "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "references": ["59761abc736397539bdd01ad7f9d91c8607c0457", "7ece4e8d31f872d928369ac2cf58a616a7182112", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "bc1022b031dc6c7019696492e8116598097a8c12", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "ade0c116120b54b57a91da51235108b75c28375a", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "26e743d5bd465f49b9538deaf116c15e61b7951f", "2c821e2ec8ef976d3abb36fb0dc1946f04208512", "189e6bb7523733c4e524214b9e6ae92d4ed50dac"], "page_rank": 0.0005008210180623973}, {"id": "ba15f09796d53adfbe9e78cf79182e59b6045543", "title": "On the Ky Fan inequality and related inequalities II", "authors": ["Edward Neuman", "J{\\'o}zsef S{\\'a}ndor"], "date": 2005, "abstract": "Disclosed is an information processing apparatus including a display unit displaying information on a display screen, an operation unit including a text input key, and a control unit. The control unit displays text in response to input from the operation unit in a state of displaying an initial screen on the display screen, shows an application using text to a user for selection, starts up the application in accordance with the user's selection, and executes the application, using the text inputted from the operation unit.", "references": ["718b1ccd4a794207dd12c0264042a12a7d4e7797", "a419456b897eb646d5c9b8f075ffd9d283fff26e", "8861a5a674c2a6c06f781095bd1c15811a16be01", "6d02ef4ef79f3cb57dcf4eb20f890aa1be859219", "a1ea19d292460e3b7fca3df1313482f5abc0897e", "7b0b4826a3919f4a1a5559d4dba68c515fb95d3a", "a211a693915981acea30e9360b11e055baed8299", "94fae0f11be9f67bc6818553e10599ced559422d", "1295e0ec83fa4e5efbbfaf156d8525fdc8907929", "ed16d9f71c3b5db08b95abf6e2800e7af6179f08"], "page_rank": 0.0001231527093596059}, {"id": "327d3df8ea2020882827d6bace1e26c9d24309c2", "title": "The dropout learning algorithm", "authors": ["Pierre Baldi", "Peter Sadowski"], "date": 2014, "abstract": "Dropout is a recently introduced algorithm for training neural network by randomly dropping units during training to prevent their co-adaptation. A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates. The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case. The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables. The results are also extended to other classes of transfer functions, including rectified linear functions. Approximation errors tend to cancel each other and do not accumulate. Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network. Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent. Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.", "references": ["cc46229a7c47f485e090857cbab6e6bf68c09811", "f9f19bee621faf46f90b023f8de8248b57becbc4", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "d124a098cdc6f99b9a152fcf8afa9327dac583be", "87c4eca6aceb29557f693fdc4efc1fbff003e02a", "f8d37cf4c4a2f15acd7c6ab2b4b4f25a3f3d7f9c", "7826ff60d2dfb24d2af18c5bc565c357ef9db4c1", "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "3c20df69865df6a627cc45c524869ccc0297048f", "052b1d8ce63b07fec3de9dbb583772d860b7c769"], "page_rank": 0.0001231527093596059}, {"id": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "authors": ["Matthew E. Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih"], "date": 2018, "abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.", "references": ["26f7305e4cf293b3daa672f0f75c1b0bac1e873a", "3febb2bed8865945e7fddc99efd791887bb7e14f", "efef34c1caef102ad5cc052642d75beaaf5adcaf", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "82364428995c29b3dcb60c1835548eeff4adcd20", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "3aa52436575cf6768a0a1a476601825f6a62e58f"], "page_rank": 4.926108374384236e-05}, {"id": "fc6b1ff29f2da985cccfa644652bb320d7720d59", "title": "Online Algorithms and Stochastic Approximations", "authors": ["L{\\'e}on Bottou"], "date": 1998, "abstract": "A process for the liquid phase oxidation of hydrocarbons with a molecular oxygen-containing gas in the presence of a dissolved cobalt salt catalyst characterized in that the oxidation is carried out in the substantial absence of chromium in the reaction medium i.e. a concentration of chromium in the liquid phase of not greater than 400 ppm.", "references": [], "page_rank": 0.0003694581280788177}, {"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "date": 2017, "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "references": ["b60abe57bc195616063be10638c6437358c81d1e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "98445f4172659ec5e891e031d8202c102135c644", "032274e57f7d8b456bd255fe76b909b2c1d7458e", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "13d9323a8716131911bfda048a40e2cde1a76a46", "d76c07211479e233f7c6a6f32d5346c983c5598f", "43428880d75b3a14257c3ee9bda054e61eb869c0", "510e26733aaff585d65701b9f1be7ca9d5afc586"], "page_rank": 0.00029556650246305416}, {"id": "7ab5ceb40c0e267ea6fcdbcaedd327d7b263bb8e", "title": "A refinement of the arithmetic mean-geometric mean inequality", "authors": ["Donald I. Cartwright", "Michael John Field"], "date": 1978, "abstract": "Upper and lower bounds are given for the difference between the arithmetic and geometric means of n positive real numbers in terms of the variance of these numbers. In this note we prove a simple refinement of the arithmetic mean-geometric mean inequality. Our result solves a problem posed by Kenneth S. Williams in [5] and generalizes an inequality on p. 215 of [3]. Other estimates for the difference between the means are discussed in [2], [3] and [4]. THEOREM. Suppose that Xk E [a, b] and Pk > 0 for k = 1, ... , n, where a > 0, and suppose that ,-lPk = 1. Then, writing x = E4=ipkxk, we have 2 Pk(Xk x)2X I (Xkp) 0, and let yt = fb t dm(t) and a2 f= b(t -_ [)2 dm(t) be the mean and variance of m. Then 2b a2 < exp(( log(t) dm(t)) 2< a2 This follows from our theorem and the weak* density of the measures of the form En lPk6kx (where 6x denotes the probability measure which is concentrated at the point x) in the set of all probability measures on [a, b]. (See [1, p. 709].) Notice that the inequality /b expt log(t) dm (t) < tt Received by the editors August 15, 1977. AMS (MOS) subject classifications (1970). Primary 26A87.", "references": ["d6f7057c714a972f4b7b6130b1c44bfab750c560", "827347d6dc00fca9a4cef8ebc03663a8cb0da91a", "e057b56b4bf8d03df7fdff534309cdf246a42fcd"], "page_rank": 0.0001231527093596059}, {"id": "57dc98cfb48247b400cc8decb93380e022864905", "title": "Introduction to the Theory of Neural Computation", "authors": ["John Hertz", "Anders Krogh", "Richard G. Palmer", "Roderick V. Jensen"], "date": 1994, "abstract": "Semantic Scholar extracted view of \"Introduction to the Theory of Neural Computation\" by John Hertz et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "8ad35df17ae4064dd174690efb04d347428f1117", "title": "Convolutional neural networks at constrained time cost", "authors": ["Kaiming He", "Jian Sun"], "date": 2015, "abstract": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than \u201cAlexNet\u201d [14] (16.0% top-5 error, 10-view test).", "references": ["021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "eb42cf88027de515750f230b23b1a057dc782108", "cbb19236820a96038d000dc629225d36e0b6294a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "6270baedeba28001cd1b563a199335720d6e0fe0", "d67175d17c450ab0ac9c256103828f9e9a0acb85"], "page_rank": 0.0002627257799671592}, {"id": "751c8884c1e857e675d85d8594c5f9b608005ed5", "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "authors": ["Yani A Ioannou", "Duncan P. Robertson", "Jamie Shotton", "Roberto Cipolla", "Antonio Criminisi"], "date": 2016, "abstract": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "eb42cf88027de515750f230b23b1a057dc782108", "f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "ebcea2d842d3d4e320500086aff0deb4cb4412ff", "5d90f06bb70a0a3dced62413346235c02b1aa086", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "5a434953b58c72fe2089531d6c4b4fc1325defcb"], "page_rank": 9.852216748768472e-05}, {"id": "b64601d509711468f5d085261d463846f36785b2", "title": "Efficient and accurate approximations of nonlinear convolutional networks", "authors": ["Xiangyu Zhang", "Jianhua Zou", "Xiang Ming", "Kaiming He", "Jian Sun"], "date": 2015, "abstract": "This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4\u00d7 is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the \u201cAlexNet\u201d [11], but is 4.7% more accurate.", "references": ["e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "d67175d17c450ab0ac9c256103828f9e9a0acb85", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "fbeaa499e10e98515f7e1c4ad89165e8c0677427"], "page_rank": 9.852216748768472e-05}, {"id": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "authors": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "date": 2014, "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model.", "references": ["021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "1109b663453e78a59e4f66446d71720ac58cec25", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "05cc38e249a6f642363b5a5cbd71cda67cea5893", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "e8650503ab80ad7299f0845b1843abf3a97f313a", "1366de5bb112746a555e9c0cd00de3ad8628aea8"], "page_rank": 0.0005747126436781608}, {"id": "6bdb186ec4726e00a8051119636d4df3b94043b5", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "authors": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross B. Girshick", "Sergio Guadarrama", "Trevor Darrell"], "date": 2014, "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.\n Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "title": "Depth-Gated LSTM", "authors": ["Kaisheng Yao", "Trevor Cohn", "Ekaterina Vylomova", "Kevin Duh", "Chris Dyer"], "date": 2015, "abstract": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.", "references": ["5b791cd374c7109693aaddee2c12d659ae4e3ec0", "8c571314311f507731296b21b56ab2c326b97392", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "cea967b59209c6be22829699f05b8b1ac4dc092d", "533ee188324b833e059cb59b654e6160776d5812", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "9819b600a828a57e1cde047bbe710d3446b30da5", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "e0945081b5b87187a53d4329cf77cd8bff635795", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"], "page_rank": 8.210180623973726e-05}, {"id": "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "title": "A Neural Conversational Model", "authors": ["Oriol Vinyals", "Quoc V. Le"], "date": 2015, "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "9819b600a828a57e1cde047bbe710d3446b30da5"], "page_rank": 0.00018062397372742197}, {"id": "944a1cfd79dbfb6fef460360a0765ba790f4027a", "title": "Recurrent Continuous Translation Models", "authors": ["Nal Kalchbrenner", "Phil Blunsom"], "date": 2013, "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "references": ["5f08df805f14baa826dbddcb002277b15d3f1556", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "6053d1693c0fab0c21ebbea0ba5408b441a3542b", "cd96a6e0b6bb099c515be8770764d2fd18e7b878", "d1275b2a2ab53013310e759e5c6878b96df643d4", "ab7b5917515c460b90451e67852171a531671ab8", "27e38351e48fe4b7da2775bf94341738bc4da07e", "79c0b2f44bbc2bc51de554b88ebe46204413f884", "57458bc1cffe5caa45a885af986d70f723f406b4"], "page_rank": 0.001026272577996716}, {"id": "c34e41312b47f60986458759d5cc546c2b53f748", "title": "End-to-end learning of semantic role labeling using recurrent neural networks", "authors": ["Jie Zhou", "Wei Xu"], "date": 2015, "abstract": "Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature, without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F1 = 81.27 on CoNLL2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "b5c6f0d18fd783536b4e6c2205d75b7c4477c6d2", "bc1022b031dc6c7019696492e8116598097a8c12", "57458bc1cffe5caa45a885af986d70f723f406b4", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "dee93d4481ac590f6debcd2816f1f8fd27b627d9", "8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1", "c92970286c535992a86539b761357761e97a37ee", "7ed7a41c275f2870b840a5e6c3eaec8888c9480c"], "page_rank": 8.210180623973726e-05}, {"id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference", "authors": ["Ankur P. Parikh", "Oscar T{\\\"a}ckstr{\\\"o}m", "Dipanjan Das", "Jakob Uszkoreit"], "date": 2016, "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0", "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "36c097a225a95735271960e2b63a2cb9e98bff83", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "ea2563467c1c472a346d165b7f97c86317d63ca4", "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "7f3ae283243e15e05f188a05779ccfae9a3567f4"], "page_rank": 0.0006568144499178981}, {"id": "30110856f45fde473f1903f686aa365cf70ed4c7", "title": "Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory", "authors": ["C. Lee GilesNEC", "Independence WayPrinceton"], "date": 1992, "abstract": "This work describes an approach for inferring De-terministic Context-free (DCF) Grammars in a Connectionist paradigm using a Recurrent Neu-ral Network Pushdown Automaton (NNPDA). The NNPDA consists of a recurrent neural network connected to an external stack memory through a common error function. We show that the NNPDA is able to learn the dynamics of an underlying push-down automaton from examples of grammatical and non-grammatical strings. Not only does the network learn the state transitions in the automaton , it also learns the actions required to control the stack. In order to use continuous optimization methods, we develop an analog stack which reverts to a discrete stack by quantization of all activations, after the network has learned the transition rules and stack actions. We further show an enhancement of the network's learning capabilities by providing hints. In addition, an initial comparative study of simulations with rst, second and third order recurrent networks has shown that the increased degree of freedom in a higher order networks improve generalization but not necessarily learning speed.", "references": ["25c19c8c1d6778a16f8b27beac4d9c6a55357580", "872cdc269f3cb59f8a227818f35041415091545f", "b185742930fd959aaccdfdecdb31641839a787c4", "eb512c593f089cdb5ed63c8acd44eff5a531601c", "a64ca771a733d58dcbf8f7a3fe65a09310424bf8", "6a835df43fdc2f79126319f6fa033bb42147c6f6"], "page_rank": 0.00016420361247947453}, {"id": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["S{\\'e}bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "date": 2015, "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.", "references": ["1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "53ca064b9f1b92951c1997e90b776e95b0880e52", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "0b544dfe355a5070b60986319a3f51fb45d1348e", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "b3e89f05876d47b9bd6ece225aaeee457a6824e8"], "page_rank": 0.0006568144499178981}, {"id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "date": 2014, "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "f9def788d4ae040edb8bde18b8aeea635444a4d1", "5d90f06bb70a0a3dced62413346235c02b1aa086", "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "523b12db4004b89284387f978c2af8ae0e79d54b", "b3d8dffb73bc93de239998548386c84177caa2ad", "38f35dd624cd1cf827416e31ac5e0e0454028eca"], "page_rank": 0.0006157635467980295}, {"id": "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "authors": ["Armand Joulin", "Tomas Mikolov"], "date": 2015, "abstract": "Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "1811f708b8b7456a3708fabd2fd638da36bd7ba0", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "57dbba9281cbd1b4dd5d1932f0dd605b8f498322", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "bff3ea999978e8c9503b62510bba11c2a5f24e51", "0b7005984749cf5f2caa1072866b36e17713ab84", "73ce506d16ad040aca09a4460018010b833e73d7", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "b185742930fd959aaccdfdecdb31641839a787c4"], "page_rank": 0.00016420361247947453}, {"id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "title": "Recurrent Neural Network Regularization", "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "date": 2014, "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "d1275b2a2ab53013310e759e5c6878b96df643d4", "9819b600a828a57e1cde047bbe710d3446b30da5", "0894b06cff1cd0903574acaa7fcf071b144ae775", "4ef03716945bd3907458efbe1bbf8928dafc1efc", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "533ee188324b833e059cb59b654e6160776d5812"], "page_rank": 0.00032840722495894905}, {"id": "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "title": "Reasoning about Entailment with Neural Attention", "authors": ["Tim Rockt{\\\"a}schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom{\\'a}s Kocisk{\\'y}", "Phil Blunsom"], "date": 2015, "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "fca1e631b8f93036065311eb92727c509423475a", "cea967b59209c6be22829699f05b8b1ac4dc092d", "9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3", "5082a1a13daea5c7026706738f8528391a1e6d59", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "11ec56898a9e7f401a2affe776b5297bd4e25025", "3276b9487b2336f662488f2a180622f3bcac6e82", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40"], "page_rank": 0.0006568144499178981}, {"id": "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "title": "Temporal Attention Model for Neural Machine Translation", "authors": ["Baskaran Sankaran", "Haitao Mi", "Yaser Al-Onaizan", "Abe Ittycheriah"], "date": 2016, "abstract": "Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles.", "references": ["33108287fbc8d94160787d7b2c7ef249d3ad6437", "93499a7c7f699b6630a86fad964536f9423bb6d0", "d5631abafe3003381d735d7385b07ab15c04182a", "ada937c9f51316c6ac87f9d1d4509383d23e0c21", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "22358c1e6f371db45a0d237baff6052e0a50e498", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "1af68821518f03568f913ab03fc02080247a27ff", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7"], "page_rank": 0.00016420361247947453}, {"id": "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "title": "Multilingual Distributed Representations without Word Alignment", "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "date": 2013, "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "references": ["d1f37d9cab68eb8cda669cc949394732f33264b4", "6a4007e60346e4501acc936b49b7a476e73afa1e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "3b6b08fc7709016a6444389f129323fa8fce66b1", "57458bc1cffe5caa45a885af986d70f723f406b4", "0d3233d858660aff451a6c2561a05378ed09725a", "0157dcd6122c20b5afc359a799b2043453471f7f", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "3c5126da7ce388c64b796c80d15a3c3629d6ad58"], "page_rank": 0.0004926108374384236}, {"id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "date": 2016, "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "references": ["eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "4d376d6978dad0374edfa6709c9556b42d3594d3", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "5d90f06bb70a0a3dced62413346235c02b1aa086", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "8ad35df17ae4064dd174690efb04d347428f1117"], "page_rank": 0.0002463054187192118}, {"id": "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "authors": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "date": 2011, "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.", "references": ["c6afe8a8aa13de8e3f2710ef07b22ce86a005419", "7acfdc905f734abf966aed58abb983bc015ff7fe", "6d1792f1871a99cb89dcf713abd24592630f32b7", "10b8f21e57b3392ce623c374c2c039f811ce5f69", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "4f65a57f4511629109544a9ff7d326e03e56c35e", "cadc3dbd73f0cbbe04b2a66f832c3cf34c877b41", "cfa2646776405d50533055ceb1b7f050e9014dcb", "57458bc1cffe5caa45a885af986d70f723f406b4", "9c0ddf74f87d154db88d79c640578c1610451eec"], "page_rank": 0.0003694581280788177}, {"id": "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "title": "Annotation Artifacts in Natural Language Inference Data", "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R. Bowman", "Noah A. Smith"], "date": 2018, "abstract": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6", "3c092128a2c98e5e3be5f8872cf05c635430cd60", "1778e32c18bd611169e64c1805a51abff341ca53", "b4317b8a4490c84301907a61f5b8ebb26ab8828d", "05dd7254b632376973f3a1b4d39485da17814df5", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "c333778104f648c385b4631f7b4a859787e9d3d3", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "72d7c465ef199a9670b3da7a318b0227f5cc3229"], "page_rank": 0.0003694581280788177}, {"id": "489955574c435169abd72285cfe2f055f538a401", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "authors": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun"], "date": 2016, "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "references": ["f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "ba30df190664193514d1d309cb673728ed48f449", "cea967b59209c6be22829699f05b8b1ac4dc092d", "7a67159fc7bc76d0b37930b55005a69b51241635", "5f176a929d9eaa569b430cb784280802cf8fca79", "a9614b05461bb306cc47c8cd645b9b67bb1227ba", "5082a1a13daea5c7026706738f8528391a1e6d59", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "29a294eaec7b485245aa21d994f7300f6b5da8fc", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"], "page_rank": 0.00016420361247947453}, {"id": "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales", "authors": ["Bo Pang", "Lillian Lee"], "date": 2005, "abstract": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.", "references": ["167e1359943b96b9e92ee73db1df69a1f65d731d", "9e7c7853a16a378cc24a082153b282257a9675b7", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "0650df86ad901fb9aadd9033a83c328a6f595666", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "338a891907dce447da9a0fa2f27221bd35164163", "3bc4736f9b8512043ed47357a81f26b93a1204b6", "b419fb257870fe8024fb3f07dddd66328ae644cb", "0e644cd910443bb21e87191e6c604f2ba5c1d90d", "a3b3aad58ecc6aed599c7567d4fe07ad3480a866"], "page_rank": 0.0004926108374384236}, {"id": "8f1c9b656157b1d851563fb42129245701d83175", "title": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "authors": ["Dorottya Demszky", "Kelvin Guu", "Percy Liang"], "date": 2018, "abstract": "Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA resources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenomena rarely seen in previous NLI datasets.", "references": ["c40665520563fb872b051bd27d3b43e042869ba4", "2dec6a802cbac1f640980b5106d88ae72c45ece4", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "05dd7254b632376973f3a1b4d39485da17814df5", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "1a210410493fbc052f0b7a54e7bc89cee20e8d28", "3eda43078ae1f4741f09be08c4ecab6229046a5c"], "page_rank": 0.0002463054187192118}, {"id": "ed16d9f71c3b5db08b95abf6e2800e7af6179f08", "title": "On Ky Fan's inequality", "authors": ["Ioan Gavrea", "Tiberiu Trif"], "date": 2001, "abstract": "In this paper we prove several Ky Fan type inequalities involving certain StolarskyTobey means. Mathematics subject classification (2000): 26D15.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks", "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "date": 2017, "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.", "references": ["8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "ad945987071d3c5b4b915b85e09ae3488b2212c0", "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "bc1022b031dc6c7019696492e8116598097a8c12", "eb42a490cf4f186d3383c92963817d100afd81e2", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "e2d4df61a787210c67041929f6a43203bee99edf", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "af88ce6116c2cd2927a4198745e99e5465173783", "4dabd6182ce2681c758f654561d351739e8df7bf"], "page_rank": 0.0004105090311986863}, {"id": "c25a67ad7e8629a9d12b9e2fc356cd73af99a060", "title": "Learning to Skim Text", "authors": ["Adams Wei Yu", "Hongrae Lee", "Quoc V. Le"], "date": 2017, "abstract": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "62a7fc51bcca0f242cfadedff98a553afc3a1704", "46147f08468e873ff90d1d51e65493f262c7bb57", "687bac2d3320083eb4530bf18bb8f8f721477600", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d"], "page_rank": 0.0002463054187192118}, {"id": "1295e0ec83fa4e5efbbfaf156d8525fdc8907929", "title": "Bounds for symmetric elliptic integrals", "authors": ["Edward Neuman"], "date": 2003, "abstract": "Lower and upper bounds for the four standard incomplete symmetric elliptic integrals are obtained. The bounding functions are expressed in terms of the elementary transcendental functions. Sharp bounds for the ratio of the complete elliptic integrals of the second kind and the first kind are also derived. These results can be used to obtain bounds for the product of these integrals. It is shown that an iterative numerical algorithm for computing the ratios and products of complete integrals has the second order of convergence.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "7647a06965d868a4f6451bef0818994100a142e8", "title": "Empower Sequence Labeling with Task-Aware Neural Language Model", "authors": ["Liyuan Liu", "Jingbo Shang", "Frank F. Xu", "Xiang Ren", "Huan Gui", "Juan Peng", "Jiawei Han"], "date": 2018, "abstract": "Linguistic sequence labeling is a general modeling approach that encompasses a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a novel neural framework to extract abundant knowledge hidden in raw texts to empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra annotation.", "references": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "ac17cfa150d802750b46220084d850cfdb64d1c1", "ade0c116120b54b57a91da51235108b75c28375a", "bc1022b031dc6c7019696492e8116598097a8c12", "17ff6fb495872abe1b09d3330d5c4676bd190568", "4bb8107199208080123d1bf5d5ddf233cf530adf", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b"], "page_rank": 0.0001231527093596059}, {"id": "e057b56b4bf8d03df7fdff534309cdf246a42fcd", "title": "Functional Analysis: Theory and Applications", "authors": ["Robert E. Edwards"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Functional Analysis: Theory and Applications\" by Robert E. Edwards", "references": [], "page_rank": 0.0002463054187192118}, {"id": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "title": "Character-Aware Neural Language Models", "authors": ["Yoon Kim", "Yacine Jernite", "David A Sontag", "Alexander M. Rush"], "date": 2016, "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.", "references": ["1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "91a93f751f912b2f96d6771018d8f06c41e11152", "f9a1b3850dfd837793743565a8af95973d395a4e", "b7cfccf123f86785476a06c8039889a2eb1e2d73", "d1275b2a2ab53013310e759e5c6878b96df643d4", "750f26d613d3bda4ce043944aa3ef358b0c5de68", "53ab89807caead278d3deb7b6a4180b277d3cb77", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "cea967b59209c6be22829699f05b8b1ac4dc092d"], "page_rank": 0.00028735632183908046}, {"id": "827347d6dc00fca9a4cef8ebc03663a8cb0da91a", "title": "On lower and upper bounds of the difference between the arithmetic and the geometric mean", "authors": ["S. H. Tung"], "date": 1975, "abstract": "Lower and upper bounds of the difference between the arithmetic and the geometric mean of n quantities are given here in terms of n, the smallest value a and the largest value A of given n quantities. Also, an upper bound for the difference, independent of n, is given in terms of a and A. All the bounds obtained are sharp.", "references": ["a3b8146c7950597628689d14551e74d46cc3543d", "f1f2981c3146fbd9300bd8fbfd0401ba65f8ce5a", "95b18ea66847bf1765fbdfb0d6785e91fa473842"], "page_rank": 0.0002463054187192118}, {"id": "3aa52436575cf6768a0a1a476601825f6a62e58f", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "authors": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg"], "date": 2016, "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "references": ["fafa541419b3756968fe5b3156c6f0257cb29c23", "47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f", "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "5d1d86c1990a21cdb71aea8f1939eddc36dbbe9e", "e442a3ca917b8b491375c9662843f7fd8c729598", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "9462eee3e5eff15df5e97c38e24072c65e581cee", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "f828b401c86e0f8fddd8e77774e332dfd226cb05", "f9a1b3850dfd837793743565a8af95973d395a4e"], "page_rank": 0.00016420361247947453}, {"id": "d67175d17c450ab0ac9c256103828f9e9a0acb85", "title": "Some Improvements on Deep Convolutional Neural Network Based Image Classification", "authors": ["Andrew G. Howard"], "date": 2014, "abstract": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "398c296d0cc7f9d180f84969f8937e6d3a413796", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "7e1291583873fb890e7922ec0dfefd4846df46c9", "bd4318cd5129cf0d6268876888359f87b410d719", "4dc21a168070dc87266accd9ce2c06ee6115a285"], "page_rank": 0.00032840722495894905}, {"id": "05cc38e249a6f642363b5a5cbd71cda67cea5893", "title": "Tiled convolutional neural networks", "authors": ["Quoc V. Le", "Jiquan Ngiam", "Zhenghao Chen", "Daniel Jin hao Chia", "Pang Wei Koh", "Andrew Y. Ng"], "date": 2010, "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.", "references": ["3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "5d90f06bb70a0a3dced62413346235c02b1aa086", "54a9c2553138932426faebcaa67a63a84a56b55d", "1e80f755bcbf10479afd2338cec05211fdbd325c", "265069b3670930fd884b02062d7e7b79ff2a49d5", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "f354310098e09c1e1dc88758fca36767fd9d084d"], "page_rank": 0.00016420361247947453}, {"id": "5a434953b58c72fe2089531d6c4b4fc1325defcb", "title": "Learning Separable Filters", "authors": ["Amos Sironi", "Bugra Tekin", "Roberto Rigamonti", "Vincent Lepetit", "Pascal Fua"], "date": 2013, "abstract": "Learning filters to produce sparse image representations in terms of over complete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-the-art methods on the linear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.", "references": ["6436ab45779f90a4d9c473354672b86e465e3ff7", "6d5b6e0ec6957a7e22245f5e9d2305f8fa46c189", "59882b92d0183163e897a671b8c9298f89df5df3", "e07416eabd4ba6c69fa473756bb04ae7161177be", "4dbc68cf2e14155edb6da0def30661aca8c96c22", "6c0eb85c99044d19cc59b2cf46b20eadb302ce0e", "b1c23451334b0f9294a6a8de5be59d361547a946", "498efaa51f5eda731dc6199c3547b9465717fa68", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "bfd771fba9aed481cd3395e3e9ae3c284659f0d4"], "page_rank": 0.0004926108374384236}, {"id": "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "title": "Adaptive deconvolutional networks for mid and high level feature learning", "authors": ["Matthew D. Zeiler", "Graham W. Taylor", "Rob Fergus"], "date": 2011, "abstract": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.", "references": ["83dfe3980b875c4e5fe6f2cb1df131cc46d175c8", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "1e80f755bcbf10479afd2338cec05211fdbd325c", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "8d6227e26a4bfc5482c12b8f072496ac6e97ed21", "498efaa51f5eda731dc6199c3547b9465717fa68", "7f6869c37da690e7d53ee28d597ab5e2b179f9e5", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "8978cf7574ceb35f4c3096be768c7547b28a35d0"], "page_rank": 0.00016420361247947453}, {"id": "6270baedeba28001cd1b563a199335720d6e0fe0", "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition", "authors": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "date": 2014, "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.", "references": ["b8de958fead0d8a9619b55c7299df3257c624a96", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "a9ce496186120df8f9ed3367e76a4947419e992e", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "25d0fa49ca846370ff4796a6ac6688a42cf50f77", "9bc0295460089592d04e754a5fd427060b7bfa8c", "6286a82f72f632672c1890f3dd6bbb15b8e5168b", "189b1859f77ddc08027e1e0f92275341e5c0fdc6"], "page_rank": 0.00016420361247947453}, {"id": "e0945081b5b87187a53d4329cf77cd8bff635795", "title": "Highway Networks", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J{\\\"u}rgen Schmidhuber"], "date": 2015, "abstract": "There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on information highways. The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.", "references": ["abc866e30163ec67f1cf4f4380e5f8323a6c598d", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "cd85a549add0c7c7def36aca29837efd24b24080", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "99c970348b8f70ce23d6641e201904ea49266b6e", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "eb42cf88027de515750f230b23b1a057dc782108", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "8c571314311f507731296b21b56ab2c326b97392"], "page_rank": 0.0002463054187192118}, {"id": "7ed7a41c275f2870b840a5e6c3eaec8888c9480c", "title": "A Global Joint Model for Semantic Role Labeling", "authors": ["Kristina Toutanova", "Aria Haghighi", "Christopher D. Manning"], "date": 2008, "abstract": "We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments. We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees. The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively. We also present results on the CoNLL 2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.", "references": ["1ae5c1646ea445a670fe6cc8bf72b589dd9f6e5c", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "844db702be4bc149b06b822b47247e15f5894cc3", "6f78ac1460e6b7f2f2594179f3b7fcf07aa207df", "502a0987e09450129a4ab22492e69448a08bedc9", "31274eabb84407e3bc2c1d14b804cb7bcc068111", "971849feaae420079b6843bf8b74db849bc8291d", "a584e4b607e972783cea22daaaf1114ea94a8035", "f11e1b8c08898a34f29e6bef0bf5b29dc93ebe11", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb"], "page_rank": 0.00016420361247947453}, {"id": "c92970286c535992a86539b761357761e97a37ee", "title": "Towards Robust Linguistic Analysis using OntoNotes", "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Bj{\\\"o}rkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong"], "date": 2013, "abstract": "Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks. Up till now, however, most of the evaluation has been done on monolithic corpora such as the Penn Treebank, the Proposition Bank. As a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains. The completion of the OntoNotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation. This paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus. This should set the benchmark for future development of various NLP components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.", "references": ["95067ce3aa49d52f684904a193d42e3aecf82ff2", "2c72257ae7a4a32dc60569f4e1fe4504b2678112", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "f8cdf754fb7c08caf6e2f82b176819230910be5b", "a584e4b607e972783cea22daaaf1114ea94a8035", "5cb6ce78710b22cda87d423a9e54235804a01321", "2ed51de85d3aa1a921c334bcd6293b58edbe5798", "44acd22bfb89f6e3d5529b2fa92b18b035e6bab5", "86128b764ea86caa207298a092f70f8bedc8ba3f", "37a3235ffb8eed862f69752b69c2d878c8f541f1"], "page_rank": 0.00016420361247947453}, {"id": "a64ca771a733d58dcbf8f7a3fe65a09310424bf8", "title": "Induction of Finite-State Languages Using Second-Order Recurrent Networks", "authors": ["Raymond L. Watrous", "Gary M. Kuhn"], "date": 1992, "abstract": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length.", "references": ["25c19c8c1d6778a16f8b27beac4d9c6a55357580", "cea0fe8c2a77c84f9bbb172a9f65f2cb9ba9d24d", "b185742930fd959aaccdfdecdb31641839a787c4", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "c30b63e849b313ce8b32a966b40c750b6f60aa2d"], "page_rank": 0.0008210180623973726}, {"id": "79c0b2f44bbc2bc51de554b88ebe46204413f884", "title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "date": 2013, "abstract": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.", "references": ["745d86adca56ec50761591733e157f84cfb19671", "27e38351e48fe4b7da2775bf94341738bc4da07e", "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "a0e3dd2480d2a3e90e0f04b6362a676cb047b85a", "607cca37c1429b7380df35b3f761ae1499aa84ab", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "57458bc1cffe5caa45a885af986d70f723f406b4", "ae5e6c6f5513613a161b2c85563f9708bf2e9178"], "page_rank": 0.0004926108374384236}, {"id": "eb512c593f089cdb5ed63c8acd44eff5a531601c", "title": "Training Second-Order Recurrent Neural Networks using Hints", "authors": ["Christian W. Omlin", "C. Lee Giles"], "date": 1992, "abstract": "We investigate a method for inserting rules into discrete-time second-order recurrent neural networks which are trained to recognize regular languages. The rules defining regular languages can be expressed in the form of transitions in the corresponding deterministic finite-state automaton. Inserting these rules as hints into networks with second-order connections is straightforward. Our simulation results show that even weak hints seem to improve the convergence time by an order of magnitude.", "references": ["a64ca771a733d58dcbf8f7a3fe65a09310424bf8", "cea0fe8c2a77c84f9bbb172a9f65f2cb9ba9d24d", "872cdc269f3cb59f8a227818f35041415091545f", "b185742930fd959aaccdfdecdb31641839a787c4", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "e3cd36c092abd65d6ac8e648f3468eeee90ee1fc", "dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8", "40c39070027c046407d7654ee2934df610b1a69a", "b8b84c7509c34c77d2984405ba7fe5336fc8788d", "3bda562fbdd64624e8c9f3c893bec2d4c1925ab6"], "page_rank": 0.00016420361247947453}, {"id": "1109b663453e78a59e4f66446d71720ac58cec25", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "authors": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha{\\\"e}l Mathieu", "Rob Fergus", "Yann LeCun"], "date": 2014, "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "references": ["48adff169c044c674e7cbcc033c81d77c7ac9b43", "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "fd790b061082571e20be7892ce4a97e156497c9f", "79ef1a3843a2dc01bde67c3a9a17c6deb352e285", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "a1306ce652f556fbb9e794d91084a29294298e6d", "f566b1f24e63151ddae652826638af054973a27f", "38b6540ddd5beebffd05047c78183f7575559fb2", "398c296d0cc7f9d180f84969f8937e6d3a413796"], "page_rank": 0.00016420361247947453}, {"id": "6a835df43fdc2f79126319f6fa033bb42147c6f6", "title": "Recursive Distributed Representations", "authors": ["Jordan B. Pollack"], "date": 1990, "abstract": "Abstract A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.", "references": ["668087f0ae7ce1de6e0bd0965dbb480c08103260", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "0e0efb073163b35bae48b95c540240a000a18ca2", "114b2df7ba63669c2f901d4e3c298d9360c4ae7d", "c96fe25817c5fca96719cfa56cdaeeb2d17c93d7", "de996c32045df6f7b404dda2a753b6a9becf3c08", "59ab7e538b055f4b8396edb74eee83c3319d6fe4", "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "9928cac725ebe6db7b974bbd65738d33dc95332d", "70623eb30f076d6de599c4dff13b04bd62e29e0c"], "page_rank": 0.00016420361247947453}, {"id": "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "title": "Gated Feedback Recurrent Neural Networks", "authors": ["Junyoung Chung", "\u00c7aglar G{\\\"u}l\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "date": 2015, "abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "references": ["5522764282c85aea422f1c4dc92ff7e0ca6987bc", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "8c571314311f507731296b21b56ab2c326b97392", "cea967b59209c6be22829699f05b8b1ac4dc092d", "d0be39ee052d246ae99c082a565aba25b811be2d"], "page_rank": 0.0002463054187192118}, {"id": "3276b9487b2336f662488f2a180622f3bcac6e82", "title": "UNAL-NLP: Combining Soft Cardinality Features for Semantic Textual Similarity, Relatedness and Entailment", "authors": ["Sergio Jim{\\'e}nez", "George Due{\\~n}as", "Julia Baquero", "Alexander Gelbukh"], "date": 2014, "abstract": "This paper describes our participation in the SemEval-2014 tasks 1, 3 and 10. We used an uniform approach for addressing all the tasks using the soft cardinality for extracting features from text pairs, and machine learning for predicting the gold standards. Our submitted systems ranked among the top systems in all the task and sub-tasks in which we participated. These results confirm the results obtained in previous SemEval campaigns suggesting that the soft cardinality is a simple and useful tool for addressing a wide range of natural language processing problems.", "references": ["8fbc260a6e56b4b2464bbbee16722e70bdf5463e", "6cc61c010d7c7d3d58ca789fa5ce6c9bdf75c27a", "0d388dfd33c6617a6491551172cb4cb9857b4bd4", "fa77ce54cc62281ffada8d914b5ab5d92f1337e6", "ab62669c45e00bf35997c1a5e135dd5da00e3fc3", "16084914bc3729f86f46ac6267ea7a42e7951d41", "28161cd0ef7d69d2006e3e49c65cc0cd4308222e", "78b3d9cd136bb04e79d5d52ec7e4f5d068759d88", "c333778104f648c385b4631f7b4a859787e9d3d3", "ff5cbefc6766df788919db4c060daedb303ed3e3"], "page_rank": 0.0002463054187192118}, {"id": "73ce506d16ad040aca09a4460018010b833e73d7", "title": "A Recurrent Network that performs a Context-Sensitive Prediction Task", "authors": ["Mark Steijvers", "P{\\'e}ter Gr"], "date": 1996, "abstract": "We address the problem of processing a context-sensitive language with a recurrent neural network (RN). So far, the language processing capabilities of RNs have only been investigated for regular and context-free languages. We present an extremely simple RN with only one parameter z for its two hidden nodes that can perform a prediction task on sequences of symbols from the language f(bak)n j k 0; n > 0g, a language that is context-sensitive but not context-free. The input to the RN consists of any string of the language, one symbol at a time. The network should then, at all times, predict the symbol that should follow. This means that the network must be able to count the number of a\u2019s in the first subsequence and to retain this number for future use. We present a value for the parameter z for which our RN can solve the task for k = 1 up to k = 120. As we do not give any method to find a good value for z, this does not say anything about the learning capabilities of our network. It does, however, show that context-sensitive information (the count of a\u2019s) can be represented by the network; we analyse in detail how this is done. Hence our work shows that, at least from a representational point of view, connectionist architectures can handle more complex formal languages than was previously known.", "references": ["0b7005984749cf5f2caa1072866b36e17713ab84", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "2a1483397106807e74ab422dd8330d56a3cc6db5", "b185742930fd959aaccdfdecdb31641839a787c4", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "872cdc269f3cb59f8a227818f35041415091545f", "677d17e00fcdad5baffc4fcd3925442f62fc9307", "41a88a490d7ba9e383ecb16c4290083413a08258", "d76aafbeb54575859441a442376766c597f6bb52"], "page_rank": 0.0002463054187192118}, {"id": "11ec56898a9e7f401a2affe776b5297bd4e25025", "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment", "authors": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "date": 2014, "abstract": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).", "references": ["997bfd6c322909ac66fae417c553f104a6dcf18b", "c4fa5bd2f9c774b4d5ad96388603843868fe2da8", "ac5637276be614f5c89bd8fe2ed82f412568eba1", "f795853dba1b4dc7ead4c4c5d94d4e1666a5df24", "ac991aa2072cf22fee83db7aa536137e666ed9d1", "3276b9487b2336f662488f2a180622f3bcac6e82", "d9daa826f5989be59be27d7ca9f05401e12ab9da", "c333778104f648c385b4631f7b4a859787e9d3d3", "5f3c2557d8ae7b9025535ee021c545e2d6a92cf0", "3c5126da7ce388c64b796c80d15a3c3629d6ad58"], "page_rank": 0.001108374384236453}, {"id": "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "date": 2011, "abstract": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.", "references": ["96fa75d886643fa1621cc30f4f55c0a22c2e49d5", "73e897104540642698321c106cc9c35af369fe12", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "459745250d453dacf8373b7a34a353bb1fa9e147", "f5e23b61597f3d2b51515c557a213be1f0e6ed6f", "37efe2ef1b9d27cc598361a8013ec888a6f7c4d8", "44da6a8fc4aafad47db93bc164de9513b6adc7e5", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617"], "page_rank": 0.0007553366174055828}, {"id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "title": "A Structured Vector Space Model for Word Meaning in Context", "authors": ["Katrin Erk", "Sebastian Pad{\\'o}"], "date": 2008, "abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "references": ["7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "00162f43964fd457a9158408c1ac0e8990489782", "a38083e2dab2aae2e4161e96f8c4a6767ab939e5", "c212eab7a430dd931ac17e7cd9779838296dd578", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "2f55b6959997e8464c19140644c6e3d6781a55b4", "fd1901f34cc3673072264104885d70555b1a4cdc"], "page_rank": 0.0011247947454844007}, {"id": "29a294eaec7b485245aa21d994f7300f6b5da8fc", "title": "Neural Summarization by Extracting Sentences and Words", "authors": ["Jianpeng Cheng", "Mirella Lapata"], "date": 2016, "abstract": "Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.", "references": ["5082a1a13daea5c7026706738f8528391a1e6d59", "3d6b0146482d5ca8bc1ee4c34e20870503d6d977", "f2a665c064f84a301d64a0ab79f9ed6047dc74e2", "9d08213ede54c4e205d18b4400288831af918ec8", "b631b2cad1771418c2c71251bd608277c2797a1e", "6d9f0bcb44ac9a266e5de7eeada0f01b5f957d84", "d008dae6c602c0586e49d169e57a24d536949446", "52ebb5873677dd900a97e4bd4042fb40be14f937", "7453a974d355883f342aaa6e29eb86a13edcedb9", "123b9de009865472c660192f8072493a48352dc2"], "page_rank": 0.00016420361247947453}, {"id": "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons", "authors": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"], "date": 2012, "abstract": "We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a lowdimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.", "references": ["5d90f06bb70a0a3dced62413346235c02b1aa086", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "843959ffdccf31c6694d135fad07425924f785b1", "3e1638d1ce6ec0d18c82fb3235a521db0a559da2", "5a767a341364de1f75bea85e0b12ba7d3586a461", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "48e1de7d085808004d5f0493d486669a3d2930b5", "5763bd6b3f24a01c3bc7cd15d3c916b4840b759d", "4c46347fbc272b21468efe3d9af34b4b2bad6684", "75a026ddfdd9c219d69fe8af816f085ea1b3877d"], "page_rank": 0.0006568144499178981}, {"id": "9c0ddf74f87d154db88d79c640578c1610451eec", "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "authors": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "date": 2011, "abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.", "references": ["81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "57458bc1cffe5caa45a885af986d70f723f406b4", "c58dd287a476b4722c5b6b1316629e2874682219", "adcf1552e759f9cade8ef9e59ecf6159e25a055e", "1e80f755bcbf10479afd2338cec05211fdbd325c", "f52de7242e574b70410ca6fb70b79c811919fc00", "8e523721feebeaee18e487607b7d0920ac6cd3b4", "eed4e6967c7a96e4cc2c590db40269cd97c8c98e", "8bc26623a700b564934221dd34c9cfbfeedc5efa", "6eb3a15108dfdec25b46522ed94b866aeb156de9"], "page_rank": 0.0004926108374384236}, {"id": "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "title": "Pointing the Unknown Words", "authors": ["\u00c7aglar G{\\\"u}l\u00e7ehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "date": 2016, "abstract": "The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context.~We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known.~We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.", "references": ["1956c239b3552e030db1b78951f64781101125ed", "1af68821518f03568f913ab03fc02080247a27ff", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "53ca064b9f1b92951c1997e90b776e95b0880e52", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "ba30df190664193514d1d309cb673728ed48f449", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5082a1a13daea5c7026706738f8528391a1e6d59", "9653d5c2c7844347343d073bbedd96e05d52f69b"], "page_rank": 0.00016420361247947453}, {"id": "1a210410493fbc052f0b7a54e7bc89cee20e8d28", "title": "Crowdsourcing Question-Answer Meaning Representations", "authors": ["Julian Michael", "Gabriel Stanovsky", "Luheng He", "Ido Dagan", "Luke Zettlemoyer"], "date": 2017, "abstract": "We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, QA-SRL, and AMR) along with many previously under-resourced ones, including implicit arguments and relations. The QAMR data and annotation code is made publicly available to enable future work on how best to model these complex phenomena.", "references": ["7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "05dd7254b632376973f3a1b4d39485da17814df5", "a8b21f72bdc251689f636d3d7ff52a6b85ab7ce9", "4deedbe82a9de796e111260dd194bd2d84220b39", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "564257469fa44cdb57e4272f85253efb9acfd69d", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "b2392b8f4928f8033dd9a0a542c9e4a0c9e249c6", "fda558136b2d2b812a608b21fe22959d48db1078", "d8259bcbe9cb0cf5bad6ea25645f4407fc544a1c"], "page_rank": 0.0001231527093596059}, {"id": "3eda43078ae1f4741f09be08c4ecab6229046a5c", "title": "NewsQA: A Machine Comprehension Dataset", "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "date": 2017, "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "references": ["05dd7254b632376973f3a1b4d39485da17814df5", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "f2e50e2ee4021f199877c8920f1f984481c723aa", "b1e20420982a4f923c08652941666b189b11b7fe", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "564257469fa44cdb57e4272f85253efb9acfd69d", "46147f08468e873ff90d1d51e65493f262c7bb57", "e4600ece1f09236d082eca4537ee9c1efe687f6c", "e94697b98b707f557436e025bdc8498fa261d3bc"], "page_rank": 0.0001231527093596059}, {"id": "46147f08468e873ff90d1d51e65493f262c7bb57", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "authors": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Philip Bachman"], "date": 2016, "abstract": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\\% absolute).", "references": ["6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "564257469fa44cdb57e4272f85253efb9acfd69d", "9c03d14520c897ca8536e165507f568d1980dabd", "452059171226626718eb677358836328f884298e", "d1505c6123c102e53eb19dff312cb25cea840b72", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "86c2a7dc48445d75ec9bc71f7d9fdec622687e90", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "71ae756c75ac89e2d731c9c79649562b5768ff39"], "page_rank": 0.0005911330049261083}, {"id": "7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language", "authors": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer"], "date": 2015, "abstract": "This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb \u201cintroduce\u201d in the previous sentence would be labeled with the questions \u201cWhat is introduced?\u201d, and \u201cWhat introduces something?\u201d, each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for predefined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.", "references": ["1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "2791e0d36ba23763195ac984453d61dbaff555da", "c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "b29cdc426784534ad5c83ccf646fde67c4695607", "1de2f2d45e505eab0bc520ee554353c6d509f32a", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "5894c9fbe9d14be08a48e34d0467da4213b6399c", "c7d3f610b528226f1c862c4f9cd6b37623f7390f", "b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14"], "page_rank": 0.0003694581280788177}, {"id": "4dabd6182ce2681c758f654561d351739e8df7bf", "title": "Multilingual Language Processing From Bytes", "authors": ["Daniel Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "date": 2016, "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text.", "references": ["91a93f751f912b2f96d6771018d8f06c41e11152", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "93a9694b6a4149e815c30a360347593b75860761", "696b505083d34c6f995aef88d0352d70d7f7e8c8", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "cea967b59209c6be22829699f05b8b1ac4dc092d", "96526726f87233fb017f6ea9483090f04e0f0530", "bc1022b031dc6c7019696492e8116598097a8c12", "d9674edc47fd41c27c0c6497c0e7d6bf1910d84e"], "page_rank": 0.0004926108374384236}, {"id": "95b18ea66847bf1765fbdfb0d6785e91fa473842", "title": "On the difference between the geometric and the arithmetric mean of n quantities", "authors": ["Charles Loewner", "Henry B. Mann"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"On the difference between the geometric and the arithmetric mean of n quantities\" by Charles Loewner et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "title": "Early results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons", "authors": ["Andrew McCallum", "Wei Li"], "date": 2003, "abstract": "Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character n-grams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998).", "references": ["1c0ece611643cfb8f3a23e4802c754ea583ebe37", "0e494ad9265fcb4e87ff585c65db4c795940cf9b", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "4df361d65a15ca9a7fc27c58c38b04d1f41e6f62", "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3", "50df0dd6c8f9811e99f6c73b2f88330aeda2eb98", "b951b9f78b98a186ba259027996a48e4189d37e5", "a574e320d899e7e82e341eb64baef7dfe8a24642", "878783964ab23c97052ea82685368099d85c500d"], "page_rank": 0.00028735632183908046}, {"id": "62a7fc51bcca0f242cfadedff98a553afc3a1704", "title": "Hierarchical Question Answering for Long Documents", "authors": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste", "Illia Polosukhin", "Jakob Uszkoreit", "Jonathan Berant"], "date": 2016, "abstract": "We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate the state of the art performance on a challenging subset of the Wikireading and on a new dataset, while speeding up the model by 3.5x-6.7x.", "references": ["4cfad7889dc12825309325cd4b4f3febed424e36", "828dbeb7cf922dc9b6657dd169b8d26d2b58eedb", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "05dd7254b632376973f3a1b4d39485da17814df5", "832fc9327695f7425d8759c6aaeec0fa2d7b0a90", "f2e50e2ee4021f199877c8920f1f984481c723aa", "452059171226626718eb677358836328f884298e", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "59018eb4d0a5161a12cdca42cbcb6bf78d73612f"], "page_rank": 9.852216748768472e-05}, {"id": "53ab89807caead278d3deb7b6a4180b277d3cb77", "title": "Better Word Representations with Recursive Neural Networks for Morphology", "authors": ["Thang Luong", "Richard Socher", "Christopher D. Manning"], "date": 2013, "abstract": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.", "references": ["2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "27e38351e48fe4b7da2775bf94341738bc4da07e", "dac72f2c509aee67524d3321f77e97e8eff51de6", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "b4fcb3921b42d156a812484dcabf60c3f4410d42", "d1275b2a2ab53013310e759e5c6878b96df643d4", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "bc1022b031dc6c7019696492e8116598097a8c12", "68c03788224000794d5491ab459be0b2a2c38677"], "page_rank": 0.0002463054187192118}, {"id": "bfd771fba9aed481cd3395e3e9ae3c284659f0d4", "title": "Filter learning for linear structure segmentation", "authors": ["Roberto Rigamonti", "Engin T{\\\"u}retken", "Germ{\\'a}n Gonz{\\'a}lez Serrano", "Pascal Fua", "Vincent Lepetit"], "date": 2011, "abstract": "We introduce an approach to learning convolution filters whose joint output can be fed to a classifier that labels them as belonging to linear structures or not. The filters are learned using sparse synthesis techniques but we show that enforcing constraints is not required at run-time to achieve good classification performance. In practice, this is important as it drastically reduces the computational cost. We show that our approach outperforms the state-of-the-art on difficult, and very different, images of roads, retinal scans, and dendritic networks.", "references": ["be9a17321537d9289875fe475b71f4821457b435", "498efaa51f5eda731dc6199c3547b9465717fa68", "8c8c9c0ff996da9a2e8f8387788903e21a4c3668", "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec", "4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a", "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f"], "page_rank": 0.0002463054187192118}, {"id": "f354310098e09c1e1dc88758fca36767fd9d084d", "title": "Learning methods for generic object recognition with invariance to pose and lighting", "authors": ["Yann LeCun", "Fu Jie Huang", "L{\\'e}on Bottou"], "date": 2004, "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.", "references": ["472ff211d18aa2ca2b65b69d86499430ad287499", "3d081b80b1850df9b1e382f97a7a244890d6485e", "33b98b5dc150680fc02a14c0cf629168dd0af08b", "ed7935cc73cc16cf6123eff0d17949c1aff5a847", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "1fda96d554f4e5a21e35bf33b9720141da47664b", "397f22d68805551c500077f4a9b4dbea868d1fb3", "3473ce22ada00842b355883a5ddde5a8c7c76ba6", "48d28acc0f3ac2b25bc62ae71525fa099b2d1052"], "page_rank": 0.0004926108374384236}, {"id": "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization", "authors": ["Adam Coates", "Andrew Y. Ng"], "date": 2011, "abstract": "While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find it is more important to choose a good encoder\u2014which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB.", "references": ["e8f811399746c059bf4d4c3d43334045e0222209", "498efaa51f5eda731dc6199c3547b9465717fa68", "be9a17321537d9289875fe475b71f4821457b435", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "5d90f06bb70a0a3dced62413346235c02b1aa086", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "bea5780d621e669e8069f05d0f2fc0db9df4b50f"], "page_rank": 0.0002463054187192118}, {"id": "189b1859f77ddc08027e1e0f92275341e5c0fdc6", "title": "Sparse Representations and Distance Learning for Attribute Based Category Recognition", "authors": ["Grigorios Tsagkatakis", "Andreas E. Savakis"], "date": 2010, "abstract": "While traditional approaches in object recognition require the specification of training examples from each class and the application of class specific classifiers, in real world situations, the immensity of the number of image classes makes this task daunting. A novel approach in object recognition is attribute based classification, where instead of training classifiers for the recognition of specific object class instances, classifiers are trained on attributes of the object images and these attributes are subsequently used for the object recognition. The attributes based paradigm offers significant advantages including the ability to train classifiers without any visual examples. We begin by discussing a scenario for object recognition on mobile devices where the attribute prediction and the attribute-to-class mapping are decoupled in order to meet the specific resource constraints of mobile systems. We next present two extensions on the attribute based classification paradigm by introducing alternative approaches in attribute prediction and attribute-to-class mapping. For the attribute prediction, we employ the recently proposed Sparse Representations Classification scheme that offers significant benefits compared to the previous SVM based approaches, such as increased accuracy and elimination of the training stage. For the attribute-to-class mapping, we employ a Distance Metric Learning algorithm that automatically infers the significance of each attribute instead of assuming uniform attribute importance. The benefits of the proposed extensions are validated through experimental results.", "references": ["0566bf06a0368b518b8b474166f7b1dfef3f9283", "a251dac6589a83e0bbcf9bef9a80c21222aeecbb", "461d2c494d0353834c54f13e74cc80cd56dbe365", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "c6a8aef1bf134294482d8088f982d5643347d2ff", "82635fb63640ae95f90ee9bdc07832eb461ca881", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "d5eec41043d91964879c4c745c7165f823967f29", "58b0be2db0aeda2edb641273fe52946a24a714c3"], "page_rank": 0.0004926108374384236}, {"id": "37a3235ffb8eed862f69752b69c2d878c8f541f1", "title": "Word Sense Disambiguation Using OntoNotes: An Empirical Study", "authors": ["Zhi Zhong", "Hwee Tou Ng", "Yee Seng Chan"], "date": 2008, "abstract": "The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.", "references": ["689b4bec1cf9dbfd6af293eff2aeefd8c18085b2", "176ce079bd5efa86677aea8804158dc29b651e45", "e421fc9b8d368d6a9542cc384abe0d43cdd57209", "7f98a89bfff160c66a00d814ff626853324f6fed", "f5d5641eddef44843b0a5c9d2ba1d2a160da626f", "0d4d9deebc83aa7c94876f240b96cb1b0ea60be1", "63936c6ce3ffedd4e4bc653a7ce3c3d295adb8c0", "e5019ace68169b17e3c83a0cbe92b2217c30523a", "4c2a642effd543babace0c565b48cadcb6fce14f", "bbe013543e9c1d8f00499036121c363ad3ab3d7d"], "page_rank": 0.0002463054187192118}, {"id": "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "title": "Developing a large semantically annotated corpus", "authors": ["Valerio Basile", "Johan Bos", "Kilian Evang", "Noortje Venhuizen"], "date": 2012, "abstract": "What would be a good method to provide a large collection of semantically annotated texts with formal, deep semantics rather than shallow? We argue that a bootstrapping approach comprising state-of-the-art NLP tools for parsing and semantic interpretation, in combination with a wiki-like interface for collaborative annotation of experts, and a game with a purpose for crowdsourcing, are the starting ingredients for fulfilling this enterprise. The result is a semantic resource that anyone can edit and that integrates various phenomena, including predicate-argument structure, scope, tense, thematic roles, rhetorical relations and presuppositions, into a single semantic formalism: Discourse Representation Theory. Taking texts rather than sentences as the units of annotation results in deep semantic representations that incorporate discourse structure and dependencies. To manage the various (possibly conflicting) annotations provided by experts and non-experts, we introduce a method that stores \" Bits of Wisdom \" in a database as stand-off annotations.", "references": ["1bf6428b55c5e793c4b7d1377a4eb9a12197af1e", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "e6b2650789630e88a64f517ce41dfa2a7cf7a7e4", "351b175603e9a5a4c80372c54460d0c589bb5c47", "88930ee973b6aa875df14f32500dab5eb91e17fe", "547f23597f9ec8a93f66cedaa6fbfb73960426b1", "b349b855f47a3000062e1b08ec48651b28ce10f4", "4f74a7a6ccd4197865a9e9ff91e360c36e89ba53", "cbc2a7afcfd45c2fbad42a97e2609ead37ef7b74", "dcceb47a102bb1a634a494e43030364e7f8e7533"], "page_rank": 0.0006157635467980295}, {"id": "607cca37c1429b7380df35b3f761ae1499aa84ab", "title": "Multi-Step Regression Learning for Compositional Distributional Semantics", "authors": ["Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni"], "date": 2013, "abstract": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face.", "references": ["3c5126da7ce388c64b796c80d15a3c3629d6ad58", "96fa75d886643fa1621cc30f4f55c0a22c2e49d5", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "27e38351e48fe4b7da2775bf94341738bc4da07e", "5974441a0bebfb45579491a9a28bca4fff6bc256", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "745d86adca56ec50761591733e157f84cfb19671", "3cd32f1a3d8a090a75658528eeecc38cba8a2bf1", "228d9e4b69926594fd26080f4cfaa9ecfca44eb3", "bc96de1cc022a0425e9f4f607c8d95064a6b3811"], "page_rank": 0.0001231527093596059}, {"id": "184ac0766262312ba76bbdece4e7ffad0aa8180b", "title": "Representation Learning: A Review and New Perspectives", "authors": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "date": 2013, "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.", "references": ["5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "8a9a10170ee907acb3e582742bec5fa09116f302", "01373261bfbba42a806d21d5b759d5a27f509892", "aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "843959ffdccf31c6694d135fad07425924f785b1", "f93844c68d96f2f01da973b2ed3c236c8a369e57", "dc1d132c79a72dba386dc47750a49b0c29b54568", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14"], "page_rank": 0.0002463054187192118}, {"id": "99c970348b8f70ce23d6641e201904ea49266b6e", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "authors": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "date": 2013, "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.", "references": ["b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "43c8a545f7166659e9e21c88fe234e0323855216", "aa7bfd2304201afbb19971ebde87b17e40242e91", "b27736aad8c0fd11bde6f9300c9f6755e2eb8420", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "9505f8c9e320fc51417ea5acbe6fad5afdcb37ec", "84069287da0a6b488b8c933f3cb5be759cb6237e", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04"], "page_rank": 0.0002463054187192118}, {"id": "86128b764ea86caa207298a092f70f8bedc8ba3f", "title": "CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes", "authors": ["Sameer Pradhan", "Lance A. Ramshaw", "Mitchell P. Marcus", "Martha Palmer", "Ralph M. Weischedel", "Nianwen Xue"], "date": 2011, "abstract": "The CoNLL-2011 shared task involved predicting coreference using OntoNotes data. Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ace entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.", "references": ["5cb6ce78710b22cda87d423a9e54235804a01321", "7719679e6255c51d157116fcfbc858b7e7dfdb59", "ca2c7d02cbb0d533c3831ee0feb8d61664f52dfb", "bc0b93f9320d7c8eb03132652363f4135d9fd4dd", "455a1994727ced91e81891821778376a4e867563", "4ec4776f938fb15862facce71f7db62884f1ef1e", "14c146d457bbd201f3a117ee9c848300d341e5d0", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "cbb786e7e42d02de0080299dc2e114357e816002", "c7d3f610b528226f1c862c4f9cd6b37623f7390f"], "page_rank": 0.0002463054187192118}, {"id": "f566b1f24e63151ddae652826638af054973a27f", "title": "Supervised Learning of Image Restoration with Convolutional Networks", "authors": ["Viren Jain", "Joseph F. Murray", "Fabian Roth", "Srinivas C. Turaga", "Valentin P. Zhigulin", "Kevin L. Briggman", "Moritz Helmstaedter", "Winfried Denk", "H. Sebastian Seung"], "date": 2007, "abstract": "Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition. Here we show that they can also be used as a general method for low-level image processing. As an example of our approach, convolutional networks are trained using gradient learning to solve the problem of restoring noisy or degraded images. For our training data, we have used electron microscopic images of neural circuitry with ground truth restorations provided by human experts. On this dataset, Markov random field (MRF), conditional random field (CRF), and anisotropic diffusion algorithms perform about the same as simple thresholding, but superior performance is obtained with a convolutional network containing over 34,000 adjustable parameters. When restored by this convolutional network, the images are clean enough to be used for segmentation, whereas the other approaches fail in this respect. We do not believe that convolutional networks are fundamentally superior to MRFs as a representation for image processing algorithms. On the contrary, the two approaches are closely related. But in practice, it is possible to train complex convolutional networks, while even simple MRF models are hindered by problems with Bayesian learning and inference procedures. Our results suggest that high model complexity is the single most important factor for good performance, and this is possible with convolutional networks.", "references": ["b9701ad65e256bd8841c4f80ced09b4ca1d5e331", "1c71164b88a016516d99be72ebf49056daea842a", "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa", "496c3d75b81b336411e53da1ac632a8139655604", "3120324069ec20eed853d3f9bbbceb32e4173b93", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "133809cf62bf67f0a63b35e5ef5180d20c9aec19", "ed91ce023b6500c586802de7d23d8f8f01e5aa1b"], "page_rank": 0.0002463054187192118}, {"id": "f1f2981c3146fbd9300bd8fbfd0401ba65f8ce5a", "title": "H", "authors": ["Yu-Qin Cao", "Lei Yuan", "Qin Zhao", "Jian-Lin Yuan", "Yung-Fu Chang", "Xin-Tian Wen", "Rui Wu", "Xiao-Bo Huang", "Xin-Feng Han", "Xiao-Ping Ma", "San-Jie Cao"], "date": 1940, "abstract": "(1962): Die Chondrinen der S\u00fcdalpen. \u2014 Archiv f\u00fcr Molluskenkunde, 91 (1/2): 1-20. Das Genus Chondrina REICHENBACH 1828 (Chondrinida, Vertiginacea) entwickelt in den S\u00fcdalpen \u00e4hnlich wie in den Pyren\u00e4en eine F\u00fclle von Formen, deren systematische Ordnung bis jetzt nicht hinreichend bekannt ist. Die beiden, auch in den n\u00f6rdlichen Kalkalpen und Teilen Deutschlands verbreiteten Arten avenacea und clienta, die von EHRMANN (1931) ausreichend charakterisiert wurden, besiedeln die s\u00fcdlichen Kalkalpen nur in Teilgebieten. Das hier behandelte Gebiet umfa\u00dft die Ketten der s\u00fcdlichen Kalkalpen vom Lago Maggiore im Westen bis K\u00e4rnten, Slowenien und dem kroatischen Karst im Osten. Der Westen des genannten Gebiets wird in niedrigen Lagen fast ausschlie\u00dflich von der gr\u00f6\u00dften s\u00fcdalpinen Art, megacheilos, besiedelt, die in h\u00f6hern Lagen vieler T\u00e4ler von multidentata abgel\u00f6st wird (so z. B. in den T\u00e4lern Valsassina, Valbrembana, Valle Seriana, Valle di Scalve, Vallarsa, Valle d'Illasi, u. a.). Ch. megacheilos mu\u00df als ausgesprochen w\u00e4rmeliebende Art der westlichen S\u00fcdalpent\u00e4ler im Bereich der gro\u00dfen italienischen Seen bezeichnet werden. Ihr Verbreitungsgebiet, in dem sie zwei gro\u00dfe geographische Rassen ausbildet, reicht von der Provence bis ins Veronesische. Die zweite genannte Art, multidentata, ist dagegen eine mehr montane Form, die oft in gr\u00f6\u00dferen H\u00f6hen und in den oberen Teilenn der T\u00e4ler lebt und verwandtschaftlich stark zur avenacea neigt, von der sie jedoch durch eine Anzahl eigent\u00fcmlicher Merkmale abweicht. Ihr Verbreitungsgbiet reicht vom Come See im Westen \u00fcber die h\u00f6heren Teile der Bergamasker Alpen durch Dolomiten und Venetianer Alpen bis zu den Julischen Alpen des Kanaltals im Osten. F. SCHROTT (1935) hat ihr eine eingehende Arbeit gewidmet. Ein merkw\u00fcrdiger Endemismus der Bergamasker Alpen ist Ch. bergomensis, die bis jetzt nur aus der Gegend um Bergamo bekannt ist und dort in niedrigen Lagen die megacheilos ersetzt. Neben den genannten Arten treten nun die bekannten Arten, avenacea und clienta, auf. Die erstere ist in den h\u00f6heren Lagen der mittleren und \u00f6stichen S\u00fcdalpen von S\u00fcdtirol bis K\u00e4rnten udn Krain verbreitet und bildet mehrere geographische Rassen aus. Die zweite, kontinentale Art, clienta, besiedelt den ganzen Osten und dringt im Westen bis Osttirol und zum Kanaltal vor. Sie ist im Gebiet recht einheitlich ausgebildet und l\u00e4\u00dft keine Rassenbildung erkennen.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a3b8146c7950597628689d14551e74d46cc3543d", "title": "R", "authors": ["J. M. Lackie"], "date": 2013, "abstract": "Semantic Scholar extracted view of \"R\" by J. M. Lackie", "references": [], "page_rank": 0.0004105090311986863}, {"id": "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies", "authors": ["Salah El Hihi", "Yoshua Bengio"], "date": 1995, "abstract": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "b107cfe948f5d3c6e00b7cf1ebeacadd40224e0d", "2dc4d6d7d55f9f0f1de53bb7f6816502f8f38892", "50c770b425a5bb25c77387f687a9910a9d130722", "a87889c316aaeee117d73ada27a9f0913e44483a", "13369d124474b5f8dcbc70d12296a185832192b2", "914fa99500eb779e33d22609f8a0fdf3fd2799e1", "9d076613d7c36dbda4a6ff42fbdd076604b96630", "29085cdffb3277c1c8fd10ac09e0d89452c8db83", "e08d090d1e586610d636a46004876e9f3ded8209"], "page_rank": 0.00016420361247947453}, {"id": "d76aafbeb54575859441a442376766c597f6bb52", "title": "Attractor dynamics and parallelism in a connectionist sequential machine", "authors": ["Michael I. Jordan"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Attractor dynamics and parallelism in a connectionist sequential machine\" by Michael I. Jordan", "references": [], "page_rank": 0.0004926108374384236}, {"id": "38b6540ddd5beebffd05047c78183f7575559fb2", "title": "Selective Search for Object Recognition", "authors": ["Jasper R. R. Uijlings", "Koen E. A. van de Sande", "Theo Gevers", "Arnold W. M. Smeulders"], "date": 2013, "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html).", "references": ["6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc", "0e19e69403501be0c4e9cb19bd5a11632721ba58", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "0b43f48c933c615e305ebd25521635cff8df4707", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "120081166fc0d780c84e198622d638152a7cdf3e", "5fae850e7b85e91b11a2874252ec617c3cb064c6"], "page_rank": 0.0002463054187192118}, {"id": "5f3c2557d8ae7b9025535ee021c545e2d6a92cf0", "title": "UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment", "authors": ["Rohit Gupta", "Hannah B{\\'e}chara", "Isma{\\\"i}l El Maarouf", "Constantin Orasan"], "date": 2014, "abstract": "This paper presents the system submitted by University of Wolverhampton for SemEval-2014 task 1. We proposed a machine learning approach which is based on features extracted using Typed Dependencies, Paraphrasing, Machine Translation evaluation metrics, Quality Estimation metrics and Corpus Pattern Analysis. Our system performed satisfactorily and obtained 0.711 Pearson correlation for the semantic relatedness task and 78.52% accuracy for the textual entailment task.", "references": ["a508159293cdfc7fcc8d5027097a7c699a1aa10b", "11ec56898a9e7f401a2affe776b5297bd4e25025", "dd3065499d6462f62ab8fe20b175dcdc0edb5df3", "96dbc7bcf00b0a1aac86ef449dd2e5e86e81440e", "5c97a26fd66d7413681cb74041347a47d0c97178", "c333778104f648c385b4631f7b4a859787e9d3d3", "0c1fdf0e79e2f759c8bdbf7b91784a10d05d6d82", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "505da4d6f9618028c7977a2b4d4b625b26897080", "4e8d9c5bf3c02e716935bcd00e5dcdd2557e75a7"], "page_rank": 0.0001231527093596059}, {"id": "ff5cbefc6766df788919db4c060daedb303ed3e3", "title": "*SEM 2013 shared task: Semantic Textual Similarity", "authors": ["Eneko Agirre", "Daniel Matthew Cer", "Mona T. Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo"], "date": 2013, "abstract": "In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED). CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. Several types of similarity have been defined, including similar author, similar time period or similar location. The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%. The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.", "references": ["528fa9bb03644ba752fb9491be49b9dd1bce1d52", "5c97a26fd66d7413681cb74041347a47d0c97178", "bfc214ce7ab5b101425e5cabd631176bb427adff", "547f23597f9ec8a93f66cedaa6fbfb73960426b1", "51951073580f6995e55be873db9a7f6a9736ca86", "67863c9dc84d431cbe911326e9e277b9f6828d27", "46309506f8d687e4963e3d0850a0506cfdc47563", "e54d8b07ef659f9ee2671441c4355e414e408836", "fc24fe37c2bda8ddeabc01c0df57814648eec054"], "page_rank": 0.0004926108374384236}, {"id": "d9daa826f5989be59be27d7ca9f05401e12ab9da", "title": "CECL: a New Baseline and a Non-Compositional Approach for the Sick Benchmark", "authors": ["Yves Bestgen"], "date": 2014, "abstract": "This paper describes the two procedures for determining the semantic similarities between sentences submitted for the SemEval 2014 Task 1. MeanMaxSim, an unsupervised procedure, is proposed as a new baseline to assess the efficiency gain provided by compositional models. It outperforms a number of other baselines by a wide margin. Compared to the wordoverlap baseline, it has the advantage of taking into account the distributional similarity between words that are also involved in compositional models. The second procedure aims at building a predictive model using as predictors MeanMaxSim and (transformed) lexical features describing the differences between each sentence of a pair. It finished sixth out of 17 teams in the textual similarity sub-task and sixth out of 19 in the textual entailment subtask.", "references": ["c333778104f648c385b4631f7b4a859787e9d3d3", "11ec56898a9e7f401a2affe776b5297bd4e25025", "bdc9efc20b45368ac045bfe05a8f9e979ab8a62e", "553fb89d5858826c02f26e94262e8958debc777e", "bdd05c56ade7794b65024d79b7e72eac7b8ed544", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "745d86adca56ec50761591733e157f84cfb19671", "eb42a490cf4f186d3383c92963817d100afd81e2", "23bcfedcbcdc0789e64995222827ee8fdf9aba3a", "a2a407f6123f57ae2c4c1205f6551cb47efd3297"], "page_rank": 0.0001231527093596059}, {"id": "73e897104540642698321c106cc9c35af369fe12", "title": "Combining Symbolic and Distributional Models of Meaning", "authors": ["Stephen Clark", "Stephen G. Pulman"], "date": 2007, "abstract": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed.", "references": ["b54bcfca3fddc26b8889739a247a25e445818149", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "36c4c51917b1f53ee85c459f2597e115df53eb05", "913c0fe31e99379dcc3e9e8c584c596bf6113941", "b9eea85e590f6e522e3681b8e45012684c60b0fd", "36eff99a7f23cec395e4efc80ff7f937934c7be6", "99614334a3e9b809e43384777409af7eccde3db6", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac"], "page_rank": 0.000779967159277504}, {"id": "7453a974d355883f342aaa6e29eb86a13edcedb9", "title": "Automatic Generation of Story Highlights", "authors": ["Kristian Woodsend", "Mirella Lapata"], "date": 2010, "abstract": "In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating \"story highlights\"---a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model's output is comparable to human-written highlights in terms of both grammaticality and content.", "references": ["9d08213ede54c4e205d18b4400288831af918ec8", "f9a00e41500c5e23fcc1ab3502a2c20acf4b8f6a", "0d8e3db6fcd99313773fcaa16074f2cc76ce1fef", "fa07fa673d8c908e91d22c4566572a72548ccee9", "f9ad221c2627e2b0d16edfa05e65656447e00b77", "92ba9c288cbd0089cf6e9d988c9672f095a67109", "aa5d79f1c2ec606b836a4fe3fd6cee795d336c1e", "b4d4304597b6a6c56e71846f68d5eb64385837d3", "23292c9e0f19572d5aa0aa21807a9c89f1466ecd", "fb56d57d9e64fb2c0af7f19120aae94485df59e2"], "page_rank": 0.0002463054187192118}, {"id": "123b9de009865472c660192f8072493a48352dc2", "title": "Phrase-based Image Captioning", "authors": ["R{\\'e}mi Lebret", "Pedro H. O. Pinheiro", "Ronan Collobert"], "date": 2015, "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "references": ["15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "2a0d0f6c5a69b264710df0230696f47c5918e2f2", "05e074abddd3fe987b9bebd46f6cf4bf8465c37e", "169b847e69c35cfd475eb4dcc561a24de11762ca", "87f40e6f3022adbc1f1905e3e506abad05a9964f"], "page_rank": 0.0002463054187192118}, {"id": "6eb3a15108dfdec25b46522ed94b866aeb156de9", "title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "authors": ["Richard Socher", "Li Fei-Fei"], "date": 2010, "abstract": "We propose a semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels. Given photos of a sports event, all that is necessary to provide a pixel-level labeling of objects and background is a set of newspaper articles about this sport and one to five labeled images. Our model is motivated by the observation that words in text corpora share certain context and feature similarities with visual objects. We describe images using visual words, a new region-based representation. The proposed model is based on kernelized canonical correlation analysis which finds a mapping between visual and textual words by projecting them into a latent meaning space. Kernels are derived from context and adjective features inside the respective visual and textual domains. We apply our method to a challenging dataset and rely on articles of the New York Times for textual features. Our model outperforms the state-of-the-art in annotation. In segmentation it compares favorably with other methods that use significantly more labeled training data.", "references": ["eed4e6967c7a96e4cc2c590db40269cd97c8c98e", "df46c8c5e613c62a976a2013e0de21b92ab26450", "88482475e5dffab106149c7b358732e6c973e611", "56766bab76cdcd541bf791730944a5e453006239", "8e523721feebeaee18e487607b7d0920ac6cd3b4", "f39d3e88cce063ccd3ca01100efd44dcabc9d3b4", "c4d13788112f0fec457d31e1f7de9a53bbcec8e6", "aeeffe327e6c93e9010c7b1e401caa9113723851", "473f4b7f8ae2b03dda2593f54b316ff7d55db26b", "15d48f8e1739ab85bb409b5a8813787534cabafc"], "page_rank": 0.0002463054187192118}, {"id": "b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14", "title": "Semantic Role Features for Machine Translation", "authors": ["Ding Liu", "Daniel Gildea"], "date": 2010, "abstract": "We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation.", "references": ["d2b78ad97078a8bfef6c2492b478532fb14b14de", "9e427c9ec538ad2e985257383814c5c38b50c91d", "9d8612724225d85f7e0e218358ee3df52581f4a6", "03a52216777990f504d95e0d30cc53573055d07c", "53b4aaf51c6d1c164b19e8f9df5cfde560eeb6a7", "7e982f360b44094552264010781a476d85ac78a7", "d01737b617acc555153f4660417908bf3971b1a5", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "35199edf885516904f0fc0f8e2f35e14040568ab"], "page_rank": 0.0002463054187192118}, {"id": "35b91b365ceb016fb3e022577cec96fb9b445dc5", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "date": 2016, "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "references": ["71ae756c75ac89e2d731c9c79649562b5768ff39", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "fac2ca048fdd7e848f0b9ba2f7be25bb49186770", "452059171226626718eb677358836328f884298e", "be1fed9544830df1137e72b1d2396c40d3e18365", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "564257469fa44cdb57e4272f85253efb9acfd69d", "15de5528b04bf3d9cf741122677588140c25ebff", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "6e565308c8081e807709cb4a917443b737e6cdb4"], "page_rank": 0.0002463054187192118}, {"id": "d9674edc47fd41c27c0c6497c0e7d6bf1910d84e", "title": "A Universal Part-of-Speech Tagset", "authors": ["Slav Petrov", "Dipanjan Das", "Ryan T. McDonald"], "date": 2012, "abstract": "To facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices, we propose a tagset that consists of twelve universal part-ofspeech categories. In addition to the tagset, we develop a mapping from 25 different treebank tagsets to this universal set. As a result, when combined with the original treebank data, this universal tagset and mapping produce a dataset consisting of common partsof-speech for 22 different languages. We highlight the use of this resource via two experiments, including one that reports competitive accuracies for unsupervised grammar induction without gold standard part-of-speech tags.", "references": ["8766f3686f809628dac1c92d1dc34f8c1bb3106e", "eb42a490cf4f186d3383c92963817d100afd81e2", "343733a063e491d234a36d3e1090a739318b3566", "0eebaf8dbc32eabcf9c62e337ec7f947db650538", "d560a8d279075a529e9cadb0d664b27957aac5a2", "a43f0fd5fec3df14c395cec3abb331efe116c57f", "69164782f9b3b34acb07423151716b47c89d11b4", "327c88dd06722a967be9c6b1176fbd79554967e7", "0f531f8db68e981bbf1f4e543d15b519ba3325b0", "dceb8b78cebc56ebb99dcb05de252999487fd28d"], "page_rank": 0.0001231527093596059}, {"id": "9653d5c2c7844347343d073bbedd96e05d52f69b", "title": "Pointer Networks", "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "date": 2015, "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "71ae756c75ac89e2d731c9c79649562b5768ff39", "f01fc808592ea7c473a69a6e7484040a435f36d9", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "c3823aacea60bc1f2cabb9283144690a3d015db5"], "page_rank": 0.00016420361247947453}, {"id": "c19fbefdeead6a4154a22a9c8551a18b1530033a", "title": "Hierarchical Probabilistic Neural Network Language Model", "authors": ["Frederic Morin", "Yoshua Bengio"], "date": 2005, "abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.", "references": ["d6fb7546a29320eadad868af66835059db93d99f", "e41498c05d4c68e4750fb84a380317a112d97b01", "3bb45466dfb9770e706d1e63205e266e7761f915", "9b30e7c50d8aa3d872a63d7ca2e18ebf6a23c031", "bfab4ffa229c8af0174a683ff1eda524c4f59d00", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "09c76da2361d46689825c4efc37ad862347ca577", "a8ca92770bce439a207cc75fd28a749b51b5a516", "3de5d40b60742e3dfa86b19e7f660962298492af"], "page_rank": 0.0011494252873563218}, {"id": "59018eb4d0a5161a12cdca42cbcb6bf78d73612f", "title": "A Joint Model for Answer Sentence Ranking and Answer Extraction", "authors": ["Md Arafat Sultan", "Vittorio Castelli", "Radu Florian"], "date": 2016, "abstract": "Answer sentence ranking and answer extraction are two key challenges in question answering that have traditionally been treated in isolation, i.e., as independent tasks. In this article, we (1) explain how both tasks are related at their core by a common quantity, and (2) propose a simple and intuitive joint probabilistic model that addresses both via joint computation but task-specific application of that quantity. In our experiments with two TREC datasets, our joint model substantially outperforms state-of-the-art systems in both tasks.", "references": ["4cfad7889dc12825309325cd4b4f3febed424e36", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "eaf63218521a6678d46e77767aaf23f4ff12920c", "228920ddc0d376c376ae534ceed589005f51867a", "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e", "97ef1e8e5860ba2102058cccb39bae94c0d441fb", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "5ab6ddd1d45302bf635cce5cb93fbaf4ea79458a", "1375a19dd4429a064d7fee959ff07d495ba8c06c", "fcd6cbdf5b6f93a65cfd8e6fc4a73a442d6a4c2b"], "page_rank": 0.0002463054187192118}, {"id": "86c2a7dc48445d75ec9bc71f7d9fdec622687e90", "title": "Machine Comprehension with Discourse Relations", "authors": ["Karthik Narasimhan", "Regina Barzilay"], "date": 2015, "abstract": "This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is computed using off-the-shelf discourse analyzers. This design provides limited opportunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces relations between sentences while optimizing a task-specific objective. This approach enables the model to benefit from discourse information without relying on explicit annotations of discourse structure during training. The model jointly identifies relevant sentences, establishes relations between them and predicts an answer. We implement this idea in a discriminative framework with hidden variables that capture relevant sentences and relations unobserved during training. Our experiments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems.1", "references": ["a6e0701de9fd89c1d74c997d9f264b0177c3c86f", "28d6c0111ab3d1020aee3eb80fe67698a904012c", "16fee45352cad81fae2dd58b3bd9624eb1141c86", "6396ab37641d36be4c26420e58adeb8665914c3b", "d1d4712f267b2a65d5210a77e91d2a24888a4da0", "27c18ff7782c227e9fdd972b749d9f9a8556305e", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "5985e41f8b1bdca631ff7bc95dde59b25e03f5c3", "b8d2f988642099b6d7bd93cb1563c2459c323631", "38dd6e69be875011d9b73c1304777586e468e397"], "page_rank": 0.0002463054187192118}, {"id": "96526726f87233fb017f6ea9483090f04e0f0530", "title": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs", "authors": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith"], "date": 2015, "abstract": "We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.", "references": ["0fac3a87cf6f214a10f4e39d73e119ea81b52e46", "a76aac7697a9ad2f0d522fbd15b553697f1eb7bc", "2c22177821ffebc8bbdd8fd0d10b91a48c18d495", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "0ec546aa8f6f9340020ddaa3f77b7b5b2541b559", "061ab5336eb1a59ff89645a8540131df74ca2091", "eb42a490cf4f186d3383c92963817d100afd81e2", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "7d53d45bb95c8f2e7f00341f6739c7792667035e", "46f418bf6fab132f193661226c5c27d67f870ea5"], "page_rank": 0.0001231527093596059}, {"id": "8bc26623a700b564934221dd34c9cfbfeedc5efa", "title": "Spatial Random Tree Grammars for Modeling Hierarchal Structure in Images with Regions of Arbitrary Shape", "authors": ["Jeffrey Mark Siskind", "James Sherman", "Ilya Pollak", "Mary P. Harper", "Charles A. Bouman"], "date": 2007, "abstract": "We present a novel probabilistic model for the hierarchical structure of an image and its regions. We call this model spatial random tree grammars (SRTGs). We develop algorithms for the exact computation of likelihood and maximum a posteriori (MAP) estimates and the exact expectation-maximization (EM) updates for model-parameter estimation. We collectively call these algorithms the center-surround algorithm. We use the center-surround algorithm to automatically estimate the maximum likelihood (ML) parameters of SRTGs and classify images based on their likelihood and based on the MAP estimate of the associated hierarchical structure. We apply our method to the task of classifying natural images and demonstrate that the addition of hierarchical structure significantly improves upon the performance of a baseline model that lacks such structure.", "references": ["b2c4d65fc481310b81445a5f140cffee7a395d0d", "979f7dde5dc3e6e42ae81213cd11d440d338173c", "843beaaa56f6de56eff986f4c779ddedce874889", "1dc2443a955a6d3ef06c10d1071998c267e3a229", "363b56f85e12389017ba8894056a1b309e46a5f7", "a5f39edf5d270c6fd67d8a1ffeab2cc357deb118", "5e3a9d335f999e4f32e5708da6122f1c46ed3ada", "d71126dfdfef0f5d0b2eeb12fe3c988e505dc008", "65c87167e783262395cff83c23f57863bb74f243", "3fc5f96b85654154a4d78944e9a2ee26e5aba789"], "page_rank": 0.0007389162561576354}, {"id": "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f", "title": "Probabilistic modeling based vessel enhancement in thoracic CT scans", "authors": ["Gady Agam", "Changhua Wu"], "date": 2005, "abstract": "Vessel enhancement in volumetric data is a necessary prerequisite in various medical imaging applications with particular importance for automated nodule detection. Ideally, vessel enhancement filters should enhance vessels and vessel junctions while suppressing nodules and other non-vessel elements. A distinction between vessels and nodules is normally obtained through eigenvalue analysis of the curvature tensor which is a second order differential quantity and so is sensitive to noise. Furthermore, by relying on principal curvatures alone, existing vessel enhancement filters are incapable of distinguishing between nodules and vessel junctions. In this paper we propose probabilistic vessel models from which novel vessel enhancement filters capable of enhancing junctions while suppressing nodules are derived. The proposed filters are based on eigenvalue analysis of the structure tensor which is a first order differential quantity and so are less sensitive to noise. The proposed filters are evaluated and compared to known techniques based on actual clinical data.", "references": ["9dadf602ba5849e5db34301b36b7081c99dcc7e7", "c3a6c534776c0953b04e7852e5e0773119c96268", "c8486104d2ce791485c03f4bf55c31c5218c0110", "2c4570a6e2ff603b0c8c78c7c4de5617639e2b01", "7a8115243c57cae484961da102767ebdc7ef849e", "4aa6f029a4d63da6f4d9b5a5cb87f8207aab88b0", "f196cef7e67c9443c049fd175092316119d37ffe", "696459e0c67f729a05a819699adae5d64aaab4b3", "cbf130a89881ec8046fe2b20bda7940b38877153", "52faef9f54f7c741b4ac574a0f1a596ea9c22ee4"], "page_rank": 0.0002463054187192118}, {"id": "832fc9327695f7425d8759c6aaeec0fa2d7b0a90", "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia", "authors": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "date": 2016, "abstract": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "references": ["f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "eb9fb8385c5824b029633c0cb68a8fb8573380ad", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "822f1ed9a76a57cc19d8fda7745365b97130b97a", "d1505c6123c102e53eb19dff312cb25cea840b72", "1956c239b3552e030db1b78951f64781101125ed"], "page_rank": 0.0002463054187192118}, {"id": "58b0be2db0aeda2edb641273fe52946a24a714c3", "title": "Attribute-based people search in surveillance environments", "authors": ["Daniel A. Vaquero", "Rog{\\'e}rio Schmidt Feris", "Duan Tran", "Lisa M. Brown", "Arun Hampapur", "Matthew Turk"], "date": 2009, "abstract": "We propose a novel framework for searching for people in surveillance environments. Rather than relying on face recognition technology, which is known to be sensitive to typical surveillance conditions such as lighting changes, face pose variation, and low-resolution imagery, we approach the problem in a different way: we search for people based on a parsing of human parts and their attributes, including facial hair, eyewear, clothing color, etc. These attributes can be extracted using detectors learned from large amounts of training data. A complete system that implements our framework is presented. At the interface, the user can specify a set of personal characteristics, and the system then retrieves events that match the provided description. For example, a possible query is \u201cshow me the bald people who entered a given building last Saturday wearing a red shirt and sunglasses.\u201d This capability is useful in several applications, such as finding suspects or missing people. To evaluate the performance of our approach, we present extensive experiments on a set of images collected from the Internet, on infrared imagery, and on two-and-a-half months of video from a real surveillance environment. We are not aware of any similar surveillance system capable of automatically finding people in video based on their fine-grained body parts and attributes.", "references": ["6b4778ce78ad7186db1f08ec548d3984b4e440d7", "ac7f973658b55563f4d56e5b763c9049dd1034e0", "28f232498c6e14950ef9b6a3ae0781f2348be3b7", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "d363d3343cb70ad757d472d80622e1a7c5982fd9", "f797ed7a4220278c25d73657365c39231f5ad0be", "06a1382a0fc63fb173fe570e7b6a84158d4e06a5", "42cd5b7f483b974a19d64f2ac397cc7f15fbafba", "e5890f4ad85c545684505e3cdedaaf59d3b8bac5", "461efc87636ff4e48323ffcb9f8fdf79cf736fb0"], "page_rank": 0.0004926108374384236}, {"id": "689b4bec1cf9dbfd6af293eff2aeefd8c18085b2", "title": "Domain Adaptation with Active Learning for Word Sense Disambiguation", "authors": ["Yee Seng Chan", "Hwee Tou Ng"], "date": 2007, "abstract": "When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.", "references": ["57111d4896b116f1b253db54bb944118b9f26b7f", "fd303a438cef5eb647c63cd1f25bad12a5babba3", "a859c95269fdfe81b1a4ebf1f8255bfdbabad25a", "efa8b1c728a06a4f01a60f586ea684592cb746a2", "4c2a642effd543babace0c565b48cadcb6fce14f", "1aac9a51700b4f548ed4d406d3987c8008876521", "176ce079bd5efa86677aea8804158dc29b651e45", "8014b26403cc1085bc86ce5a6a3d7ff54eb49bc2", "89a3d24b4d6e28706c75c9794e35502138fa29fb", "63936c6ce3ffedd4e4bc653a7ce3c3d295adb8c0"], "page_rank": 0.0004926108374384236}, {"id": "472ff211d18aa2ca2b65b69d86499430ad287499", "title": "Towards automatic discovery of object categories", "authors": ["Markus Weber", "Max Welling", "Pietro Perona"], "date": 2000, "abstract": "We propose a method to learn heterogeneous models of object classes for visual recognition. The training images contain a preponderance of clutter and learning is unsupervised. Our models represent objects as probabilistic constellations of rigid parts (features). The variability within a class is represented by a join probability density function on the shape of the constellation and the appearance of the parts. Our method automatically identifies distinctive features in the training set. The set of model parameters is then learned using expectation maximization. When trained on different, unlabeled and unsegmented views of a class of objects, each component of the mixture model can adapt to represent a subset of the views. Similarly, different component models can also \"specialize\" on sub-classes of an object class. Experiments on images of human heads, leaves from different species of trees, and motor-cars demonstrate that the method works well over a wide variety of objects.", "references": ["1cf1527807ebb16020b04d4166e7ba8d27652302", "25c3065da19b967caf4aa8a8e1ac826ae85448b6", "102cf35af78c14019edfe28c8cd624da7d5b3fac", "de6f7d242b78bf5e87966fb72565ba43d277084e", "4fb52984078d75ec5655962dc94dc7848182286b", "62ccf3020ac41088b39c3982d119752b0ba6b261", "846b99d151892e70046e5ca541430be4ff67209f", "248fb34a10dcd3cfb7e606692b920e4bbca0ea6a", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "b07ce649d6f6eb636872527104b0209d3edc8188"], "page_rank": 0.0004926108374384236}, {"id": "228d9e4b69926594fd26080f4cfaa9ecfca44eb3", "title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "authors": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "date": 2010, "abstract": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek. This mathematical framework enables us to compute the meaning of a well-typed sentence from the meanings of its constituents. Concretely, the type reductions of Pregroups are `lifted' to morphisms in a category, a procedure that transforms meanings of constituents into a meaning of the (well-typed) whole. Importantly, meanings of whole sentences live in a single space, independent of the grammatical structure of the sentence. Hence the inner-product can be used to compare meanings of arbitrary sentences, as it is for comparing the meanings of words in the distributional model. The mathematical structure we employ admits a purely diagrammatic calculus which exposes how the information flows between the words in a sentence in order to make up the meaning of the whole sentence. A variation of our `categorical model' which involves constraining the scalars of the vector spaces to the semiring of Booleans results in a Montague-style Boolean-valued semantics.", "references": ["314782b81f04378471c27e1a91a2ea0bd670e1ab", "f47fa369a91ec995a02c1c0ec846436e32808ebe", "6d8018bd8b288baca0c55522877efd1b49258747", "73e897104540642698321c106cc9c35af369fe12", "de5e318b0045b7df732f74a772eee314c8e8d2be", "c4732c036d0b00d435bcd41ae904a9e936e4f683", "de5861000ea45ceace5fb74aafecea6e8d05f542", "069ca4d3ce5fdcdad511ec6cb1a6bb5fccf1f298", "03380e7083807d3264472871dc0582036cf79479", "509a2ca90a85c62d66a16b37e0de28715dd4e89f"], "page_rank": 0.00022167487684729062}, {"id": "4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a", "title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "authors": ["Ingrid Daubechies", "Michel Defrise", "C. De Mol"], "date": 2003, "abstract": "We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted p-penalties on the coefficients of such expansions, with 1 \u2264 p \u2264 2, still regularizes the problem. Use of such p-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. \u00a9 2004 Wiley Periodicals, Inc.", "references": ["2ced7c0cf116b241ddc8908918161ff995d0238a", "392ab05014a0e2301c1b1edb40a253a3a1bd8936", "19da2b7f74493b5a1f8876abcc3db2332a51f256", "2399029a4111e560f3f3925d6035408ba1542174", "7f9ffc3a186204076ad3b7a9086d7104c79e5e06", "df8d148efe659fb56e1deafa78ac94d3415f97ba", "9f6058c289adb7a91b1ddcc904ff23094edaa92f", "48d9d162588aa024be03bdea1a98823c389de6d2", "601ba02449e03c7b36e882b80de24c42b887aa4d", "7368898189d0b617659c5d20bf89d34b245d58c9"], "page_rank": 0.0002463054187192118}, {"id": "dcceb47a102bb1a634a494e43030364e7f8e7533", "title": "Linguistically Motivated Large-Scale NLP with C&C and Boxer", "authors": ["James R. Curran", "Stephen Clark", "Johan Bos"], "date": 2007, "abstract": "The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a break-through in NLP technology.", "references": ["913c0fe31e99379dcc3e9e8c584c596bf6113941", "662cc508b94f3a99585053caff7d0d5a63857700", "8af70241b409aa12af225b9e17b346fbd101d2bc", "b9eea85e590f6e522e3681b8e45012684c60b0fd", "5a8399a28aa322ed6b27b6408d34f44abdbf7b46", "6cc701c93616b8308877a569db4a2e880de74cbc", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "15b6080c3dfdfba4869020d03d089bc443ce022b", "7410c010a38e7e23f38c3c6e898d5695a4874c61", "e979925b15861153a0e9ce8ace39a28d319e613d"], "page_rank": 0.0004926108374384236}, {"id": "3cd32f1a3d8a090a75658528eeecc38cba8a2bf1", "title": "Concrete Sentence Spaces for Compositional Distributional Models of Meaning", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh", "Stephen Clark", "Bob Coecke", "Stephen G. Pulman"], "date": 2011, "abstract": "Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map, by constructing a corpus-based vector space for the type of sentence. Our construction method is based on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This enables us to compare meanings of sentences by simply taking the inner product of their vectors.", "references": ["228d9e4b69926594fd26080f4cfaa9ecfca44eb3", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "37efe2ef1b9d27cc598361a8013ec888a6f7c4d8", "73e897104540642698321c106cc9c35af369fe12", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "36c4c51917b1f53ee85c459f2597e115df53eb05", "99eadd5e29a85f30cafef7f2c915f384715e3b89"], "page_rank": 9.852216748768472e-05}, {"id": "cbb786e7e42d02de0080299dc2e114357e816002", "title": "Conditional Models of Identity Uncertainty with Application to Noun Coreference", "authors": ["Andrew McCallum", "Ben Wellner"], "date": 2004, "abstract": "Coreference analysis, also known as record linkage or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational\u2014they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies\u2014paralleling the advantages of conditional random fields over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets.", "references": ["a20bfec3c95aad003dcb45a21a220c19cca8bb66", "e20d2d6c57032114ee57118ff620fdef7c548c46", "08c81389b3ac4b8253d718a7cebe04a5536efa78", "25308818aa94ffb3a9809540b530f6e8c7bfb83c", "9cc36397e1fef5c922d64e88211a7e08ecc64759", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "02537122c7f00d63c8c9c861791a2026a4879e33", "71c0698edd0cf489cd837c91ad22bbf51643bf6c", "ae287b7ecbb3dffaa0e302acfd7bbf44f733acfc", "611dac316bf03112c778cf7365d08e4a9d171876"], "page_rank": 0.0002463054187192118}, {"id": "bc96de1cc022a0425e9f4f607c8d95064a6b3811", "title": "Experimenting with Transitive Verbs in a DisCoCat", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "date": 2011, "abstract": "Formal and distributional semantic models offer complementary benefits in modeling meaning. The categorical compositional distributional model of meaning of Coecke et al. (2010) (abbreviated to DisCoCat in the title) combines aspects of both to provide a general framework in which meanings of words, obtained distributionally, are composed using methods from the logical setting to form sentence meaning. Concrete consequences of this general abstract setting and applications to empirical data are under active study (Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011). In this paper, we extend this study by examining transitive verbs, represented as matrices in a DisCoCat. We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011).", "references": ["3c5126da7ce388c64b796c80d15a3c3629d6ad58", "96fa75d886643fa1621cc30f4f55c0a22c2e49d5", "37efe2ef1b9d27cc598361a8013ec888a6f7c4d8", "73e897104540642698321c106cc9c35af369fe12", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "459745250d453dacf8373b7a34a353bb1fa9e147", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c"], "page_rank": 9.852216748768472e-05}, {"id": "ed91ce023b6500c586802de7d23d8f8f01e5aa1b", "title": "Efficiency of pseudolikelihood estimation for simple Gaussian fields", "authors": ["Julian Besag"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"Efficiency of pseudolikelihood estimation for simple Gaussian fields\" by Julian Besag", "references": [], "page_rank": 0.0002463054187192118}, {"id": "133809cf62bf67f0a63b35e5ef5180d20c9aec19", "title": "Large Scale Online Learning", "authors": ["L{\\'e}on Bottou", "Yann LeCun"], "date": 2003, "abstract": "We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.", "references": ["3380f30e85577f67f7e178b70bf9f120ec16a3bc", "fc6b1ff29f2da985cccfa644652bb320d7720d59", "5a767a341364de1f75bea85e0b12ba7d3586a461", "ded14685a23df94d93e8662578d4132c9f4aa1c7", "233e47fb2f97ec8f587c62c3f84c7971fae8e568"], "page_rank": 0.0002463054187192118}, {"id": "29085cdffb3277c1c8fd10ac09e0d89452c8db83", "title": "An Input Output HMM Architecture", "authors": ["Yoshua Bengio", "Paolo Frasconi"], "date": 1994, "abstract": "We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation.", "references": ["ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "13369d124474b5f8dcbc70d12296a185832192b2", "d0be39ee052d246ae99c082a565aba25b811be2d", "830ccb44084d9d6cdcb70d623df5012ae4835142", "a64ca771a733d58dcbf8f7a3fe65a09310424bf8", "b185742930fd959aaccdfdecdb31641839a787c4", "872cdc269f3cb59f8a227818f35041415091545f", "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "7077a8802ac5f65991bf1086b07d0d6f17e9d894"], "page_rank": 0.00016420361247947453}, {"id": "9d076613d7c36dbda4a6ff42fbdd076604b96630", "title": "An introduction to hidden Markov models", "authors": ["Lawrence R. Rabiner", "Biing Hwang Juang"], "date": 1986, "abstract": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.", "references": ["090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "e90c15e0de8b5452c6291359e98ddc099e3b93f6", "999aa520f3f5523286613d17a349d8a0935bbcc1", "c180f387357d9302a558bcd643209831744c639b", "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "3ef4ec3a5606b23275e191aafff2ef1e9356b716", "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4"], "page_rank": 0.00016420361247947453}, {"id": "e08d090d1e586610d636a46004876e9f3ded8209", "title": "A time-delay neural network architecture for isolated word recognition", "authors": ["Kevin J. Lang", "Alexander H. Waibel", "Geoffrey E. Hinton"], "date": 1990, "abstract": "Abstract A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.", "references": ["3e6bea2649298c68d17b9421fc7dd19eeacc935e", "dbe8c61628896081998d1cd7d10343a45b7061bd", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "e69606729837aa1d0168c47f812cbccaba09dc83", "969f1eb7e976174a6a3150d05e0b3e6f0f5aeda9", "9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5", "a57c6d627ffc667ae3547073876c35d6420accff", "7257eacd80458e70c74494eb1b6759b52ff21399"], "page_rank": 0.00016420361247947453}, {"id": "14c146d457bbd201f3a117ee9c848300d341e5d0", "title": "The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies", "authors": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Llu{\\'i}s M{\\`a}rquez i Villodre", "Joakim Nivre"], "date": 2008, "abstract": "The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies. This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year's syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems.", "references": ["bc03890a03a2a284f550f2cd1c722b3677a77955", "10a9abb4c78f0be5cc85847f248d3e8277b3c810", "9940201164381c95746edb6d2ef4350d46840157", "dc4a6ee4276fa077014f8a1755def3c559764b61", "34eba45dc13082f00739d112ca309bfb4a466832", "83c1afce3c4ee706223d63f258ec79d2e9cd5093", "dff4eb6721a72cfb6b090ef7b80f9cd2e9384653", "5f3ea64359eb8b7de1928af683f1748f535d4fc4", "d8a5a08f35f7dd5019cebe768092f268843cd9b4", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d"], "page_rank": 0.0002463054187192118}, {"id": "fc24fe37c2bda8ddeabc01c0df57814648eec054", "title": "Numerical Recipes: The Art of Scientific Computing", "authors": ["William H. Press", "Brian P. Flannery", "Saul A. Teukolsky", "William T. Vetterling", "Harvey Gould"], "date": 1987, "abstract": "From the Publisher: \nThis is the revised and greatly expanded Second Edition of the hugely popular Numerical Recipes: The Art of Scientific Computing. The product of a unique collaboration among four leading scientists in academic research and industry, Numerical Recipes is a complete text and reference book on scientific computing. In a self-contained manner it proceeds from mathematical and theoretical considerations to actual practical computer routines. With over 100 new routines (now well over 300 in all), plus upgraded versions of many of the original routines, this book is more than ever the most practical, comprehensive handbook of scientific computing available today. The book retains the informal, easy-to-read style that made the first edition so popular, with many new topics presented at the same accessible level. In addition, some sections of more advanced material have been introduced, set off in small type from the main body of the text. Numerical Recipes is an ideal textbook for scientists and engineers and an indispensable reference for anyone who works in scientific computing. Highlights of the new material include a new chapter on integral equations and inverse methods; multigrid methods for solving partial differential equations; improved random number routines; wavelet transforms; the statistical bootstrap method; a new chapter on \"less-numerical\" algorithms including compression coding and arbitrary precision arithmetic; band diagonal linear systems; linear algebra on sparse matrices; Cholesky and QR decomposition; calculation of numerical derivatives; Pade approximants, and rational Chebyshev approximation; new special functions; Monte Carlo integration in high-dimensional spaces; globally convergent methods for sets of nonlinear equations; an expanded chapter on fast Fourier methods; spectral analysis on unevenly sampled data; Savitzky-Golay smoothing filters; and two-dimensional Kolmogorov-Smirnoff tests. All this is in addition to material on such basic top", "references": [], "page_rank": 0.0001231527093596059}, {"id": "46309506f8d687e4963e3d0850a0506cfdc47563", "title": "HyTER: Meaning-Equivalent Semantics for Translation Evaluation", "authors": ["Markus Dreyer", "Daniel Marcu"], "date": 2012, "abstract": "It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.", "references": ["51951073580f6995e55be873db9a7f6a9736ca86", "ed909ad651011dab1c30d587a0574442581a2ead", "0f8992ee6418d367d8e50ecbb59b08ea15e8431f", "d7da009f457917aa381619facfa5ffae9329a6e9", "0cfe82145332522ed6665bd039f4fd089ae24141", "443516aeb2819d4d362ffe7d5418a54e5427a016", "2ccbdf9e9546633ee58009e0c0f3eaee75e6f576", "58b543783ed43edaa57b1b3a3c4ea81b5b23aad9", "2d60175fa4c4d2f9339d95f7ae2c5ce30c11575d", "c81dd2ef15abf0471f037639d822378165475dbf"], "page_rank": 0.0001231527093596059}, {"id": "67863c9dc84d431cbe911326e9e277b9f6828d27", "title": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics", "authors": ["Adam Kilgarriff"], "date": 2012, "abstract": "Linking implicit semantic roles is a challenging problem in discourse processing. Unlike prior work inspired by SRL, we cast this problem as an anaphora resolution task and embed it in an entity-based coreference resolution (CR) architecture. Our experiments clearly show that CR-oriented features yield strongest performance exceeding a strong baseline. We address the problem of data sparsity by applying heuristic labeling techniques, guided by the anaphoric nature of the phenomenon. We achieve performance beyond state-of-the art.", "references": ["03dabb1027b39e417373f33a44bb34553002e889", "0dad0da221dea30c3a0e90c45a0699aeb850af49", "775e95e3368c96840f2c33f4c51142f887abecc5", "3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4", "8cef20c98d41018c44c78772424112c5b5679144", "11ec56898a9e7f401a2affe776b5297bd4e25025", "6006f32532a62a84e02c891a5cb453d729d30f48", "bde226ef8a08626b066a93a46fa07d19df0e3a5b", "1d1d8900596aec8759239e2739f777a9ed77b717", "5aa553ad1dc7b2be021294486b86fc187069278d"], "page_rank": 0.0001231527093596059}, {"id": "e54d8b07ef659f9ee2671441c4355e414e408836", "title": "OntoNotes: The 90% Solution", "authors": ["Eduard H. Hovy", "Mitchell P. Marcus", "Martha Palmer", "Lance A. Ramshaw", "Ralph M. Weischedel"], "date": 2006, "abstract": "We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "1138cd2527252cc0a2e30d5f61125b0da81e716f", "a881f2ffc610d6ca3a87390dbd359584771d5976", "255d6867cb5c57810c909d5e488c9ae86e0d6d3e", "396abe7c248551be931880c97aacc584260806b7", "793c70bf07760a2a3ad8a5ac32fd3a37fe8849ff", "19a8d8a2e3c8b7b9f5359a69534b2c9e331d65d0", "547f23597f9ec8a93f66cedaa6fbfb73960426b1", "c005a495d2f3d9a864108a8bf6b85a2755b7a038", "d7d6a05eb6276cfbb3106acba3c203f7223841c7"], "page_rank": 0.0001231527093596059}, {"id": "4e8d9c5bf3c02e716935bcd00e5dcdd2557e75a7", "title": "Stanford: Probabilistic Edit Distance Metrics for STS", "authors": ["Mengqiu Wang", "Daniel Matthew Cer"], "date": 2012, "abstract": "This paper describes Stanford University's submission to SemEval 2012 Semantic Textual Similarity (STS) shared evaluation task. Our proposed metric computes probabilistic edit distance as predictions of semantic similarity. We learn weighted edit distance in a probabilistic finite state machine (pFSM) model, where state transitions correspond to edit operations. While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model. Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features. The performance of our edit distance based models is contrasted with an adaptation of the Stanford textual entailment system to the STS task. Our results show that the most advanced edit distance model, pPDA, outperforms our entailment system on all but one of the genres included in the STS task.", "references": ["57edd35b451531f8f08ac409245e4192b4a2731e", "982923236f3bc380e5ebdd8f56c46686029dc66a", "3fe5d08d701c3d5ac32e9c3df8b994448ab5fdb8", "3519b5dfde676c91b0adefad0e6f5d03cd4b83e6", "c4138748eb5dc1bbd1df2951f299d701304147a2", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "f0177ff2ee4e6b1c001465fdb96429b02541089b", "d215c7a1f7176c8a536e1ca79e56fcd405cfa770", "7f3e90eae5d24f502603163aed4bdfc32203207c", "603ccac1c7836ef05f0b92a1f986d44becfa5a34"], "page_rank": 0.0002463054187192118}, {"id": "8766f3686f809628dac1c92d1dc34f8c1bb3106e", "title": "Searching for Part of Speech Tags That Improve Parsing Models", "authors": ["Mart{\\'i}n Ariel Dom{\\'i}nguez", "Gabriel G. Infante L{\\'o}pez"], "date": 2008, "abstract": "We introduce a technique for inducing a refinement of the set of part of speech tags related to verbs. We cluster verbs according to their syntactic behavior in a dependency structure setting. The set of clusters is automatically determined by means of a quality measure over the probabilistic automata that describe words in a bilexical grammar. Each of the resulting clusters defines a new part of speech tag. We try out the resulting tag set in a state-of-the art phrase structure parser and we show that the induced part of speech tags significantly improve the accuracy of the parser.", "references": ["cd6a64424354e7f601e9f8a624f9e92d9fa081fe", "a6268f7ec37bccab46cb8b6d8ed7300f90699799", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "85d1f9caecdb03fa8e754c280afe9191b44c0619", "996b5cf724a5151b1fb14da2de8544ded7974b50", "e88fdc9c861c2e6f618d31cd4825328fc1e1718c", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "f52de7242e574b70410ca6fb70b79c811919fc00", "348a4a34f934692c58751b0284c5cfeb8f0c96f2"], "page_rank": 0.0004926108374384236}, {"id": "15d48f8e1739ab85bb409b5a8813787534cabafc", "title": "Context and Hierarchy in a Probabilistic Image Model", "authors": ["Ya Jin", "Stuart Geman"], "date": 2006, "abstract": "It is widely conjectured that the excellent ROC performance of biological vision systems is due in large part to the exploitation of context at each of many levels in a part/whole hierarchy. We propose a mathematical framework (a \"composition machine\") for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, and we build a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport. The demonstration system detects and correctly reads more than 98% of the plates, with a negligible rate of false detection. Unlike a formal grammar, the architecture of a composition machine does not exclude the sharing of sub-parts among multiple entities, and does not limit interpretations to single trees (e.g. a scene can have multiple license plates, or no plates at all). In this sense, the architecture is more like a general Bayesian network than a formal grammar. On the other hand, unlike a Bayesian network, the distribution is non-Markovian, and therefore more like a probabilistic context-sensitive grammar. The conceptualization and construction of a composition machine is facilitated by its formulation as the result of a series of non-Markovian perturbations of a \"Markov backbone.\"", "references": ["09256ded3d662e8fa1272f1e28d0e1a8d9277465", "cca9200d9da958b7f90eab901b2f30c04f1e0e9c", "861897df39716877fb1e03a7d09a234faca076e9", "d41963bb5af33ba1a764f659a4067f1ff56866e0", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "8bc26623a700b564934221dd34c9cfbfeedc5efa", "e596a25fe95ab91df1db20f66e37eca9aee39616", "626a742de034cba3a8706510aca63bd287691995", "61b933b8ef5b10ae4f6491a89f89972322534cf0", "2077d0f30507d51a0d3bbec4957d55e817d66a59"], "page_rank": 0.0004926108374384236}, {"id": "7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering", "authors": ["Mats Rooth", "Stefan Riezler", "Detlef Prescher"], "date": 1999, "abstract": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.", "references": ["0f0ab8eb169dafb221f60e98413262fa306dcbb6", "096fd36589d4cec91d9a08d5c1da6d031bc498a2", "fed64bc406adaef9d993f34363f6f1818bbb118e", "f1c25c6d030605df497ac599deaf6c0693e6c80e", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "7d37dff2d8e65764e7293750051d519359d8835d", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "f7463aad3b5182820995101602788ea4c9bb4d9f", "3e9dfc4f5ced83a745249e409223ddab85381c88", "99860c33faa88c42418e58b606738e3dc5dbe1b3"], "page_rank": 0.0006568144499178981}, {"id": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "date": 2013, "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "references": ["27e38351e48fe4b7da2775bf94341738bc4da07e", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "9606ff5bdeb6b9bde63c5bf6ad22edeca51d35db", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "98d2bb15fa31f6e9df73b8e7e8e038a99b420190", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "3a0e788268fafb23ab20da0e98bb578b06830f7d"], "page_rank": 0.0007389162561576354}, {"id": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "title": "Moses: Open Source Toolkit for Statistical Machine Translation", "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "date": 2007, "abstract": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.", "references": ["694b3c58712deefb59502847ba1b52b192c413e5", "659f1f754954d093e684ead4842832052f7bf748", "3036c284f5913f6dbcdec9382c67ae4d3bfe9040", "e920bfe724d9661add513965e7296cd621aa1fe8", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "50b8e8d48f4973cdeefa835807b4e1a8ca65ced3", "0b8f0e60a648880ddeaed371c339714f66f24624", "1f12451245667a85d0ee225a80880fc93c71cc8b", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "20c11546a035d2fa2fa1121a7b31e890d20d6b6b"], "page_rank": 0.0006568144499178981}, {"id": "35199edf885516904f0fc0f8e2f35e14040568ab", "title": "Automatic Learning of Chinese English Semantic Structure Mapping", "authors": ["Pascale Fung", "Zhaojun Wu", "Yongsheng Yang", "Dekai Wu"], "date": 2006, "abstract": "We present twin results on Chinese semantic parsing, with application to English-Chinese cross- lingual verb frame acquisition. First, we describe two new state-of-the-art Chinese shallow semantic parsers leading to an F-score of 82.01 on simultaneous frame and argument boundary identification and labeling. Subsequently, we propose a model that applies the separate Chinese and English semantic parsers to learn cross-lingual semantic verb frame argument mappings with 89.3% accuracy. The only training data needed by this cross-lingual learning model is a pair of non-parallel monolingual Propbanks, plus an unannotated parallel corpus. We also present the first reported controlled comparison of maximum entropy and SVM approaches to shallow semantic parsing, using the Chinese data.", "references": ["df87a18357f3c462fc21df0ce9f60160714fd1e3", "ee4c4fe7fd24125531a0e9eafb6d110cf3c27398", "e5eacb904a977a131d3bf53d0ead9d0a746c2ed4", "268f1d5341f61fd46530cc91d038109bbd45e56a", "40a90af58c913d377f6a9772124133006941b8f6", "a584e4b607e972783cea22daaaf1114ea94a8035", "a16e484824b2580e092c985aa659e8680aeda5ee", "b951b9f78b98a186ba259027996a48e4189d37e5"], "page_rank": 0.00016420361247947453}, {"id": "a8ca92770bce439a207cc75fd28a749b51b5a516", "title": "Comparison of part-of-speech and automatically derived category-based language models for speech recognition", "authors": ["Thomas R Niesler", "Edward W. D. Whittaker", "Philip C. Woodland"], "date": 1998, "abstract": "This paper compares various category-based language models when used in conjunction with a word-based trigram by means of linear interpolation. Categories corresponding to parts-of-speech as well as automatically clustered groupings are considered. The category-based model employs variable-length n-grams and permits each word to belong to multiple categories. Relative word error rate reductions of between 2 and 7% over the baseline are achieved in N-best rescoring experiments on the Wall Street Journal corpus. The largest improvement is obtained with a model using automatically determined categories. Perplexities continue to decrease as the number of different categories is increased, but improvements in the word error rate reach an optimum.", "references": ["aa6b50b28e450d91aa181581109b6013022f4c7f", "24653b3b33d48c409deb672f8d8ee0eff31cd418", "a208a5a99e2a4d8f48673e980e8cc0479df890f8", "2e6dd008f2837dea054cd968ae6a50325809833d", "8648dbfff9662fa9c62a95622712dd2951b5b3a3", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "0b26fa1b848ed808a0511db34bce2426888f0b68", "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf", "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87", "b9ed0b35c9eaba0328492de65c4cdc5545094df4"], "page_rank": 0.0002463054187192118}, {"id": "38dd6e69be875011d9b73c1304777586e468e397", "title": "Text-level Discourse Parsing with Rich Linguistic Features", "authors": ["Vanessa Wei Feng", "Graeme Hirst"], "date": 2012, "abstract": "In this paper, we develop an RST-style text-level discourse parser, based on the HILDA discourse parser (Hernault et al., 2010b). We significantly improve its tree-building step by incorporating our own rich linguistic features. We also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourse-parsing performance under different discourse conditions.", "references": ["0b3858c0c31c6f0826c891a42367671f6e76d46c", "5981b4a71ed1236cdd58df1c09cda351670151d0", "a81619ebf10f6cd73e8a5fa85d8f7882f02b9fbd", "e365e8af684bd2337cb81f65f90e1597406b5f77", "7cc84d84ad87b9a1f6496fb4acbbc758a9be6345", "2b23050d9746fef1105175493bf605f97f04a9af", "17cfa8c38326a8c57c4c8510bfbdeac5dcfd7ede", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "738ab73b8a20ce9c831e8ae6cf05c0944fb4054d", "a6e0701de9fd89c1d74c997d9f264b0177c3c86f"], "page_rank": 0.0002463054187192118}, {"id": "3de5d40b60742e3dfa86b19e7f660962298492af", "title": "Class-Based n-gram Models of Natural Language", "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Peter V. de Souza", "Jennifer C. Lai", "Robert L. Mercer"], "date": 1992, "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "references": ["0ae4756313dc6e8e33ab69593a6f7475025a64fd", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "7e8b2d1c6c8be30cbf8e19b99a20186411ef1ba8", "e0f5f3ec67edc870e546ced63e9db09bd215e6db", "a1066659ec1afee9dce586f6f49b7d44527827e1", "03b11aec8deafa2572ab5a8fac43bfc94db9ec37"], "page_rank": 0.00046798029556650243}, {"id": "52faef9f54f7c741b4ac574a0f1a596ea9c22ee4", "title": "Analysis of the Pulmonary Vascular Tree Using Differential Geometry Based Vector Fields", "authors": ["James P. Williams", "Lawrence B. Wolff"], "date": 1997, "abstract": "We extract topological and local structural information from X-ray-computed tomography (CT) volume images of the vascular tree of the lung. We then produce an abstract model of the topology and geometry of the tree which is suitable for generating physical models. Physiologists model the vascular tree as a connected network of tubes and, to date, the only accurate data available to produce such models has come from dissection and measurement. Modeling the dynamic behavior of the tree in a living organism has been, until now, impossible. Building models from CT scans will increase the volume and quality of information available for both the study of physiology and diagnosis. We present an efficient, accurate analysis technique for volume images of tube networks. Using local operations that link techniques based on morphology to differential geometry, we transform the volume into a vector field which resembles idealized, axis-parallel fluid flow through the tube network. This field provides information to condense the salient features of the image into an augmented Euclidean minimum spanning tree (EMST). This augmented EMST proves to be a convenient and logical abstract representation of the vascular tree.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "b8d2f988642099b6d7bd93cb1563c2459c323631", "title": "Discourse Structure for Context Question Answering", "authors": ["Joyce Chai", "R. H. Jin"], "date": 2004, "abstract": "In a real-world setting, questions are not asked in isolation, but rather in a cohesive manner that involves a sequence of related questions to meet user\u2019s information needs. The capability to interpret and answer questions based on context is important. In this paper, we discuss the role of discourse modeling in context question answering. In particular, we motivate a semantic-rich discourse representation and discuss the impact of refined discourse structure on question answering.", "references": ["b1bde182cddbd23e3930a8b7538a227673bda7ab", "53073cf42228f7e1d23c513eb474c7e599f82c02", "989aaca363a201fac4961965d1ea805ad96bc3f4", "8bdb479b6c837c148209be3e51c5e82c1dc92448", "18d04fa815cbc9e7a2eeab189a4d7388cb43b0cb", "07a78850c0c2ff11acf21fccca40bfcb79da282b", "1a5a60233da0feec4d6c3c22f9b3b8656d0dbd84", "0b3858c0c31c6f0826c891a42367671f6e76d46c", "3e5652cca6464981a065184dceb2672bd13984b7", "88a2432fcc758f4a0a469df1f0a26d836d2d0b09"], "page_rank": 0.0002463054187192118}, {"id": "89a3d24b4d6e28706c75c9794e35502138fa29fb", "title": "Supervised and unsupervised PCFG adaptation to novel domains", "authors": ["Brian Roark", "Michiel Bacchiani"], "date": 2003, "abstract": "This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation. The MAP framework is general enough to include some previous model adaptation approaches, such as corpus mixing in Gildea (2001), for example. Other approaches falling within this framework are more effective. In contrast to the results in Gildea (2001), we show F-measure parsing accuracy gains of as much as 2.5% for high accuracy lexicalized parsing through the use of out-of-domain treebanks, with the largest gains when the amount of indomain data is small. MAP adaptation can also be based on either supervised or unsupervised adaptation data. Even when no in-domain treebank is available, unsupervised techniques provide a substantial accuracy gain over unadapted grammars, as much as nearly 5% F-measure improvement.", "references": ["3eb3a37a72087c96937a0e21f736c6e661a1d6de", "844db702be4bc149b06b822b47247e15f5894cc3", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "6c9f553e723a40a6713453b734b552c1928bf52b", "80b9a42a1bbb24dd57b60b9c30d8ebdcc5845b27", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "8d827b3a07b50903a7c4e07b7404d7168dd72d61", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "2c97115be3d650276b48e3c738eb415c5db3ad4b", "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3"], "page_rank": 0.0002463054187192118}, {"id": "3fc5f96b85654154a4d78944e9a2ee26e5aba789", "title": "Multiresolution image classification by hierarchical modeling with two-dimensional hidden Markov models", "authors": ["Jia Li", "Robert M. Gray", "Richard A. Olshen"], "date": 2000, "abstract": "This paper treats a multiresolution hidden Markov model for classifying images. Each image is represented by feature vectors at several resolutions, which are statistically dependent as modeled by the underlying state process, a multiscale Markov mesh. Unknowns in the model are estimated by maximum likelihood, in particular by employing the expectation-maximization algorithm. An image is classified by finding the optimal set of states with maximum a posteriori probability. States are then mapped into classes. The multiresolution model enables multiscale information about context to be incorporated into classification. Suboptimal algorithms based on the model provide progressive classification that is much faster than the algorithm based on single-resolution hidden Markov models.", "references": ["a7b354156832680a2a87fe9f1347e22219f5b894", "bb69b58fb402ba3690b7a6411867f6195a1ca843", "09cdb8512d64bec57dd79d1b22326085d3d5f4c6", "456998abb44891680b0e4bf47326a13692cec80b", "1dc2443a955a6d3ef06c10d1071998c267e3a229", "cbfb4404756d7caffe011c6e9ea2f5ec63ef3811", "93e573990a777deff02399406e953a268beecf28", "aed0004e4d52d48ff39681fd0bb6b5483608b00a", "1e6ea43de5f6ee6b9457d6b875d2abb33f318013", "4f7e47e5875f9c2b322bdef47b6e66013c08994f"], "page_rank": 0.0004926108374384236}, {"id": "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "authors": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "date": 2015, "abstract": "We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.", "references": ["4cfad7889dc12825309325cd4b4f3febed424e36", "228920ddc0d376c376ae534ceed589005f51867a", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e", "46be284f1e1ece64465af6fe3a69ce544e0c7e33", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "87f40e6f3022adbc1f1905e3e506abad05a9964f"], "page_rank": 0.0002463054187192118}, {"id": "8014b26403cc1085bc86ce5a6a3d7ff54eb49bc2", "title": "Updating an NLP system to fit new domains: an empirical study on the sentence segmentation problem", "authors": ["Tong Zhang", "Fred J. Damerau", "David Johnson"], "date": 2003, "abstract": "Statistical machine learning algorithms have been successfully applied to many natural language processing (NLP) problems. Compared to manually constructed systems, statistical NLP systems are often easier to develop and maintain since only annotated training text is required. From annotated data, the underlying statistical algorithm can build a model so that annotations for future data can be predicted. However, the performance of a statistical system can also depend heavily on the characteristics of the training data. If we apply such a system to text with characteristics different from that of the training data, then performance degradation will occur. In this paper, we examine this issue empirically using the sentence boundary detection problem. We propose and compare several methods that can be used to update a statistical NLP system when moving to a different domain.", "references": ["08e39912a54fc46f25f9e79bfa06ee44311b051a", "6edceaf0fada3588ee5f036e944c1a00661df77a", "68d1ef141ab947ab2098a2d672bb2789eafa4dd4", "4614650c3bb3e835c80612d3bca9586f81db95a3", "48649e3cf38d711cbaea177519becdd696c12b4c", "ec2f036f1e6f56ad6400e42cf1eb14f4ebe122c6", "9755f9993553131e5cc796d34ecaa624fe0ddffa", "3ee3f666a1d6f0ddfe48acfac16e1c0a1a52bee8", "a1dace286582d91916fe470d08f30381cf453f20", "2c5eec22c4dabb124b2bbf85966d88dd38e62193"], "page_rank": 0.0002463054187192118}, {"id": "b07ce649d6f6eb636872527104b0209d3edc8188", "title": "Pattern classification and scene analysis", "authors": ["Richard O. Duda", "Peter E. Hart"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Pattern classification and scene analysis\" by Richard O. Duda et al.", "references": [], "page_rank": 0.00032840722495894905}, {"id": "ae287b7ecbb3dffaa0e302acfd7bbf44f733acfc", "title": "Coreference for NLP Applications", "authors": ["Thomas S. Morton"], "date": 2000, "abstract": "This paper presents several techniques for performing automatic coreference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1cf1527807ebb16020b04d4166e7ba8d27652302", "title": "Unsupervised Learning of Models for Recognition", "authors": ["Markus Weber", "Max Welling", "Pietro Perona"], "date": 2000, "abstract": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.", "references": ["25c3065da19b967caf4aa8a8e1ac826ae85448b6", "102cf35af78c14019edfe28c8cd624da7d5b3fac", "de6f7d242b78bf5e87966fb72565ba43d277084e", "4fb52984078d75ec5655962dc94dc7848182286b", "61b2d0383b186c4d634c5f51421cab67c16d90a1", "37d46a2355a95abee6576986a460399adbebf6b9", "3aed15f35f59c65dd0857b29edaf141ebf77126b", "846b99d151892e70046e5ca541430be4ff67209f", "191c3c15cc4c957ee3437fc27ba3178bae292e7f", "49a84c0ac2865f7fb78393a31e51570846b9fa11"], "page_rank": 0.0002463054187192118}, {"id": "6d8018bd8b288baca0c55522877efd1b49258747", "title": "Environmental Determinants of Lexical Processing Effort", "authors": ["Scott A. McDonald"], "date": 2000, "abstract": "A central concern of psycholinguistic research is explaining the relative ease or difficulty involved in processing words. In this thesis, we explore the connection between lexical processing effort and measurable properties of the linguistic environment. Distributional information (information about a word\u2019s contexts of use) is easily extracted from large language corpora in the form of co-occurrence statistics. We claim that such simple distributional statistics can form the basis of a parsimonious model of lexical processing effort. Adopting the purposive style of explanation advocated by the recent rational analysis approach to understanding cognition, we propose that the primary function of the human language processor is to recover meaning from an utterance. We assume that for this task to be efficient, a useful processing strategy is to use prior knowledge in order to build expectations about the meaning of upcoming words. Processing effort can then be seen as reflecting the difference between \u2018expected\u2019 meaning and \u2018actual\u2019 meaning. Applying the tools of information theory to lexical representations constructed from simple distributional statistics, we show how this quantity can be estimated as the amount of information conveyed by a word about its contexts of use. The hypothesis that properties of the linguistic environment are relevant to lexical processing effort is evaluated against a wide range of empirical data, including both new experimental studies and computational reanalyses of published behavioural data. Phenomena accounted for using the current approach include: both singleword and multiple-word lexical priming, isolated word recognition, the effect of contextual constraint on eye movements during reading, sentence and \u2018feature\u2019 priming, and picture naming performance by Alzheimer\u2019s patients. Besides explaining a broad range of empirical findings, our model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature, such as the word frequency effect and the need for distinct mechanisms to explain semantic and associative priming. We conclude by emphasising the important role of distributional information in explanations of lexical processing effort, and suggest that environmental factors in general should given a more prominent place in theories of human language processing.", "references": ["857e9b3037e4ec7780bc5d2773074a25ff1ddadd", "f26ab52b2b191a6ada08645c95a1282378c02455", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "6c9d2ffc90d1ae5b0593e922f2035f1f767ad5ba", "bfac528c546d46d5047ac19da1d8563aebd738b9", "70a2f872625ce84e491cbc99683e9647ffd897de", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "5bbc3d8ca0f62ce85e4b1fbb6de7a8624ebaeda7", "a8a3ff991718aaeceb03948e1a694070675763c1", "9e2caa39ac534744a180972a30a320ad0ae41ea3"], "page_rank": 0.0002463054187192118}, {"id": "99eadd5e29a85f30cafef7f2c915f384715e3b89", "title": "Mathematical structures of language", "authors": ["Zellig S. Harris"], "date": 1968, "abstract": "We may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full.", "references": ["069ca4d3ce5fdcdad511ec6cb1a6bb5fccf1f298", "aae199b92e0863fd6c78351fdad2f0f0b2dd7ff8", "14b59ec3d034de8aa32297713ffbec3f957001c8", "95bae20554a0cb6f27930f83c748df598cac8dbd", "ab7790485f26ce65f9d83dd700c43e49058bdd2b", "c45877ff654f6dc3a2e3b9381a7c50669195e592", "00f3bc14bc3662558b677549ee6cc46ca66e5227", "414aad0223595e32cad0758f23cdf98a2520fb14", "99090aa80f1043b2606e70b83dc5f367708de6ba", "b6a7d807136b42376dce54db3c66d18d8db9e7cf"], "page_rank": 0.0001231527093596059}, {"id": "71c0698edd0cf489cd837c91ad22bbf51643bf6c", "title": "A Statistical Approach to Anaphora Resolution", "authors": ["Niyu Ge", "John Hale", "Eugene Charniak"], "date": 1998, "abstract": "This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm. We incorporate multiple anaphora resolution factors into a statistical framework specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition. We combine them into a single probability that enables us to identify the referent. Our first experiment shows the relative contribution of each source Of information and demonstrates a success rate of 82.9% for all sources combined. The second experiment investigates a method for unsupervised learning of gender/number/animaticity information. We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy. 1 I n t r o d u c t i o n We present a statistical method for determining pronoun anaphora. This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text (Marcus et al., 1993) that has been marked with co-reference information. The first sections of this paper describe this program: the probabilistic model behind it, its implementation, and its performance. The second half of the paper describes a method for using (portions of) t~e aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program. In particular, the scheme infers the gender of a referent from the gender of the pronouns that 161 refer to it and selects referents using the pronoun anaphora program. We present some typical results as well as the more rigorous results of a blind evaluation of its output. 2 A P r o b a b i l i s t i c M o d e l There are many factors, both syntactic and semantic, upon which a pronoun resolution system relies. (Mitkov (1997) does a detailed study on factors in anaphora resolution.) We first discuss the training features we use and then derive the probability equations from them. The first piece of useful information we consider is the distance between the pronoun and the candidate antecedent. Obviously the greater the distance the lower the probability. Secondly, we look at the syntactic situation in which the pronoun finds itself. The most well studied constraints are those involving reflexive pronouns. One classical approach to resolving pronouns in text that takes some syntactic factors into consideration is that of Hobbs (1976). This algorithm searches the parse tree in a leftto-right, breadth-first fashion that obeys the major reflexive pronoun constraints while giving a preference to antecedents that are closer to the pronoun. In resolving inter-sentential pronouns, the algorithm searches the previous sentence, again in left-to-right, breadth-first order. This implements the observed preference for subject position antecedents. Next, the actual words in a proposed nounphrase antecedent give us information regarding the gender, number, and animaticity of the proposed referent. For example: M a r i e Giraud carries historical significance as one of the last women to be ezecuted in France. S h e became an abortionist because it enabled her to buy jam, cocoa and other war-rationed goodies. Here it is helpful to recognize that \"Marie\" is probably female and thus is unlikely to be referred to by \"he\" or \"it\". Given the words in the proposed antecedent we want to find the probability that it is the referent of the pronoun in question. We collect these probabilities on the training data, which are marked with reference links. The words in the antecedent sometimes also let us test for number agreement. Generally, a singular pronoun cannot refer to a plural noun phrase, so that in resolving such a pronoun any plural candidates should be ruled out. However a singular noun phrase can be the referent of a plural pronoun, as illustrated by the following example: \"I think if I tell Viacom I need more time, they will take 'Cosby' across the street,\" says the general manager ol a network a~liate. It is also useful to note the interaction between the head constituent of the pronoun p and the antecedent. For example: A Japanese company might make television picture tubes in Japan, assemble the T V sets in Malaysia and extort them to Indonesia. Here we would compare the degree to which each possible candidate antecedent (A Japanese company, television picture tubes, Japan, T V sets, and Malaysia in this example) could serve as the direct object of \"export\". These probabilities give us a way to implement selectional restriction. A canonical example of selectional restriction is that of the verb \"eat\", which selects food as its direct object. In the case of \"export\" the restriction is not as clearcut. Nevertheless it can still give us guidance on which candidates are more probable than others. The last factor we consider is referents' mention count. Noun phrases that are mentioned repeatedly are preferred. The training corpus is marked with the number of times a referent has been mentioned up to that point in the story. Here we are concerned with the probability that a proposed antecedent is correct given that it has been repeated a certain number of times. 162 In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun. The idea is similar to tha t used in the centering approach (Brennan et al., 1987) where a continued topic is the highest-ranked candidate for pronominalization. Given the above possible sources of informar tion, we arrive at the following equation, where F(p) denotes a function from pronouns to their antecedents: F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~') where A(p) is a random variable denoting the referent of the pronoun p and a is a proposed antecedent. In the conditioning events, h is the head constituent above p, l~ r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent (always a noun-phrase in this s tudy), I is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and M\" is the number of times the referent is mentioned. Note that 17r \", d'~ and A~ are vector quantities in which each entry corresponds to a possible antecedent. When viewed in this way, a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent. This equation is decomposed into pieces that correspond to all the above factors but are more statistically manageable. The decomposition makes use of Bayes' theorem and is based on certain independence assumptions discussed below. P( A(p) = alp, h, fir, t, l, sp, d~ .Q') = P(alA~)P(p,h, fir, t,l, sp,~a, 2~) (1) P(p, h, fir, t, t, sp, diM ) o\u00a2 PCalM)P(p, h, fir, t, l, sp, ~a, .Q') (2) = P(a[:Q)P(.%, ~a, :~'I) P(p,h, fir, t, l la ,~ ,sp , i) (3) = P(all~)P(sp, d~a,.Q ) PCh, t, Zla, ~'0\", so, i) PC.. ~ la , .~', so, d, h, t, l) (4) oc P(a]l~)P(So,~a,M') P(p, 14tin, ]Q, s o, d, h, t, I) (5) = P(al.Q)P(sp, d~a, 3~r) P(ffrla, I~, s o, d, h, t, I). (6) P(pla. l~, sf,, d. h, t, l, l~) cx P(a163P(dtt la)P(f f ' lh , t, I, a) P(plw\u00b0) (7) Equation (1) is simply an application of Bayes' rule. The denominator is eliminated in the usual fashion, resulting in equation (2). Selectively applying the chain rule results in equations (3) and (4). In equation (4), the term P(h. t, lla, .~, So, d) is the same for every antecedent and is thus removed. Equat ion (6) follows when we break the last component of (5) into two probability distributions. In equation (7) we make the following independence assumptions: \u2022 Given a particular choice of the antecedent candidates, the distance is independent of distances of candidates other than the antecedent (and the distance to non-referents can be ignored): P(so, d~a, 2~) o\u00a2 P(so, dola , IC4) \u2022 The syntnctic s t ructure st, and the distance from the pronoun da are independent of the number of times the referent is mentioned. Thus P(sp, dola, M) = P(sp, d.la) Then we combine sp and de into one variable dIt, Hobbs distance, since the Hobbs algorithm takes both the syntax and distance into account. The words in the antecedent depend only on the parent consti tuent h, the type of the words t, and the type of the parent I. Hence e(ff'la, M, sp, ~, h, t, l) = P ( ~ l h , t, l, a) \u2022 The choice pronoun depends only on the words in the antecedent, i.e. P{pla, M, sp, d, h, t, l, ~ = P(pla, W) 163 \u2022 If we treat a as an index into the vector 1~, then (a, I.V') is simply the a th candidate in the list ffz. We assume the selection of the pronoun is independent of the candidates other than the antecedent. Hence P(pla, W) = P(plw,~) Since I~\" is a vector, we need to normalize P(ff ' lh, t,l, a) to obtain the probability of each element in the vector. It is reasonable to assume tha t the antecedents in W are independent of each other; in other words, P(wo+llwo, h , t , l ,a ) = P(wo+llh, t , l ,a}. Thus,", "references": ["f977f5e25adef5f0569cb9eb9b930e5146be2571", "d6bbd70be1453eb9968bdca1d9f6da81a23071bc", "6f7ac41dc9321cb7478919de1da6da70009023da", "c1e0017518042b6f7efd6f62b6b40e115c2a961c", "834d400f167fcf6ba27642eafa42e63f9e699a94", "2a5e619f2c5f4220438b1357e596db5b1578398d", "b62b80384f402d38c3425db6a99899d1cb9c50c6", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "025464b73f805e76689a7a20a48a9e9c0f4ff3ef"], "page_rank": 0.0001231527093596059}, {"id": "461efc87636ff4e48323ffcb9f8fdf79cf736fb0", "title": "A survey on visual surveillance of object motion and behaviors", "authors": ["Weiming Hu", "Tieniu Tan", "Liang Wang", "Stephen J. Maybank"], "date": 2004, "abstract": "Visual surveillance in dynamic scenes, especially for humans and vehicles, is currently one of the most active research topics in computer vision. It has a wide spectrum of promising applications, including access control in special areas, human identification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactive surveillance using multiple cameras, etc. In general, the processing framework of visual surveillance in dynamic scenes includes the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, human identification, and fusion of data from multiple cameras. We review recent developments and general strategies of all these stages. Finally, we analyze possible research directions, e.g., occlusion handling, a combination of twoand three-dimensional tracking, a combination of motion analysis and biometrics, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, behavior understanding and natural language description, fusion of information from multiple sensors, and remote surveillance.", "references": ["74298daec5d7bdf797e7fc2a62e3bc2a000c7910", "8b588f0b9c8cc866460e979d1970551ad64e6625", "6b95376da376203ac19eca43df9f96186bc4cc2c", "5d3e56c19608a32751203ef5367ade3e6e7159d5", "7a1da0d5b962a77a76008cb2f283c0606bd962f8", "c843e252588e122547fa0283681b99cd95509e75", "33c37e71351b1ffaf032478ea2765f9f963708a4", "7ef197525d18e2fbd1ef5a313776d4a7d65803ab", "9ab2f58e7e27817f6480e59f8aebbc096e7b915a", "fb9abd2681ae1883a6567aafc974bd67759fe373"], "page_rank": 0.0004926108374384236}, {"id": "233e47fb2f97ec8f587c62c3f84c7971fae8e568", "title": "Stochastic Approximations and Efficient Learning", "authors": ["L{\\'e}on Bottou", "Noboru Murata"], "date": 2002, "abstract": "A brush is rigidly attached to one end of a housing having a handle at the other end and, between the ends, a cylindrical opening supporting a rotor whose center of mass is displaced laterally from the axis of rotation. As the rotor turns, it exerts an unbalanced centrifugal force on the housing, causing it to oscillate. The rotor is preferably a single metal roller gyrating or rotating about the axis of the cylindrical opening, and it may be rotated by a tangential stream of fluid, e.g., water at utility district pressure. The rotor body may be solid or a hollow member filled with a heavy liquid such as mercury, and it is so supported that during rotation its outer peripheral surface approaches but does not touch the borewall of the opening in which it operates.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "7077a8802ac5f65991bf1086b07d0d6f17e9d894", "title": "Mixtures of Controllers for Jump Linear and Non-Linear Plants", "authors": ["Timothy W. Cacciatore", "Steven J. Nowlan"], "date": 1993, "abstract": "We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior.", "references": ["f6d8a7fc2e2d53923832f9404376512068ca2a57", "6faae01dfab9bdefd4854cc4e058391e8522c8c9", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "8c3b56709b8bbf173849f03ba69bc638067a902b", "231bbf7c7a39ad7200b6e68c9a5fc64a7f06593f", "b1f5f5b6903fdf2472561c4a16f9734d53c78d9e"], "page_rank": 0.00016420361247947453}, {"id": "611dac316bf03112c778cf7365d08e4a9d171876", "title": "Learning Probabilistic Relational Models", "authors": ["Lise Getoor"], "date": 1999, "abstract": "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \u201cflat\u201d data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning \u2014 the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", "references": ["21003bf7f5fd09bc4fc374b7bd2d27a9fcdfd96d", "2086fe71a443512cd7efe50828fef81be59fe719", "da94000d2def6235ab60b88a6da31876bc6c5fc3", "8446830f3c05b97c4d12a0751c022d1ae6a5115b", "5fb874a1c8106a5b2b2779ee8e1433149109ba00", "b97ec7b4f8b3cd921bd44b962be00dbb199499be", "bfd1c958b525fdeee6c7efaa3becc42072c75a72", "8a59ce04b6a0d923cd60dc1c16b6b7d99a60bffb", "8089729d8711b3ef0de37eb6016ca3a311b491b5", "e9ef7893ae7ee6826a43b5f58365092194c0e213"], "page_rank": 0.0001231527093596059}, {"id": "7257eacd80458e70c74494eb1b6759b52ff21399", "title": "Using fast weights to deblur old memories", "authors": ["Geoffrey E. Hinton"], "date": 1987, "abstract": "Connectionist models usually have a single weight on each connection. Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associations are \"blurred\" by subsequent learning, all the original associations can be \"deblurred\" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning.", "references": ["052b1d8ce63b07fec3de9dbb583772d860b7c769", "b0cb3be87b7f4f50d62d8dbba5a2e8d78c7d90a9", "9928cac725ebe6db7b974bbd65738d33dc95332d", "8592e46a5435d18bba70557846f47290b34c1aa5", "ae9a8c850c8e7ac8397d1a60b52d052d6c73be5d", "a70bbaad23fab36ee02604130114c278d87e55fb", "1d453386011ef21285fa81fb4f87fdf811c6ad7a", "ebfc4748436f1eed9947f7c33d2dba496bdf2d74"], "page_rank": 0.0006568144499178981}, {"id": "9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5", "title": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition", "authors": ["Kunihiko Fukushima", "Sei Miyake"], "date": 1982, "abstract": "A neural network model, called a \u201cneocognitron\u201d, is proposed for a mechanism of visual pattern recognition. It is demonstrated by computer simulation that the neocognitron has characteristics similar to those of visual systems of vertebrates.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a57c6d627ffc667ae3547073876c35d6420accff", "title": "Connectionist Learning Procedures", "authors": ["Geoffrey E. Hinton"], "date": 1989, "abstract": "A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.", "references": ["aa109a5c8440332a05ac538d98c4f93d25500c81", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "97f7d20e1e82347d78cef335218692207b29d23f", "3e6bea2649298c68d17b9421fc7dd19eeacc935e", "9fccf16e5205eaa44aa084b785372df1a0b44255", "ca7425bdbfea6b1a2f9492452eaafe3972745712", "8a7acaf6469c06ae5876d92f013184db5897bb13", "7257eacd80458e70c74494eb1b6759b52ff21399", "ab4aec5e0714b352e6c90d063fe830cbc70912bc", "865787016949fefd4f0a31862a76db18077f2cf3"], "page_rank": 0.0002463054187192118}, {"id": "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "title": "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition", "authors": ["Stephen E. Levinson", "Lawrence R. Rabiner", "Man Mohan Sondhi"], "date": 1983, "abstract": "In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call \u201cleft-to-right\u201d models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper.", "references": ["be2389f98a05b7e98d78e439d8cac51c34e9e97d", "de7ec8b4086ea7b166906fc35f3b02d372a3433d", "2069762a92a0efee1f09206ee4f82b7d28a40856", "2543dea11cbfce22c38fc2f57bd1dc3c502794b1", "69fc8c03d21e22e30d6642824c37158b314f36c3", "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "c180f387357d9302a558bcd643209831744c639b", "1fd7e526e4a7ccf50cfad47e94425f144aec818b", "3f6db1b368ebaeace796f78d1ee02807051487d4", "5809b6c737863bb6a82588e8760637e25c27bc10"], "page_rank": 0.0011494252873563218}, {"id": "d7d6a05eb6276cfbb3106acba3c203f7223841c7", "title": "Making fine-grained and coarse-grained sense distinctions , both manually and automatically", "authors": ["L MARTHAPA", "G HOATRANGDAN"], "date": 2005, "abstract": "In this paper we discuss a persistent problem arising from polysemy: namely the difficulty of finding consistent criteria for making fine-grained sense distinctions, either manually or automatically. We investigate sources of human annotator disagreements stemming from the tagging for the English Verb Lexical Sample Task in the Senseval-2 exercise in automatic Word Sense Disambiguation. We also examine errors made by a high-performing maximum entropy Word Sense Disambiguation system we developed. Both sets of errors are at least partially reconciled by a more coarse-grained view of the senses, and we present the groupings we use for quantitative coarse-grained evaluation as well as the process by which they were created. We compare the system\u2019s performance with our human annotator performance in light of both fine-grained and coarse-grained sense distinctions and show that well-defined sense groups can be of value in improving word sense disambiguation by both humans and machines.", "references": ["557005002436d57f4dda46052425f83b6b028b3a", "aded6e5edfd25b1f981fa8451595164d4f381d62", "3a2c3cf32c8c214cccebc45ebc2a2a1dd7d1e658", "63799c4850df5a60cbe8763cff9b1ffe5e27ae3c", "6557bd2f2b6a2b95a381bd073561c75786406118", "1138cd2527252cc0a2e30d5f61125b0da81e716f", "a877ccea302725880a0eee16d0f9f27e045c7b5d", "e5c4aa82e5a35a8ecafb242fad400237dca0f686", "ec62927e369f364cc20bd80ae6e70a36a955d3aa", "b216a312c7b421748123a43eba9b45eb6418d7bb"], "page_rank": 0.0004926108374384236}, {"id": "2d60175fa4c4d2f9339d95f7ae2c5ce30c11575d", "title": "Findings of the 2011 Workshop on Statistical Machine Translation", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Omar Zaidan"], "date": 2011, "abstract": "This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.", "references": ["319ac2dd69f75ef281fe4652dad97a32a0b8f4ac", "279fee6f6334d09e90341f676c3138c86dce494b", "da6918ed87095d1313bd20606a934f899d4084b0", "c5276e7853624bfd1aedcb22fea20a5371775673", "be9bca1e9b0192fc49b316f2701242b50d98d456", "06f710f31a3c586f9368f0abdeb9a216f7809ac4", "3740b4c8697f23662be3455a04b8cd1064b5f2d9", "c0c9f18324226a29cccb8b8b85a35e39da922f08", "20c11546a035d2fa2fa1121a7b31e890d20d6b6b", "a018819150a741cbbbdc5ce5abfaa099903f8fe7"], "page_rank": 0.0002463054187192118}, {"id": "603ccac1c7836ef05f0b92a1f986d44becfa5a34", "title": "Quantitative analysis of probabilistic pushdown automata: expectations and variances", "authors": ["Javier Esparza", "Anton{\\'i}n Kucera", "Richard Mayr"], "date": 2005, "abstract": "Probabilistic pushdown automata (pPDA) have been identified as a natural model for probabilistic programs with recursive procedure calls. Previous works considered the decidability and complexity of the model-checking problem for pPDA and various probabilistic temporal logics. In this paper we concentrate on computing the expected values and variances of various random variables defined over runs of a given probabilistic pushdown automaton. In particular, we show how to compute the expected accumulated reward and the expected gain for certain classes of reward functions. Using these results, we show how to analyze various quantitative properties of pPDA that are not expressible in conventional probabilistic temporal logics.", "references": ["94a9e890b4118cb228a0d6e8f89506ece97f90c9", "6074f08e44ba23aad31087f0ed418b68dd37c21d", "87f35751bcbe760f4bf486679358854a2bd80be9", "8ca0e8be0f48826f8eb3ce31a9681d3cf52d4952", "36d2270199fec474008378577d00ec3f8a4d2bf9", "14dd9a0db49526f86dccdfaabbc2f5e857d57408", "e23e2a750f7368424da6498f811a0dac270125a3", "7d9dbc41b320b1e624c1fd3ce3a54b2ffad03b16", "d80e4a98a65e03c957357fe0bdd4c4337173f506", "7f181d64f9a19f956ea43889e7918ebf0ead8b1c"], "page_rank": 0.0004926108374384236}, {"id": "c81dd2ef15abf0471f037639d822378165475dbf", "title": "Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation", "authors": ["Joseph P. Olive", "Caitlin Christianson", "John P. McCary"], "date": 2011, "abstract": "This comprehensive handbook, written by leading experts in the field, details the groundbreaking research conducted under the breakthrough GALE program--The Global Autonomous Language Exploitation within the Defense Advanced Research Projects Agency (DARPA), while placing it in the context of previous research in the fields of natural language and signal processing, artificial intelligence and machine translation.The most fundamental contrast between GALE and its predecessor programs was its holistic integration of previously separate or sequential processes. In earlier language research programs, each of the individual processes was performed separately and sequentially: speech recognition, language recognition, transcription, translation, and content summarization. The GALE program employed a distinctly new approach by executing these processes simultaneously. Speech and language recognition algorithms now aid translation and transcription processes and vice versa. This combination of previously distinct processes has produced significant research and performance breakthroughs and has fundamentally changed the natural language processing and machine translation fields.This comprehensive handbook provides an exhaustive exploration into these latest technologies in natural language, speech and signal processing, and machine translation, providing researchers, practitioners and students with an authoritative reference on the topic.", "references": ["206e23d7081d8fbe813e496058d9b8bdb91deeeb", "85f1038d327afee1deba38cfa85a05afad72fc21", "b676dfa0111a213132c5c317f458b2136bd96bd3", "cea1f8612719a851183f13e348a2ddac34cbd508", "8b77e3ddb536146e2a8ca6a978c0aaac0469c736", "bc91891db3cb06c74116ba08fe1e542bbe61b6aa", "3e1d03bc7fb15381573b02b531b9a7e3ce90e88b", "9a77278b192431a4be9b1b667a40e376ac5bd3df", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "9a09452d026f80bfb93b32a9cfe0c091b80c9956"], "page_rank": 0.0002463054187192118}, {"id": "348a4a34f934692c58751b0284c5cfeb8f0c96f2", "title": "Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models", "authors": ["James Henderson", "Ivan Titov"], "date": 2005, "abstract": "Previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees, which are hand-crafted based on domain knowledge and computational considerations. In this paper we propose a method for defining kernels in terms of a probabilistic model of parsing. This model is then trained, so that the parameters of the probabilistic model reflect the generalizations in the training data. The method we propose then uses these trained parameters to define a kernel for reranking parse trees. In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model.", "references": ["844db702be4bc149b06b822b47247e15f5894cc3", "fe638b5610475d4524684fb2c2b7b08c119c8700", "a46152d8ad27ae47086334c33c8376185b40340d", "1e19a94d547ee023837c14c361139185e2353fc0", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "84cccc9e14e49a6c56147e4cd36bda2ffd70b683", "1174297ddcf08937a94d8efe4c1efb65f3b92fd8", "2ae8397c07bd3c76f84c7cdac7897e8b7dec9029", "707bd4840863f2cc8c3984c1eacb546914a31644"], "page_rank": 0.00016420361247947453}, {"id": "e88fdc9c861c2e6f618d31cd4825328fc1e1718c", "title": "Alternative approaches for Generating Bodies of Grammar Rules", "authors": ["Gabriel G. Infante L{\\'o}pez", "Maarten de Rijke"], "date": 2004, "abstract": "We compare two approaches for describing and generating bodies of rules used for natural language parsing. In today's parsers rule bodies do not exist a priori but are generated on the fly, usually with methods based on n-grams, which are one particular way of inducing probabilistic regular languages. We compare two approaches for inducing such languages. One is based on n-grams, the other on minimization of the Kullback-Leibler divergence. The inferred regular languages are used for generating bodies of rules inside a parsing procedure. We compare the two approaches along two dimensions: the quality of the probabilistic regular language they produce, and the performance of the parser they were used to build. The second approach outperforms the first one along both dimensions.", "references": ["3fc44ff7f37ec5585310666c183c65e0a0bb2446", "adfef97814b292a09520d8c78a141e7a4baf8726", "33be02735525a3eb6111ee790ea2e15775019d21", "844db702be4bc149b06b822b47247e15f5894cc3", "2a5e619f2c5f4220438b1357e596db5b1578398d", "3764baa7465201f054083d02b58fa75f883c4461", "aaaee4a7f71f030536d67aa801dd07f2532838ee", "0ffa423a5283396c88ff3d4033d541796bd039cc", "b876ddde80881cbad2397fede9e7499c32260104", "c356cdc3f3293938a82662c727439be82b97cbc8"], "page_rank": 0.00016420361247947453}, {"id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "title": "A Maximum-Entropy-Inspired Parser", "authors": ["Eugene Charniak"], "date": 2000, "abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", "references": ["f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "a4c0e02d99de82149efd719260e5a5549a13854a", "aad8c5ae265e8f645101245afb9d9c9cdf40b4ca", "bd9a50da8dc4dc63d80e698dacccf18ca2ae2e56", "6c9f553e723a40a6713453b734b552c1928bf52b", "2a5e619f2c5f4220438b1357e596db5b1578398d", "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "0ffa423a5283396c88ff3d4033d541796bd039cc", "29fdbbd3bb0b3c798a57e10576d318281d37dd2a"], "page_rank": 0.0008210180623973726}, {"id": "7d37dff2d8e65764e7293750051d519359d8835d", "title": "Similarity-Based Models of Word Cooccurrence Probabilities", "authors": ["Ido Dagan", "Lillian Lee", "Fernando C Pereira"], "date": 1999, "abstract": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \u201ceat a peach\u201d and \u201deat a beach\u201d is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words.We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.", "references": ["3cb09327e68400bf05e6f373e046a3a08e82510e", "e3aaab199ee6baaa15f09c1f01ccf3d4b56212ed", "1d922631a6bf8361d7602e12cafb9e15d421c827", "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0", "297e478f92cef1cd090706fc59fde5ea0836ce80", "36c4c51917b1f53ee85c459f2597e115df53eb05", "2eae0f08186952643c3a7ead2eba2d41fda58cec", "3de5d40b60742e3dfa86b19e7f660962298492af", "fcd8599fa26d7742516cc88baf36a0ca5a96216a", "1cf6f1209d29c151b693861e083850f1b385c595"], "page_rank": 0.0004926108374384236}, {"id": "03b11aec8deafa2572ab5a8fac43bfc94db9ec37", "title": "An Introduction to Probability Theory and its Applications, Volume I", "authors": ["Boyd Harshbarger"], "date": 1958, "abstract": "Semantic Scholar extracted view of \"An Introduction to Probability Theory and its Applications, Volume I\" by Boyd Harshbarger", "references": [], "page_rank": 0.0004926108374384236}, {"id": "b9ed0b35c9eaba0328492de65c4cdc5545094df4", "title": "Improved clustering techniques for class-based statistical language modelling", "authors": ["Reinhard Kneser", "Hermann Ney"], "date": 1993, "abstract": "Semantic Scholar extracted view of \"Improved clustering techniques for class-based statistical language modelling\" by Reinhard Kneser et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "20c11546a035d2fa2fa1121a7b31e890d20d6b6b", "title": "(Meta-) Evaluation of Machine Translation", "authors": ["Chris Callison-Burch", "Cameron S. Fordyce", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "date": 2007, "abstract": "This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies.", "references": ["f493b8a1f05fd788b1bcaecf18eb87a3f5965b35", "d5967e0d6e9d3bcd1b623c8372dff04197cdb590", "d7da009f457917aa381619facfa5ffae9329a6e9", "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7", "009f60203ac86cda58a783a7003c575f06c475fd", "42fc4c6580bfa54d57b5d6c56b5dfde58c6f6abb", "9dfef4a603b1db53cac6917fb3a5e724633f76d9", "63c8326257a3fc0ba57d85cc1e559b9afdb73c4c", "34d7a07c493ca6336c92156806a2947e115caadc", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f"], "page_rank": 0.0006568144499178981}, {"id": "738ab73b8a20ce9c831e8ae6cf05c0944fb4054d", "title": "Probabilistic Head-Driven Parsing for Discourse Structure", "authors": ["Jason Baldridge", "Alex Lascarides"], "date": 2005, "abstract": "We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues. We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques. We show that dialogue-based features regarding turn-taking and domain specific goals have a large positive impact on performance. Our best model achieves an f-score of 43.2% for labelled discourse relations and 67.9% for unlabelled ones, significantly beating a right-branching baseline that uses the most frequent relations.", "references": ["a4f98df6113b0cae123ad892f24d1fb9452003e9", "b349b855f47a3000062e1b08ec48651b28ce10f4", "0b3858c0c31c6f0826c891a42367671f6e76d46c", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "87ae5843d13582a244d5f797f0a62c1df1c64559", "9f3cb28d91d8091c5f1723923471ac1b43713409", "30e36a103258d18c6c7ca9570b405bd738266a55", "22d45dadde6b5837eff11dc031045754bc5901c3", "1cf5c3f9adb4c254c90e7a2726059b17caccf464", "57d78d899b1977c8170a9d2ca98b07ad0e71a832"], "page_rank": 0.0002463054187192118}, {"id": "88a2432fcc758f4a0a469df1f0a26d836d2d0b09", "title": "On the Relation Between the Informational and Intentional Perspectives on Discourse", "authors": ["Jerry R. Hobbs"], "date": 1996, "abstract": "In the paper \u201cInterpretation as Abduction\u201d (hereafter IA) Hobbs et al. (1993) presented and elaborated the view that to interpret an utterance is to find the best explanation of why it would be true. We may call this the \u201cInformational Perspective\u201d on discourse interpretation. The only thing to be explained is the information explicitly conveyed by the utterance, and the explanation does not necessarily involve any knowledge of the specific goals of the speaker.", "references": ["fa5ac9da63704100d03256cdc3ef99ab9e63390e", "895dfdabbbd5155bbb559d3f144db6f7c195ce75", "08f92a1fe09457280bab9d34934e89c7f50c5e80", "57d78d899b1977c8170a9d2ca98b07ad0e71a832", "0b144f3ca00d98df1d8f9456d29c7fce3290924d", "39cf55b9500a38b5b32f29cc7a9b8457a804f1b9", "205a38b9a32f7907f07dd6778805a0b6ee7b109d", "9af92ec374b2aa31a44edcefa2abcbf60facf74d", "987767d4d3652be8e78ba8e6ac0658b7ab1fd432", "28d3c6bf2275a40a6dc11062e0aef9be0084c005"], "page_rank": 0.0001231527093596059}, {"id": "f5b1146b7ca79322aab124fd63825b9c175c02cf", "title": "Clause Restructuring for Statistical Machine Translation", "authors": ["Michael Collins", "Philipp Koehn", "Ivona Kucerova"], "date": 2005, "abstract": "We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.", "references": ["8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "4791cb189d2c07090051f39f2cb63bf4767faa3d", "da336ed15bd24c268cb2a09efe2aa4f298cda3ba", "9aca99a65c399a62092c4332257918c7011daea7", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "cfa9d0a994d730281b06bc8c66b637115a97d787", "1f12451245667a85d0ee225a80880fc93c71cc8b", "12cad1bf2c6beb9fe6fe8cadf31630c15884a506"], "page_rank": 0.0002463054187192118}, {"id": "0b3858c0c31c6f0826c891a42367671f6e76d46c", "title": "Sentence Level Discourse Parsing using Syntactic and Lexical Information", "authors": ["Radu Soricut", "Daniel Marcu"], "date": 2003, "abstract": "We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.", "references": ["c1474182c1073948817be976b93cb6d4a931e8ac", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "fcb7e6cf3b4c349b729fa0444bbb5ac1f99c3f3a", "844db702be4bc149b06b822b47247e15f5894cc3", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "99991d22a1e0dac7955255ee0d05476179372d4d", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1", "3b14be12cecba09db74a901bf25a33a67ac784a6", "87834ded1d151355d2028dc6efb26fcaa48ab9fc"], "page_rank": 0.0006157635467980295}, {"id": "1a5a60233da0feec4d6c3c22f9b3b8656d0dbd84", "title": "Overview of the TREC 2002 Question Answering Track", "authors": ["Ellen M. Voorhees"], "date": 2004, "abstract": "The TREC 2005 Question Answering track contained three tasks: the main question answering task, the document ranking task, and the relationship task. The main task was the same as the single TREC 2004 QA task. In the main task, question series were used to define a set of targets. Eac h series was about a single target and contained factoid and list questions. The final question in the series was an \u201cOt her\u201d question that asked for additional information about the target that was not covered by previous questions in the series. The document ranking task was to return a ranked list of documents for each question from a subset of the questions in the main task, where the documents were thought to contain an answer to the question. In the relationship tas k, systems were given TREC-like topic statements that ended with a question asking for evidence for a particular relationship. The goal of the TREC question answering (QA) track is to foster research on systems that return answers themselves, rather than documents containing answers, in response to a question. The track started in TREC-8 (1999), with the first several editions of the track focused on factoid questions. A factoid question is a fact-based, short answer question such as How many calories are there in a Big Mac? . The task in the TREC 2003 QA track was a combined task that contained list and definition questions in additio n to factoid questions [1]. A list question asks for differen t instances of a particular kind of information to be returned , such as List the names of chewing gums . Answering such questions requires a system to assemble an answer from information located in multiple documents. A definition question asks for interesting information about a particul ar person or thing such as Who is Vlad the Impaler?or What is a golden parachute?. Definition questions also require systems to locate inform ation in multiple documents, but in this case the information of interest is much less crisply de lineated. The TREC 2004 test set contained factoid and list questions grouped into different series, where each series had the target of a definition associated with it [2]. Each questi on in a series asked for some information about the target. In addition, the final question in each series was an explicit \u201cOther\u201d question, which was to be interpreted as \u201cTell me other interesting things about this target I don\u2019t know enou gh to ask directly\u201d. This last question is roughly equivalen t to the definition questions in the TREC 2003 task. Several concerns regarding the TREC 2005 QA track were raised during the TREC 2004 QA breakout session. Since the TREC 2004 task was rather different from previous years\u2019 tasks, there was the desire to repeat the task largely unchanged. There was also the desire to build infrastructure that would allow a closer examination of the role document retrieval techniques play in supporting QA technology. As a result of this discussion, the main task for the 2005 QA track was decided to be essentially the same as the 2004 task in that the test set would consist of a set of questio n series where each series asks for information regarding a particular target. As in TREC 2004, the targets included people, organizations, and other entities (things); unlike TREC 2004 the target could also be an event. Events were added since the document set from which the answers are to be drawn are newswire articles. The runs were evaluated using the same methodology as in TREC 2004, except that the primary measure was the per-series score instead of the combined component score. The document ranking task was added to the TREC 2005 track to address the concern regarding document retrieval and QA. The task was to submit, for a subset of 50 of the questions in the main task, a ranked list of up to 1000 documents for each question. Groups whose primary emphasis was document retrieval rather than QA, were allowed to participate in the document ranking task without submitting actual answers for the main task. However, all TREC 2005 submissions to the main task were required to include a ranked list of documents for each question in the document", "references": ["46be284f1e1ece64465af6fe3a69ce544e0c7e33", "d2161251488dbba08616a9cdd4223a0ac1190cef", "36f73f6dc29b6cb7438d4820699ca07908cfdc51", "9e5c8edc5e1a66a92b74dc87a976bfcd0824b611", "0ff65ac698013cdd9d61326cab49a1d75404e001", "f649b57fac0b384dc8405dfc643f40824d54e2a0"], "page_rank": 0.0001231527093596059}, {"id": "3e5652cca6464981a065184dceb2672bd13984b7", "title": "The Necessity of Parsing for Predicate Argument Recognition", "authors": ["Daniel Gildea", "Martha Palmer"], "date": 2002, "abstract": "Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification.", "references": ["2a5e619f2c5f4220438b1357e596db5b1578398d", "fc090a68e45e0e6337136777d21c87b76a90ae72", "9e85832b04cc3700c2c26d6ba93fdeae39cac04a", "547f23597f9ec8a93f66cedaa6fbfb73960426b1", "87d69e2a64a3059e810321872595042f89efaa3c", "e408cfd6b8433fea02e2fd13d0c75cead0dc8023", "5d2d6034c5afd4ab047fe4687d47559722142c90", "d1b63980781545e76e4b74eca493ca4b4fe07b9f", "e9292ba3230f01b9f6990362fdf06783b9347bf6", "bece46ed303f8eaef2affae2cba4e0aef51fe636"], "page_rank": 0.0001231527093596059}, {"id": "3ee3f666a1d6f0ddfe48acfac16e1c0a1a52bee8", "title": "Relational Learning for NLP using Linear Threshold Elements", "authors": ["Roni Khardon", "Dan Roth", "Leslie G. Valiant"], "date": 1999, "abstract": "We describe a coherent view of learning and reasoning with relational representations in the context of natural language processing. In particular, we discuss the Neuroidal Architecture, Inductive Logic Programming and the SNoW system explaining the relationships among these, and thereby offer an explanation of the theoretical basis for the SNoW system. We suggest that extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality.", "references": ["31db8efb7fc7ea3b1c8f9f1ce5c2c80f575d782d", "3ed17a1114e2dc48597ab17cc8d5234006f525c9", "547d483ed1e80066693af561f63daa30ffa8e9fa", "14a2e8174947556d4743cc11dae49751ea9867ab", "db83cb23a49d8d0df539835d4149752e408b3d94", "58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "a6c001bfbade469a9f9056b812706cf4914197fb", "b1d3c5653dd4717df19b99709d81b08be44f268a", "adf42271b2817f06459b56ba84790a7c2afe3227", "58095bae1d836943bdaa52b76fa8d17cf77d06b3"], "page_rank": 0.0001231527093596059}, {"id": "a1dace286582d91916fe470d08f30381cf453f20", "title": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm", "authors": ["Nick Littlestone"], "date": 1987, "abstract": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space.", "references": ["3948494b79dcda6d53237397123efdbb7a9954b2", "95f08ef99f0f0b16ce4a916f2983451ed722fcbc", "c5609ee7a8c7432c0f502b2a6dcfe9c0039206ab", "5fc0c7afc6bb27fb3752eae0ea5869413b1259b7", "6bee88f11407225011db66795bf0d74f9bedffad", "ebc3afef7c5455ab8bd0c06755ce926f8fe707fb", "e0b8fa3496283d4d808fba9ff62d5f024bcf23be", "0df1aac45ff562089a3bdbcb34e2481a71478651", "80feac7b0c5f1a0880cc8b0a980eb0583d68b711", "71448a9b8d35d5f3123405d3d7a66288618daab2"], "page_rank": 0.00022167487684729062}, {"id": "99090aa80f1043b2606e70b83dc5f367708de6ba", "title": "On categorial and phrase structure grammars", "authors": ["Yehoshua Bar-Hillel", "Haim Gaifman", "Eliahu Shamir"], "date": 1960, "abstract": "Semantic Scholar extracted view of \"On categorial and phrase structure grammars\" by Yehoshua Bar-Hillel et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "b6a7d807136b42376dce54db3c66d18d8db9e7cf", "title": "ENTSCHEIDUNGSPROBLEM REDUCED TO THE AEA CASE.", "authors": ["A. S. Kahr", "Edward F. Moore", "H. Wang"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"ENTSCHEIDUNGSPROBLEM REDUCED TO THE AEA CASE.\" by A. S. Kahr et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers", "authors": ["Xin Li", "Dan Roth"], "date": 2002, "abstract": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.", "references": ["dd53ce7b4e9e1f4f236ae741b95d0553314f0cc4", "f24e2d8df650e8c8b0bf61af8ce5aed83e476be4", "5afbc0e1cb4cfb6f22bb1b11de3498a610a99ec7", "3ed17a1114e2dc48597ab17cc8d5234006f525c9", "3fab92869cfab684b3ffb1c16a771e9c3b774acd", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "7d8b12abbaf0283f9f7612f48d1b46e981a0539d", "cb352f6df85ada5f90ab0301e3bdbf37c93b3190", "58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "46e765ac3b3d163fdf60040f8742d199c0508675"], "page_rank": 0.0002463054187192118}, {"id": "49a84c0ac2865f7fb78393a31e51570846b9fa11", "title": "Locating salient facial features using image invariants", "authors": ["Kevin N. Walker", "Timothy F. Cootes", "Christopher J. Taylor"], "date": 1998, "abstract": "We present a method, based on the idea of salient points, for locating corresponding features between two different faces independently of scale, orientation and position. Salient points are those which have a low probability of being mistaken with other points in the face, and therefore are more likely to be correctly located in a second face. The local image structure at each image point is described by vectors of Cartesian differential invariants computed at a range of scales. Salient points lie in low density regions of the distribution of all vectors of invariants found in an image. The vectors of invariants of salient points are used to locate similar features in a second image. Results are presented, showing that salient facial features can be relocated more reliably than arbitrary facial features.", "references": ["3aed15f35f59c65dd0857b29edaf141ebf77126b", "de6f7d242b78bf5e87966fb72565ba43d277084e", "bf474fd5b89b6b38bec83cd1e8d3b11166ba2a1a", "de2fa641d79397c3b383aea77776d8ebe49de08a", "05f900fb69a333e2573da5ad94176f6b90b88930", "429b7fd0760aa426961b2a7bedb951fb21829dcd", "6149234c26ee464faa32685d81bfb68d1d087960", "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6", "e8a21cf04cd7dc7a49510240515627a438cae7ae", "d6fb9fb0bf112abde285d14ac59a2607c21bf04f"], "page_rank": 0.0004926108374384236}, {"id": "fb9abd2681ae1883a6567aafc974bd67759fe373", "title": "Path detection in video surveillance", "authors": ["Dimitrios Makris", "Tim J. Ellis"], "date": 2002, "abstract": "Abstract This paper addresses the problem of automatically extracting frequently used pedestrian pathways from video sequences of natural outdoor scenes. Path models are learnt from the accumulation of trajectory data over long time periods, and can be used to augment the classification of subsequent track data. In particular, labelled paths provide an efficient means for compressing the trajectory data for logging purposes. In addition, the model can be used to compute a probabilistic prediction of the pedestrian's location many time steps ahead, and to aid the recognition of unusual behaviour identified as atypical object motion.", "references": ["f8352355d5ff967f86cea99326ad5a513ae65436", "8b2b7ab62edf4da8135095d8442c32c1b0ac8242", "c968aa8f8b031301316cc9379f8ca1f0c7e7b36c", "7c05eb1563da0a3f8fa31363f4d8ecae40b7e3ce", "d77dfbc29f104384877f8ca61c5e42b340932d37", "b9fba724701418bc220569218f6833c5aee49444", "59e4a7955897bde43aff59de6b9f37fbd2402b97", "e9670d548f40013648e1984bd36e06134f63ecdf", "54bfd42964f85355765a3de5ef6a464ca8aa00c3"], "page_rank": 0.00016420361247947453}, {"id": "2c5eec22c4dabb124b2bbf85966d88dd38e62193", "title": "Mean and variance adaptation within the MLLR framework", "authors": ["Mark John Francis Gales", "Philip C. Woodland"], "date": 1996, "abstract": "Abstract One of the key issues for adaptation algorithms is to modify a large number of parameters with only a small amount of adaptation data. Speaker adaptation techniques try to obtain near speaker-dependent (SD) performance with only small amounts of speaker-specific data, and are often based on initial speaker-independent (SI) recognition systems. Some of these speaker adaptation techniques may also be applied to the task of adaptation to a new acoustic environment. In this case an SI recognition system trained in, typically, a clean acoustic environment is adapted to operate in a new, noise-corrupted, acoustic environment. This paper examines the maximum likelihood linear regression (MLLR) adaptation technique. MLLR estimates linear transformations for groups of model parameters to maximize the likelihood of the adaptation data. Previously, MLLR has been applied to the mean parameters in mixture-Gaussian HMM systems. In this paper MLLR is extended to also update the Gaussian variances and re-estimation formulae are derived for these variance transforms. MLLR with variance compensation is evaluated on several large vocabulary recognition tasks. The use of mean and variance MLLR adaptation was found to give an additional 2% to 7% decrease in word error rate over mean-only MLLR adaptation.", "references": ["778015de7b81dfde54367dd57fb76c86faa72be4", "490915864dc44c40c393229a591783746ff1ae81", "ae5c4527f4e587b6d99a09195d5fba906b62a0e6", "e368a35ae6b1c4bd6d2b49ba3962d9146b824a84", "b1c057eed83f4ab82c0400303d7b9ca14d09aa91", "1c6d7d30d95a869e2a7dca975a2405b2d8795242", "44f02a80a58c55bc0ade176527a8df197ac58a3e", "583d605b8c632d130e3779af7205066e2ca78d00", "88009fb591e3477c2cacfdd71efef0091576a1e0", "7b3175be6b38c271558723fa1f95f0ca223939c6"], "page_rank": 0.0001231527093596059}, {"id": "9ab2f58e7e27817f6480e59f8aebbc096e7b915a", "title": "Multiple camera tracking of interacting and occluded human motion", "authors": ["Shiloh L. Dockstader", "A. Murat Tekalp"], "date": 2001, "abstract": "We propose a distributed, real-time computing platform for tracking multiple interacting persons in motion. To combat the negative effects of occlusion and articulated motion we use a multiview implementation, where each view is first independently processed on a dedicated processor. This monocular processing uses a predictor-corrector filter to weigh reprojections of three-dimensional (3-D) position estimates, obtained by the central processor, against observations of measurable image motion. The corrected state vectors from each view provide input observations to a Bayesian belief network, in the central processor, with a dynamic, multidimensional topology that varies as a function of scene content and feature confidence. The Bayesian net fuses independent observations from multiple cameras by iteratively resolving independency relationships and confidence levels within the graph, thereby producing the most likely vector of 3-D state estimates given the available data. To maintain temporal continuity, we follow the network with a layer of Kalman filtering that updates the 3-D state estimates. We demonstrate the efficacy of the proposed system using a multiview sequence of several people in motion. Our experiments suggest that, when compared with data fusion based on averaging, the proposed technique yields a noticeable improvement in tracking accuracy.", "references": ["7a1da0d5b962a77a76008cb2f283c0606bd962f8", "f33e0aa9940d5fd5db44c122093581cd044ca28b", "a27d84ff0f5bec9cb6a791bf916328cb30774ec0", "ee1d075126f1fcdf6a49ad9976e84ebee5f00caf", "a6fad775a27cc172479fdb6f291e361b35fd2f1e", "b9df7f4ef4e164a9b8bbdebadaf379b4db71db4e", "77c7dbbb9ee85a9676021a2ec5b997de7e481371", "4c2e1b22689c59086f938cb275de021127a92789", "eaa3bc2a3291855f91a85939dfdaeb75f9e0cffb", "6eb807ae65f393e2b1544619bd7faf7985897b07"], "page_rank": 0.00016420361247947453}, {"id": "ebfc4748436f1eed9947f7c33d2dba496bdf2d74", "title": "McClelland and the PDP Research Group", "authors": ["David E. Rumelhart"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"McClelland and the PDP Research Group\" by David E. Rumelhart", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a70bbaad23fab36ee02604130114c278d87e55fb", "title": "Modulatory actions of neurotransmitters on voltage-dependent membrane currents in bullfrog sympathetic neurones.", "authors": ["Takashi Akasu", "Kyozo Koketsu"], "date": 1981, "abstract": "The concept that neurotransmitters are able to modulate voltage-dependent membrane currents during initiation of action potentials has been proposed on the basis of experimental evidences that catecholamine and acetylcholine (ACh) modify the configuration of action potentials of bullfrog sympathetic ganglion cells (Koketsu 1974a, b, 1977; Koketsu and Minota, 1975; Kuba and Koketsu, 1975, 1976; Minota and Koketsu, 1977). Indeed, both adrenaline (Koketsu and Minota, 1975; Minota and Koketsu, 1977) and ACh (Kuba and Koketsu, 1975, 1976) are able to depress the after-hyperpolarization and the maximum rate of fall of action potentials of these ganglion cells in the Ringer solution. Furthermore, these neurotransmitters or neuromodulators in sympathetic ganglia (Kuba and Koketsu, 1978) are able to depress the peak amplitude as well as the maximum rate of rise of Ca2+ spike potential produced in the isotonic Ca2+ solution (Kuba and Koketsu, 1976; Minota and Koketsu, 1977). These results suggested that both adrenaline and ACh depressed the K+ conductance (gK) and presumably also the Ca+ conductance (gCa) during an initiation of action potentials of sympathetic ganglion cells in the physiological condition.", "references": ["46f80af5616911d7815f7205931d26a4cda487a2", "8552a94f99ae996a1b039ad289c5c05869c70387", "1ffea1afcfba67e5fb298ce6ffc9ea0dff02b9bf", "b0e9f69e74f045400b04e20f991547a449ecd91b", "799e0839474cf02b6d9b5366e2bb88f03b98d205", "4d786143a3b854b59b35f56273349d9c9384455e", "27a774612fc87a491d0f15b432750dcc867d05c3"], "page_rank": 0.00016420361247947453}, {"id": "1d453386011ef21285fa81fb4f87fdf811c6ad7a", "title": "Learning internal representations by back-propagating errors", "authors": ["David E. Rumelhart"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"Learning internal representations by back-propagating errors\" by David E. Rumelhart", "references": [], "page_rank": 0.00016420361247947453}, {"id": "e9ef7893ae7ee6826a43b5f58365092194c0e213", "title": "Probabilistic Horn Abduction and Bayesian Networks", "authors": ["David Poole"], "date": 1993, "abstract": "This paper presents a simple framework for Horn-clause abduction, with probabilities associated with hypotheses. The framework incorporates assumptions about the rule base and independence assumptions amongst hypotheses. It is shown how any probabilistic knowledge representable in a discrete Bayesian belief network can be represented in this framework. The main contribution is in finding a relationship between logical and probabilistic notions of evidential reasoning. This provides a useful representation language in its own right, providing a compromise between heuristic and epistemic adequacy. It also shows how Bayesian networks can be extended beyond a propositional language. This paper also shows how a language with only (unconditionally) independent hypotheses can represent any probabilistic knowledge, and argues that it is better to invent new hypotheses to explain dependence rather than having to worry about dependence in the language.", "references": ["e47aeca1b10e48a35943ed0257e6397d9e74fac0", "e5fa3ce2ff2484914224ffba31b8649e10db18e9", "9115822f4b4e2832be382e026301691c8cc61c8d", "a180017f7b17b7ef4f261b31139ed15ebaa0701f", "bdf379596eac144908c1ae83bf1f1753c1f1c7fe", "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93", "e55a07e0d51ba0b920883a231a47e5b8631da995", "1c6b2f410531caf564725806a53618338f185bec", "433da15efcf72ecb2323990c2f6a2aee9b125ddc", "2ad87ede21c26877c4f98069f543b41e2f26e05f"], "page_rank": 0.0004926108374384236}, {"id": "7ef197525d18e2fbd1ef5a313776d4a7d65803ab", "title": "Model-based object tracking in monocular image sequences of road traffic scenes", "authors": ["D. Roller", "Kostas Daniilidis", "Hans-Hellmut Nagel"], "date": 1993, "abstract": "Moving vehicles are detected and tracked automatically in monocular image sequences from road traffic scenes recorded by a stationary camera. In order to exploit the a priori knowledge about shape and motion of vehicles in traffic scenes, a parameterized vehicle model is used for an intraframe matching process and a recursive estimator based on a motion model is used for motion estimation. An interpretation cycle supports the intraframe matching process with a state MAP-update step. Initial model hypotheses are generated using an image segmentation component which clusters coherently moving image features into candidate representations of images of a moving vehicle. The inclusion of an illumination model allows taking shadow edges of the vehicle into account during the matching process. Only such an elaborate combination of various techniques has enabled us to track vehicles under complex illumination conditions and over long (over 400 frames) monocular image sequences. Results on various real-world road traffic scenes are presented and open problems as well as future work are outlined.", "references": ["84d200872bc18a93935f9b126ddb6f614b228336", "a62958d4cfdae21761d699544573d9af12ab4b6c", "96d429be5feb846bb46e8d649f023c0d54ac6e64", "3e544752fc984893339aad649b3b1018335a2451", "936bb5cfea3ad2861086e16f2285e168061467e4", "289e8386dc69651047acc55f6a753a9a42ed772e", "c031e39b2d78b4b7ac0a5f0ec6b6a0c25682d1ad", "c656481ddaae9f2319d39fd6b66cd18a3ee6dacd", "eb18ff8bd5aaa7e75577d8e5ee54e9766cc7a1aa", "ed9839b734c359a84135da5a0dee1085a98795a1"], "page_rank": 0.00016420361247947453}, {"id": "9755f9993553131e5cc796d34ecaa624fe0ddffa", "title": "A probability analysis on the value of unlabeled data for classification problems", "authors": ["Tong Zhang"], "date": 2000, "abstract": "Recently, there has been increasing interest in using unlabeled data for classiica-tion. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given. We demonstrate that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data. We apply this methodology to both \\passive partially supervised learning\" and \\active learning\", and draw conclusions from this analysis. Experiments will be provided to support our claims.", "references": ["278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "334867ed99a0af07d8a53dae4f7fdeffffdecc09", "87dc3bbe5de567d884a3cee9c47bd9fdbd2ce090", "e2de29049d62de925cf709024b92774cd82b0a5a", "3b3b54848c1bc6ffea2625ce79302abed8e8deb9", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "910688d01c01856dd20715907af44157de8d3d1d", "941ef255d31b5becbf0a3281bcf7ac0122e4c833", "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "22834aa74138de7f4da42fb9dfb480cef4e7b177"], "page_rank": 0.0001231527093596059}, {"id": "3f6db1b368ebaeace796f78d1ee02807051487d4", "title": "Further results on the recognition of a continuously read natural corpus", "authors": ["Lalit R. Bahl", "Raimo Bakis", "Paul S. Cohen", "A. G. Cole", "Frederick Jelinek", "Burn L. Lewis", "Robert L. Mercer"], "date": 1980, "abstract": "Further results have been obtained on the recognition of continuously read sentences from a natural language corpus of laser patents. The vocabulary is limited to the 1000 most frequently occurring words in the corpus. Our model of the task language has a perplexity of 24.1 words (corresponding to an entropy of 4.6 bits/word). This paper describes modifications and improvements to the system which have resulted in the lowering of the word error rate from the previously reported 33.1% to 8.9%.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "5809b6c737863bb6a82588e8760637e25c27bc10", "title": "The Gradient Projection Method for Nonlinear Programming. Part I. Linear Constraints", "authors": ["J. B. Rosen"], "date": 1960, "abstract": "more constraints or equations, with either a linear or nonlinear objective function. This distinction is made primarily on the basis of the difficulty of solving these two types of nonlinear problems. The first type is the less difficult of the two, and in this, Part I of the paper, it is shown how it is solved by the gradient projection method. It should be noted that since a linear objective function is a special case of a nonlinear objective function, the gradient projection method will also solve a linear programming problem. In Part II of the paper [16], the extension of the gradient projection method to the more difficult problem of nonlinear constraints and equations will be described. The basic paper on linear programming is the paper by Dantzig [5] in which the simplex method for solving the linear programming problem is presented. The nonlinear programming problem is formulated and a necessary and sufficient condition for a constrained maximum is given in terms of an equivalent saddle value problem in the paper by Kuhn and Tucker [10]. Further developments motivated by this paper, including a computational procedure, have been published recently [1]. The gradient projection method was originally presented to the American Mathematical Society", "references": [], "page_rank": 0.0002463054187192118}, {"id": "ec62927e369f364cc20bd80ae6e70a36a955d3aa", "title": "Analysis of a Hand-Tagging Task", "authors": ["Christiane Fellbaum", "Joachim Grabowski", "Shari Land"], "date": 1997, "abstract": "We analyze the results of a semantic annotation task performed by novice taggers as part of the WordNet SemCor project (Landes et al., in press). Each polysemous content word in a text was matched to a sense from WordNet. Comparing the performance of the novice taggers with that of experienced lexicographers, we find that the degree of polysemy, part of speech, and the position within the WordNet entry of the target words played a role in the taggers' choices. The taggers agreed on a sense choice more often than they agreed with two lexicographers, suggesting an effect of experience on sense distinction. Evidence indicates that taggers selecting senses from a list ordered by frequency of occurrence, where salient, core senses are found at the beginning of the entry, use a different strategy than taggers working with a randomly ordered list of senses. 1 I n t r o d u c t i o n Our present understanding of how the meanings of polysemous words are represented in speakers' minds and accessed during language use is poor. One model of the mental lexicon, implicit in much of computational linguistics, likens it to a dictionary, with a discrete entry for each word form and each sense of a polysemous word form. Language production and comprehension then would simply require \"looking up\" the appropriate entry and selecting the intended meaning. If this model of the mental lexicon, with its discrete and non-overlapping sense representations, were correct, both the creation and the use of dictionaries would be straightforward. Lexicographers collect large numbers of occurrences of words from a corpus. Interpreting the dif3 4 ferent meanings of polysemous words from the corpus presents no dit~culty, since lexicographers simply do what they do as competent speakers of the language. The step that is particular to lexicography is transforming the corpus occurrences of a given word form into a number of discrete senses in the format of dictionary entries. Cross-dictionary comparisons show that carving up the different meanings of a polysemous word into discrete dictionary senses is difficult. The number of senses for a polysemous word often differs, reflecting \"lumping\" versus \"splitting\" strategies; some senses are absent from one but not another dictionary. Yet postulating different mental lexicons seems unwarranted, given our rapid and successful communication. Rather, the mapping process from occurrence to dictionary entry may give rise to difficulties and discrepancies across dictionaries because speakers' meaning representations may not resemble those of dictionaries with their fiat and discrete senses, thus making lexicography an artificial and therefore challenging task. Semantic tagging is the inverse of lexicography, in that taggers identify and interpret dictionary entries with respect to words occurring in a text. Taggers, like lexicographers, first interpret the target word in the text, and then match the meaning they have identified for a given occurrence of a polysemous word with one of several dictionary senses. Our goal was to examine the difficulties associated with semantic tagging. Because taggers are faced with the same task as lexicographers-although the the former select, rather than create, dictionary senses to match word occurrences in text-we expected to see discrepancies among the results of the semantic annotation task across taggers. Moreover, we guessed that those polysemous words that receive very different treatments across dictionaries would also be tagged differently by the annotators.", "references": ["63936c6ce3ffedd4e4bc653a7ce3c3d295adb8c0", "cb2577675cb3c46af3a3d226cd8b697c5fdec661", "553fa529ba615e4bddea81e9a231ae19d5a870a4", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "e373a86843c179e9852e7029684cf73956483625", "dbd1994e7e63e8aef509604d3c38652cbf30f9de", "634ec8cce662d1f67d9de909dd14b14042a3ee00", "f886dc055a420bec6bf01aea5c0bca4b9a4c9999"], "page_rank": 0.0002463054187192118}, {"id": "c356cdc3f3293938a82662c727439be82b97cbc8", "title": "Bilexical Grammars and their Cubic-Time Parsing Algorithms", "authors": ["Jason Eisner"], "date": 2000, "abstract": "This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other. Such \u2018bilexicalism\u2019 has been a theme of much current work in parsing. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight modification yields link grammars. Its scoring approach is compatible with a wide variety of probability models.", "references": ["96395cd7494bcfa43cd62c14b4cf428bddbd3063", "fff984e57e6d12fa92fa89d8417c64abf389115a", "5cd28d8da08176bc22eb3a33fa4a68d282ef0cd8", "9d3a863b71f093e1cbc9304a3287c1ddc48c6f31", "2a5e619f2c5f4220438b1357e596db5b1578398d", "5752b8dcec5856b7ad6289bbe1177acce535fba4", "407d55ee40f1bdb10745155fde211d566c2d0c71", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "adfef97814b292a09520d8c78a141e7a4baf8726", "17ae3bda93abc40e758a1074c86baa041e977703"], "page_rank": 0.0004926108374384236}, {"id": "a018819150a741cbbbdc5ce5abfaa099903f8fe7", "title": "CMU Haitian Creole-English Translation System for WMT 2011", "authors": ["Sanjika Hewavitharana", "Nguyen Bach", "Qin Gao", "Vamshi Ambati", "Stephan E. Vogel"], "date": 2011, "abstract": "This paper describes the statistical machine translation system submitted to the WMT11 Featured Translation Task, which involves translating Haitian Creole SMS messages into English. In our experiments we try to address the issue of noise in the training data, as well as the lack of parallel training data. Spelling normalization is applied to reduce out-of-vocabulary words in the corpus. Using Semantic Role Labeling rules we expand the available training corpus. Additionally we investigate extracting parallel sentences from comparable data to enhance the available parallel data.", "references": ["e9dbbb76171bc48dfe42503af68a3e45ed4e5cff", "45410934f5eddd2bdecd9f8e49c16223ba5fda44", "15c28b0edbd2296324c07a0f218de83033781831", "3d309aa1629ef9ca43e252eb6bf539286ed872f9", "1f12451245667a85d0ee225a80880fc93c71cc8b", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "1efffabe9c9fe5c4c2d7afae9d0e60c12e50bc9a", "d7da009f457917aa381619facfa5ffae9329a6e9", "ab7b5917515c460b90451e67852171a531671ab8", "a16e484824b2580e092c985aa659e8680aeda5ee"], "page_rank": 0.00016420361247947453}, {"id": "b216a312c7b421748123a43eba9b45eb6418d7bb", "title": "Word Sense Ambiguation: Clustering Related Senses", "authors": ["William B. Dolan"], "date": 1994, "abstract": "This paper describes a heuristic approach to automatically identifying which senses of a machinereadable dictionary (MRD) headword are semantically related versus those which correspond to fundamentally different senses of the word. The inclusion of this information in a lexical database profoundly alters the nature of sense disambiguation: the appropriate \"sense\" of a polysemous word may now correspond to some set of related senses. Our technique offers benefits both for on-line semantic processing and for the challenging task of mapping word senses across multiple MRDs in creating a merged lexical database.", "references": ["5516b0babd5b9ebe95a3a2ad80789269e2e1d9bb", "a302f40fb0af3658d5f478cf712534385254d76d", "43865ad56b3364b39ae3badf1fc212547292b335", "b4ea3e2e980010703ff12466cd7413ee78d59f5e", "6f3fce7cecdd9d83421ecb87788393474aaac7dc", "73b75154b319e3352895c4cb387ab76095ceebfb", "882f393ab1c5d13f7ea3fa74284fcfcedd60d509", "7c871b62aa17e7f2af9c0c35ca85245d9e3a3822"], "page_rank": 0.0002463054187192118}, {"id": "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "title": "Equations for Part-of-Speech Tagging", "authors": ["Eugene Charniak", "Curtis Hendrickson", "Neil Jacobson", "Mike Perkowitz"], "date": 1993, "abstract": "We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.", "references": ["fe30dc915eefa40755b25a363813fcc575536661", "307c05b07c845a815c577d6bb43aea151efbce80", "d64cae6538bb3b8e595007585477a9fdef106602", "3a8de92b304729f15d9bd6c3d22a56ab9b31e212", "9463e3eca9f3b053fca7ca64abb157aaeac35f4f", "d811628d5f8230720c13ee9bb844badc3c3b6ae2", "980b2c3b44840daf4103f0fa3d2f7ded90437001", "f853daccfcb2350f9adcd75331d148b04c21e5ef", "729316fbded86763104f3412cadf98f00a9a3993", "8c23a242622abc0bf7e7b93f3822b1fc4d9d1f6a"], "page_rank": 0.0004926108374384236}, {"id": "319ac2dd69f75ef281fe4652dad97a32a0b8f4ac", "title": "Findings of the 2009 Workshop on Statistical Machine Translation", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "date": 2009, "abstract": "This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.", "references": ["b7594ec51b278482cef9b1e338b5cfcc629e1655", "20c11546a035d2fa2fa1121a7b31e890d20d6b6b", "ba3b8388f771f6e2caad8e77dcfb317c4f01569d", "be9bca1e9b0192fc49b316f2701242b50d98d456", "bfcd2fb80c0f7725baef1e093dbeac404e650480", "5932feb2b6c1f60410fc269b20c08110f74966e6", "fe32c5af2df4e9a07ea1d64c7282fdb99816d909", "dea44a9f9c6d446875ab2224d9ae1d8f1a067786", "0d551f3310a33ddccfb969c6ccb827ff1a5196bf", "710f79804a50d93521a15a1ce9663d1f221ea7f4"], "page_rank": 0.00016420361247947453}, {"id": "1cf6f1209d29c151b693861e083850f1b385c595", "title": "A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis", "authors": ["Claire Cardie"], "date": 1993, "abstract": "This paper describes a case-based approach to knowledge acquisition for natural language systems that simultaneously learns part of speech, word sense, and concept activation knowledge for all open class words in a corpus. The parser begins with a lexicon of function words and creates a case base of context-sensitive word definitions during a humansupervised training phase. Then, given an unknown word and the context in which it occurs, the parser retrieves definitions from the case base to infer the word's syntactic and semantic features. By encoding context as part of a definition, the meaning of a word can change dynamically in response to surrounding phrases without the need for explicit lexical disambiguation heuristics. Moreover, the approach acquires all three classes of knowledge using the same case representation and requires relatively little training and no hand-coded knowledge acquisition heuristics. We evaluate it in experiments that explore two of many practical applications of the technique and conclude that the case-based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains. In addition, we present a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric.", "references": ["9103d52184919be9946e50306093104031165abd", "5e6fdf2dfcefe043394298c9e437f962aad0f81e", "cdd603b4377ec75cc83a952277dbbf9f6c53e974", "1d922631a6bf8361d7602e12cafb9e15d421c827", "7484b2addbaa57561a40ea3fc60f11b0e5f73d56", "5bdf95f516303d46a38bfa8c7bf7a3112286e064", "b132192076c65ee9c16c851728827634991d6868", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "5213e864b82c4dde46ce9f6b82c403729426d3a2", "f8a044c3935bd18aba4064e861ea854a84f2f52a"], "page_rank": 0.00022167487684729062}, {"id": "fcd8599fa26d7742516cc88baf36a0ca5a96216a", "title": "Isolated word recognition using hidden Markov models", "authors": ["Kenji Sugawara", "Masafumi Nishimura", "Koichi Toshioka", "M. Okochi", "Takaomi Kaneko"], "date": 1985, "abstract": "In this paper we investigated smoothing techniques for alleviating the problem due to insufficient amount of training data. Hidden Markov Models (HMM) require a large amount of training data to obtain reliable probability estimates. But for isolated word recognition (100 words or more), we can not expect a user to speak each word more than several times. We found that the confusion matrix between a pair of label prototypes was particularly effective for the problem. We investigated two ways of computing the confusion matrix. One is based on distance among labels, and the other is based on the correspondence of labels in several utterances of the same word. Performance of these techniques was tested by using 100 Japanese city names spoken in an isolated word mode by three speakers. It was found that the smoothing technique reduced recognition errors from 1% to 0.1%. To visualize such performance improvement, we used, together with recognition rate, \"two-dimensional score plot,\" which shows the distribution of the best score of the true word and that of the remaining false ones in the vocabulary.", "references": ["090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58"], "page_rank": 9.852216748768472e-05}, {"id": "29fdbbd3bb0b3c798a57e10576d318281d37dd2a", "title": "Statistical Techniques for Natural Language Parsing", "authors": ["Eugene Charniak"], "date": 1997, "abstract": "I review current statistical work on syntactic parsing and then consider part-of-speech tagging, which was the first syntactic problem to successfully be attacked by statistical techniques and also serves as a good warm-up for the main topic-statistical parsing. Here, I consider both the simplified case in which the input string is viewed as a string of parts of speech and the more interesting case in which the parser is guided by statistical information about the particular words in the sentence. Finally, I anticipate future research directions.", "references": ["2a5e619f2c5f4220438b1357e596db5b1578398d", "0ffa423a5283396c88ff3d4033d541796bd039cc", "3764baa7465201f054083d02b58fa75f883c4461", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "8ad8e98574a275930bf04a477ce3532fd13c503c", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "ad120a6635aeb1f0fbf798e5a1b97b65e25b716e", "aad8c5ae265e8f645101245afb9d9c9cdf40b4ca", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "24e5031c567464b858f86c6ca46331f153399bc6"], "page_rank": 0.0002463054187192118}, {"id": "2eae0f08186952643c3a7ead2eba2d41fda58cec", "title": "Disambiguating Noun Groupings with Respect to Wordnet Senses", "authors": ["Philip Resnik"], "date": 1995, "abstract": "Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word senses, not words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns \u2014 the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.", "references": ["1d922631a6bf8361d7602e12cafb9e15d421c827", "f6921e3c7219c5246a3f6105156cdcbd708603c7", "a877ccea302725880a0eee16d0f9f27e045c7b5d", "162078c7170d5f6a3a2805439a5529e1a636f12e", "52af59382abca0fd549074353020f846a8731165", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "76e4e034c20bea86edcc6e71bbaddb47fafeecbc", "b3bf0d51eb6aa2bba6c557156f7679c627ebfee8", "dc69c680484633f27962510ced7afb20f60065cb"], "page_rank": 9.852216748768472e-05}, {"id": "297e478f92cef1cd090706fc59fde5ea0836ce80", "title": "WordNet and Distributional Analysis: A Class-based Approach to Lexical Discovery", "authors": ["Philip Resnik"], "date": 1992, "abstract": "It has become common in statistical studies of natural language data to use measures of lexical association, such as the information-theoretic measure of mutual information, to extract useful relationships between words (e.g. [Church et al., 1989; Church and Hanks, 1989; Hindle, 1990]). For example, [Hindle, 1990] uses an estimate of mutual information to calculate what nouns a verb can take as its subjects and objects, based on distributions found within a large corpus of naturally occurring text.", "references": ["f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "ff97fa9912e4e0517208bf45ef0f646f995e09d2", "1d922631a6bf8361d7602e12cafb9e15d421c827", "09a3882e627373100090f52f1f6ae8542e782afa", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "96e543aa48f7b78f1f34efc540fc5524cdee3488", "d98b3f138c70970897f962940c4e2682a18b3bcd", "92b56deb337bf272bc2e24c7f01c319602cbabd3", "6a923c9f89ed53b6e835b3807c0c1bd8d532687b"], "page_rank": 9.852216748768472e-05}, {"id": "57d78d899b1977c8170a9d2ca98b07ad0e71a832", "title": "A Problem for RST: The Need for Multi-Level Discourse Analysis", "authors": ["Johanna D. Moore", "Martha E. Pollack"], "date": 1992, "abstract": "Rhetorical Structure Theory (RST) (Mann and Thompson 1987), argues that in most coherent discourse, consecutive discourse elements are related by a small set of rhetorical relations. Moreover, RST suggests that the information conveyed in a discourse over and above what is conveyed in its component clauses can be derived from the rhetorical relation-based structure of the discourse. A large number of natural language generation systems rely on the rhetorical relations defined in RST to impose structure on multi-sentential text (Hovy 1991; Knott 1991; Moore and Paris 1989; Rosner and Stede 1992). In addition, many descriptive studies of discourse have employed RST (Fox 1987; Linden, Cumming, and Martin 1992; Matthiessen and Thompson 1988). However, recent work by Moore and Paris (1992) noted that RST cannot be used as the sole means of controlling discourse structure in an interactive dialogue system, because RST representations provide insufficient information to support the generation of appropriate responses to \"follow-up questions.\" The basic problem is that an RST representation of a discourse does not fully specify the intentional structure (Grosz and Sidner 1986) of that discourse. Intentional structure is crucial for responding effectively to questions that address a previous utterance: without a record of what an utterance was intended to achieve, it is impossible to elaborate or clarify that utterance. 1 Further consideration has led us to conclude that the difficulty observed by Moore and Paris stems from a more fundamental problem with RST analyses. RST presumes that, in general, there will be a single, preferred rhetorical relation holding between consecutive discourse elements. In fact, as has been noted in other work on discourse structure (Grosz and Sidner 1986), discourse elements are related simultaneously on multiple levels. In this paper, we focus on two levels of analysis. The first involves the relation between the information conveyed in consecutive elements of a coherent discourse. Thus, for example, one utterance may describe an event that can be presumed to be the cause of another event described in the subsequent utterance. This causal relation is at what we will call the informational level. The second level of relation results from the fact that discourses are produced to effect changes in the mental state of the discourse participants. In coherent discourse, a speaker is carrying out a consistent plan to achieve the intended changes, and consecutive discourse elements are related to one another by means of the ways in which they participate in that plan. Thus, one utterance may be intended to increase the likelihood that the hearer will come to", "references": ["0b144f3ca00d98df1d8f9456d29c7fce3290924d", "ffd588709f6bfa33a293b20a6964c826c1b07f0b", "cba94fae9df48b938ac2ec6b1aaadd232944f791", "d652208b8f93222516c13cbfb610bb1e03fcc7b9", "a8912e439ac787c91892ea3d5223e7dd0fea4052", "97683c025984ac6c9cb0cb91b961b65fdb3dc960", "c3993259a2ad144678b784fc65ab014a90bad3de", "4fb573e6ead5b1986b6afddd5148b16f36f7735a", "54531c0ce77465573dcc3fe0ffef1bea5c3837a6", "e381520518153da5e700235d5142e771055aeceb"], "page_rank": 0.0007389162561576354}, {"id": "3b14be12cecba09db74a901bf25a33a67ac784a6", "title": "Adaptive Multilingual Sentence Boundary Disambiguation", "authors": ["David D. Palmer", "Marti A. Hearst"], "date": 1997, "abstract": "The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.", "references": ["2510c7da837cf4ad083a6aa97a857e524cb4f142", "41945ebde5eea2186c5970cbd59de04cef05d0fb", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "34d7f231fe2b9f243a60a4a64c06028ad7ba776b", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "51f81f7bf9bb777f57a952ec3b09f3c245a72aec", "42fd4d469c53e4eedd7eb76e7859e3270367f795", "1cf01798f07e1f13cd44daad6119f4978721c61d", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "af386a4e0f2615ed929fdc64a86df8e383bd6121"], "page_rank": 0.00016420361247947453}, {"id": "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "title": "Improved Alignment Models for Statistical Machine Translation", "authors": ["Franz Josef Och", "Christoph Tillmann", "Hermann Ney"], "date": 1999, "abstract": "In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 S t a t i s t i c a l M a c h i n e T r a n s l a t i o n The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)}", "references": ["a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "ab7b5917515c460b90451e67852171a531671ab8", "7526e9f65a24804a1686ab4a39f0e3a5598f4dae", "5a8246be154ca2c9965c6573f14e570e314aebdd", "0991b1a89f9046cdc37e1db1f3f8d2d56b00162c", "15b222eae7cea8f458aed3d70956553c45ee2e43"], "page_rank": 0.0008784893267651887}, {"id": "adf42271b2817f06459b56ba84790a7c2afe3227", "title": "PAC-learnability of determinate logic programs", "authors": ["Saso Dzeroski", "Stephen Muggleton", "Stuart J. Russell"], "date": 1992, "abstract": "The field of Inductive Logic Programming (ILP) is concerned with inducing logic programs from examples in the presence of background knowledge. This paper defines the ILP problem, and describes the various syntactic restrictions that are commonly used for learning first-order representations. We then derive some positive results concerning the learnability of these restricted classes of logic programs, by reduction to a standard propositional learning problem. More specifically, k-clause predicate definitions consisting of determinate, function-free, non-recursve Horn clauses with variables of bounded depth are polynomially learnable under simple distributions. Similarly, recursive k-clause definitions are polynomially learnable under simple distributions if we allow existential and membership queries about the target concept.", "references": ["3e36f22685b8d3db73532d3104b325cea5288a66", "a0e2db510e61f33307a85c08368e9bbc6428965a", "ec0f5460a2943cacaa67eb16483adea9b6ff2220", "218a899ba7ff3094c0fc871b9605d8ff4f529336", "a62bfef14defb2b8d7e22316f7a34657c265861a"], "page_rank": 0.0002463054187192118}, {"id": "71448a9b8d35d5f3123405d3d7a66288618daab2", "title": "Occam's Razor", "authors": ["Anselm Blumer", "Andrzej Ehrenfeucht", "David Haussler", "Manfred K. Warmuth"], "date": 1987, "abstract": "Abstract We show that a polynomial learning algorithm, as defined by Valiant (1984), is obtained whenever there exists a polynomial-time method of producing, for any sequence of observations, a nearly minimum hypothesis that is consistent with these observations.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "58095bae1d836943bdaa52b76fa8d17cf77d06b3", "title": "Inductive logic programming - techniques and applications", "authors": ["Nada Lavrac", "Saso Dzeroski"], "date": 1993, "abstract": "Part 1 Empirical inductive logic programming: introduction empirical ILP systems - an overview LINUS - using attribute-value learners in an ILP framework experiments in learning relations with LINUS ILP as search for program clauses. Part 2 Learning relations from imperfect data: handling imperfect data in ILP using heuristics to handle noise in ILP mFOIL - extending noise-handling in FOIL experiments in learning relations from noisy examples. Part 3 Applications of inductive logic programming: learning rules for early diagnosis of rheumatic diseases finite element mesh design an overview of selected ILP applications.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "bece46ed303f8eaef2affae2cba4e0aef51fe636", "title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "authors": ["Andrew McCallum", "Dayne Freitag", "Fernando C Pereira"], "date": 2000, "abstract": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s.", "references": ["3b9217ac8d4fdd9528442389425b792b1ef0ad93", "ee276a682f0ee2e01a61265c5e92b8d8d89e4de2", "352dbd26580856ba4b9877d43aeba304343af66d", "c11fb8460b4e2e8cf76cc3abe1dc3eaf153b67d9", "4d8cb09f19c0afdc68d39cc55743104ec396d86e", "0b26fa1b848ed808a0511db34bce2426888f0b68", "442fff5c760643c1fd46bd97b0877de0bcb8ba2c", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1", "8ec2d52a2c7ac954adfdbe0f3a314379d89b3858", "7fad831935254c9c9ec39ffb03752a3f736c3f76"], "page_rank": 0.0004926108374384236}, {"id": "80feac7b0c5f1a0880cc8b0a980eb0583d68b711", "title": "Quantifying the Inductive Bias in Concept Learning (Extended Abstract)", "authors": ["David Haussler"], "date": 1986, "abstract": "We show that the notion of bias in inductive concept learning can be quantified in a way that directly relates to learning performance, and that this quantitative theory of bias can provide guidance in the design of effective learning algorithms. We apply this idea by measuring some common language biases, including restriction to conjunctive concepts and conjunctive concepts with internal disjunction, and, guided by these measurements, develop learning algorithms or these classes of concepts that have provably good convergence properties.", "references": ["5fc0c7afc6bb27fb3752eae0ea5869413b1259b7", "6bee88f11407225011db66795bf0d74f9bedffad", "aa4eb6549d9afcafd7104869d3b4c7166c00fbf9", "bcee7c85d237b79491a773ef51e746bbbcf48e35", "80c82e8e212ad03d5456088127b50e480631334c", "95f08ef99f0f0b16ce4a916f2983451ed722fcbc", "a40262fc39c2611906d4b69481035a5d4b05fc0e", "5e6dfb46ed298ff037e166291c128a465f90bfc0", "a36b028d024bf358c4af1a5e1dc3ca0aed23b553", "4315baed930e06dac39875eba9e289e95d964309"], "page_rank": 0.0002463054187192118}, {"id": "87834ded1d151355d2028dc6efb26fcaa48ab9fc", "title": "The FrameNet Database and Software Tools", "authors": ["Charles J. Fillmore", "Collin F. Baker", "Hiroaki Sato"], "date": 2002, "abstract": "The FrameNet Project is producing a lexicon of English for both human use and NLP applications, based on the principles of Frame Semantics, in which sentences are described on the basis of predicators which evoke semantic frames and other constituents which express the participants (frame elements) in these frames. Our lexicon contains detailed information about the possible syntactic realizations of frame elements, derived from annotated corpus examples. In the process, we have developed a suite of tools for the definition of semantic frames, for annotating sentences, for searching the results, and for creating a variety ofreports. We will discuss the conceptual basis ofour work and demonstrate the tools we work with, the results we produce, and how they may be of use to other NLP projects.", "references": ["547f23597f9ec8a93f66cedaa6fbfb73960426b1", "782d19b7b07c8cdab8174a0c9b2b0a7e28917d6f", "2423eeafa0c087ec9b0a5381de11dab5fca025ef", "72cc80486744320d5b9a2ea75e3fb1cde4ca669c", "74999bc24a4ebf42577735fb158068718574860a", "957ddc4ca4c8f7be5efba176716a4ff6b23d80d5"], "page_rank": 0.00016420361247947453}, {"id": "d6fb9fb0bf112abde285d14ac59a2607c21bf04f", "title": "Higher order differential structure of images", "authors": ["Bart M. ter Haar Romeny", "Luc Florack", "Alfons H. Salden", "Max A. Viergever"], "date": 1994, "abstract": "This paper is meant as a tutorial on the basic concepts for vision in the \u2018Koenderink\u2019 school. The concept of scale-space is a necessity, if the extraction of structure from measured physical signals (i.e. images) is at stage. The Gaussian derivative kernels for physical signals are then the natural analogues of the mathematical differential operators. This paper discusses some interesting properties of the Gaussian derivative kernels, like their orthogonality and behaviour with noisy input data. Geometrical structure to extract is expressed in terms of differential invariants, in this paper limited to invariants under orthogonal transformations. Three representations are summarized: Cartesian, gauge and manifest invariant notation. Many explicit examples are given. A section is included about the computer implementation of the calculation of higher order invariant structure.", "references": ["4e7aec276e93e76f66e3a3c659ca519953e30226", "00aa5220d49f3fcf357c1b64ac14f24cd8afb76d", "482fd5984f3b83c0e5772a00a39a4ff6e14eeb41", "fd94cb99e6c4b917f77f524d72925be393dd785d"], "page_rank": 0.0004926108374384236}, {"id": "46e765ac3b3d163fdf60040f8742d199c0508675", "title": "AT&T at TREC-8", "authors": ["Amit Singhal", "Steven P. Abney", "Michiel Bacchiani", "Michael Collins", "Donald Hindle", "Fernando C Pereira"], "date": 1999, "abstract": "In 1999, AT&T participated in the ad-hoc task and the Question Answering (QA), Spoken Document Retrieval (SDR), and Web tracks. Most of our e ort for TREC-8 focused on the QA and SDR tracks. Results from SDR track show that our document expansion techniques, presented in [8, 9], are very e ective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can nd the correct answer for about 45% of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.", "references": ["238202b2134fb58560291af34aa0532102b5f106", "209a76fdec6d4c342b24fea9548c967d41d1e2ad", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "44235141062263ee4d5186eb780583c9b3ab8bed", "742b1a4bdd944e8685b4eef76cbdfb7d2a52cd01", "3ed7dfac6641dc4bbd1ff4d6abedceae57d6ddf4", "f631121deb8c26d1fff60799b2306e85852aaff5"], "page_rank": 0.0004926108374384236}, {"id": "7b3175be6b38c271558723fa1f95f0ca223939c6", "title": "Hidden Markov model decomposition of speech and noise", "authors": ["A. P. Varga", "Roger K.G. Moore"], "date": 1990, "abstract": "The problem of automatic speech recognition in the presence of interfering signals and noise with statistical characteristics ranging from stationary to fast changing and impulsive is discussed. A technique of signal decomposition using hidden Markov models is described. This is a generalization of conventional hidden Markov modeling that provides an optimal method of decomposing simultaneous processes. The technique exploits the ability of hidden Markov models to model dynamically varying signals in order to accommodate concurrent processes, including interfering signals as complex as speech. This form of signal decomposition has wide implications for signal separation in general and improved speech modeling in particular. The application of decomposition to the problem of recognition of speech contaminated with noise is emphasized.>", "references": ["62b3c348331666b651bfbcb93531a26de4f82f34", "ca18fe0b7a9f964d920be843a9674779be2bf832", "8b49fce77d75f671e5f41326e85794501835b0fc"], "page_rank": 0.0004926108374384236}, {"id": "6eb807ae65f393e2b1544619bd7faf7985897b07", "title": "Simultaneous stereo-motion fusion and 3-D motion tracking", "authors": ["Y{\\\"u}cel Altunbasak", "A. Murat Tekalp", "Gozde Bozdagi Akar"], "date": 1995, "abstract": "Presents a new framework for combining maximum likelihood (ML) stereo-motion fusion with adaptive iterated extended Kalman filtering (IEKF) for 3-D motion tracking. The ML stereo-fusion step, with two stereo-pairs, generates observations of 3-D feature matches to be used by the IEKF step. The IEKF step, in turn, computes updated 3-D motion parameter estimates to be used by the ML stereo-motion fusion step. The covariance of the observation noise process is regulated by the value of the ML cost function to address occlusion related problems. The proposed simultaneous approach is compared with performing the 3-D feature correspondence estimation and the Kalman filtering separately using simulated stereo imagery.", "references": ["85275c7ede7e3417ad37d4d1823f12c7abdd9b9a", "bf76e048e36be1c1d1ee6a31c546950d1ce1444b", "ace2f420d1b7f3423e664c02d5782e5caf56b185", "a62958d4cfdae21761d699544573d9af12ab4b6c", "a72a4f3d2dda391c262a3070586d3961fe7bd381"], "page_rank": 9.852216748768472e-05}, {"id": "eaa3bc2a3291855f91a85939dfdaeb75f9e0cffb", "title": "Estimating the Kinematics and Structure of a Rigid Object from a Sequence of Monocular Images", "authors": ["Ted J. Broida", "Rama Chellappa"], "date": 1991, "abstract": "The problem considered involves the use of a sequence of noisy monocular images of a three-dimensional moving object to estimate both its structure and kinematics. The object is assumed to be rigid, and its motion is assumed to be smooth. A set of object match points is assumed to be available, consisting of fixed features on the object, the image plane coordinates of which have been extracted from successive images in the sequence. Structure is defined as the 3-D positions of these object feature points, relative to each other. Rotational motion occurs about the origin of an object-centered coordinate system, while translational motion is that of the origin of this coordinate system. In this work, which is a continuation of the research done by the authors and reported previously (ibid., vol.PAMI-8, p.90-9, Jan. 1986), results of an experiment with real imagery are presented, involving estimation of 28 unknown translational, rotational, and structural parameters, based on 12 images with seven feature points. >", "references": ["bb117effec8b8b86d19de517ff2d6550f207f445", "c031e39b2d78b4b7ac0a5f0ec6b6a0c25682d1ad", "3733124d9fedd380e7e0c6b82eef9f2db3344fc3", "3c00e2575feb5e5fc07bf63f28c51b951094a8bf", "d74086e6e571edecff8d2be85684f3e3a66718ef", "a4c8279d962b1a45a5768f457633574b5beef5d3", "06e1ae83b3cc014158652eb0c42aff6aa28f7368", "b6f8b84544ad2387e9caf31962ae700caa44efa0", "df35abc5edd50b3a4d89cff2c85dbde76b885a45", "53ac115c2d8ddb81f1cfb611447e778c45c575e1"], "page_rank": 0.0003448275862068965}, {"id": "54bfd42964f85355765a3de5ef6a464ca8aa00c3", "title": "Parameter estimation techniques: a tutorial with application to conic fitting", "authors": ["Zhengyou Zhang"], "date": 1997, "abstract": "Almost all problems in computer vision are related in one form or another to the problem of estimating parameters from noisy data. In this tutorial, we present what is probably the most commonly used techniques for parameter estimation. These include linear least-squares (pseudo-inverse and eigen analysis); orthogonal least-squares; gradient-weighted least-squares; bias-corrected renormalization; Kalman filtering; and robust techniques (clustering, regression diagnostics, M-estimators, least median of squares). Particular attention has been devoted to discussions about the choice of appropriate minimization criteria and the robustness of the different techniques. Their application to conic fitting is described.", "references": ["1e112e8bb31ebcf2fc019796cdabec1e5f34f480", "36cbd6f1064576381a2006087d8bd79cf0b9828d", "18408f7fc067f0cbb42a66695e63ab22df5d831c", "ef3e96eb1fd60b4aa3f7f45179e5eda8464000e4", "8be5efc2b356414e3e0a5f2f7ec54875b2e2559f", "c22361e3060d5eb88e503e022747cbffa8bce1cf", "1f28637a8618fa76b4052f04293ddc0cf0c5b9b4", "568aa2b4e9fc6266fd86d202852fa67ab54a3b7d", "b3ec4ceea040b7b4129ee5d71b4f95539bf876b7", "47b6ffcd715eebd5418d42b17f4e64da8a6e9747"], "page_rank": 0.0004926108374384236}, {"id": "4c2e1b22689c59086f938cb275de021127a92789", "title": "Tracking interacting people", "authors": ["Stephen J. Mckenna", "Sumer Jabri", "Zoran Duric", "Harry Wechsler"], "date": 2000, "abstract": "A computer vision system for tracking multiple people in relatively unconstrained environments is described. Tracking is performed at three levels of abstraction: regions, people and groups. A novel, adaptive background subtraction method that combines colour and gradient information is used to cope with shadows and unreliable colour cues. People are tracked through mutual occlusions as they form groups and part from one another. Strong use is made of colour information to disambiguate occlusions and to provide qualitative estimates of depth ordering and position during occlusion. Some simple interactions with objects can also be detected. The system is tested using indoor and outdoor sequences. It is robust and should provide a useful mechanism for bootstrapping and reinitialisation of tracking using more-specific but less-robust human models.", "references": ["211e65af22d95f35e58a05312956b58c41fc29fd", "6ca68f34e1431f4e6114bc5d2b3a2cb0d9e5ae33", "c8cfe5de48221c42d80482a79a5338fdc156f50f", "89d3cd1a3a189cfd8a712ae0015fe6d9d6435ffc", "deecabcf3ad46a7188085edde63d45ec319d7112", "18b02beb27288f6bd9d4376ca41e70655a698084", "eac7287d7ef69252358c1fbddedf123e11012370", "7c05eb1563da0a3f8fa31363f4d8ecae40b7e3ce", "67eac618325d5f5f31ce17922d51fad995a57749", "f8352355d5ff967f86cea99326ad5a513ae65436"], "page_rank": 9.852216748768472e-05}, {"id": "b9df7f4ef4e164a9b8bbdebadaf379b4db71db4e", "title": "Multiple-human tracking using multiple cameras", "authors": ["Akira Utsumi", "Hiroki Mori", "Jun Ohya", "Masahiko Yachida"], "date": 1998, "abstract": "We propose a human motion detection method using multiple-viewpoint images. In vision-based human tracking, self-occlusions and human-human occlusions are a part of the more significant problems. Employing multiple viewpoints and a viewpoint selection mechanism, however, can reduce these problems. The vision system in this case should select the best viewpoints for extracting human motion information; the \"best\" selections can be changed among different types of target information. We address the problem of tracking human bodies. We divide the task into three primitive sub-tasks (position detection, rotation angle detection and body side detection). Each sub-task has a different criterion for selecting viewpoints and an estimation result of one sub-task can help another sub-task. We describe the criteria for accomplishing the individual sub-tasks and the relationships between sub-tasks. We have built an experimental system based on a small number of reliable image features and performed fundamental examinations on the viewpoint selection approach.", "references": ["86de16b4a992f702e2f3a4862d5f9fa611fcaef0", "cc9b263c1af95ea803c4f5c8888ef8e37f0cef80", "b76cc89484dd46dab66b22b19723ba358318e712", "2d7bcb538b03c67e5e7daecabd26d4941fa1bb07", "f9695c6ebbbdc1341f8197d7c0a0cf6a6a1cbd47", "fc4db08246dc3ed4b0f7ff1df290de763f343b03", "92ab4fc76e2f085dde81626794b79b5e9d1d00e0", "d7986359bb4c91b10da1015f11ade69e89e212c0", "9df0428c30b8aab4f7e6f367e70126efdfb8fc45", "556d76ca9fb4d2f53959d3e211cdba90ca0647ce"], "page_rank": 9.852216748768472e-05}, {"id": "77c7dbbb9ee85a9676021a2ec5b997de7e481371", "title": "Tracking a dynamic set of feature points", "authors": ["Yi-Sheng Yao", "Rama Chellappa"], "date": 1995, "abstract": "We address the problems of tracking a set of feature points over a long sequence of monocular images as well as how to include and track new feature points detected in successive frames. Due to the 3-D movement of the camera, different parts of the images exhibit different image motion. Tracking discrete features can therefore be decomposed into several independent and local problems. Accordingly, we propose a localized feature tracking algorithm. The trajectory of each feature point is described by a 2-D kinematic model. Then to track a feature point, an interframe motion estimation scheme is designed to obtain the estimates of interframe motion parameters. Subsequently, using the estimates of motion parameters, corresponding points are identified to subpixel accuracy. Afterwards, the temporal information is processed to facilitate the tracking scheme. Since different feature points are tracked independently, the algorithm is able to handle the image motion arising from general 3-D camera movements. On the other hand, in addition to tracking feature points detected at the beginning, an efficient way to dynamically include new points extracted in subsequent frames is devised so that the information in a sequence is preserved. Experimental results for several image sequences are also reported.", "references": ["bfcc05670a8f49d59db68c4e6f186bfc5b474c5d", "a62958d4cfdae21761d699544573d9af12ab4b6c", "d5e08bcdb62c4fbda1bbb09eeeb0fec5430873e4", "bb117effec8b8b86d19de517ff2d6550f207f445", "b09a2094220df80068163483e2e4b4ff145efb55", "eaa3bc2a3291855f91a85939dfdaeb75f9e0cffb", "c031e39b2d78b4b7ac0a5f0ec6b6a0c25682d1ad", "5ac387d575db7ac013e1d1edd5a29f3e7a4f56e5", "a5ddb09310eee52d97074afc496b16df238ae5b9", "a4c8279d962b1a45a5768f457633574b5beef5d3"], "page_rank": 9.852216748768472e-05}, {"id": "3e544752fc984893339aad649b3b1018335a2451", "title": "Recursive 3-D motion estimation from a monocular image sequence", "authors": ["Ted J. Broida", "S. Chandrashekhar", "Rama Chellappa"], "date": 1990, "abstract": "Consideration is given to the design and application of a recursive algorithm to a sequence of images of a moving object to estimate both its structure and kinematics. The object is assumed to be rigid, and its motion is assumed to be smooth in the sense that it can be modeled by retaining an arbitrary number of terms in the appropriate Taylor series expansions. Translational motion involves a standard rectilinear model, while rotational motion is described with quaternions. Neglected terms of the Taylor series are modeled as process noise. A state-space model is constructed, incorporating both kinematic and structural states, and recursive techniques are used to estimate the state vector as a function of time. A set of object match points is assumed to be available. The problem is formulated as a parameter estimation and tracking problem which can use an arbitrarily large number of images in a sequence. The recursive estimation is done using an iterated extended Kalman filter (IEKF), initialized with the output of a batch algorithm run on the first few frames. Approximate Cramer-Rao lower bounds on the error covariance of the batch estimate are used as the initial state estimate error covariance of the IEKF. The performance of the recursive estimator is illustrated using both real and synthetic image sequences. >", "references": ["59da095ce0ac2127fa02e9cf818ba33931d70070", "bdda730f1ff4e4b45e10aa68162b3d21b259d670", "aab5d2323a29a160b1feab5c7175b237436abce4", "9e53c6e29fe888c848728fc31a7754da3c5da8b8", "a9098e0b17b1ebc754f62a033c8a33b1a8e21654", "8c369eb4534705488e7060da949273bfb499a67a"], "page_rank": 0.0004926108374384236}, {"id": "f886dc055a420bec6bf01aea5c0bca4b9a4c9999", "title": "Semantic Theory and the Meaning of 'Good'", "authors": ["Jerrold J. Katz"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Semantic Theory and the Meaning of 'Good'\" by Jerrold J. Katz", "references": [], "page_rank": 0.00016420361247947453}, {"id": "22834aa74138de7f4da42fb9dfb480cef4e7b177", "title": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter", "authors": ["Vittorio Castelli", "Thomas M. Cover"], "date": 1996, "abstract": "We observe a training set Q composed of l labeled samples {(X/sub 1/,/spl theta//sub 1/),...,(X/sub l/, /spl theta//sub l/)} and u unlabeled samples {X/sub 1/',...,X/sub u/'}. The labels /spl theta//sub i/ are independent random variables satisfying Pr{/spl theta//sub i/=1}=/spl eta/, Pr{/spl theta//sub i/=2}=1-/spl eta/. The labeled observations X/sub i/ are independently distributed with conditional density f/sub /spl theta/i/(/spl middot/) given /spl theta//sub i/. Let (X/sub 0/,/spl theta//sub 0/) be a new sample, independently distributed as the samples in the training set. We observe X/sub 0/ and we wish to infer the classification /spl theta//sub 0/. In this paper we first assume that the distributions f/sub 1/(/spl middot/) and f/sub 2/(/spl middot/) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter /spl eta/. We then assume that two densities g/sub 1/(/spl middot/) and g/sub 2/(/spl middot/) are given, but we do not know whether g/sub 1/(/spl middot/)=f/sub 1/(/spl middot/) and g/sub 2/(/spl middot/)=f/sub 2/(/spl middot/) or if the opposite holds, nor do we know /spl eta/. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples.", "references": ["dbf110e1e786d71b974392c018a64ec06eb08bef", "55ab64d91344cdde7d4959a181c7652245c19597", "c2c4aa2580e53ae163fa69d43c5ed9c21956cc08", "1349b5a746d02023bd8704165e54ce2255b889df", "e85a68602abf92fcc1efb8b7aa90d27d141a80c2", "4b82bce6a021aea889a1e4f734277ffa73b1595b", "6cf994cc7cec3c76f49f98aa6ded0824187e786d", "4caff7fdf821594f4ffe98fda02c62df92d07291", "7e983c395a26710bed4fb2577d166a1811ba6e44", "87360f5df38866a171f96c046dc9f6feb755745f"], "page_rank": 0.00016420361247947453}, {"id": "941ef255d31b5becbf0a3281bcf7ac0122e4c833", "title": "Query by committee", "authors": ["H. Sebastian Seung", "Manfred Opper", "Haim Sompolinsky"], "date": 1992, "abstract": "We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.", "references": [], "page_rank": 0.00032840722495894905}, {"id": "dbd1994e7e63e8aef509604d3c38652cbf30f9de", "title": "Semantic networks of english", "authors": ["George Armitage Miller", "Christiane Fellbaum"], "date": 1991, "abstract": "Principles of lexical semantics developed in the course of building an on-line lexical database are discussed. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages.", "references": [], "page_rank": 0.0006568144499178981}, {"id": "634ec8cce662d1f67d9de909dd14b14042a3ee00", "title": "Building Semantic Concordances", "authors": ["Christiane Fellbaum", "George A. Miller"], "date": 1998, "abstract": "Semantic Scholar extracted view of \"Building Semantic Concordances\" by Christiane Fellbaum et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "title": "Selective Sampling Using the Query by Committee Algorithm", "authors": ["Yoav Freund", "H. Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "date": 1997, "abstract": "We analyze the \u201cquery by committee\u201d algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.", "references": ["941ef255d31b5becbf0a3281bcf7ac0122e4c833", "586a31a02a798f5752a853792c5c8524980bc909", "b3c19172a59922fa7cbed8f09f30ed3a1ea74b4e", "a7404527c3a6aa542ea183da9c821efda05a2afc", "e449143111fa4a270aa36d1515ca9bba9b172304", "0f05520f728a890a5c806ad7f3b27e3144ecc4d6", "10ddb646feddc12337b5a755c72e153e37088c02", "107ed240fd5a94aa84d9f6297fd6e43a88008755", "e0b8fa3496283d4d808fba9ff62d5f024bcf23be", "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b"], "page_rank": 0.00016420361247947453}, {"id": "8c23a242622abc0bf7e7b93f3822b1fc4d9d1f6a", "title": "Parsing the LOB Corpus", "authors": ["C. Demarcken"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Parsing the LOB Corpus\" by C. Demarcken", "references": [], "page_rank": 0.0004926108374384236}, {"id": "882f393ab1c5d13f7ea3fa74284fcfcedd60d509", "title": "Structural Patterns vs. String Patterns for Extracting Semantic Information from Dictionaries", "authors": ["Simonetta Montemagni", "Lucy Vanderwende"], "date": 1992, "abstract": "This chapter presents evidence for preferring to extract semantic information from a syntactic analysis of a dictionary definition rather than directly from the definition string itself when the information to be extracted is found in the differentiae. We present examples of how very complex information can be extracted from the differentiae of the definition using structural analysis patterns, and why string patterns would fail to do the same.", "references": ["e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "d6d84b46dbe786becb7e854dc195f210bc1410c3", "c8bca113c9cfecc103d80b5c0d0d2e2b6f13e69f", "6c51b5d1cb194c2ebf1876e2bb4d9f610973b6b1", "43865ad56b3364b39ae3badf1fc212547292b335", "6436696a7c81ac3beb6d264e31d6bdeaa73ce73e", "358e36ab58fc7f7f1738cc37ebf2a4be7c04cd23", "3d5eabf859c29670e3b91adf6f0193a8d7bc4981", "7aaf5f7e51509d27faf3d578f8dc635c73f169c0"], "page_rank": 0.00016420361247947453}, {"id": "7c871b62aa17e7f2af9c0c35ca85245d9e3a3822", "title": "An empirical approach to circularity in dictionary definitions", "authors": ["Nicoletta Calzolari"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"An empirical approach to circularity in dictionary definitions\" by Nicoletta Calzolari", "references": [], "page_rank": 0.00016420361247947453}, {"id": "73b75154b319e3352895c4cb387ab76095ceebfb", "title": "Automatically Deriving Structured Knowledge Bases From On-Line Dictionaries", "authors": ["Stephen D. Richardson", "Lucy Vanderwende", "William B. Dolan"], "date": 1993, "abstract": "keywords: computational lexicography; lexical knowledge bases We describe an automated strategy which exploits on -line dictionaries to construct a richly-structured lexical knowledge bas e. In particular, we show how the Longman Dictionary of Contemporary English (LDO CE) can be used to build a directed graph which captures semantic asso ciati ns between words. The result is a huge and highly interconnected network f words linked by arcs labeled with semantic relations such as Hypernym, Part_of, Location, and Purpose. We argue that this knowledge base provides much more detailed information about word meanings than can be obtaine d using standard lexical lookup procedures or by relying on statistical meas ure of semantic associations among words. 1We would like thank the other members of the Micros oft Natural Language group: Joseph Pentheroudakis, Karen Jensen, George Heidorn, and Diana Peterson.", "references": ["6c6a9548e18e372fd381794f18dcee518c9f506d", "dbfd191afbbc8317577cbc44afe7156df546e143", "e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "43865ad56b3364b39ae3badf1fc212547292b335", "bbca172c218e8a3904ad99e259b755cb4b24a9d7", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "c2d7252950d51f67fb4613c6a364be48baa709b5", "50ae0b1667ac1089d5ddbb5d52902ca5e484ff00", "31f9017a0e1922c7f7de2a81f4d5f6ab24a04097", "b4ea3e2e980010703ff12466cd7413ee78d59f5e"], "page_rank": 0.00016420361247947453}, {"id": "710f79804a50d93521a15a1ce9663d1f221ea7f4", "title": "Regression for Sentence-Level MT Evaluation with Pseudo References", "authors": ["Joshua Albrecht", "Rebecca Hwa"], "date": 2007, "abstract": "Many automatic evaluation metrics for machine translation (MT) rely on making comparisons to human translations, a resource that may not always be available. We present a method for developing sentence-level MT evaluation metrics that do not directly rely on human reference translations. Our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy ( pseudo references). Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances.", "references": ["f6881f51393704cda399733078d571d964e909cd", "4d3f45009baa00e8f6d4e9ba4dd788a512b82c4a", "443516aeb2819d4d362ffe7d5418a54e5427a016", "ea41a2d1b3c846ad8325cad830f24032aec375e9", "fde1630afaa9fd1a4568f50c8f1f2303bfd3cd3f", "fc077ee1308620505f16c55ac0fddb5a9de40496", "cb826a3899752b796f14df1c50378c64954a6b0a", "7a2afdf19a760a8c9419d00edfa336b42001b2ab", "d7da009f457917aa381619facfa5ffae9329a6e9", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f"], "page_rank": 0.0002463054187192118}, {"id": "e381520518153da5e700235d5142e771055aeceb", "title": "ASCRIBING PLANS TO AGENTS Preliminary Report", "authors": ["Kurt Konolige", "Martha E. Pollack"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"ASCRIBING PLANS TO AGENTS Preliminary Report\" by Kurt Konolige et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "24e5031c567464b858f86c6ca46331f153399bc6", "title": "New Figures of Merit for Best-First Probabilistic Chart Parsing", "authors": ["Sharon A. Caraballo", "Eugene Charniak"], "date": 1998, "abstract": "Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars.", "references": ["fe3788f2de079b08a6b2b130bba17ec0429c7f04", "403c1578fbab8bf17fcb69409e6dff3f9ecdc23e", "92426670b72300c251bf7a63d6761307c75e1748", "7b462310672d53386719224686f3bd0a0ce9fad1", "1ee14b826073af5f39757e8526d44499900c3dea", "8f6a71452a7e81cebd6537879974aff5655de4f4", "aad8c5ae265e8f645101245afb9d9c9cdf40b4ca", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "ac3947d7f4b33a773d1cd2e88c8937eab3205af5", "eb34c9981c50bde33d165a7f5faeb72018aa4d09"], "page_rank": 0.0002463054187192118}, {"id": "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "title": "Inside-Outside Reestimation from Partially Bracketed Corpora", "authors": ["Fernando C Pereira", "Yves Schabes"], "date": 1992, "abstract": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.", "references": ["9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74", "c74111b54850727e48b180e1a8d76cfdf18df902", "d607ed3aa8a1762e06988329aeb0c05b997023db", "8cf9b7c08655dadad0cad00771f3c9670181004e", "617241818e8ddd6edcb4ee7682992673c18c6f3d", "6c79a9bb8f885050cad70b4c69e016b186ffa538", "e7470c416e13fcf91396cc29fa43a7903ea6d519", "7689778171dc100bb636fc0e4e2ce4063967d3c9", "1d19708290ef3cc3f43c2c95b07acdd4f52f5cda", "f71ab539337bb1e496df363a3cc2a66849117314"], "page_rank": 0.0004926108374384236}, {"id": "54531c0ce77465573dcc3fe0ffef1bea5c3837a6", "title": "Planning English Sentences", "authors": ["Douglas E. Appelt"], "date": 1988, "abstract": "From the Publisher: \nThis book is an investigation into the problems of generating natural language utterances to satisfy specific goals in the speaker's mind. It is thus an ambitious and significant contribution to research on language generation in artificial intelligence.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b", "title": "Interpolated estimation of Markov source parameters from sparse data", "authors": ["Frederick Jelinek"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"Interpolated estimation of Markov source parameters from sparse data\" by Frederick Jelinek", "references": [], "page_rank": 0.0004926108374384236}, {"id": "a62bfef14defb2b8d7e22316f7a34657c265861a", "title": "Interactive concept-learning and constructive induction by analogy", "authors": ["Luc De Raedt", "Maurice Bruynooghe"], "date": 2004, "abstract": "The available concept-learners only partially fulfill the needs imposed by the learning apprentice generation of learners. We present a novel approach to interactive concept-learning and constructive induction that better fits the requirements imposed by the learning apprentice paradigm. The approach is incorporated in the system Clint-Cia, which integrates several user-friendly features into one working whole: it is interactive, generates examples, shifts its bias, identifies concepts in the limit, copes with indirect relevance, recovers from errors, performs constructive induction and invents new concepts by analogy to previously learned ones.", "references": ["baa4ce887cb65edf94fafcdf50cf9093313ef79a", "5d0b895738ac5e0236dbedbb5abfe4c6cba4dbbe", "bfb41c22445344a7e0adfeae7f73958399212ef0", "b2e3e096c613f49bcaca3ce9baf019621eea7ff8", "eb2f539a17487db2c93785214da2fc7a67a57840", "886b9cd28b77e1a73da7112b95196e661ae8d5a1", "9e65b35e8bc74cf87c93d1c396a7200b6b9fd5e0", "2c6090c519124977ef63b47bd590aab65c311c82", "d3c615f09b36ded265b08962cb528fdb755c105a", "ff0ad997632bacc7fb9336751596c5d1009e5083"], "page_rank": 0.0001231527093596059}, {"id": "3e36f22685b8d3db73532d3104b325cea5288a66", "title": "Algorithmic Program Debugging", "authors": ["Ehud Y. Shapiro"], "date": 1983, "abstract": "The thesis lays a theoretical framework for program debugging, with the goal of partly mechanizing this activity. In particular, we formalize and develop algorithmic solutions to the following two questions: (1) How do we identify a bug in a program that behaves incorrectly? (2) How do we fix a bug, once one is identified? \nWe develop interactive diagnosis algorithms that identify a bug in a program that behaves incorrectly, and implement them in Prolog for the diagnosis of Prolog programs. Their performance suggests that they can be the backbone of debugging aids that go far beyond what is offered by current programming environments. \nWe develop an inductive inference algorithm that synthesizes logic programs from examples of their behavior. The algorithm incorporates the diagnosis algorithms as a component. It is incremental, and progresses by debugging a program with respect to the examples. The Model Inference System is a Prolog implementation of the algorithm. Its range of applications and efficiency is comparable to existing systems for program synthesis from examples and grammatical inference. \nWe develop an algorithm that can fix a bug that has been identified, and integrate it with the diagnosis algorithms to form an interactive debugging system. By restricting the class of bugs we attempt to correct, the system can debug programs that are too complex for the Model Inference System to synthesize.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "4fb573e6ead5b1986b6afddd5148b16f36f7735a", "title": "Planning Text for Advisory Dialogues", "authors": ["Johanna D. Moore", "C{\\'e}cile Paris"], "date": 1989, "abstract": "Explanation is an interactive process requiring a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates.", "references": ["44ff6701d0ca7685b4d51f153ef8888fe2c2de2d", "cf1db9d4b45752edb368855224e0572d41c6a169", "5b35af11c60b5acf403f2f671396587895364491", "b1891bc6fcf24096fed0354213a49396bd729ef2", "908b0130d8563afb96e29e743a8f638fb3312ba9", "e556f546574f82d06bb159aec3ac2f862a1485cd", "79d87845d01eafd43264ec640d572a5cfa6c68a5", "169256845d0c4c4d8b0d95cf76fb3b62ad3b0a59", "9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "377e186570d22fe66b6381bf6b672a8691021eca"], "page_rank": 0.00016420361247947453}, {"id": "7fad831935254c9c9ec39ffb03752a3f736c3f76", "title": "A Memory-Based Approach to Learning Shallow Natural Language Patterns", "authors": ["Shlomo Argamon", "Ido Dagan", "Yuval Krymolowski"], "date": 1998, "abstract": "Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.", "references": ["1cf6f1209d29c151b693861e083850f1b385c595", "68805e30441ed92575459ee40b3bdc2774b6bc3d", "6082156a2270b6567ebdc85f6570ffacc3d903c2", "f5bb34e38e3403054d4396fc48882f02eae1ffcc", "fed3002240ebfcfbad4ff472748f46191e17e4e0", "75288ecdeb29f093190c1a0130be2d24619238ed", "3ed17a1114e2dc48597ab17cc8d5234006f525c9", "da838db79e7593018894ada44db35eee670941d6", "ad120a6635aeb1f0fbf798e5a1b97b65e25b716e", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036"], "page_rank": 0.0004926108374384236}, {"id": "f631121deb8c26d1fff60799b2306e85852aaff5", "title": "Implementation of the SMART Information Retrieval System", "authors": ["Chris Buckley"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Implementation of the SMART Information Retrieval System\" by Chris Buckley", "references": [], "page_rank": 0.00016420361247947453}, {"id": "218a899ba7ff3094c0fc871b9605d8ff4f529336", "title": "Learning Logical Definitions from Relations", "authors": ["J. R. Quinlan"], "date": 1990, "abstract": "This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "957ddc4ca4c8f7be5efba176716a4ff6b23d80d5", "title": "Automatic Labeling of Semantic Roles", "authors": ["Daniel Gildea", "Dan Jurafsky"], "date": 2000, "abstract": "We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.", "references": ["7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "c07c690601169dc1155b2dcf90941b32f606a9d4", "2a5e619f2c5f4220438b1357e596db5b1578398d", "d16b4d012ca313d00fd3b1ed7d57a5a51d747bc5", "547f23597f9ec8a93f66cedaa6fbfb73960426b1", "fed64bc406adaef9d993f34363f6f1818bbb118e", "e408cfd6b8433fea02e2fd13d0c75cead0dc8023", "45de5bfc4895b1cdb8177cf312327c60ca513099", "ff22ac5921ed92cf442fa67329cedb0eea0ca963"], "page_rank": 0.0002463054187192118}, {"id": "74999bc24a4ebf42577735fb158068718574860a", "title": "Statistical language understanding using frame semantics", "authors": ["Daniel Gildea"], "date": 2001, "abstract": "Statistical Language Understanding Using Frame Semantics", "references": ["a1066659ec1afee9dce586f6f49b7d44527827e1", "51f393365ad74d7e20db44325d96cd65cfea0e36", "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "01d88ddd3e7a9c5af2acc91a05734b7f066908dc", "9f9e479b3b330fbbfde375a23c2c759f91ca0f75", "5752b8dcec5856b7ad6289bbe1177acce535fba4", "b888cae7e6e288b108f9d119fc23b84b4d447029", "351752e0080d7635aef227b5d0bd6461cf0b14bd", "a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd", "e408cfd6b8433fea02e2fd13d0c75cead0dc8023"], "page_rank": 0.0002463054187192118}, {"id": "ec0f5460a2943cacaa67eb16483adea9b6ff2220", "title": "Foundations of Logic Programming", "authors": ["John Wylie Lloyd"], "date": 1984, "abstract": "This is the second edition of an account of the mathematical foundations of logic programming. Its purpose is to collect, in a unified and comprehensive manner, the basic theoretical results of the field, which have previously only been available in widely scattered research papers. In addition to presenting the technical results, the book also contains many illustrative examples and problems. The text is intended to be self-contained, the only prerequisites being some familiarity with PROLOG and knowledge of some basic undergraduate mathematics. The material is suitable either as a reference book for researchers or as a textbook for a graduate course on the theoretical aspects of logic programming and deductive database systems.", "references": ["3430d7929dc9c0c9278dca858e785ee3d89ce2b0", "a587b0bb6b1b2399f14d1a879d90e6a32ddfb2dc", "1f3419ce809adf58e38d96c03a3d7271e9bc312c", "7dbf4c5424c676f7e04010a0a6678cab40e71332", "b025fbd8a0c44d421bb379a93c8dad935856def2", "79c0f70569a8c5c14ddc0951a73ece06dde6136f", "3c0ec2f83739bc4844f2ea3e594146942a839815", "7851bdf7d804da4d6a67be3d8c48a04757561dd6", "89fa5c754594418549e818cf73fbfe7f98373e89", "57fb4b0c63400dc984893461b1f5a73244b3e3eb"], "page_rank": 0.0001231527093596059}, {"id": "3ed7dfac6641dc4bbd1ff4d6abedceae57d6ddf4", "title": "Partial parsing via finite-state cascades", "authors": ["Steven P. Abney"], "date": 1996, "abstract": "Finite state cascades represent an attractive architecture for parsing unrestricted text. Deterministic parsers specified by finite state cascades are fast and reliable. They can be extended at modest cost to construct parse trees with finite feature structures. Finally, such deterministic parsers do not necessarily involve trading off accuracy against speed \u2014 they may in fact be more accurate than exhaustive search stochastic context free parsers.", "references": ["1ef69ff7760477d26d6c8d06f07dd021d60f9413", "cb34e27e0900c691c83950543b6e9edf2858fff8", "4c38b7b68393e53e712341e16954064d2ba13319", "9dd4169b8db81701ee68eb2f30c9e03bb4951310", "834b4d23d539bd8e5f7e37cdc2baa3d7e15b9936", "56d7826f3afaa374077f87ca3529709b1ca7e044", "20ba61450c943a8b980f52376f8bbf95a5ef7418", "0f7bfb9d2739a53d15290a7145e3cede221eec5d", "b9167023806d16b8b76f5be04964a704e9709771"], "page_rank": 0.00016420361247947453}, {"id": "742b1a4bdd944e8685b4eef76cbdfb7d2a52cd01", "title": "The SMART Retrieval System-Experiments in Automatic Document Retrieval", "authors": ["Gerard Salton", "Michael J Mcgill"], "date": 1971, "abstract": "This invention relates to di-olefinically unsaturated compounds which contain two quaternary ammonium moieties. Polymers and copolymers of these polyfunctional compounds are characterized by high charge density, and find application as flocculants, paper sizes, and electroconductive coatings.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8b49fce77d75f671e5f41326e85794501835b0fc", "title": "An algorithm for connected word recognition", "authors": ["John S. Bridle", "Michael D. Brown", "Richard M. Chamberlain"], "date": 1982, "abstract": "The principles of an efficient one-pass dynamic programming whole-word pattern matching algorithm for the recognition of spoken sequences of connected words are described. Particular attention is given to the technique for keeping track of word-sequence decisions, which may be constrained by a finite-state syntax. Some extensions of the technique are discussed.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "556d76ca9fb4d2f53959d3e211cdba90ca0647ce", "title": "Hand image segmentation using sequential-image-based hierarchical adaptation", "authors": ["Akira Utsumi", "Jun Ohya"], "date": 1997, "abstract": "Two methods to extract a moving target region from a series of images are presented. Pixel value distributions for both the target object and background region are estimated for each pixel with roughly extracted moving regions. Using the distributions, stable target extraction is performed. In the first method, the distributions are approximated with Gaussian distribution functions and the probability of a pixel being associated with the target object is calculated. In the second method, a Markov random field model is applied to perform region segmentation on regularized input images using the estimated pixel value distributions. The texture parameters for the target object region can be calculated from the estimated pixel value distributions. Experimental results obtained by these two methods using hand motion images are presented.", "references": ["9d8f0cfb7a79b698f2d050a22fbcec248c72b8c1", "dc67a48a9bb2ca8037f47245667a67601d2c5366", "5b45f31aaf7b3b3deb4b3867d4196a36ec2682c1", "7c4f9d0b2f878e0b03c6895aaaed94bdb4811651", "7862fc4099b31f0a21fcf681403c2e594c2dd5bc", "ee253d0cee399bbe1a692a819ae49771a584b49e", "d7986359bb4c91b10da1015f11ade69e89e212c0"], "page_rank": 0.0004926108374384236}, {"id": "8c369eb4534705488e7060da949273bfb499a67a", "title": "Kinematics and structure of a rigid object from a sequence of noisy images", "authors": ["Ted J. Broida", "Rama Chellappa"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"Kinematics and structure of a rigid object from a sequence of noisy images\" by Ted J. Broida et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "47b6ffcd715eebd5418d42b17f4e64da8a6e9747", "title": "A survey of the hough transform", "authors": ["John Illingworth", "Josef Kittler"], "date": 1988, "abstract": "We present a comprehensive review of the Hough transform, HT, in image processing and computer vision. It has long been recognized as a technique of almost unique promise for shape and motion analysis in images containing noisy, missing, and extraneous data but its adoption has been slow due to its computational and storage complexity and the lack of a detailed understanding of its properties. However, in recent years much progress has been made in these areas. In this review we discuss ideas for the efficient implementation of the HT and present results on the analytic and empirical performance of various methods. We also report the relationship of Hough methods and other transforms and consider applications in which the HT has been used. It seems likely that the HT will be an increasingly used technique and we hope that this survey will provide a useful guide to quickly acquaint researchers with the main literature in this research area.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "87360f5df38866a171f96c046dc9f6feb755745f", "title": "The General Distribution of the Error Rate of a Classification Procedure With Application to Logistic Regression Discrimination", "authors": ["Terence W O'Neill"], "date": 1980, "abstract": "Abstract The large-sample distribution of the error rate of an arbitrary estimator of the optimal classification rule is given. The asymptotic distribution of the logistic regression estimator is found. These results are used to show that the efficiency of logistic regression classification in some nonnormal cases is low. This suggests that maximum likelihood discrimination should be used whenever possible.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7e983c395a26710bed4fb2577d166a1811ba6e44", "title": "A review of optimality of multivariate tests", "authors": ["Ashis SenGupta"], "date": 1991, "abstract": "We consider unrestricted (unordered) parametric hypotheses for multivariate or multiparameter distributions and review some optimality aspects, both exact and asymptotic, for testing of hypotheses possibly in the presence of nuisance parameters. The aim is not to provide an exhaustive review but to represent the widely used classical approaches, expose some promising recent ones and present some interesting practical problems requiring the development of new methods.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a5ddb09310eee52d97074afc496b16df238ae5b9", "title": "Matching Two Perspective Views", "authors": ["Juyang Weng", "Narendra Ahuja", "Thomas S. Huang"], "date": 1992, "abstract": "A computational approach to image matching is described. It uses multiple attributes associated with each image point to yield a generally overdetermined system of constraints, taking into account possible structural discontinuities and occlusions. In the algorithm implemented, intensity, edgeness, and cornerness attributes are used in conjunction with the constraints arising from intraregional smoothness, field continuity and discontinuity, and occlusions to compute dense displacement fields and occlusion maps along the pixel grids. The intensity, edgeness, and cornerness are invariant under rigid motion in the image plane. In order to cope with large disparities, a multiresolution multigrid structure is employed. Coarser level edgeness and cornerness measures are obtained by blurring the finer level measures. The algorithm has been tested on real-world scenes with depth discontinuities and occlusions. A special case of two-view matching is stereo matching, where the motion between two images is known. The algorithm can be easily specialized to perform stereo matching using the epipolar constraint. >", "references": [], "page_rank": 0.0007389162561576354}, {"id": "b3ec4ceea040b7b4129ee5d71b4f95539bf876b7", "title": "A Robust Technique for Matching two Uncalibrated Images Through the Recovery of the Unknown Epipolar Geometry", "authors": ["Zhengyou Zhang", "Rachid Deriche", "Olivier D. Faugeras", "Quang-Tuan Luong"], "date": 1995, "abstract": "Abstract This paper proposes a robust approach to image matching by exploiting the only available geometric constraint, namely, the epipolar constraint. The images are uncalibrated, namely the motion between them and the camera parameters are not known. Thus, the images can be taken by different cameras or a single camera at different time instants. If we make an exhaustive search for the epipolar geometry, the complexity is prohibitively high. The idea underlying our approach is to use classical techniques (correlation and relaxation methods in our particular implementation) to find an initial set of matches, and then use a robust technique\u2014the Least Median of Squares (LMedS)\u2014to discard false matches in this set. The epipolar geometry can then be accurately estimated using a meaningful image criterion. More matches are eventually found, as in stereo matching, by using the recovered epipolar geometry. A large number of experiments have been carried out, and very good results have been obtained. Regarding the relaxation technique, we define a new measure of matching support, which allows a higher tolerance to deformation with respect to rigid transformations in the image plane and a smaller contribution for distant matches than for nearby ones. A new strategy for updating matches is developed, which only selects those matches having both high matching support and low matching ambiguity. The update strategy is different from the classical \u201cwinner-take-all\u201d, which is easily stuck at a local minimum, and also from \u201closer-take-nothing\u201d, which is usually very slow. The proposed algorithm has been widely tested and works remarkably well in a scene with many repetitive patterns.", "references": ["f0261505e0086c33c03035e24963c9481896e1e1", "958ebd4eacadc18d539eec5a3f009482d5792d74", "a5ddb09310eee52d97074afc496b16df238ae5b9", "b87b3c5eea6382351e3c1631448f35d74760bf4f", "22d262e5dfd45776f5616fe405cf153c8466e965", "d7730295185f1e98ef087525e2dc1518114a9306", "dab947b341bc642e4588ec59d938c6ce6c5b5405", "449cac591cd5418ed2896c5a49e9c77048e58bb5", "52e11059695b96c88a3ffa6c67635d7c1edf4b8a", "7ac7a75e63d28d89f5e6f934639b6ed436927aa2"], "page_rank": 0.0002463054187192118}, {"id": "67eac618325d5f5f31ce17922d51fad995a57749", "title": "An efficient method for contour tracking using active shape models", "authors": ["Adam Michael Baumberg", "David C. Hogg"], "date": 1994, "abstract": "There has been considerable research interest recently, in the areas of real time contour tracking and active shape models. This paper demonstrates how dynamic filtering can be used in combination with a modal-based flexible shape model to track an articulated non-rigid body in motion. The results show the method being used to track the silhouette of a walking pedestrian in real time. The active shape model used was generated automatically from real image data and incorporates variability in shape due to orientation as well as object flexibility. A Kalman filter is used to control spatial scale for feature search over successive frames. Iterative refinement allows accurate contour localisation where feasible. The shape model incorporates knowledge of the likely shape of the contour and speeds up tracking by reducing the number of system parameters. A further increase in speed is obtained by filtering the shape parameters independently.>", "references": ["cdb0a19d9ee3a36a2954bf991f817f43aa645127", "a28e2fe4ccde9a0e14123c126459be3e01ebe40f", "b992336681135bd3e013a2c12b01dbbc3a469af2", "d7f4e5d48bb21c2b0c5672688ea8a8072f6a35f0", "f244c6d0f9abe7564a48653eb6726c814bb5fd27", "d741141eab521b66b979848f33a93ecf38411b94", "22b125851acd9ab6d16485104eb554a13367b7a7", "b1d308a92a166900351c1e460ac6fc8463a9a002", "bd13c71fa6a6242e702041d54eadd7f4cb19108e"], "page_rank": 0.0004926108374384236}, {"id": "4caff7fdf821594f4ffe98fda02c62df92d07291", "title": "Combined use of unsupervised and supervised learning for dynamic security assessment", "authors": ["Yoh-Han Pao", "Dejan J. Sobajic"], "date": 1991, "abstract": "It is highly desirable that the security and stability of electric power systems after exposure to large disturbances be assessable. In this connection, the critical clearing time (CCT) is an attribute which provides significant information about the quality of the post-fault system behavior. It may be regarded as a complex mapping of the prefault, fault-on, and post-fault system conditions in the time domain. Y.-H. Pao and D.J. Solajic (1989) showed that a feedforward neural network can be used to learn this mapping and successfully perform under variable system operating conditions and topologies. In that work the system was described in terms of some conventionally used parameters. In contrast to using those pragmatic features selected on the basis of the engineering understanding of the problem, the possibility of using unsupervised and supervised learning paradigms to discover what combination of raw measurements are significant in determining CCT is considered. Correlation analysis and Euclidean metric are used to specify interfeature dependencies. An example of a 4-machine power system is used to illustrate the suggested approach. >", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4b82bce6a021aea889a1e4f734277ffa73b1595b", "title": "Normal Discrimination with Unclassified Observations", "authors": ["Terence W O'Neill"], "date": 1978, "abstract": "Abstract Fisher's linear discriminant rule may be estimated by maximum likelihood estimation using unclassified observations. It is shown that the ratio of the relevant information contained in unclassified observations to that in classified observations varies from approximately one-fifth to two-thirds for the statistically interesting range of separation of the populations. Thus, more information may be obtained from large numbers of inexpensive unclassified observations than from a small classified sample. Also, all available unclassified and classified data should be used for estimating Fisher's linear discriminant rule.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "31f9017a0e1922c7f7de2a81f4d5f6ab24a04097", "title": "Combining NLP and statistical techniques for lexical acquisition", "authors": ["Roberto Basili", "Maria Teresa Pazienza", "Paola Velardi"], "date": 1992, "abstract": "The growing availability of large on-line corpora encourages the study of word behaviour directly from accessible raw texts. However the methods by which lexical knowledge should be extracted from plain texts are still matter of debate and experimentation. In this paper it is presented an integrated tool for lexical acquisition from corpora, ARIOSTO, based on a hybrid methodology that combines typical NLP techniques, such as (shallow) syntax and semantic markers, with numerical processing.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2", "title": "Automatic Pattern Recognition: A Study of the Probability of Error", "authors": ["Luc Devroye"], "date": 1988, "abstract": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >", "references": ["0efb841403aa6252b39ae6975c1cc5410554ef7b", "80acea2684b1a04ec06360cf7966ebafd311cfc3", "b0eae4c19136a014517a57faa414324079698ffa", "b6c835ed3f011f1221006586989a73998a75fbcb", "e60ccd8a6aa8759f9af77796526e451bdbe899eb", "0c4a39e43d22b9e39507cde0ccae50859d447a7e", "f53c22ed4f130cf1e703af4cb212de4518d8b405", "942f37ae066e1db5b6660241c84e689d2f6b5c20", "66ee4bb5d704856f43d751c259a7b5de9c77b764", "a4f9442c9c8c835e069dc9cb812e2748541590e2"], "page_rank": 0.00018062397372742197}, {"id": "107ed240fd5a94aa84d9f6297fd6e43a88008755", "title": "Version spaces: an approach to concept learning.", "authors": ["Tom Michael Mitchell"], "date": 1979, "abstract": "Abstract : A method is presented for learning general descriptions of concepts from a sequence of positive and negative training instances. This method involves examining a predetermined space or language of possible concept descriptions, finding those which are consistent with the observed training instances. Rather than use heuristic search techniques to examine this concept description space, the subspace (version space) of all plausible concept descriptions is represented and updated with each training instance. This version space approach determines all concept descriptions consistent with the training instances, without backtracking to reexamine past training instances or previously rejected concept descriptions. Proofs are given for the correctness of the method for representing version spaces, and of the associated concept learning algorithm, for any countably infinite concept description language. Empirical results obtained from computer implementations in two domains are presented. The version space approach has been implemented as one component of the Meta-DENDRAL program for learning production rules in the domain of chemical spectroscopy. Its implementation in this program is described in detail.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "eb34c9981c50bde33d165a7f5faeb72018aa4d09", "title": "Two Experiments on Learning Probabilistic Dependency Grammars from Corpora", "authors": ["Glenn Carroll", "Eugene Charniak"], "date": 1992, "abstract": "We present a scheme for learning probabilistic dependency grammars from positive training examples plus constraints on rules. In particular, we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.", "references": ["ddbef7c7990c4e412056df98ece9ef0df04b6c9a", "7f23a4a3aedc8f7cc00cc843e829862fe83b148a", "835b96a6c4a6926ebe5ad1be29d2c538d177b5a9", "617241818e8ddd6edcb4ee7682992673c18c6f3d", "9d8b5cefb292f94328b95fa6fae7bf875f4f2343", "6c79a9bb8f885050cad70b4c69e016b186ffa538", "8cf9b7c08655dadad0cad00771f3c9670181004e", "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4"], "page_rank": 0.00016420361247947453}, {"id": "6cf994cc7cec3c76f49f98aa6ded0824187e786d", "title": "Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners", "authors": ["Sarunas Raudys", "Anil K. Jain"], "date": 1991, "abstract": "The effects of sample size on feature selection and error estimation for several types of classifiers are discussed. The focus is on the two-class problem. Classifier design in the context of small design sample size is explored. The estimation of error rates under small test sample size is given. Sample size effects in feature selection are discussed. Recommendations for the choice of learning and test sample sizes are given. In addition to surveying prior work in this area, an emphasis is placed on giving practical advice to designers and users of statistical pattern recognition systems. >", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7a2afdf19a760a8c9419d00edfa336b42001b2ab", "title": "BLANC: Learning Evaluation Metrics for MT", "authors": ["Lucian Vlad Lita", "Monica Rogati", "Alon Lavie"], "date": 2005, "abstract": "We introduce BLANC, a family of dynamic, trainable evaluation metrics for machine translation. Flexible, parametrized models can be learned from past data and automatically optimized to correlate well with human judgments for different criteria (e.g. adequacy, fluency) using different correlation measures. Towards this end, we discuss ACS (all common skip-ngrams), a practical algorithm with trainable parameters that estimates reference-candidate translation overlap by computing a weighted sum of all common skip-ngrams in polynomial time. We show that the BLEU and ROUGE metric families are special cases of BLANC, and we compare correlations with human judgments across these three metric families. We analyze the algorithmic complexity of ACS and argue that it is more powerful in modeling both local meaning and sentence-level structure, while offering the same practicality as the established algorithms it generalizes.", "references": ["fd31bf60e450e06967c6d280b50d2c44231d9b30", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "b038147589432947cd47b2c75d46e43613a1a91b", "d7da009f457917aa381619facfa5ffae9329a6e9", "f7e5e8dcfe85e4fd431eb29a2370ab8383c3b718", "ebe4dabf2bd0f798d827881bc206dc50fa281c04", "9ca86842aad16797d0fe0323358f3beb1ac6a5c6", "f9e0a6f47f450a80d3db5ca13e7f3f51521108e1", "15315ed05451c88f83c50d56a66a0b85517c5f4f", "72c58a58ba1684dc8337c8e64e0b5dacfc94499e"], "page_rank": 0.0002463054187192118}, {"id": "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b", "title": "On the Density of Families of Sets", "authors": ["Norbert Sauer"], "date": 1972, "abstract": "If T is a family of sets and A some set we denote by T \u2229 A the following family of subsets of A: T \u2229 A = {F \u2229 A; F \u03f5 T}. P. Erdos (oral communication) transmitted to me in Nice the following question: Is it true that if T is a family of subsets of some infinite set S then either there exists to each number n a set A \u2282 S with |A| = n such that |T \u2229 A| = 2n or there exists some number N such that |T \u2229 A| \u2a7d |A|c for each A \u2282 S with |A| \u2a7e N and some constant c? In this paper we will answer this question in the affirmative by determining the exact upper bound. (Theorem 2).1", "references": [], "page_rank": 0.0002627257799671592}, {"id": "ac3947d7f4b33a773d1cd2e88c8937eab3205af5", "title": "Statistical Parsing of Messages", "authors": ["Mahesh Chitrao", "Ralph Grishman"], "date": 1990, "abstract": "The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold - firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language.", "references": ["b3792b3382711d1de777c6de1a68acc882144e4f", "a2bb4898f96296843f87e18613edc9d4bc8f6a93", "6b0773366c1de1b01a1d2bf8bbae9f20ad1adfe5", "5c0f99e72bd539171a4dedcffd0e0a424ad3aad6", "affcf19551b01c4c8009d061750700d91c2f79e9", "645ae1b0a3b0e18cfde3f2d1845bdfa70be736f1", "2c4abc117b0617c131b9e383697dc81232ed6d85"], "page_rank": 0.0004926108374384236}, {"id": "cb826a3899752b796f14df1c50378c64954a6b0a", "title": "Statistical Significance Tests for Machine Translation Evaluation", "authors": ["Philipp Koehn"], "date": 2004, "abstract": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.", "references": ["1f12451245667a85d0ee225a80880fc93c71cc8b", "d7da009f457917aa381619facfa5ffae9329a6e9", "e0cf7771a02921f9d4725f973a01c240d1a20634", "a6ed57eabb350b220836a910045c45d069817e98", "223dcd0e44532fc02444709e61327432c74fe46d", "e2a68774f92d1e894cbbbef2c819e4592990eb4b", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "f013d1eaf0b5e089281c64c8a75d2b7ec390891d", "ab7b5917515c460b90451e67852171a531671ab8", "e4d3bf856ce5259360a8033d50abcdd22873bcd6"], "page_rank": 0.0002463054187192118}, {"id": "ff0ad997632bacc7fb9336751596c5d1009e5083", "title": "Integrity Constraints and Interactive Concept-Learning", "authors": ["Luc De Raedt", "Maurice Bruynooghe", "Bern Martens"], "date": 1991, "abstract": "Abstract We show how our interactive concept-learner Clint can be enhanced with capabilities to handle integrity constraints. In this version of Clint, the user may supply general first order logic clauses as integrity constraints to the system. The system will then assure that these constraints are satisfied by the learned knowledge base.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "baa4ce887cb65edf94fafcdf50cf9093313ef79a", "title": "Constructive Induction by Analogy", "authors": ["Luc De Raedt", "Maurice Bruynooghe"], "date": 1989, "abstract": "ABSTRACT Concept-learners can be viewed as problem-solvers : they receive as input a set of examples and produce as output a consistent concept-description. In this paradigm, we derive from each solved concept-learning problem a second-order schema, and apply it in analoguous concept-learning situations in order to learn and/or invent new concepts. In this way, a change of representation is achieved and the learning process itself is learned.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "8f6a71452a7e81cebd6537879974aff5655de4f4", "title": "Context-Sensitive Statistics For Improved Grammatical Language Models", "authors": ["Eugene Charniak", "Glenn Carroll"], "date": 1994, "abstract": "We develop a language model using probabilistic context-free grammars (PCFGs) that is \"pseudo context-sensitive\" in that the probability that a nonterminal N expands using a rule r depends on N's parent. We give the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction.", "references": ["d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "4d286045910b498f848a5581f24b7b1c7871a031", "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "3a8de92b304729f15d9bd6c3d22a56ab9b31e212", "d64cae6538bb3b8e595007585477a9fdef106602", "b84276fe751ca4f1389549281383b151a746107b", "7d6b34002d6dab558943f55e37317a572600393b", "fe3788f2de079b08a6b2b130bba17ec0429c7f04", "d811628d5f8230720c13ee9bb844badc3c3b6ae2", "025464b73f805e76689a7a20a48a9e9c0f4ff3ef"], "page_rank": 0.00016420361247947453}, {"id": "f71ab539337bb1e496df363a3cc2a66849117314", "title": "Probabilistic Representation of Formal Languages", "authors": ["Taylor L. Booth"], "date": 1969, "abstract": "The problem of assigning a probability to each string of a language L(G) generated by a grammar G is considered. Two methods are considered. One method assigns a probability to each production associated with G and the other assigns the probabilities on the basis of particular features of the language. Several necessary conditions that must be satisfied by these probability assignment techniques if they are to be consistant are presented. The problem of recognizing languages is also considered. It is shown that under some conditions it is possible to recognize a non finitestate language with a finite state acceptor if one is willing to accept a small probability of making an error.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "377e186570d22fe66b6381bf6b672a8691021eca", "title": "Speech Acts and Rationality", "authors": ["Philip R. Cohen", "Hector J. Levesque"], "date": 1985, "abstract": "This paper derives the basis of a theory of communication from a formal theory of rational interaction. The major result is a demonstration that illocutionary acts need not be primitive, and need not be recognized. As a test case. we derive Searle's conditions on requesting from principles of rationality coupled with a Gricean theory of imperatives. The theory is shown to distinguish insincere or nonserious imperatives from true requests. Extensions to indirect speech acts, and ramifications for natural language systems are also briefly discussed.", "references": ["27a84c9145fff4255aa2a30a556a2900a65916ae", "895dfdabbbd5155bbb559d3f144db6f7c195ce75", "21ff0196e1b0054b5acfb203467609d723d51519", "21eb3100980f7aa04d85713d305c60d05a7e1693", "00b9cdc710201b6593bba8ed7f9c6715ce3a8ac4", "c7f9e423b32ae6523d993062c274be5254559df2", "a0189fbb2bf75baea2826283d4e899e91059e097", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "6a6bfc2d27fe3c3a88d8fd5c21d9c3e2cc93ccaf", "ffa3ba4cb713ab3b2c33c6071d2f74fb0a3a00b6"], "page_rank": 0.0002463054187192118}, {"id": "75288ecdeb29f093190c1a0130be2d24619238ed", "title": "FASTUS: A Finite-state Processor for Information Extraction from Real-world Text", "authors": ["Douglas E. Appelt", "Jerry R. Hobbs", "John Bear", "David J. Israel", "Mabry Tyson"], "date": 1993, "abstract": "Approaches to text processing that rely on parsing the text with a context-free grammar tend to be slow and error-prone because of the massive ambiguity of long sentences. In contrast, FASTUS employs a nondeterministic finite-state language model that produces a phrasal decomposition of a sentence into noun groups, verb groups and particles. Another finite-state machine recognizes domain-specific phrases based on combinations of the heads of the constituents found in the first pass. FASTUS has been evaluated on several blind tests that demonstrate that state-of-the-art performance on information-extraction tasks is obtainable with surprisingly little computational effort.", "references": ["ebc2defb23b4667d460304eabe68fbc8395a829a", "a42397893746da6488f70bfc0d48e66763860f0f", "189b516bcb97aac7645bb57d21ebb83a60b4bc0d", "c1c5d946e819b65f5823072b46a6905e8d677f55", "ff7180abf889a4a55b09d43200bbc696b8bfaf1c", "2a99c350f73f54ef78493481f533dc1b9d8455a1"], "page_rank": 0.0001231527093596059}, {"id": "9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "title": "A Structure For Plans And Behavior", "authors": ["Earl D. Sacerdoti"], "date": 1977, "abstract": "Abstract : This report describes progress to date in the ability of a computer system to understand and reason about actions. A new method of representing actions within a computer's memory has been developed, and this new representation, called the \"procedural net,\" has been employed in developing new strategies for solving problems and monitoring the execution of the resulting solutions. A set of running computer programs, called the NOAH (Nets Of Action Hierarchies) system, embodies this representation. Its major goal is to provide a framework for storing expertise about the actions of a particular task domain, and to impart that expertise to a human in the cooperative achievement of nontrivial tasks. A problem is presented to NOAH as a statement that is to be made true by applying a sequence of actions in an initial state of the world. The actions are drawn from a set of actions previously defined to the system. NOAH first creates a one-step solution to the problem, then it progressively expands the level of detail of the solution, filling in ever more detailed actions. All the individual actions, composed into plans at differing levels of detail, are stored in the procedural net. The system avoids imposing unnecessary constraints on the order of the actions in a plan. Thus, plans are represented as partial orderings of actions, rather than as linear sequences. The same data structure is used to guide the human user through a task. Since the system has planned the task at varying levels of detail, it can issue requests for action to the user at varying levels of detail, depending on his/her competence and understanding of the higher level actions. If more detail is needed than was originally planned for, or if an unexpected event causes the plan to go awry, the system can continue to plan from any point during execution. In essence, the structure of a plan of actions is as important for problem solving and execution monitoring as the nature of the actions themselves.", "references": [], "page_rank": 0.0003557744937055282}, {"id": "a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd", "title": "Using Subcategorization to Resolve Verb Class Ambiguity", "authors": ["Mirella Lapata", "Chris Brew"], "date": 1999, "abstract": "Levin's (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these ambiguous cases the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context. In others it is not, and an application which wants to recover this information will be forced to rely on some more or less elaborate process of inference. We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference.", "references": ["7d717c80e868db69e1f2225a27fe0f319c643c7b", "9b77ca011896f79d9014704aaa63ecf4ffb3485c", "fed64bc406adaef9d993f34363f6f1818bbb118e", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "7d6775b3b8a3775821a01d2a54ddc471dac6e570", "6f532be9406b0fe10a8d71e0973b19939403268b", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "384720361a8cae1f2bbb87e05215cc58fef74ac1"], "page_rank": 0.00016420361247947453}, {"id": "fed3002240ebfcfbad4ff472748f46191e17e4e0", "title": "Evaluation Techniques for Automatic Semantic Extraction: Comparing Syntactic and Window Based Approaches", "authors": ["Gregory Grefenstette"], "date": 1993, "abstract": "As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words.", "references": ["b132192076c65ee9c16c851728827634991d6868", "4966f2d75734b4abd4ad105b85eff675cb781b5d", "2510562fc1f7ff1eba53731961d3b4c4bc5a5b09", "1574fc4a1454fc21fa6f2e33716b860e7c2164a2", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4471e3117cdac2fae74d305d54b237bb3addd749", "18dde31c2716b0bf10a35f334ed24af4e4e37fa5", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "3de5d40b60742e3dfa86b19e7f660962298492af", "1e5c33ab632310d827155002311e0adceebb6ec0"], "page_rank": 0.0001231527093596059}, {"id": "b9167023806d16b8b76f5be04964a704e9709771", "title": "Analyse syntaxique transformationnelle du francais par transducteurs et lexique-grammaire", "authors": ["Emmanuel Roche"], "date": 1993, "abstract": "A de rares exceptions pres, le cheminement de l'analyse syntaxique automatique suit la creation de modeles de grammaires formelles (gb, hpsg, etc. ) censees refleter les mecanismes internes de la langue. Pour notre part nous avons utilise le formalisme le plus simple et le plus neutre possible, celui des listes, listes que sont les dictionnaires morphologiques et syntaxiques. Ces dictionnaires forment cependant des listes trop importantes pour etre manipulees directement. La notion d'automate, et plus particulierement celle de transducteur, permet de pallier a cet inconvenient en factorisant un tres grand nombre d'informations communes a plusieurs entrees. Nous avons represente par un transducteur un dictionnaire syntaxique de plus de 2000000 d'entrees. Ce dictionnaire, dont la taille est appelee a augmenter tres notablement est appele delsyn, le transducteur qui le representera f-delsyn. Par ailleurs, de telles representations conduisent a des programmes d'analyse extremement simples, l'analyse d'une phrase ph consistant simplement a appliquer la fonction f-delsyn cycliquement a un automate representant ph jusqu'a l'obtention d'un point fixe. L'application cyclique d'une transduction peut en effet effectuer une analyse descendante d'identification de l'element predicatif de la phrase (le verbe en general) puis passer a l'analyse de chacun des arguments de cet element (le sujet et les complements le plus souvent). Mais une transduction peut egalement simuler une analyse de type ascendant qui reconnait des segments de plus en plus grand; enfin nous verrons qu'une transduction peut egalement appliquer des regles de grammaires locales dans une analyse qu'on peut dire transversale par rapport aux deux types precedents", "references": [], "page_rank": 0.00016420361247947453}, {"id": "b888cae7e6e288b108f9d119fc23b84b4d447029", "title": "Towards better integration of semantic predictors in statistical language modeling", "authors": ["Noah Coccaro", "Dan Jurafsky"], "date": 1998, "abstract": "We introduce a number of techniques designed to help integrate semantic knowledge with N-gram language models for automatic speech recognition. Our techniques allow us to integrate Latent Semantic Analysis (LSA), a word-similarity algorithm based on word co-occurrence information, with N-gram models. While LSA is good at predicting content words which are coherent with the rest of a text, it is a bad predictor of frequent words, has a low dynamic range, and is inaccurate when combined linearly with N-grams. We show that modifying the dynamic range, applying a per-word confidence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal testset than a baseline N-gram model.", "references": ["04e71ae0a7b82c7529f2d8e9b4b89ed8184b3f2c", "d8ad76470164e7ac1d374bcfb393983c0113725f", "076fa8d095c37c657f2aff39cf90bc2ea883b7cb", "d3f869c5c74212c68e8879b2e7123394441f0706", "232e66748382ded9d217de554574fbf70df0f6b6", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "2928de5400a920a6a29af41821c680cef5d35f91", "7c294104a5add3010805e93102b3194a613f6454"], "page_rank": 0.00016420361247947453}, {"id": "0f7bfb9d2739a53d15290a7145e3cede221eec5d", "title": "Automatic indexing using selective NLP and first-order thesauri", "authors": ["David A. Evans", "Kimberley Ginther-Webster", "Mary Hart", "Robert G. Lefferts", "Ira Monarch"], "date": 1991, "abstract": "As one approach to automatic indexing, the CLARIT System utilizes selective natural-language processing (NLP) to identify candidate noun phrases in free text and maps them into candidate terms, in a morphologically-normalized form, emphasizing modifier and head relations. Candidate terms are matched against a first-order thesaurus of certified domain-specific terminology. Terms are scored and ranked based on the distribution statistics of the term (and its lexical items) in a document. Terms are weighted, as well, according to their distribution both in a reference domain database and a large, general corpus of English. The result is a tripartite indexing of a document by terms classified as exact (or certified), general, and novel, each ranked for relevance. In an evaluation comparing CLARIT automatic indexing of ten full-text articles in the domain of artificial intelligence to the indexing of two human subjects, it was found that CLARIT performed as well---and in some respects better---than the humans.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "da838db79e7593018894ada44db35eee670941d6", "title": "Pearl: A Probabilistic Chart Parser", "authors": ["David M. Magerman", "Mitchell P. Marcus"], "date": 1991, "abstract": "This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.", "references": ["a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "098de23f08e080bed8a224bf1ad2e504688d3db3", "ac3947d7f4b33a773d1cd2e88c8937eab3205af5", "264e107971a50eaf6dfcbd02e9c3fa8fdb83c20f", "2d30aa623fd96da99a16c0c3bde73f50c92a5c42", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "d24a489657026723e83a712ab7f01d00fdf585e5", "108d05ba4d16e5991dd4b271360441b810767543"], "page_rank": 0.0001231527093596059}, {"id": "351752e0080d7635aef227b5d0bd6461cf0b14bd", "title": "Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar", "authors": ["David Chiang"], "date": 2000, "abstract": "We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.", "references": ["eafd684ab19ded0e5819cd9d5ea326276a86d9ed", "e1b4d6ba9970fb69545d20d6e7882e4d478a812f", "0ffa423a5283396c88ff3d4033d541796bd039cc", "fc0cad9c8449d7858843ed0e13b6564d094c576a", "9aea6cf433a3ad6b845864f81087ce64a6cb5457", "4535a8309c1a84ad885865200de13eaae3ebe754", "01a0f4d9dc810751401360a453fdf2e0438c6735", "2a5e619f2c5f4220438b1357e596db5b1578398d", "d607ed3aa8a1762e06988329aeb0c05b997023db", "01d88ddd3e7a9c5af2acc91a05734b7f066908dc"], "page_rank": 0.00016420361247947453}, {"id": "ee253d0cee399bbe1a692a819ae49771a584b49e", "title": "Multi-camera hand pose recognition system using skeleton image", "authors": ["Akira Utsumi", "Tsutomu Miyasato", "Fumio Kishino"], "date": 1995, "abstract": "We propose a vision-based hand pose recognition system. The system we propose expresses a hand pose by a plane model that consists of hand's center of gravity (COG) and fingertip points. These points of reference can be relatively more stable and easily detected than other points (e.g., finger base points). However, since it has been assumed in the COG detection process that a hand region in an image is separate from other regions, detection becomes unstable when, for instance, a hand region is connected with an arm region. Moreover, finger occlusion which occurs in specific ranges of palm direction, makes angle detection unstable. The technique we present here solves the former problem by using hand skeleton images detected by a multi-camera system. We picked many candidates as the COG and selected a candidate according to its attributes. The multi-camera system also solves the latter problem. Results of a series of experiments on the former problem are also presented.", "references": ["6aaafd4819e45438569bfc52fd6460ca382d4707", "8a9e0f134e439232fa88000f204df63e071ecd24", "3cd5327dec2a7c62cfa2fc7658d977ef7e197202", "db537487d5efaa8fda6790237b1148107ba2852b"], "page_rank": 8.210180623973726e-05}, {"id": "7c4f9d0b2f878e0b03c6895aaaed94bdb4811651", "title": "Gray Level Thresholding in Badly Illuminated Images", "authors": ["J. R. Parker"], "date": 1991, "abstract": "The thresholding method involves first locating objects in an image by using the intensity gradient, then noting the levels that correspond to the objects in various areas of the image, and finally using these levels as initial guesses at a threshold. This method is capable of thresholding images that have been produced in the context of variable illumination. The thresholding method, called the local intensity gradient (LIG) method, was implemented in C using a Sun4 host running UNIX. The LIG method was compared against iterative selection (IS), gray level histograms (GLHs) and two correlation based algorithms on a dozen sample images under three different illumination effects. Overall, the LIG method, while it takes significantly longer, properly thresholds a larger set of images than does any other method examined over the sample images tested. >", "references": [], "page_rank": 0.00032840722495894905}, {"id": "20ba61450c943a8b980f52376f8bbf95a5ef7418", "title": "SRI International FASTUS system: MUC-4 test results and analysis", "authors": ["Douglas E. Appelt", "John Bear", "Jerry R. Hobbs", "David J. Israel", "Mabry Tyson"], "date": 1992, "abstract": "The system that SRI used for the MUC-4 evaluation represents a significant departure from system architectures that have been employed in the past. In MUC-2 and MUC-3, SRI used the TACITUS text processing system [1], which was based on the DIALOGIC parser and grammar, and an abudctive reasoner for horn-clause logic. In MUC-4, SRI designed a new system called FASTUS (a permutation of the initial letters in Finite State Automata-based Text Understanding System) which we feel represents a significant advance in the state of the art of text processing. The system shares certain modules with the earlier TACITUS system, namely modules for text preprocessing and standardization, spelling correction, Hispanic name recognition, and the core lexicon. However, the DIALOGIC system and abductive reasoner, which were the heart and soul of the previous system, were replaced by a system whose architecture is based on cascaded finite-state automata. Using this system we were capable of achieving a significant level of performance on the MUC-4 task with less than one month devoted to domain-specific development. In addition, the system is extremely fast, and is capable of processing texts at the rate of approximately 3,200 words per minute, measured in CPU time on a Sun SPARC-2 processor. (Measured according to elapsed real time, the system about 50% slower, but the observed time depends on the particular hardware configuration involved.)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5b45f31aaf7b3b3deb4b3867d4196a36ec2682c1", "title": "Automatic target segmentation by locally adaptive image thresholding", "authors": ["Wen-Nung Lie"], "date": 1995, "abstract": "A locally adaptive thresholding algorithm, concerning the extraction of targets from a given field of background, is proposed. Conventional histogram-based or global-type methods are deficient in detecting small targets of possibly low contrast as well. The present research is notable for solving the mentioned problems by introducing (1) shape connectivity measure based on co-occurrence statistics for threshold evaluation; and (2) no-target identification procedure for modeling a local-processing paradigm. In this manner, thresholds are determined adaptively even in the presence of space-varying noise or clutter. Experiments show that the results are reliable and even outperform those that manual operations can achieve for global thresholding.", "references": ["a050eb794e7cb65207d998371f8f0287e7ed53ab", "7c4f9d0b2f878e0b03c6895aaaed94bdb4811651", "1d4816c612e38dac86f2149af667a5581686cdef", "63b279a7f0d389cf1f9e4b42511efd124e60638c", "5644141ac8dc0b7893f4a403c41015ed5dbb15de", "1ecd0d5d19e0e1fb5ca398c0172f95e959ff5473", "a7fdfd358483a108ecc7d3ebc76dd5ace61fa975", "e917b5d6342adf799137d4c6f316a4ce4f586a68", "f66f886bfdef9c1b639122e5aba601cf2606c0a2", "bd9565a800e478638342cc55ee8945b23e5f8e4e"], "page_rank": 8.210180623973726e-05}, {"id": "dc67a48a9bb2ca8037f47245667a67601d2c5366", "title": "Unsupervised segmentation of noisy and textured images using Markov random fields", "authors": ["Chee Sun Won", "Haluk Derin"], "date": 1992, "abstract": "Abstract This paper proposes a general unsupervised segmentation algorithm which estimates all model parameters, including the number of regions, as part of the segmentation. The general image model is a hierarchical one, consisting of Markov random fields as components of the model. The MAP criterion is adopted, in principle, for the simultaneous image segmentation and parameter estimation procedures. Due to the difficulty of implementing the MAP segmentation with a large number of unknown parameters, a novel modification of the MAP criterion is proposed. For the model parameters having closed-form ML estimates, these estimates are substituted back into the objective function to reduce the difficulty of the maximization. The remaining maximization is implemented by a recursive segmentation-parameter estimation algorithm, which yields a partial optimal solution (POS) to the maximization problem. In the special case where all model parameters have closed-form ML estimates, the proposed algorithm is equivalent to implementing the MAP criterion. The number of regions in the image is determined through a model fitting criterion tagged on to the segmentation algorithm. Special forms of the general unsupervised segmentation algorithm are developed for the segmentation of noisy and textured images. For noisy images, the image is assumed to consist of uniform graylevel regions modeled by a class of Gibbs random fields and corrupted by additive, white, region-dependent, Gaussian noise. For textured images, the image is assumed to consist of regions, modeled by a class of Gibbs random fields, which are filled with textures, modeled by Gaussian Markov random fields. The algorithms for both classes of images are applied to a wide range of images\u2014generated according to the model, hand-drawn, natural and Brodatz textures, their combinations, and outdoor images\u2014with notable success. Despite the large number of unknown parameters (as many as 14 for some noisy images and 36 for some textured images), the algorithms yield good segmentations, accurate estimates for the parameters, and the correct number of regions.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7862fc4099b31f0a21fcf681403c2e594c2dd5bc", "title": "Markov Random Field Texture Models", "authors": ["George R. Cross", "Anil K. Jain"], "date": 1983, "abstract": "We consider a texture to be a stochastic, possibly periodic, two-dimensional image field. A texture model is a mathematical procedure capable of producing and describing a textured image. We explore the use of Markov random fields as texture models. The binomial model, where each point in the texture has a binomial distribution with parameter controlled by its neighbors and ``number of tries'' equal to the number of gray levels, was taken to be the basic model for the analysis. A method of generating samples from the binomial model is given, followed by a theoretical and practical analysis of the method's convergence. Examples show how the parameters of the Markov random field control the strength and direction of the clustering in the image. The power of the binomial model to produce blurry, sharp, line-like, and blob-like textures is demonstrated. Natural texture samples were digitized and their parameters were estimated under the Markov random field model. A hypothesis test was used for an objective assessment of goodness-of-fit under the Markov random field model. Overall, microtextures fit the model well. The estimated parameters of the natural textures were used as input to the generation procedure. The synthetic microtextures closely resembled their real counterparts, while the regular and inhomogeneous textures did not.", "references": ["fcfd56c6fc2a1632da285a0cbd1f657956b23cea", "ae30b6d182d0a722a4c9d43dc717a6b3639140bd", "45666cef42b1a6423a030cfc0581062a45641434", "40a6fff1cf2b6f43d4ecdefdf32a7138cf33be93", "83fb6d2721cd26be4964882ce929ff8c98ebb688", "cce7c8c9a96ab12b80353b2eef298399a81361d2", "20538226c1247e0ffc59b2281400472ec9cb14b2", "31db124111ca4a6d76b202bcaf2f3bb8ea64aa1e", "d67be230a6d13ee2304cf4baf6c0e12fffdf7f23", "e74b530f0943d1eb688cedd7ba628d2400589154"], "page_rank": 8.210180623973726e-05}, {"id": "b1d308a92a166900351c1e460ac6fc8463a9a002", "title": "Dynamic contours: real-time active splines", "authors": ["Rupert W. Curwen", "Andrew Blake"], "date": 1993, "abstract": "Semantic Scholar extracted view of \"Dynamic contours: real-time active splines\" by Rupert W. Curwen et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "9d8f0cfb7a79b698f2d050a22fbcec248c72b8c1", "title": "Data-driven segmentation of textured images using hierarchical markov random fields", "authors": ["Hideki Noda", "Mehdi N. Shirazi"], "date": 1995, "abstract": "This paper proposes an algorithm for a data-driven segmentation of an image consisting of multiple textures. The method uses hierarchical Markov random fields (MRF) consisting of two layers: the first layer MRF representing invisible regional images; and the second layer MRF representing a different texture in each region. This method is based on the EM algorithm for estimating the maximum likelihood of incomplete data, to treat invisible regional processes. The algorithm repeats an estimation of the MRF parameters and a segmentation (estimation of region processes). The former is carried out by maximizing a pseu-dolikelihood function; the latter is carried out by applying [the deterministic iterative relaxation algorithm] (developed by the authors) to the textured images. The proposed algorithm has been confirmed successfully by computer simulations using artificially composed textures.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "bd13c71fa6a6242e702041d54eadd7f4cb19108e", "title": "Tracking with Kalman snakes", "authors": ["Demetri Terzopoulos", "Richard Szeliski"], "date": 1993, "abstract": "Semantic Scholar extracted view of \"Tracking with Kalman snakes\" by Demetri Terzopoulos et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "d741141eab521b66b979848f33a93ecf38411b94", "title": "Model-Based Interpretation of 3D Medical Images", "authors": ["Andrew Hill", "Ann Thornham", "Christopher J. Taylor"], "date": 1993, "abstract": "The automatic segmentation and labelling of anatomical structures in 3D medical images is a challenging task of practical importance. We describe a model-based approach which allows robust and accurate interpretation using explicit anatomical knowledge. Our method is based on the extension to 3D of Point Distribution Models (PDMs) and associated image search algorithms. A combination of global, Genetic Algorithm (GA), and local, Active Shape Model (ASM), search is used. We have built a 3D PDM of the human brain describing a number of major structures. Using this model we have obtained automatic interpretations for 30 3D Magnetic Resonance head images from different individuals. The results have been evaluated quantitatively and support our claim of robust and accurate interpretation.", "references": ["b992336681135bd3e013a2c12b01dbbc3a469af2", "6a576bc9d7d40c1c9fdd7f1c94e69aabd9279400", "4d79f7ebf833c549dd1429fd9440628d6c0824c3", "a041199f33d69a949d9bd889068187ffe4140cc7", "d7f4e5d48bb21c2b0c5672688ea8a8072f6a35f0", "e6f2a07dde029636090d03f1ceeee2bb7200e6db", "2e62d1345b340d5fda3b092c460264b9543bc4b5", "4b4279db68b16e20fbc56f9d41980a950191d30a", "57e7cb6fa950a2c1059fbe69a9f70bcb1ab8805e"], "page_rank": 7.037297677691766e-05}, {"id": "f244c6d0f9abe7564a48653eb6726c814bb5fd27", "title": "Recovery of non-rigid motion and structure", "authors": ["Bradley Horowitz", "Alex Pentland"], "date": 1991, "abstract": "The elastic properties of real materials provide constraint on the types of non-rigid motion that can occur, and thus allow overconstrained estimates of 3-D non-rigid motion from optical flow data. It is shown that by modeling and simulating the physics of non-rigid motion it is possible to obtain good estimates of both object shape and velocity. Examples using grey-scale and X-ray imagery are presented, including an example of tracking a complex articulated figure.>", "references": ["50da6cb08bccec7b24764be5c4c364dca1d0015a", "9a5c253fa1da8ceca64f572b39c9ec366e789141", "4845c38569b417565751f38fc61c1eb5d154a7f5", "f9b3e5785bcf1743b3d7212122cc0f61ed3f49f3", "223f5be5170a74aab4d1d57ae3df90ec28a247e4"], "page_rank": 7.037297677691766e-05}, {"id": "a4f9442c9c8c835e069dc9cb812e2748541590e2", "title": "A Comparison of Some Multivariate Discrimination Procedures", "authors": ["Margaret Palmer Gessaman", "Paul H. Gessaman"], "date": 1972, "abstract": "Abstract A sequence of observations is obtained; each observation is known to be a value on one of two distinct absolutely continuous p-dimensional random variables, X and Y. The problem is to decide whether each observation is on X or Y. Some discrimination procedures are suggested and, using Monte Carlo methods, they are compared with other discrimination procedures to be found in the literature. The overall best performer in the comparisons of this study is one of the suggested procedures, a \u201cnearest neighbor\u201d type procedure based on statistically equivalent blocks.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "d7f4e5d48bb21c2b0c5672688ea8a8072f6a35f0", "title": "Training Models of Shape from Sets of Examples", "authors": ["Timothy F. Cootes", "Christopher J. Taylor", "David H. Cooper", "Jim Graham"], "date": 1992, "abstract": "A method for building flexible shape models is presented in which a shape is represented by a set of labelled points. The technique determines the statistics of the points over a collection of example shapes. The mean positions of the points give an average shape and a number of modes of variation are determined describing the main ways in which the example shapes tend to deform from the average. In this way allowed variation in shape can be included in the model. The method produces a compact flexible \u2018Point Distribution Model\u2019 with a small number of linearly independent parameters, which can be used during image search. We demonstrate the application of the Point Distribution Model in describing two classes of shapes.", "references": ["f53c1156ea4836d3c38478f46b3608ad1ee2d392", "df59d8c95025bcc540b31e04fa2d3a702404fb6f", "48ad3a569dc6946d44328191164052d9f23bd821", "e4e2429c43193cf204292400ba9e5a9f0b67f4e2", "f0c33d23bdb0dc35019859b752cc2bd42fadf1ef", "9394a5d5adcb626128b6a42c8810b9505a3c6487", "30316bf8c4522a7f2617a014b8a824ecc7db9d41", "84eb3581684121874119cbe7ad9e88c97ec08699", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "6ed4a2833ae3adaebaa73d24fd5f06bc9d9a02c9"], "page_rank": 0.0005629838142153413}, {"id": "b992336681135bd3e013a2c12b01dbbc3a469af2", "title": "The Use of Active Shape Models for Locating Structures in Medical Images", "authors": ["Timothy F. Cootes", "Andrew Hill", "Christopher J. Taylor", "J. Haslam"], "date": 1993, "abstract": "This paper describes a technique for building compact models of the shape and appearance of flexible objects (such as organs) seen in 2-D images. The models are derived from the statistics of sets of labelled images of examples of the objects. Each model consists of a flexible shape template, describing how important points of the object can vary, and a statistical model of the expected grey levels in regions around each model point. The shape models are parameterised in such a way as to allow \u2018legal\u2019 configurations. Such models have proved useful in a wide variety of applications. We describe how the models can be used in local image search and give examples of their application to medical images. We also describe how the method can be simply extended to segment 3-D objects in volume images and to track structures in image sequences.", "references": ["d7f4e5d48bb21c2b0c5672688ea8a8072f6a35f0", "f0c33d23bdb0dc35019859b752cc2bd42fadf1ef", "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1", "6a576bc9d7d40c1c9fdd7f1c94e69aabd9279400"], "page_rank": 0.0003166783954961295}, {"id": "22b125851acd9ab6d16485104eb554a13367b7a7", "title": "Applied Optimal Estimation", "authors": ["Arthur F. Gelb"], "date": 1974, "abstract": "This is the first book on the optimal estimation that places its major emphasis on practical applications, treating the subject more from an engineering than a mathematical orientation. Even so, theoretical and mathematical concepts are introduced and developed sufficiently to make the book a self-contained source of instruction for readers without prior knowledge of the basic principles of the field. The work is the product of the technical staff of the The Analytic Sciences Corporation (TASC), an organization whose success has resulted largely from its applications of optimal estimation techniques to a wide variety of real situations involving large-scale systemsArthur Gelb writes in the Foreword that \"It is our intent throughout to provide a simple and interesting picture of the central issues underlying modern estimation theory and practice. Heuristic, rather than theoretically elegant, arguments are used extensively, with emphasis on physical insights and key questions of practical importance.\"Numerous illustrative examples, many based on actual applications, have been interspersed throughout the text to lead the student to a concrete understanding of the theoretical material. The inclusion of problems with \"built-in\" answers at the end of each of the nine chapters further enhances the self-study potential of the text.After a brief historical prelude, the book introduces the mathematics underlying random process theory and state-space characterization of linear dynamic systems. The theory and practice of optimal estimation is them presented, including filtering, smoothing, and prediction. Both linear and non-linear systems, and continuous- and discrete-time cases, are covered in considerable detail. New results are described concerning the application of covariance analysis to non-linear systems and the connection between observers and optimal estimators. The final chapters treat such practical and often pivotal issues as suboptimal structure, and computer loading considerations.This book is an outgrowth of a course given by TASC at a number of US Government facilities. Virtually all of the members of the TASC technical staff have, at one time and in one way or another, contributed to the material contained in the work", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "f9e0a6f47f450a80d3db5ca13e7f3f51521108e1", "title": "A New Quantitative Quality Measure for Machine Translation Systems", "authors": ["Keh-Yih Su", "Ming-Wen Wu", "Jing-Shin Chang"], "date": 1992, "abstract": "In this paper, an objective quantitative quality measure is proposed to evaluate the performance of machine translation systems. The proposed method is to compare the raw translation output of an MT system with the final revised version for the customers, and then compute the editing efforts required to convert the raw translation to the final version. In contrast to the other proposals, the evaluation process can be done quickly and automatically. Hence, it can provide a quick response on any system change. A system designer can thus quickly find the advantages or faults of a particular performance dynamically. Application of such a measure to improve the system performance on-line on a parameterized and feedback-controlled system will be demonstrated. Furthermore, because the revised version is used directly as a reference, the performance measure can reflect the real quality gap between the system performance and customer expectation. A system designer can thus concentrate on practically important topics rather than on theoretically interesting issues.", "references": ["cc60440bc8eeee1f857eef47b42924fe9e2b336c", "b85dad35a5d1f68449907ff651279865f851ccf1", "d4533193ea63095c4125c33f4608c73fcac62d12", "bf9439a68a0c797d866c67d146d5a84642b4d373", "585931069bd9584c774152014ade1f13bd346b3d", "a7af8485d8db639cf96494df57fc09929e358db8", "c1698406207c34143fc56222c37dd439a8f078d3", "455e1168304e0eb2909093d5ab9b5ec85cda5028", "947a56ab45f2c269135d1d82303ae81f0f2fa3dc", "c7b09e5764317db0da5e9a42ad90351271030b2f"], "page_rank": 0.00016420361247947453}, {"id": "15315ed05451c88f83c50d56a66a0b85517c5f4f", "title": "An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research", "authors": ["Sonja Nie{\\ss}en", "Franz Josef Och", "Gregor Leusch", "Hermann Ney"], "date": 2000, "abstract": "In this paper we present a tool for the evaluation of translation quality. First, the typical requirements of such a tool in the framework of machine translation (MT) research are discussed. We define evaluation criteria which are more adequate than pure edit distance and we describe how the measurement along these quality criteria is performed semi-automatically in a fast, convenient and above all consistent way using our tool and the corresponding graphical user interface.", "references": ["c18264c08f45f2283c343829ddf9d4d107822e6e", "fd0101dfbdd6768efe1e99a5ccd3ec0415fe723f", "b66072f7dd9950d6ed2d53d41cb4656a5d7806c1", "7052b435e9846436d15f9c386cfdc5e969728362", "0991b1a89f9046cdc37e1db1f3f8d2d56b00162c", "15b222eae7cea8f458aed3d70956553c45ee2e43", "0efb632e3fe804dd18133f6f4f09f19e6fd14d61"], "page_rank": 0.00016420361247947453}, {"id": "f013d1eaf0b5e089281c64c8a75d2b7ec390891d", "title": "A Projection Extension Algorithm for Statistical Machine Translation", "authors": ["Christoph Tillmann"], "date": 2003, "abstract": "In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks -- pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional high-recall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an Arabic-English translation task.", "references": ["297b0d80575ae36e3e26772ba7e70fa6b570c68d", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "c9214ebe91454e6369720136ab7dd990d52a07d4", "ab7b5917515c460b90451e67852171a531671ab8", "0ce9c261a7f69668da2066da0ad736e6eccdcd36", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "d7da009f457917aa381619facfa5ffae9329a6e9"], "page_rank": 0.0001231527093596059}, {"id": "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "title": "Statistical machine translation: from single word models to alignment templates", "authors": ["Franz Josef Och"], "date": 2002, "abstract": "In this work, new approaches for machine translation using statistical methods are described. In addition to the standard source-channel approach to statistical machine translation, a more general approach based on the maximum entropy principle is presented. Various methods for computing single-word alignments using statistical or heuristic models are described. Various smoothing techniques, methods to integrate a conventional dictionary and training methods are analyzed. A detailed evaluation of these models is performed by comparing the automatically produced word alignment with a manually produced reference alignment. Based on these fundamental single-word based alignment models, a new phrase-based translation model\u2014the alignment template model\u2014is suggested. For this model, a training and an efficient search algorithm is developed. For two specific applications (interactive translation and multi-source translation) specific search algorithms are developed. The suggested machine translation approach has been tested for the German-English Verbmobil task, the French-English Hansards task and for Chinese-English news text translation. Often, the obtained results are significantly better than those obtained with alternative approaches to machine translation.", "references": ["b165a1c826e77b0fca1e3e8aa24de6c87924e8c7", "027224cecc5cc50f664e7fd249bd4f4545397b96", "d7da009f457917aa381619facfa5ffae9329a6e9", "a8912e439ac787c91892ea3d5223e7dd0fea4052", "dda03bf2b6103e358ee6ce340c1ebf022285783f", "aa56223830d6e7ff73bd02c8e7e88f2d99126b5e", "fff1b293b45d06c8462021aa6c90c81e743e131b"], "page_rank": 0.0003694581280788177}, {"id": "e2a68774f92d1e894cbbbef2c819e4592990eb4b", "title": "Minimum Bayes-Risk Decoding for Statistical Machine Translation", "authors": ["Shankar Kumar", "William J. Byrne"], "date": 2004, "abstract": "Abstract : We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.", "references": ["1f12451245667a85d0ee225a80880fc93c71cc8b", "7181f7a664fbbf34c7c147c8a90f0343cdd1674c", "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "c4138748eb5dc1bbd1df2951f299d701304147a2", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "cd5a169879504ea91660a443b9151753cc29c42f", "08e39912a54fc46f25f9e79bfa06ee44311b051a", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "b5b0ab2d445a393341d292b26c8a9a6d01c62051", "d7da009f457917aa381619facfa5ffae9329a6e9"], "page_rank": 0.0001231527093596059}, {"id": "7d6b34002d6dab558943f55e37317a572600393b", "title": "Learn-ing probaballstic dependency grammars from labelled text", "authors": ["Glenn Carroll", "Eugene Charniak"], "date": 1992, "abstract": "We present the results of experimenting with schemes for learning probabilistic dependency grammars 1 for English from corpora labelled with part-of-speech information. We intend our system to produce widecoverage grammars which have some resemblance to the standard2 context-free grammars of English which grammarians and linguists commonly exhibit as exampies.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "223dcd0e44532fc02444709e61327432c74fe46d", "title": "Phrase-Based Statistical Machine Translation", "authors": ["Richard Zens", "Franz Josef Och", "Hermann Ney"], "date": 2002, "abstract": "This paper is based on the work carried out in the framework of the VERBMOBIL project, which is a limited-domain speech translation task (German-English). In the final evaluation, the statistical approach was found to perform best among five competing approaches.In this paper, we will further investigate the used statistical translation models. A shortcoming of the single-word based model is that it does not take contextual information into account for the translation decisions. We will present a translation model that is based on bilingual phrases to explicitly model the local context. We will show that this model performs better than the single-word based model. We will compare monotone and non-monotone search for this model and we will investigate the benefit of using the sum criterion instead of the maximum approximation.", "references": ["8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "e7feffbba87f227e3de9244dbb75d39bd707d70a", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "ffc1b0032e704185658ff36d0cfa5f7cfa5df362", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "7ad3903a0c2645585946d00d075676dfe5f220f1", "ab7b5917515c460b90451e67852171a531671ab8", "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "8b4819c3622a4b5a38c3286be00fbedb381eae2d", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0"], "page_rank": 0.0001231527093596059}, {"id": "72c58a58ba1684dc8337c8e64e0b5dacfc94499e", "title": "Information Retrieval", "authors": ["Alexander Dekhtyar"], "date": 1968, "abstract": "Query reformulation understanding is important for Information Retrieval (IR) tasks, such as search results reranking and query recommendation. Conventional works rely on the textual content of queries to understand reformulation behaviors, which suffer from data sparsity problems. To address this issue, We propose a novel method to efficiently represent the behaviors of query reformulation by the translating embedding from the original query to its reformulated query. We utilize two-stage training algorithm to make the learning of multilevel intentions representation more adequate. We construct a new corpus of shopping search query log and create a query reformulation graph based on this dataset. Referring to knowledge graph embedding methods, we use the accuracy of intentions prediction to evaluate experimental results. Our final result, an increase of 20.6% of the average prediction accuracy in 21 intentions, shows significant improvement compared to baselines.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2a99c350f73f54ef78493481f533dc1b9d8455a1", "title": "Interpretation as Abduction", "authors": ["Jerry R. Hobbs", "Mark E. Stickel", "Paul Antoine Martin", "Douglas Edwards"], "date": 1988, "abstract": "An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "ff7180abf889a4a55b09d43200bbc696b8bfaf1c", "title": "GE NLToolset: description of the system as used for MUC-4", "authors": ["George B. Krupka", "Paul S. Jacobs", "Lisa F. Rau", "Lois Childs", "Ira Sider"], "date": 1992, "abstract": "The GE NLTOOLSET is a set of text interpretation tools designed to be easily adapted to new domains. This report summarizes the system and its performance on the MUC-4 task.", "references": ["1e5c33ab632310d827155002311e0adceebb6ec0", "471c18589caf74f7e149fb74df314668b239ff6f", "2797a6f03a187e76cc5fca02253ed49ac179c462"], "page_rank": 0.0002463054187192118}, {"id": "18dde31c2716b0bf10a35f334ed24af4e4e37fa5", "title": "Parsing the LOB Corpus", "authors": ["Carl de Marcken"], "date": 1990, "abstract": "This paper presents a rapid and robust parsing system currently used to learn from large bodies of unedited text. The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze. The results of applying the disambiguator and parser to large sections of the Lancaster/Oslo-Bergen corpus are presented.", "references": ["3a8de92b304729f15d9bd6c3d22a56ab9b31e212", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "ff17092b958e0d32ef078cf8c972067e6ce07a43", "bedfbe4395ead517cfa6e369a3564c8fb6876c04", "0dba12afcac3737e04b5928340112683881ada6f"], "page_rank": 0.00028735632183908046}, {"id": "27a84c9145fff4255aa2a30a556a2900a65916ae", "title": "Speech Acts: An Essay in the Philosophy of Language", "authors": ["Alice Koller", "John R. Searle"], "date": 1969, "abstract": "Part I. A Theory of Speech Acts: 1. Methods and scope 2. Expressions, meaning and speech acts 3. The structure of illocutionary acts 4. Reference as a speech act 5. Predication Part II. Some Applications of the Theory: 6. Three fallacies in contemporary philosophy 7. Problems of reference 8. Deriving 'ought' from 'is' Index.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "384720361a8cae1f2bbb87e05215cc58fef74ac1", "title": "Extracting Semantic Representations from Large Text Corpora", "authors": ["Malti Patel", "John A. Bullinaria", "Joseph P. Levy"], "date": 1997, "abstract": "Many connectionist language processing models have now reached a level of detail at which more realistic representations of semantics are required. In this paper we discuss the extraction of semantic representations from the word co-occurrence statistics of large text corpora and present a preliminary investigation into the validation and optimisation of such representations. We find that there is significantly more variation across the extraction procedures and evaluation criteria than is commonly assumed.", "references": ["c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "dbd1994e7e63e8aef509604d3c38652cbf30f9de", "5bbc3d8ca0f62ce85e4b1fbb6de7a8624ebaeda7", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "d30f3840a1a3c1e3be3580e2922d4eecc54a0b53", "a7e62ca1603444c9589be66a08825058abf44845", "ef95c2add4d5238f99150ecdd5f2314dce031158", "a8a3ff991718aaeceb03948e1a694070675763c1", "88178aa64e1b09ab173deb12e55cac4173df8936", "d859f0d4c78bc772b350854cae30e72e5610ce71"], "page_rank": 0.0002463054187192118}, {"id": "6f532be9406b0fe10a8d71e0973b19939403268b", "title": "Acquiring Lexical Generalizations from Corpora: A Case Study for Diathesis Alternations", "authors": ["Maria Lapata"], "date": 1999, "abstract": "This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data. We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations. We estimate the productivity of an alternation and the typicality of its members using type and token frequencies.", "references": ["0fabb90d6482e614bce42e2ed936b133a508c958", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "53a39651778980d82cd992efa741ccbc076e1fcf", "8bd683d5aef2704207fca32a175f0c79f871e180", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "b0e5ab189f770b7e106db429f2980510065ef125", "42053bb5bc617a3c75c56f61cc6be1e8e53f4e29", "ceee0c5d577c6d17926fd8c8bc6e590fbc7d813a", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "2a4a9c37db04225f21a5b8ce764495a4897783d6"], "page_rank": 0.0002463054187192118}, {"id": "1e5c33ab632310d827155002311e0adceebb6ec0", "title": "SCISOR: extracting information from on-line news", "authors": ["Paul S. Jacobs", "Lisa F. Rau"], "date": 1990, "abstract": "The future of natural language text processing is examined in the SCISOR prototype. Drawing on artificial intelligence techniques, and applying them to financial news items, this powerful tool illustrates some of the future benefits of natural language analysis through a combination of bottom-up and top-down processing.", "references": ["5c0d500f108199c8f0ec05c040f8bb9c0e5ead95"], "page_rank": 0.00028735632183908046}, {"id": "db537487d5efaa8fda6790237b1148107ba2852b", "title": "A Study on Detecting Finger Motion to Realize a Virtual World", "authors": ["Masayuki Nakajima", "Hirokuni Shiba"], "date": 1994, "abstract": "Semantic Scholar extracted view of \"A Study on Detecting Finger Motion to Realize a Virtual World\" by Masayuki Nakajima et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "d24a489657026723e83a712ab7f01d00fdf585e5", "title": "Towards Understanding Text with a Very Large Vocabulary", "authors": ["Damaris M. Ayuso", "Robert J. Bobrow", "Dawn MacLaughlin", "Marie Meteer", "Lance A. Ramshaw", "Richard M. Schwartz", "Ralph M. Weischedel"], "date": 1990, "abstract": "In order to meet the information processing demands of the next decade, natural language systems must have the capability of processing very large amounts of text, commonly called \"messages\", from highly diverse sources written in any of a few dozen languages. One of the key issues in building systems with this scale of competence is handling large numbers of different words and word senses. Natural language understanding systems today are typically limited to vocabularies of less than 10,000 words; tomorrow's systems will need vocabularies at least 5 times that to effectively handle the volume and diversity of messages needing to be processed.", "references": ["ac3947d7f4b33a773d1cd2e88c8937eab3205af5", "be1fed9544830df1137e72b1d2396c40d3e18365", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "ff97fa9912e4e0517208bf45ef0f646f995e09d2", "18dde31c2716b0bf10a35f334ed24af4e4e37fa5", "af1b311bf37b34e177b7effe164a155395125c3f", "ac50078e58b33fc98d30187a88b1364aff716ba9", "88e2729a43316b88623d11e30e7b354e9697baf0"], "page_rank": 0.00016420361247947453}, {"id": "108d05ba4d16e5991dd4b271360441b810767543", "title": "Structural ambiguity and lexical relations", "authors": ["HindleDonald", "RoothMats"], "date": 1993, "abstract": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on t...", "references": [], "page_rank": 0.0004105090311986863}, {"id": "8a9e0f134e439232fa88000f204df63e071ecd24", "title": "Real time hand shape recognition using pipe-line image processor", "authors": ["Koichi Ishibuchi", "Haruo Takemura", "Fumio Kishino"], "date": 1992, "abstract": "This paper describes a new hand shape recognition method for man-machine interfaces using a pipeline image process. The basic strategy is to use simple but effective real time dynamical image processing to enhance the reliability of hand shape recognition since motion between frames is minimized. The algorithm for a noncontact-type hand shape recognizer has been developed for incorporation into a virtual reality environment.>", "references": ["b1a672fc01064dcb0658050bf0ea5df2eaab4f49"], "page_rank": 0.0003694581280788177}, {"id": "3cd5327dec2a7c62cfa2fc7658d977ef7e197202", "title": "Uncalibrated Stereo Vision with Pointing for a Man-Machine Interface", "authors": ["Roberto Cipolla", "Paul A. Hadfield", "Nicholas J. Hollinghurst"], "date": 1994, "abstract": "Here we report preliminary work on a gesturebased interface for robot guidance. The system requires no physical contact with the op erator, but uses uncalibrated s tereo vision with active c ontours to track the position and pointing direction of a hand. With a ground plane c onstraint, it is then possible to find the indicated position in the robot's workspace, by considering only two-dimensional collineations. The system is accurate to about 2cm in a 40cm workspace; natural operator feedback improves this to within Icm. It is initialised by observing just 4 points on the plane.", "references": ["0d3f501026abaa3790f3c08423c2d7a612fdc801", "5d84e137ed9cf40b787fbf3535c328a3899363c2", "641187a901e907247a2ebfb214598cfffffd8f29", "ce8b0e2c27bb503163a00c735ef1e70883f89baa", "c2d2fefc1c61298059f9a160f190e6957587b74e", "5ba7042c5220548c9d5636df3cc2c84bb8641e02", "d74b052ff7a9deecc92370a815ccd21ae764a280", "a5b16bf76efe6005be17a19e4dcb3b126b37158c"], "page_rank": 0.0001231527093596059}, {"id": "6aaafd4819e45438569bfc52fd6460ca382d4707", "title": "Real time hand gesture recognition using 3D prediction model", "authors": ["Koichi Ishibuchi", "Haruo Takemura", "Fumio Kishino"], "date": 1993, "abstract": "This paper proposes a new hand gesture recognition method using a 3D prediction model used for developing human-computer interfaces. It focuses on real-time hand-pose estimation, which is indispensable for virtual object manipulation. In the first stage, simple but effective parallel pipelined algorithms are employed to extract hand features from binocular images. In the second stage, the hand pose is estimated using the hand's features. Even if occlusion occurs, a ghost correspondence eliminating table, used with the 3D prediction model, makes the estimation possible on the assumption that finger tips are occluded by other finger tips. The hand-feature extraction method, the pose estimation method, the constructed human-computer interfaces, and a hand-pose estimation experiment are discussed.>", "references": ["51d4d507f271f3c2976cb4d065764602031678dd", "8a9e0f134e439232fa88000f204df63e071ecd24"], "page_rank": 0.0001231527093596059}, {"id": "e74b530f0943d1eb688cedd7ba628d2400589154", "title": "Markov Random Field Texture Models", "authors": ["R CrossGeorge", "K JainAnil"], "date": 1983, "abstract": "We consider a texture to be a stochastic, possibly periodic, two-dimensional image field. A texture model is a mathematical procedure capable of producing and describing a textured image. We explor...", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4471e3117cdac2fae74d305d54b237bb3addd749", "title": "Explorations In Automatic Thesaurus Discovery", "authors": ["Gregory Grefenstette"], "date": 1994, "abstract": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "d67be230a6d13ee2304cf4baf6c0e12fffdf7f23", "title": "A syntactic approach to texture analysis", "authors": ["S. Y. Lu", "King-Sun Fu"], "date": 1978, "abstract": "Abstract A syntactic model for the generation of structured textures and for the discrimination of textures is proposed. A texture pattern is first divided into fixed-size windows. Windows belonging to the same texture pattern are then characterized by a tree grammar. This tree grammar is used for synthesis as well as discrimination. If it is considered necessary, distortion or noise is introduced by using stochastic tree grammars. Finally, a set of error-correcting tree automata is used as a texture discriminator. Illustrative examples of texture synthesis and discrimination are presented.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "bd9565a800e478638342cc55ee8945b23e5f8e4e", "title": "Moment-preserving thresholding: a new approach", "authors": ["Wen-Hsiang Tsai"], "date": 1995, "abstract": "A new approach to automatic threshold selection using the moment-preserving principle is proposed. The threshold values are computed deterministically in such a way that the moments of an input picture is preserved in the output picture. Experimental results show that the approach can be employed to threshold a given picture into meaningful gray-level classes. The approach is described for global thresholding, but it is applicable to local thresholding as well.", "references": ["152c18461790995879aa7f31a8e67c894be97a86", "f2131fb0cfddbda3fc8e6e8f1bc654a84296f374", "093dad9cda0a69664a8e71ed8a35d2d24386f2cc", "3211084cca0391df648750765115e75782a5d046", "6e3be395b37897bc768c009ac30b2e863d4f9433", "54b89dca4328c37e576ff8cd0fc423409e2d2047", "a8d5cfed225be1737d4db104ed2f80ebffffa8b5", "c70d1a2656ecc1335fd0c345e5014756541cae47", "f2dc23000b57db1dff031e8650b17bf6750bd2f4"], "page_rank": 0.0002463054187192118}, {"id": "cce7c8c9a96ab12b80353b2eef298399a81361d2", "title": "Texture pattern image generation by regular Markov chain", "authors": ["Ryuzo Yokoyama", "Robert M. Haralick"], "date": 1979, "abstract": "Abstract A method of graytone texture pattern generation using a regular Markov chain is presented. The procedure arranges the generated gray tones in a sequence along a scan line. The transition probability matrix of the Markov chain directly determines the spatial co-occurrence probabilities of gray tones in the generated image. The generated image can be rotated and arithmetically combined to produce images of additional texture patterns. The paper illustrates the variety of texture which can be produced by this method.", "references": ["20538226c1247e0ffc59b2281400472ec9cb14b2", "57337d2cf02c26901eb8add0c2b85d97ab668aad"], "page_rank": 9.852216748768472e-05}, {"id": "223f5be5170a74aab4d1d57ae3df90ec28a247e4", "title": "Maximizing rigidity: The incremental recov-ery of 3D structure from rigid and rubbery motion", "authors": ["Shimon Ullman"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"Maximizing rigidity: The incremental recov-ery of 3D structure from rigid and rubbery motion\" by Shimon Ullman", "references": [], "page_rank": 0.0002463054187192118}, {"id": "20538226c1247e0ffc59b2281400472ec9cb14b2", "title": "Time series model for texture synthesis", "authors": ["Bruce H. McCormick", "Sadali N. Jayaramamurthy"], "date": 1974, "abstract": "A general method is proposed for the synthesis of texture. It is based on a model which treats the pixels (picture elements) of a digitized textural scene as a two-way seasonal time series. This method possesses the desirable characteristic that the parameters needed for synthesis are derived directly from the analysis of the \u201cparent\u201d texture (texture to be imitated). With the help of well-developed methods in the time series analysis the process that generates the pixels of the parent texture is identified. From a set of boundary conditions the future values of the time series are generated which in essence are the pixels of the synthesised texture. The effectiveness of this method is illustrated with examples.", "references": ["8b7c0822c1ee812df0ffb39eb11a2181aa837641", "ddc6a00071aac025b0e603c50172a290322ad6a1", "46575e6d7a8115b825064ff7864f2e6c0dae50c7", "757e3a29ce26b170c0f24cfb8f25efd41d56646c", "50757a6e2ebf0605e7310b6aa70b834a4732de23", "de0b63262321ce0526d37e79df0c19bd6891c929"], "page_rank": 0.0003448275862068965}, {"id": "31db124111ca4a6d76b202bcaf2f3bb8ea64aa1e", "title": "Texture Analysis Using Generalized Co-Occurrence Matrices", "authors": ["Larry S. Davis", "Steven A. Johns", "Jake K. Aggarwal"], "date": 1979, "abstract": "We present a new approach to texture analysis based on the spatial distribution of local features in unsegmented textures. The textures are described using features derived from generalized co-occurrence matrices (GCM). A GCM is determined by a spatial constraint predicate F and a set of local features P = {(Xi, Yi, di), i = 1,..., m} where (Xi, Yi) is the location of the ith feature, and di is a description of the ith feature. The GCM of P under F, GF, is defined by GF(i, j) = number of pairs, pk, pl such that F(pk, pl) is true and di and dj are the descriptions of pk and pl, respectively. We discuss features derived from GCM's and present an experimental study using natural textures.", "references": ["077dc845fa51fd105d694f458a7c6d59f48044f6", "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "1fdb62555eb650662dbe2a6f3985d390861597c2", "6f6784ee9cddba10ec8fd0027b2a36f44ef2c73d", "11498ac1e146f3ecd800227690a8715f41c926c1", "2811e815185c1c00de5cd6c7e34b78a17322343b", "d06d11ade5601c6b50606ef8bf8446398f452449"], "page_rank": 9.852216748768472e-05}, {"id": "84eb3581684121874119cbe7ad9e88c97ec08699", "title": "Object recognition by computer: the role of geometric constraints", "authors": ["W. Eric L. Grimson"], "date": 1991, "abstract": "With contributions from Tomas LozanoPerez and Daniel P. Huttenlocher.An intelligent system must know \"what \"the objects are and \"where \"they are in its environment. Examples of this ubiquitous problem in computer vision arise in tasks involving hand-eye coordination (such as assembling or sorting), inspection tasks, gauging operations, and in navigation and localization of mobile robots. This book describes an extended series of experiments into the role of geometry in the critical area of object recognition. It provides precise definitions of the recognition and localization problems, describes the methods used to address them, analyzes the solutions to these problems, and addresses the implications of this analysis.The solution to problems of object recognition are of fundamental importance in many real applications and versions of the techniques described here are already being used in industrial settings. Although a number of questions remain to be solved, the authors provide a valuable framework for understanding both the strengths and limitations of using object shape to guide recognition.W. Eric L. Grimson is Matsushita Associate Professor in the Department of Electrical Engineering and Computer Science at MIT.Contents: Introduction. Recognition as a Search Problem. Searching for Correspondences. Two-Dimensional Constraints. Three-Dimensional Constraints. Verifying Hypotheses. Controlling the Search Explosion. Selecting Subspaces of the Search Space. Empirical Testing. The Combinatorics of the Matching Process. The Combinatorics of Hough Transforms. The Combinatorics of Verification. The Combinatorics of Indexing. Evaluating the Methods. Recognition from Libraries. Parameterized Objects. The Role of Grouping. Sensing Strategies. Applications. The Next Steps.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "0ce9c261a7f69668da2066da0ad736e6eccdcd36", "title": "Inducing Multilingual Text Analysis Tools via Robust Projection across Aligned Corpora", "authors": ["David Yarowsky", "Grace Ngai", "Richard H Wicentowski"], "date": 2001, "abstract": "This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish.Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections.Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system.This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection.", "references": ["327c88dd06722a967be9c6b1176fbd79554967e7", "4d1dac08bb3d960baa88e4ff3477ec834446d056", "5ee96af7e166dd833fcfc43dec290115fac4e7d2", "dcd0ee8cb2d43ce4101ddc3f6b3e236aa92d5794", "d9c71db75046473f0e3d3229950d7c84c09afd5e", "16d5db716e774c60cf76ef5b3912c001e7c2754b", "79f20fb39a6e78352ebbb65b1737970837a420b5", "3491aa4a9a66ba3d1603230a70d82c7479666a7d", "19cc8befd39c2f8c555918e5be15c33ef5d1c651", "07f8dc8ac06b0b3305ea96a62ea98145dc9620f4"], "page_rank": 0.0001231527093596059}, {"id": "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1", "title": "Fitting Parameterized Three-Dimensional Models to Images", "authors": ["David G. Lowe"], "date": 1991, "abstract": "Model-based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. The author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved, parameterized objects. >", "references": ["e06d2241036f4699826da161ca8d336509c32680", "85fe26c3178b0fedab2b510217edcf107a37aae3", "4f37468a95ccc62debb9e5a4cb0d73489ca61190", "8735690a9e8f8884bf27717877ddf7f9071472e5", "140b1d27f53bd809ba55a47f55f1abe2347e742f", "85edabd5dbaefd061a10fffd7349623cc81a80ac", "7232c7a3aad15ab5d9f3de951f331eaf14169920", "339149a85a558018c321e21d8446127f4fe65b06", "e2583da9d16a6a596158a8eef220831b0d3d8f8a", "b8831f3095686131a98731d6806abdc75c6baebd"], "page_rank": 0.0002463054187192118}, {"id": "c9214ebe91454e6369720136ab7dd990d52a07d4", "title": "Improved Statistical Alignment Models", "authors": ["Franz Josef Och", "Hermann Ney"], "date": 2000, "abstract": "In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.", "references": [], "page_rank": 0.0003694581280788177}, {"id": "f9b3e5785bcf1743b3d7212122cc0f61ed3f49f3", "title": "The senses considered as perceptual systems", "authors": ["James Jerry Gibson"], "date": 1966, "abstract": "A signature feeding mechanism having a shuttle plate and an associated vacuum gripper for feeding the bottom signature from a stack in a supply magazine into the bite of pinch rolls which advance successive signature pieces to a transport system for advance to a label applicator, or the like, and having a guard plate for bridging exposed portions of the gap between the leading edge of the shuttle plate and the signature receiving end of the associated transport conveyor floor so as to prevent possible damage to, or interference with the operation of, the feeding mechanism, by the presence of a foreign object in the gap, and possible injury to an operator through careless insertion of the fingers in the same.", "references": [], "page_rank": 0.00030788177339901473}, {"id": "5e0ce7719dcb315145284fea50fd7c96df3599ab", "title": "Model-based recognition in robot vision", "authors": ["Roland T. Chin", "Charles R. Dyer"], "date": 1986, "abstract": "This paper presents a comparative study and survey of model-based object-recognition algorithms for robot vision. The goal of these algorithms is to recognize the identity, position, and orientation of randomly oriented industrial parts. In one form this is commonly referred to as the \"bin-picking\" problem, in which the parts to be recognized are presented in a jumbled bin. The paper is organized according to 2-D, 2\u00bd-D, and 3-D object representations, which are used as the basis for the recognition algorithms. Three central issues common to each category, namely, feature extraction, modeling, and matching, are examined in detail. An evaluation and comparison of existing industrial part-recognition systems and algorithms is given, providing insights for progress toward future robot vision systems.", "references": ["8845ebd09e86c0745aa09661b4cda953660e1def", "8ad87f99f2f0a6970ab2329fc2bec72351c74adb", "92aadef8a3e94989ffc22fb6b849b626aeb018d6", "111bd450219d726f8edd1d09fbfe1eb0eba2c626"], "page_rank": 0.00016420361247947453}, {"id": "6ed4a2833ae3adaebaa73d24fd5f06bc9d9a02c9", "title": "Application of the Karhunen-Lo\u00e8ve Expansion to Feature Selection and Ordering", "authors": ["Keinosuke Fukunaga", "Warren L. G. Koontz"], "date": 1970, "abstract": "The Karhunen-Lo6ve expansion has been used previously to extract important features for representing samples taken from a given distribution. A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes. Numerical examples are presented to illustrate the technique.", "references": ["20e71e4135d44a77d67c2849264332bdd2534a69", "750d0eb009cc53be501e2859cc425e8f6286ba2b", "6f9720d2d46b508091191cffe78b9106d330b90d", "fbf75104b26aec301837ffbaf29859f6cc842aaa"], "page_rank": 0.00016420361247947453}, {"id": "0efb632e3fe804dd18133f6f4f09f19e6fd14d61", "title": "Evaluating Natural Language Processing Systems: An Analysis and Review", "authors": ["Karen Jones", "Julia Galliers"], "date": 1995, "abstract": "From the Publisher: \nThis comprehensive state-of-the-art book is the first devoted to the important and timely issue of evaluating NLP systems. It addresses the whole area of NLP system evaluation, including aims and scope, problems and methodology. The authors provide a wide-ranging and careful analysis of evaluation concepts, reinforced with extensive illustrations; they relate systems to their environments and develop a framework for proper evaluation. The discussion of principles is completed by a detailed review of practice and strategies in the field, covering both systems for specific tasks, like translation, and core language processors. The methodology lessons drawn from the analysis and review are applied in a series of example cases. The book also refers NLP system evaluation to the neighbouring areas of information and speech processing, and addresses issues of tool and data provision for evaluation. A comprehensive bibliography and subject index are included as well as a term glossary. This monograph will be a valuable source of inspiration in research, practice, and teaching.", "references": ["2406b90d779c59305e2b758f225658e454e2f039"], "page_rank": 0.0002463054187192118}, {"id": "8b4819c3622a4b5a38c3286be00fbedb381eae2d", "title": "Example-Based Machine Translation with Templates", "authors": ["Marko Auerswald"], "date": 2000, "abstract": "This paper presents an approach for template based machine translation, in which the templates are generated in a highly automated way from large corpora of translation examples. The techniques described have been successfully used in one of the alternative translation modules within the Verbmobil speech-to-speech translation system. A crucial feature of this approach is the capability of processing word lattice input in an efficient way.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "297b0d80575ae36e3e26772ba7e70fa6b570c68d", "title": "A Decoder for Syntax-based Statistical MT", "authors": ["Kenji Yamada", "Kevin Knight"], "date": 2002, "abstract": "This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model.", "references": ["436772d9a916f0382800cf18581cfdfd4f83c457", "c9214ebe91454e6369720136ab7dd990d52a07d4", "ab7b5917515c460b90451e67852171a531671ab8", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "8bcabd656e6fb379221968165d3b326c55ae2fe3", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "64455eb3fd378965589a69aa8472c39af6d332d2", "d7da009f457917aa381619facfa5ffae9329a6e9"], "page_rank": 0.0001231527093596059}, {"id": "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "title": "Modeling with Structures in Statistical Machine Translation", "authors": ["Ye-Yi Wang", "Alexander H. Waibel"], "date": 1998, "abstract": "Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task.", "references": ["59c442932e9fcfcac6df5566c2bcd1ec331548c9", "fec2275eee699411d47eff115e61db5110f8b14c", "03b513b30b9d95e39285df1dc93be63e25f2744e", "93dd9a95399258c1411f8dd882be8877e25bd54e", "b0b6a4a78af426532b04cb763f4d5c0dd1ad7885", "e7c32944293cee888d6bce7363f646a22a69c59d", "ab7b5917515c460b90451e67852171a531671ab8"], "page_rank": 9.852216748768472e-05}, {"id": "7052b435e9846436d15f9c386cfdc5e969728362", "title": "Verbmobil - Translation of Face-To-Face Dialogs", "authors": ["Wolfgang Wahlster"], "date": 1993, "abstract": "Verbmobil is a long-term project on the translation of spontaneous language in negotiation dialogs. We describe the goals of the project, the chosen discourse domains and the initial project schedule. We discuss some of the distinguishing features of Verbmobil and introduce the notion of translation on demand and variable depth of processing in speech translation. Finally, the role of anytime modules for efficient dialog translation in close to real time is described.", "references": ["48257923de7681b2c3fbc5a76d3381f4920c7098", "fb8a0bafe72d2670f5cda25479eb9b3d9dd1b7c5", "308b421170b840b3fef7fe2d4788a79ee8b52795"], "page_rank": 0.0002463054187192118}, {"id": "cd5a169879504ea91660a443b9151753cc29c42f", "title": "Minimum Bayes-risk automatic speech recognition", "authors": ["Vaibhava Goel", "William J. Byrne"], "date": 2000, "abstract": "In this paper we address application of minimum Bayes-risk classifiers to tasks in automatic speech recognition (ASR). Minimum-risk classifiers are useful because they produce hypotheses in an attempt to be optimal under a specified task-dependent performance criterion. While the form of the optimal classifier is well known, its implementation is prohibitively expensive. We present efficient approximations that can be used to implement these procedures. In particular, anA* search over word lattices produced by a conventional ASR system is described. This algorithm is intended to extend the previously proposed N -best list rescoring approximation to minimum-risk classifiers. We provide experimental results showing that both the A*and N -best list rescoring implementations of minimum-risk classifiers yield better recognition accuracy than the commonly used maximum a posteriori probability (MAP) classifier in word transcription and identification of keywords. TheA* implementation is compared to the N -best list rescoring implementation and is found to obtain modest but significant improvements in accuracy at little additional computational cost. Another application of minimum-risk classifiers for the identification of named entities from speech is presented. Only the N -best list rescoring could be implemented for this task and was found to yield better named entity identification performance than the MAP classifier.", "references": ["0765c1fef84f21b89d00a6c5035d73bbf8aaa9c6", "3f1cde8cafbe2e682d1b2b41e57e4358c3f4ce5c", "445f658e3048bb1c60c003da73ef2907b0056f3e", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "8987ab010c3ad64ce7020184ad5189cfc9de743b", "3fd51769d828ae04e0a82888c5bd5047efdf112a", "219c382f29b734d0be0bbf0426aab825b328b3c1", "d95b1cbdf2e64b6b9663d302c2ba78884ab86987", "5209d1b3f57ee9fc6c08b1022cab5cb360eecc1f", "609a22c85938bb8233deeb3a8058b5862afaedbc"], "page_rank": 0.0002463054187192118}, {"id": "2797a6f03a187e76cc5fca02253ed49ac179c462", "title": "TRUMP: A transportable language understanding program", "authors": ["Paul S. Jacobs"], "date": 1992, "abstract": "Transportability has perpetually been the nemesis of natural language processing systems, in both the research and commercial sectors. During the last 20 years, the technology has not moved much closer to providing robust coverage of everyday language, and has failed to produce commercial successes beyond a few specialized interfaces and application programs. the redesign required for each application has limited the impact of natural language systems. Trump (TRansportable Understanding Mechanism Package) is a natural language analyzer that functions in a variety of domains, in both interfaces and text processing. While other similar efforts have treated transportability as a problem in knowledge engineering, Trump instead relies mainly on a \u201ccore\u201d of knowledge about language and a set of techniques for applying that knowledge within a domain. the information about words, word meanings, and linguistic relations in this generic knowledge base guides the conceptual framework of language interpretation in each domain. Turmp uses this core knowledge to piece together a conceptual representation of a natural language input by combining generic and specialized inforamtion. the result has been a language processing system that is capable of performing fairly extensive analysis with a minimum of customization for each application.", "references": [], "page_rank": 0.00022577996715927748}, {"id": "7ad3903a0c2645585946d00d075676dfe5f220f1", "title": "Word re-ordering and dynamic programming based search algorithm for statistical machine translation", "authors": ["Christoph Tillmann"], "date": 2002, "abstract": "In this work, a new search procedure for statistical machine translation (SMT) is proposed that is based on dynamic programming (DP). The starting point is a DP solution to the traveling salesman problem that works by jointly processing tours that visit the same subset of cities. For SMT, the cities correspond to source sentence positions to be translated. Imposing restrictions on the order in which the source positions are translated yields a DP algorithm for carrying out the word re-ordering in SMT efficiently. A simple data-driven search organization allows the algorithm to prune unlikely translation hypotheses. Search restrictions especially useful for the translation directions German-to-English and English-to-German are presented. A generalization of these re-ordering restrictions is given that is applicable to several different translation directions. Translation results are reported with a widely used SMT model.", "references": ["e7feffbba87f227e3de9244dbb75d39bd707d70a", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "fd0101dfbdd6768efe1e99a5ccd3ec0415fe723f", "e8e5a5270f7143214cc38e4316dd9dfdeed3fa84", "0b8f0e60a648880ddeaed371c339714f66f24624", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "62864f78fa4cb5f1ab45ebbe5a420b546b62d7a6", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "4711ff01d8eff9b9d10deeb3b68f366f7944c208"], "page_rank": 9.852216748768472e-05}, {"id": "471c18589caf74f7e149fb74df314668b239ff6f", "title": "Lexico-Semantic Pattern Matching as a Companion to Parsing in Text Understanding", "authors": ["Paul S. Jacobs", "George R. Krupka", "Lisa F. Rau"], "date": 1991, "abstract": "Ordinarily, one thinks of the problem of natural language understanding as one of making a single, left-to-right pass through an input, producing a progressively refined and detailed interpretation. In text interpretation, however, the constraints of strict left-to-right processing are an encumbrance. Multi-pass methods, especially by interpreting words using corpus data and associating units of text with possible interpretations, can be more accurate and faster than single-pass methods of data extraction. Quality improves because corpus-based data and global context help to control false interpretations; speed improves because processing focuses on relevant sections.The most useful forms of pre-processing for text interpretation use fairly superficial analysis that complements the style of ordinary parsing but uses much of the same knowledge base. Lexico-semantic pattern matching, with rules that combine lexical analysis with ordering and semantic categories, is a good method for this form of analysis. This type of pre-processing is efficient, takes advantage of corpus data, prevents many garden paths and fruitless parses, and helps the parser cope with the complexity and flexibility of real text.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5c0d500f108199c8f0ec05c040f8bb9c0e5ead95", "title": "Second message understanding conference (muck-ii) test report", "authors": ["Jonathan Nakamoto"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Second message understanding conference (muck-ii) test report\" by Jonathan Nakamoto", "references": [], "page_rank": 0.0006362889983579638}, {"id": "ffc1b0032e704185658ff36d0cfa5f7cfa5df362", "title": "Example-Based Incremental Synchronous Interpretation", "authors": ["Hans Ulrich Block"], "date": 2000, "abstract": "This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation \u201cchunks\u201d and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German \u21d4 English. German \u21d4 Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "2a4a9c37db04225f21a5b8ce764495a4897783d6", "title": "Study and Implementation of Combined Techniques for Automatic Extraction of Terminology", "authors": ["B{\\'e}atrice Daille"], "date": 1994, "abstract": "This paper presents an original method and its implementation to extract terminology from corpora by combining linguistic filters and statistical methods. Starting from a linguistic study of the terms of telecommunication domain, we designed a number of filters which enable us to obtain a first selection of sequences that may be considered as terms. Various statistical scores are applied to this selection and results are evaluated. This method has been applied to French and to English, but this paper deals only with French.", "references": ["d8ccc0846df41e225aede7a7c90ac5ccd899fe11", "094c0495ebb34c7eb61bad86a96eeebab06dab08", "025464b73f805e76689a7a20a48a9e9c0f4ff3ef", "da0cfa958dff6ef68d70d8b1df419a120c10006c", "f7a72b478b93ba757ae30754fa4753deaa517350"], "page_rank": 0.0001231527093596059}, {"id": "ceee0c5d577c6d17926fd8c8bc6e590fbc7d813a", "title": "Comlex Syntax: Building a Computational Lexicon", "authors": ["Ralph Grishman", "Catherine Macleod", "Adam Meyers"], "date": 1994, "abstract": "We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.", "references": ["bdaf232c561f1f50e88b1d24097e214890b37e8b", "108d05ba4d16e5991dd4b271360441b810767543", "b0e5ab189f770b7e106db429f2980510065ef125", "7d6775b3b8a3775821a01d2a54ddc471dac6e570", "f9a25e0dc776857fc24ebc7115c980312f2719b1", "611632f58f38487863306b56b2afcce107c2fb79", "79a0e0d47f80d9cfb3b50bf6aa6922a0026b93aa"], "page_rank": 0.0001231527093596059}, {"id": "d74b052ff7a9deecc92370a815ccd21ae764a280", "title": "Voice and gesture at the graphics interface", "authors": ["Richard A. Bolt"], "date": 1980, "abstract": "A vehicle transmission couples a plurality of input driven planetary arrangements with a plurality of selectable direct drive gear reduction means. The plurality of input driven planetary arrangements include a reverse planetary gear arrangement and in the preferred embodiment two forward planetary gear arrangements. The two forward planetary gear arrangements, each of which includes a ring gear which may be stopped, share a common planet carrier which drives the selectable direct drive gear reduction means. The reverse planetary gear arrangement includes a planet carrier which may be stopped, thus an associated reverse ring gear rotates in an opposite direction relative the input shaft. The reverse ring gear is coupled to the common planet carrier of the forward planet section. The direct drive section includes three drive gears in the preferred embodiment rotatably mounted on a quill shaft driven by the common planet carrier and coaxially mounted about the input shaft. Three driven gears are drivingly mounted on an output shaft, each intermeshing with a driving gear. Drive engagement devices are included to selectively engage one of the drive gears with the quill shaft.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "ac50078e58b33fc98d30187a88b1364aff716ba9", "title": "Unification-Based Semantic Interpretation in the BBN Spoken Language System", "authors": ["David Stallard"], "date": 1989, "abstract": "This paper describes the current state of work on unification-based semantic interpretation in HARC (for Hear and Recognize Continous speech) the BBN Spoken Language System. It presents the implementation of an integrated syntax/semantics grammar written in a unification formalism similar to Definite Clause Grammar. This formalism is described, and its use in solving a number of semantic interpretation problems is shown. These include, among others, the encoding of semantic selectional restrictions and the representation of relational nouns and their modifiers.", "references": ["fbc04a1951003ba164303b2898fb7f3c6b4e9083", "a376f76c6029cd053db3a601f95b0f4e9b3d56fc", "9274889f719b8308a0e389b2251d2a2b64c03e84", "edd8d454a9fa8163f6bc6918e06c5ef93c784bfd", "3f662d33b8c46e425522a9fe1ed1e281a559688a", "318a16558b5e03f3fa744d8136de296fcfd0e0a2"], "page_rank": 0.00016420361247947453}, {"id": "42053bb5bc617a3c75c56f61cc6be1e8e53f4e29", "title": "Role of Word Sense Disambiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues", "authors": ["Bonnie J. Dorr", "Douglas A. Jones"], "date": 1996, "abstract": "This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources.", "references": ["1d922631a6bf8361d7602e12cafb9e15d421c827", "1bcb0d197554534702640626e57b085051747fce", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "0616b0f5e6edce01f153081e53bd0152c8d0a4bd", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "d4e844b857bdfe4b560bad41b0d13aff5a87f03a", "2fa50c1037c6e1796442de57f744a4b9b8a6a08b", "b55a2d39a556e7ecbf3a6f1c3287c7cbb6d43dfc", "c847acb03f2855198b7f2a5053947ff8caa96f4b", "1b4f35e5298fc199eeb54b64dd5ee71565eb4d0d"], "page_rank": 0.0001231527093596059}, {"id": "b0e5ab189f770b7e106db429f2980510065ef125", "title": "From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax", "authors": ["Michael R. Brent"], "date": 1993, "abstract": "Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.", "references": ["8cf9b7c08655dadad0cad00771f3c9670181004e", "db52ec5e0c60d4f4cd16ec8fea822575bab17b57", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "fe30dc915eefa40755b25a363813fcc575536661", "16c762445f11fa2020994918dc4f93e76264df17", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "b4348e2b910a4a370a486c7c180741c2af712841", "f4f4d06c7d4461eaa1cb7dfa9297e69b11499efd"], "page_rank": 0.0003694581280788177}, {"id": "51d4d507f271f3c2976cb4d065764602031678dd", "title": "Real time hand shape recognition for man-machine interfaces", "authors": ["Koichi Ishibuchi", "Haruo Takemura", "Fumio Kishino"], "date": 1992, "abstract": "The authors describe a hand shape recognition method for man-machine interfaces using a pipeline image processor. The basic strategy is to use simple but effective real-time dynamical image processing to enhance the reliability of hand shape recognition. Low-level image processing reduces the data required for recognition of a hand shape. An algorithm for a noncontact-type hand shape recognizer has been developed for incorporation into a virtual reality environment. To realize this recognizer, the most important considerations are processing speed, the pointer's positional accuracy, and command recognition rate. The usefulness of the proposed method is verified through experimental results for these three points.>", "references": ["d54a2fcd45bbdc710bddb2174e54ead50f4b0844", "f706f1babe84c0728be3a06a4d3023cdc44f61c2"], "page_rank": 0.0002463054187192118}, {"id": "a5b16bf76efe6005be17a19e4dcb3b126b37158c", "title": "Surface shape from the deformation of apparent contours", "authors": ["Roberto Cipolla", "Andrew Blake"], "date": 1992, "abstract": "The spatiotemporal analysis of deforming silhouettes (apparent contours) is here extended using the mathematics of perspective projections and tools from differential geometry. Analysis of the image motion of a silhouette or apparent contour enables computation of local surface curvature along the corresponding contour generator on the surface, assuming viewer motion is known. To perform the analysis, a spatiotemporal parameterization of image-curve motion is needed, but is underconstrained (a manifestation of the well-known aperture problem). It is shown that an epipolar parameterization is most naturally matched to the recovery of surface curvature.One immediate facility afforded by the analysis is that surface patches can be reconstructed in the vicinity of contour generators. Once surface curvature is known, it is possible to discriminate extremal contours from other fixed curves in space. Furthermore, the known robustness of parallax as a cue to depth extends to the case of surface curvature. Its derivative\u2014rate of parallax\u2014is shown theoretically to be a curvature cue that is robust to uncertainties in the known viewer motion. This robustness has been confirmed in experiments.Finally, the power of the new analysis for robotics applications is demonstrated. Illustrations are given of an Adept robot, equipped with a CCD camera, circumnavigating curved obstacles. When further equipped with a suction gripper the robot manipulator can pick up an object by its curved surface, under visual guidance.", "references": ["df0b9f61e2765fa1eb541547ce403814364458c3", "6178079398696f058c0c1e85120aeb465bbc778d", "cdd1216d4d3cc42abc20089ed1292304d95cdbea", "1af25c587e3d7f664c9f14ed8494315418cae7f1", "673fe5bd0f80a35ff8d99a9bf50049152aed3d4c", "56a86116479441d3f1a96bb3940ef09d9e5bf7d5", "a4f8599c040dfd4876d9a5d99545b35b6a4e7a57", "4935da965dfc4f8824cd61268e77a8dd4b1b5bd5", "5fdc0e69cd859526a554da0b1de5380e738efaca", "75f60a199e06dd6e37a5c5b5edd44909bda89e9f"], "page_rank": 0.0001231527093596059}, {"id": "f2dc23000b57db1dff031e8650b17bf6750bd2f4", "title": "EDGE LOCATION AND DATA COMPRESSION FOR DIGITAL IMAGERY", "authors": ["Tabatabai Yazdi", "Mohammad Ali Jezu"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"EDGE LOCATION AND DATA COMPRESSION FOR DIGITAL IMAGERY\" by Tabatabai Yazdi et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "5ba7042c5220548c9d5636df3cc2c84bb8641e02", "title": "\u201cPut-that-there\u201d: Voice and gesture at the graphics interface", "authors": ["Richard A. Bolt"], "date": 1980, "abstract": "Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.\n The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "c2d2fefc1c61298059f9a160f190e6957587b74e", "title": "The Interpretation of Visual Motion", "authors": ["Shimon Ullman"], "date": 1979, "abstract": "This book uses the methodology of artificial intelligence to investigate the phenomena of visual motion perception: how the visual system constructs descriptions of the environment in terms of objects, their three-dimensional shape, and their motion through space, on the basis of the changing image that reaches the eye. The author has analyzed the computations performed in the course of visual motion analysis. Workable schemes able to perform certain tasks performed by the visual system have been constructed and used as vehicles for investigating the problems faced by the visual system and its methods for solving them.Two major problems are treated: first, the correspondence problem, which concerns the identification of image elements that represent the same object at different times, thereby maintaining the perceptual identity of the object in motion or in change. The second problem is the three-dimensional interpretation of the changing image once a correspondence has been established.The author's computational approach to visual theory makes the work unique, and it should be of interest to psychologists working in visual perception and readers interested in cognitive studies in general, as well as computer scientists interested in machine vision, theoretical neurophysiologists, and philosophers of science.", "references": [], "page_rank": 0.0003694581280788177}, {"id": "c70d1a2656ecc1335fd0c345e5014756541cae47", "title": "The analysis of cell images.", "authors": ["John M. Prewitt", "Mortimer L. Mendelsohn"], "date": 1966, "abstract": "Automation of the acquisition and interpretation of data in microscopy has been a focus of biomedical research for almost a decade. In spite of many serious mechanical perception of microscopic fields with a reliability that would inspire routine application still eludes us. Many facets of the problem appear to be well within the grasp of presentday technology. Thus, available histochemical techniques make it possible to prepare biological materials so that morphological integrity is preserved, key constituents are stained stoichiometrically, and the specimens are favorably dispersed for effective imaging one by one. Scanning microscopes now have the requisite sensitivity, resolution, and stability to sample such objects and make photometric measurements over a wide range of magnifications and wavelengths within the visible and near-visible spectrum. Furthermore, modern large capacity, high speed data facilities at last provide the ability to manipulate the hitherto unmanageable quantities of optical information contained within all but the simplest images. With the basic materials for achieving automation via mensuration finally a t hand, attention has been turned toward generating and evaluating methods for extracting meaning from quantitative optical information. Definitive concepts for image structuring and image characterization have yet to be realized, to be given satisfactory operational definitions, and to be assembled within a machine-oriented perceptual framework.6 Criteria for effective and efficient discrimination and interpretation of images must be evolved. It would be a serious mistake to presuppose that mechanical perception must mimic the human\u2019s perceptual apparatus in organizing images as complexes of picture eIements. Likewise i t would be serious to ignore traditional descriptive morphology and established taxonomies. In steering a middle course, the exploration of many complementary approaches and the introduction of numeric methods which fully utilize measurements of light intensity of optical density seemed to us to hold the most promise for augmenting and explicating the existing, largely verbal tradition of microscopic morphology. To realize these objectives, we have designed a system with three basic components: 1) a sensor which rigorously and effectively scans microscopic fields and converts the optical information into digital form; 2) human analysts who contribute heuristics, devise image processing methods and en-", "references": ["d19b4e24a811322b0146c0aa64d6c9adcc6af9b0", "40d82658aea7135674d82a5ebbb5cd171c1fe94a", "f97497477812cfca063e6716e7968785dbcefad4", "e030e50f4f828bedc9520d6ba995468359f3495e", "496de4d899424077d7b86d1e418cc367f30c8424", "d9cd44b822d094f697b2525623f51f39744454df", "0217065021a8166e88dbcdf60f23f2222c6338bc", "898f383ce8ce85c2fd0827ce2964ce1ba229d995", "c49de029ca032ea39fab5fd3a3c289b35095c542"], "page_rank": 0.0002463054187192118}, {"id": "d06d11ade5601c6b50606ef8bf8446398f452449", "title": "Understanding Natural Texture", "authors": ["Joseph Tudor Maleson", "Crw Brown", "Jerome A. Feldman"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"Understanding Natural Texture\" by Joseph Tudor Maleson et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "11498ac1e146f3ecd800227690a8715f41c926c1", "title": "Computer determination of the constituent structure of biological images.", "authors": ["Russell A. Kirsch"], "date": 1971, "abstract": "Abstract A class of algorithms is described which enables computer quantized images to be decomposed into constituent reflecting the structure of the images. This decomposition is viewed as the morphological precursor to a higher level syntactic analysis. Numerical results for a typical biological image are presented.", "references": ["6d3dc054f099cec3bdd05ec79925cbe0a821ea47"], "page_rank": 0.00016420361247947453}, {"id": "57337d2cf02c26901eb8add0c2b85d97ab668aad", "title": "Random Mosaic Models for Textures", "authors": ["Bruce J. Schachter", "Azriel Rosenfeld", "Larry S. Davis"], "date": 1978, "abstract": "Semantic Scholar extracted view of \"Random Mosaic Models for Textures\" by Bruce J. Schachter et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2811e815185c1c00de5cd6c7e34b78a17322343b", "title": "Use of Range and Reflectance Data to Find Planar Surface Regions", "authors": ["Richard O. Duda", "David Nitzan", "Phyllis Barrett"], "date": 1979, "abstract": "This paper describes a sequential procedure for processing registered range and intensity data to detect and extract regions that correspond to planar surfaces in a scene.", "references": ["2415fd60305739543105118739f7118493257af3", "9aef4c7405f4ea1b3c75e6efce4ac5976aa8ad92", "11c227d1636ecc90b60e0162fdd87f625649e962", "b30de454c99b8006db60cd53bea5f9f2d6fc8cd4", "56ac0941e95d512ee7284614b95e3b257e7236d5", "a8adc8072357c7a500fe04c82f31922cea655dac", "6f6784ee9cddba10ec8fd0027b2a36f44ef2c73d", "b6adfcae8d0207bac17e185cf5a5b07a893935d8", "798734c62a4ea75dc14b99c54bfea512e2d96b7c", "cac7561a417ac6c2d3f1b42fa4ca55d2a7466687"], "page_rank": 0.00016420361247947453}, {"id": "de0b63262321ce0526d37e79df0c19bd6891c929", "title": "The analysis, synthesis, and description of biological images.", "authors": ["Lewis E. Lipkin", "William C. Watt", "Russell A. Kirsch"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"The analysis, synthesis, and description of biological images.\" by Lewis E. Lipkin et al.", "references": ["a68886c4fa0fa4500832b3a9fcb4c0e44b2301f8", "f4cec17ea64372d48f322d9e06a1b10ea3961f1c", "a635d6eb5fc61a477f9310c5aa0973aa2df2b375", "3e30741fff23756c36fbc3670da2a6345a42eb7c", "47f2261792cef069622607ba74bf7e3178651667", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b"], "page_rank": 0.0004926108374384236}, {"id": "07f8dc8ac06b0b3305ea96a62ea98145dc9620f4", "title": "Twisted Pair Grammar: Support for Rapid Development of Machine Translation for Low Density Languages", "authors": ["Douglas Jones", "Rick Havrilla"], "date": 1998, "abstract": "We describe a streamlined knowledge acquisition method for semi-automatically constructing knowledge bases for a Knowledge Based Machine Translation (KBMT) system. This method forms the basis of a very simple Java-based user interface that enables a language expert to build lexical and syntactic transfer knowledge bases without extensive specialized training as an MT system builder. Following [Wu 1997], we assume that the permutation of binary-branching structures is a sufficient reordering mechanism for MT. Our syntactic knowledge is based on a novel, highly constrained grammar construction environment in which the only re-ordering mechanism is the permutation of binary-branching structures (Twisted Pair Grammar). We describe preliminary results for several fully implemented components of a Hindi/Urdu to English MT prototype being built with this interface.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "19cc8befd39c2f8c555918e5be15c33ef5d1c651", "title": "Aligning Noisy Parallel Corpora Across Language Groups : Word Pair Feature Matching by Dynamic Time Warping", "authors": ["Pascale Fung", "Kathleen McKeown"], "date": 1994, "abstract": "We propose a new algorithm called DK-vec for aligning pairs of Asian/Indo-European noisy parallel texts without sentence boundaries. DK-vec improves on previous alignment algorithms in that it handles better the non-linear nature of noisy corpora. The algorithm uses frequency, position and recency information as features for pattern matching. Dynamic Time Warping is used as the matching technique between word pairs. This algorithm produces a small bilingual lexicon which provides anchor points for alignment.", "references": ["1f55d2bca810edbf9870934a41d956d06ae2d9cf", "b83dbea361cd583f9fdfb134bb55c48f0335d297", "3491aa4a9a66ba3d1603230a70d82c7479666a7d", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "a76563076016fb1cb813deba45db2409772a51da", "8d021434c8da92634a49cead486826789af8ffff", "1cf01798f07e1f13cd44daad6119f4978721c61d", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "34d7f231fe2b9f243a60a4a64c06028ad7ba776b", "13a5c0e48580415e825ecba20d6db50b93949c1b"], "page_rank": 0.0002463054187192118}, {"id": "111bd450219d726f8edd1d09fbfe1eb0eba2c626", "title": "Computer Vision Systems for Industry: Comparisons", "authors": ["Igor Aleksander", "T. John Stonham", "Bruce A. Wilkie"], "date": 1982, "abstract": "This article is written for those who are not familiar with the problems of automatic, computer-based pattern recognition. It surveys known methods in the light of opportunities offered by silicon chip technology.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "fbf75104b26aec301837ffbaf29859f6cc842aaa", "title": "On the generalized Karhunen-Loeve expansion", "authors": ["Y. P. Chien", "King-Sun Fu"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"On the generalized Karhunen-Loeve expansion\" by Y. P. Chien et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "b8831f3095686131a98731d6806abdc75c6baebd", "title": "Symbolic Reasoning Among 3-D Models and 2-D Images", "authors": ["Rodney A. Brooks"], "date": 1981, "abstract": "We describe model-based vision systems in terms of four components: models, prediction of image features, description of image features, and interpretation which relates image features to models. We describe details of modelling, prediction and interpretation in an implemented model-based vision system. Both generic object classes and specific objects are represented by volume models which are independent of viewpoint. We model complex real world object classes. Variations of size, structure and spatial relations within object classes can be modelled. New spatial reasoning techniques are described which are useful both for prediction within a vision system, and for planning within a manipulation system. We introduce new approaches to prediction and interpretation based on the propagation of symbolic constraints. Predictions are two pronged. First, prediction graphs provide a coarse filter for hypothesizing matches of objects to image feature. Second, they contain instructions on how to use measurements of image features to deduce three dimensional information about tentative object interpretations. Interpretation proceeds by merging local hypothesized matches, subject to consistent derived implications about the size, structure and spatial configuration of the hypothesized objects. Prediction, description and interpretation proceed concurrently from coarse object subpart and class interpretations of images, to fine distinctions among object subclasses and more precise three dimensional quantification of objects. We distinguish our implementations from the fundamental geometric operations required by our general image understanding scheme. We suggest directions for future research for improved algorithms and representations.", "references": ["7383ddc07be0679387e890ec88ec3a3147379cdf", "092e596a5188b9fce64e038e391cb92ef50fce3f", "d83ecd7a588461dc13f579d723801a46b90144cb", "bdf9b8f4de001f5f37bc844efbce1210d581d599", "4dd5ad87ccf9b8493625e38514e2f37ab9ce99cb", "d1c6bc84f65e508c9c9fe9d3eb5f6a96abd4d5fa", "dacb05677b4a25a06c77a080f7caa0baf5b7aef6", "744689eff7096c4c8bc292b9899f011b0cebf94b", "9e2c040f507b9bd81730d92e4036d7b52b717770", "033ecd544c4e71b14a7d3ee0611a300a20b91232"], "page_rank": 0.0007389162561576354}, {"id": "64455eb3fd378965589a69aa8472c39af6d332d2", "title": "Learning Dependency Translation Models as Collections of Finite-State Head Transducers", "authors": ["Hiyan Alshawi", "Srinivas Bangalore", "Shona Douglas"], "date": 2000, "abstract": "The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction. These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese.", "references": ["e8780a2655c8e0c5ff691df1c917adc049d50f37", "d130685d04bc69397accf0b0fb1a2059dd18a5cb", "ed6c40bf6016af891f091d51cb7962c9a3f2232f", "5ce919d90ee41a8a6852677f7553e297de2b06a7", "2a9d6137c95cc7c106bf84f18fb13449eded7826", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "21e69bb8d3a36235a19da1408279290055b373bc", "40dbb25a15b63af3faccb81c8e64a3f5d659e07e", "ec2f1ca803723c0fc09d4c10e045895cde526330"], "page_rank": 0.0002463054187192118}, {"id": "308b421170b840b3fef7fe2d4788a79ee8b52795", "title": "Integration of speech recognition and language processing in spoken language translation system (SL-TRANS)", "authors": ["Tsuyoshi Morimoto", "Kiyohiro Shikano", "Hitoshi Iida", "Akira Kurematsu"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Integration of speech recognition and language processing in spoken language translation system (SL-TRANS)\" by Tsuyoshi Morimoto et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2406b90d779c59305e2b758f225658e454e2f039", "title": "An Evaluation Methodology for Natural Language Processing Systems", "authors": ["Jeannette G. Neal", "Elissa L. Feit", "Douglas J. Funke", "Christine A. Montgomery"], "date": 1992, "abstract": "Abstract : The Neal-Montgomery NLP Evaluation Methodology was developed under the 'Benchmark Investigation/Identification' project as a means of determining the linguistic competence of Natural Language Processing (NLP) systems. Embodied in an evaluation tool based on a detailed classification of linguistic phenomena with, currently, over 350 test items, the methodology produces descriptive profiles of NLP system linguistic capabilities, and can be applied without regard to the current system application, domain, or specific language processing function. To date, it has been applied to three NL data base query systems and three text processing systems. Researchers are invited to apply the evaluation methodology (included within this report) to their systems and report their critique and recommendations towards the evolution of standard evaluation procedures for NLP systems.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "e2583da9d16a6a596158a8eef220831b0d3d8f8a", "title": "The viewpoint consistency constraint", "authors": ["David G. Lowe"], "date": 2004, "abstract": "The viewpoint consistency constraint requires that the locations of all object features in an image must be consistent with projection from a single viewpoint. The application of this constraint is central to the problem of achieving robust recognition, since it allows the spatial information in an image to be compared with prior knowledge of an object's shape to the full degree of available image resolution. In addition, the constraint greatly reduces the size of the search space during model-based matching by allowing a few initial matches to provide tight constraints for the locations of other model features. Unfortunately, while simple to state, this constraint has seldom been effectively applied in model-based computer vision systems. This paper reviews the history of attempts to make use of the viewpoint consistency constraint and then describes a number of new techniques for applying it to the process of model-based recognition. A method is presented for probabilistically evaluating new potential matches to extend and refine an initial viewpoint estimate. This evaluation allows the model-based verification process to proceed without the expense of backtracking or search. It will be shown that the effective application of the viewpoint consistency constraint, in conjunction with bottom-up image description based upon principles of perceptual organization, can lead to robust three-dimensional object recognition from single gray-scale images.", "references": ["8735690a9e8f8884bf27717877ddf7f9071472e5", "27ce5a120a86632dd56f869ee65656b7d7312a3a", "880e17cd35abe8a602db11ecd3b58703a82084c0", "4f37468a95ccc62debb9e5a4cb0d73489ca61190", "b8831f3095686131a98731d6806abdc75c6baebd", "85edabd5dbaefd061a10fffd7349623cc81a80ac", "8336e347624d32b8d3762dc7a4640dfcb96dbe06", "9009c9685754346deb93f316144a9da1f70ffcd8", "7232c7a3aad15ab5d9f3de951f331eaf14169920", "ed165fa39e4ecda494049f459bbe57448101ecc4"], "page_rank": 0.0002463054187192118}, {"id": "48257923de7681b2c3fbc5a76d3381f4920c7098", "title": "Verbmobil: A Translation System for Face-to-Face Dialog", "authors": ["Martin Kay", "Peter Norvig", "Mark Gawron"], "date": 1992, "abstract": "From the Publisher: \nThe first experimental prototypes, with restricted capabilities, of Verbmobil, a system that provides simultaneous language translations, are anticipated by the year 2000. This volume is the result of a study conducted at CSLI to assess the program's realistic chances for success.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "6f9720d2d46b508091191cffe78b9106d330b90d", "title": "On the best finite set of linear observables for discriminating two Gaussian signals", "authors": ["T. T. Kadota", "Larry A. Shepp"], "date": 1967, "abstract": "Consider the problem of discriminating two Gaussian signals by using only a finite number of linear observables. How to choose the set of n observables to minimize the error probability P_{e} , is a difficult problem. Because H , the Hellinger integral, and H^{2} form an upper and a lower bound for P_{e} , we minimize H instead. We find that the set of observables that minimizes H is a set of coefficients of the simultaneously orthogonal expansions of the two signals. The same set of observables maximizes the Hajek J -divergence as well.", "references": ["df5313827e4dc0a4c59c08ddb270f7a1b943d219", "44221534c366d1056147326c56f9e60117720df5", "c85eb1279d4f3416aa8dbf97d984c37618c97d57", "d623fbed40fb161b1cfa391f8ed27520ed3e226a", "92d5bbaa9981e33d9f09f32fcd14c80c02f9352f"], "page_rank": 0.0002463054187192118}, {"id": "609a22c85938bb8233deeb3a8058b5862afaedbc", "title": "LVCSR rescoring with modified loss functions: a decision theoretic perspective", "authors": ["Vaibhava Goel", "William J. Byrne", "Sanjeev Khudanpur"], "date": 1998, "abstract": "The problem of speech decoding is considered in a decision theoretic framework and a modified speech decoding procedure to minimize the expected risk under a general loss function is formulated. A specific word error rate loss function is considered and an implementation in an N-best list rescoring procedure is presented. Methods for estimation of the parameters of the resulting decision rules are provided for both supervised and unsupervised training. Preliminary experiments on an LVCSR task show small but statistically significant error rate improvements.", "references": ["3f1cde8cafbe2e682d1b2b41e57e4358c3f4ce5c", "0765c1fef84f21b89d00a6c5035d73bbf8aaa9c6", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "d80000d84223e177d070a01a734dba56d5f5c069", "ae4614f758dfa344a04b33377c96abc10d5eeda7", "92d92fd3de1af6a46e8cd9ca841d5433e659179f"], "page_rank": 0.00016420361247947453}, {"id": "4711ff01d8eff9b9d10deeb3b68f366f7944c208", "title": "A Polynomial-Time Algorithm for Statistical Machine Translation", "authors": ["Dekai Wu"], "date": 1996, "abstract": "We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy.", "references": ["9080881918bf9f19ba34459bb97a5b2be168f8ea", "79f20fb39a6e78352ebbb65b1737970837a420b5", "ab7b5917515c460b90451e67852171a531671ab8", "9828f03fe5c356e0b757284ff2286cfb4be990c9", "a1066659ec1afee9dce586f6f49b7d44527827e1", "c9ff51bb4956e17911acce3c5984557c23152c47", "e9b1c93223ea7f7c8a835bd588a39d4aae45a5fe", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "b83dbea361cd583f9fdfb134bb55c48f0335d297", "1f0a9628b7c93f285f6d5fd34b73d6716320a10e"], "page_rank": 0.00032840722495894905}, {"id": "5209d1b3f57ee9fc6c08b1022cab5cb360eecc1f", "title": "A maximum entropy language model integrating N-grams and topic dependencies for conversational speech recognition", "authors": ["Sanjeev Khudanpur", "Jingjing Wu"], "date": 1999, "abstract": "A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topic-independent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints.", "references": ["339ccf2ce210260d0a5f36f623859525f089a38a", "e4efb4b4c07d02b4031111b1cb97a7a13b5c928a", "6612de2c77458ca746325272a99e22c156263383", "673992da19d9209434615b12d55bdd36be706e9e", "2854e19e23a0987d0989cd8dfadebec3692e3a60", "1e9f3e6da845b025a849ea98b5d93490ac91a7fc", "845fdee2b249c1c141c856251eac53dd3be2c324", "feda127528eea5d05a30810c6869d13050b0a61e", "37c931cbaa9217b829596dd196520a838562a109", "71c89de84e96a77db2accbdba04d6fda45bc044c"], "page_rank": 0.00016420361247947453}, {"id": "da0cfa958dff6ef68d70d8b1df419a120c10006c", "title": "Acquisition of Lexical Information from a Large Textual Italian Corpus.", "authors": ["Remo Bindi", "Nicoletta Calzolari"], "date": 1996, "abstract": "Semantic Scholar extracted view of \"Acquisition of Lexical Information from a Large Textual Italian Corpus.\" by Remo Bindi et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "d95b1cbdf2e64b6b9663d302c2ba78884ab86987", "title": "Finding consensus among words: lattice-based word error minimization", "authors": ["Lidia Mangu", "Eric Brill", "Andreas Stolcke"], "date": 1999, "abstract": "We describe a new algorithm for finding the hypothesis in a recognition lattice that is expected to minimize the word error rate (WER). Our approach thus overcomes the mismatch between the word-based performance metric and the standard MAP scoring paradigm that is sentence-based, and that can lead to sub-optimal recognition results. To this end we first find a complete alignment of all words in the recognition lattice, identifying mutually supporting and competing word hypotheses. Finally, a new sentence hypothesis is formed by concatenating the words with maximal posterior probabilities. Experimentally, this approach leads to a significant WER reduction in a large vocabulary recognition task.", "references": ["0765c1fef84f21b89d00a6c5035d73bbf8aaa9c6", "5c5e952f1414829947635b6a330dead4d6f4797e", "91e6bf81fbc0000e6ae62c11a46288049f7e828c", "3cedc790a215d8d2366125181f28f7a7545202c7", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "0861015b3c89d68749548c19c6f056eee34eafc6", "d80000d84223e177d070a01a734dba56d5f5c069", "b07ce649d6f6eb636872527104b0209d3edc8188"], "page_rank": 0.00016420361247947453}, {"id": "f7a72b478b93ba757ae30754fa4753deaa517350", "title": "Concordances for parallel text", "authors": ["Kenneth Ward Church", "William A. Gale"], "date": 1991, "abstract": "Semantic Scholar extracted view of \"Concordances for parallel text\" by Kenneth Ward Church et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "3f662d33b8c46e425522a9fe1ed1e281a559688a", "title": "An overview of nikl", "authors": ["M. G. Moser"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"An overview of nikl\" by M. G. Moser", "references": [], "page_rank": 0.0001231527093596059}, {"id": "318a16558b5e03f3fa744d8136de296fcfd0e0a2", "title": "In: Common Lisp the Language", "authors": ["Daniel G. Bobrow", "Linda G. DeMichiel", "Richard P. Gabriel", "Sonya E. Keene", "Gregor Kiczales", "Albert E Moon"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"In: Common Lisp the Language\" by Daniel G. Bobrow et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "title": "Machine Translation with a Stochastic Grammatical Channel", "authors": ["Dekai Wu", "Hongsing Wong"], "date": 1998, "abstract": "We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation. As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion-transduction model. However, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model.", "references": ["13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "1f0a9628b7c93f285f6d5fd34b73d6716320a10e", "c9ff51bb4956e17911acce3c5984557c23152c47", "bc37cadd2d39be8fa7e1004f24160c3c709cbd6b", "ab7b5917515c460b90451e67852171a531671ab8", "4711ff01d8eff9b9d10deeb3b68f366f7944c208", "79f20fb39a6e78352ebbb65b1737970837a420b5", "40dbb25a15b63af3faccb81c8e64a3f5d659e07e", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "1efe8329c9dec8fb276605c388a538220608b608"], "page_rank": 0.00016420361247947453}, {"id": "094c0495ebb34c7eb61bad86a96eeebab06dab08", "title": "Automatically Extracting and Representing Collocations for Language Generation", "authors": ["Frank Smadja", "Kathleen McKeown"], "date": 1990, "abstract": "Collocational knowledge is necessary for language generation. The problem is that collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.", "references": ["13470948ea2c24ceb4d309e004f5087f25487d84", "e879e354f54fcfd0efb060ddaa3a419d2e7ec607", "ed1a55cb2d4ecc213868d0072948553b894b5696", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "eeb08a3f121cde9281daa5a5208df50011e34c0d", "91b52959c7731830f1518ed35e5ab1dabadf79ec"], "page_rank": 0.00016420361247947453}, {"id": "edd8d454a9fa8163f6bc6918e06c5ef93c784bfd", "title": "Implementing Systemic Classification by Unification", "authors": ["Chris Mellish"], "date": 1988, "abstract": "985,920. Extruding hose lengths. KUNSTSTOFFMASCHINEN A.G. Aug. 25, 1961 [June 9, 1961], No. 30820/61. Heading B5A. Hose lengths of thermoplastic synthetic material are extruded through an annular inwardly tapering die orifice formed between the conical head of a nozzle core and a correspondingly shaped sleeve, the core and sleeve being relatively movable under the control of cam means whereby the orifice size and hence the thickness of the extruded hose are controlled automatically as desired in relation to the extruded hose length. In one embodiment (Fig. 1), a rod 2b extending through an upper core portion 2a carries a conical head 2 and is connected to a piston 5 urged by hydraulic fluid in a chamber 4 upwardly to hold a roller 6 against a cam 7. Rotation of the cam 7 in synchronism with other machine control cams causes movement of the head 2 to vary the orifice. Alternatively (Fig. 2, not shown), the rod 2b is displaced by a cam actuated lever, an adjustable stroke limiting stop being provided. In another embodiment (Fig. 4) the core 2, 2a is stationary, the outer die member comprising a fixed portion 22 and a movable portion 23 and the oppositely threaded portions 22, 23 being engaged and adjustably spaced by a nut 25 engaged by a rack 26 the displacement of which is controlled by a cam 7 acting through an hydraulic system including a hand adjustment 17, 17a for adjusting the fluid content of the hydraulic system. In another embodiment (Fig. 3, not shown), the hydraulic system includes a hand adjustment and the moving core is similar to that of Fig. 1. Specification 941,127 is referred to.", "references": ["3340fc907ec226a029c3ad86090359e140cefcfc", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "1007c45bdc7e77f71cb46861cb93697179ea7fd7", "ceb2380e6919f7ea2ea3d8595601f324ba34aa05", "12d092a75da54f1b971fd823216e3d8edc89f336", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "8db3d17b289caf503d7cf780019612b7aa4cda0b", "916eda20c1a516b4dbac230548813c5375420346", "3efb8558a7f8563eeb2985f55aba9f88e80ebac6", "152630e3f02d60d461215a0cc474e121c48f2a2d"], "page_rank": 0.00028735632183908046}, {"id": "9274889f719b8308a0e389b2251d2a2b64c03e84", "title": "An Improved Context-Free Recognizer", "authors": ["Susan L. Graham", "Michael A. Harrison", "Walter L. Ruzzo"], "date": 1980, "abstract": "A new algorithm for recognizing and parsing arbitrary context-free languages is presented, and several new results are given on the computational complexity of these problems. The new algorithm is of both practical and theoretical interest. It is conceptually simple and allows a variety of efficient implementations, which are worked out in detail. Two versions are given which run in faster than cubic time. Surprisingly close connections between the Cocke-Kasami-Younger and Earley algorithms are established which reveal that the two algorithms are \u201calmost\u201d identical.", "references": ["0cf8464641fc13906f0e93749ceede70dd45bf52", "c3313782db9fd777feae94cf12d2fd2170b2cab0", "5e07359ce25384c97a3695c708f913b470aa8abf", "ffce6413805bfd1026a35a56655baa9fd13bb2bd"], "page_rank": 0.0002463054187192118}, {"id": "5fdc0e69cd859526a554da0b1de5380e738efaca", "title": "Qualitative Depth from Stereo, with Applications", "authors": ["Daphna Weinshall"], "date": 1990, "abstract": "Abstract Obtaining exact depth from binocular disparities is hard if camera calibration is needed. We will show that qualitative information can be obtained from stereo disparities with little computation and without prior knowledge (or computation) of camera parameters. First, we derive two expressions that order all matched points in the images by depth in two distinct ways from image coordinates only. Using one for tilt estimation and point separation (in depth) demonstrates some anomalies observed in psychophysical experiments, most notably the \u201cinduced size effect.\u201d We apply the same approach to detect qualitative changes in the curvature of a contour on the surface of an object, with eitherx- ory-coordinate fixed. Second, we develop an algorithm to compute axes of zero-curvature from disparities alone. The algorithm is shown to be quite robust against violations of its basic assumptions for synthetic data with relatively large controlled deviations. It performs almost as well on real images, as demonstrated on an image of four cans at different orientations.", "references": ["493ce1be809e1a65c0030696f2767fdc80ab7882", "c7886c1597626ccb8ba698da74c1f3738e1fd326", "d7730295185f1e98ef087525e2dc1518114a9306", "5243efc385dcfff29d47cce432e3cbda05c745b6", "4c94c6bd45fc81a33b5a9fca42bd5a6166079325", "8380b576550f0b6e138a8df10b04d864491205c8", "511c0e70cb0f7c7f39cfe3071818ec9469135222", "9af35fdaf413687acaee8ad47e75b23534b79064", "810bd193898f9c58c93cfddd1c497eb5e2f342bc", "9ba3565169b7d6d2d8ffa8c72237fc8ca6da70b7"], "page_rank": 0.0001231527093596059}, {"id": "75f60a199e06dd6e37a5c5b5edd44909bda89e9f", "title": "Description of solid shape and its inference from occluding contours", "authors": ["Jack M. H. Beusmans", "Donald D. Hoffman", "Bruce M. Bennett"], "date": 1987, "abstract": "We explore a method of representing solid shape that is useful for visual recognition. We assume that complex shapes are constructed from convex, compact shapes and that construction involves three operations: solid union (to form humps), solid subtraction (to leave dents), and smoothing (to remove discontinuities). The boundaries between shapes joined through these operations are contours of extrema of a principal curvature. Complex objects can be decomposed along these boundaries into convex shapes, the so-called parts. We suggest that this decomposition into parts forms the basis for a shape memory. We show that the part boundaries of an object can be inferred from its occluding contours, at least up to a number of ambiguities.", "references": ["cdd1216d4d3cc42abc20089ed1292304d95cdbea", "de41855333277b5b9d9c1bf4e36f6e5cfa43202b", "cbd5fb9fc78fc6eba7ed331177fe7ffc3b6ce669", "23441c50e7180c119bf9a37f86f280286d4f3a51", "95bcfe5a35df789554e1d9d35fc9fbf35d46fff5", "3afe5f2fb915a007bf4e8a71f593e0844550c819", "5cf7575219ec7432dc344780a7927527325fbc7c", "a0a0fd62cd7236b2ac799a6230a8a62570a08527", "3e643930ac21c71c46c7f74cebbc20b366f7a8c4", "603712db9533a60935d0dce32a8afe018e59c313"], "page_rank": 0.0001231527093596059}, {"id": "16c762445f11fa2020994918dc4f93e76264df17", "title": "Aspects of the Theory of Syntax", "authors": ["Noam Chomsky"], "date": 1965, "abstract": "Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon", "references": [], "page_rank": 0.00036945812807881765}, {"id": "4935da965dfc4f8824cd61268e77a8dd4b1b5bd5", "title": "Uniqueness and Estimation of Three-Dimensional Motion Parameters of Rigid Objects with Curved Surfaces", "authors": ["Roger Y. Tsai", "Thomas S. Huang"], "date": 1984, "abstract": "Two main results are established in this paper. First, we show that seven point correspondences are sufficient to uniquely determine from two perspective views the three-dimensional motion parameters (within a scale factor for the translations) of a rigid object with curved surfaces. The seven points should not be traversed by two planes with one plane containing the origin, nor by a cone containing the origin. Second, a set of ``essential parameters'' are introduced which uniquely determine the motion parameters up to a scale factor for the translations, and can be estimated by solving a set of linear equations which are derived from the correspondences of eight image points. The actual motion parameters can subsequently be determined by computing the singular value decomposition (SVD) of a 3\u00d73 matrix containing the essential parameters.", "references": ["0464edbdff7027722208316ed69b937d3079a83e", "59da095ce0ac2127fa02e9cf818ba33931d70070", "80023a52c635ed65ab6a0a2e8cefe21a17020cb6", "45f44e091d0166559a0661545af1001720de6a1e", "ad66f034fb63ce48524af09b7f162b397e44d364", "a23d3a65df31e1523765c84972a062aea30171c3", "18d41a6590bb30fbeb3bafffdd8ab8c408fb0a7a", "df35abc5edd50b3a4d89cff2c85dbde76b885a45", "d7730295185f1e98ef087525e2dc1518114a9306", "278c9a78d4505cfaf6b709df364dbd1206a017c1"], "page_rank": 0.0001231527093596059}, {"id": "c49de029ca032ea39fab5fd3a3c289b35095c542", "title": "INITIAL APPROACHES TO THE COMPUTER ANALYSIS OF CYTOPHOTOMETRIC FIELDS.", "authors": ["Mortimer L. Mendelsohn", "Wilfred A. Kolman", "Robert C. Bostrom"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"INITIAL APPROACHES TO THE COMPUTER ANALYSIS OF CYTOPHOTOMETRIC FIELDS.\" by Mortimer L. Mendelsohn et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "d19b4e24a811322b0146c0aa64d6c9adcc6af9b0", "title": "Morphological analysis of cells and chromosomes by digital computer.", "authors": ["Mortimer L. Mendelsohn", "Wilfred A. Kolman", "Barbara H Perry", "John M. Prewitt"], "date": 1965, "abstract": "A computer-oriented approach to the analysis of microscopic images has been developed around CYDAC, a data conversion system which scans through the microscope and records the optical information in digital form on magnetic tape. The quality of the optical information thus made available to the computer is surprisingly good, as evidenced by reconstructions of the original image using the high-speed printer. Preparations for the computer- interpretation of scanned images are underway, using blood cells as a model system. Parallel attempts to classify human chromosomes by means of CYDAC scans have given very encouraging preliminary results.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "898f383ce8ce85c2fd0827ce2964ce1ba229d995", "title": "Fundamentals Of Concept Formation In Empirical Science", "authors": ["Carl Gustav Hempel"], "date": 1952, "abstract": "Semantic Scholar extracted view of \"Fundamentals Of Concept Formation In Empirical Science\" by Carl Gustav Hempel", "references": [], "page_rank": 0.0001231527093596059}, {"id": "0217065021a8166e88dbcdf60f23f2222c6338bc", "title": "Decision-Making Processes In Pattern Recognition", "authors": ["George S. Sebestyen"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"Decision-Making Processes In Pattern Recognition\" by George S. Sebestyen", "references": [], "page_rank": 0.0001231527093596059}, {"id": "a4f8599c040dfd4876d9a5d99545b35b6a4e7a57", "title": "Interpreting line drawings of curved objects", "authors": ["Jitendra Malik"], "date": 1987, "abstract": "In this paper, we study the problem of interpreting line drawings of scenes composed of opaque regular solid objects bounded by piecewise smooth surfaces with no markings or texture on them. It is assumed that the line drawing has been formed by orthographic projection of such a scene under general viewpoint, that the line drawing is error free, and that there are no lines due to shadows or specularities. Our definition implicitly excludes laminae, wires, and the apices of cones.A major component of the interpretation of line drawings is line labelling. By line labelling we mean (a) classification of each image curve as corresponding to either a depth or orientation discontinuity in the scene, and (b) further subclassification of each kind of discontinuity. For a depth discontinuity we determine whether it is a limb\u2014a locus of points on the surface where the line of sight is tangent to the surface\u2014or an occluding edge\u2014a tangent plane discontinuity of the surface. For an orientation discontinuity, we determine whether it corresponds to a convex or concave edge. This paper presents the first mathematically rigorous scheme for labelling line drawings of the class of scenes described. Previous schemes for labelling line drawings of scenes containing curved objects were heuristic, incomplete, and lacked proper mathematical justification.By analyzing the projection of the neighborhoods of different kinds of points on a piecewise smooth surface, we are able to catalog all local labelling possibilities for the different types of junctions in a line drawing. An algorithm is developed which utilizes this catalog to determine all legal labellings of the line drawing. A local minimum complexity rule\u2014at each vertex select those labellings which correspond to the minimum number of faces meeting at the vertex\u2014is used in order to prune highly counter-intuitive interpretations. The labelling scheme was implemented and tested on a number of line drawings. The labellings obtained are few and by and large in accordance with human interpretations.", "references": ["5732ad4b2e63b4bc322d2893188fba8728fd61b4", "e8f147a94d2b58c099847b963137f9779fe31325", "9bf58c1ad6f16fa65660ddfc303f5124e39004fd", "cdd1216d4d3cc42abc20089ed1292304d95cdbea", "bad4a0959b81cc2f9c3267a0efe36e07310ccb31", "2643759cac24ceb45470e731e808d7ecf0b85bce", "880e17cd35abe8a602db11ecd3b58703a82084c0", "8718bdea2c3aedc550f09d2709d2d75b25cbfab9", "1f9a9153f15655ea8a07186d42307d6d43032d7d", "92aadef8a3e94989ffc22fb6b849b626aeb018d6"], "page_rank": 0.0001231527093596059}, {"id": "6d3dc054f099cec3bdd05ec79925cbe0a821ea47", "title": "Spectre II: general-purpose microscope input for a computer.", "authors": ["Philip G. Stein", "Lewis E. Lipkin", "Howard M. Shapiro"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Spectre II: general-purpose microscope input for a computer.\" by Philip G. Stein et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "47f2261792cef069622607ba74bf7e3178651667", "title": "Sketchpad: a man-machine graphical communication system", "authors": ["Ivan E. Sutherland"], "date": 1963, "abstract": "The Sketchpad system makes it possible for a man and a computer to converse rapidly through the medium of line drawings. Heretofore, most interaction between man and computers has been slowed down by the need to reduce all communication to written statements that can be typed; in the past, we have been writing letters to rather than conferring with our computers. For many types of communication, such as describing the shape of a mechanical part or the connections of an electrical circuit, typed statements can prove cumbersome. The Sketchpad system, by eliminating typed statements (except for legends) in favor of line drawings, opens up a new area of man-machine communication.", "references": [], "page_rank": 0.0004329892876690906}, {"id": "13a5c0e48580415e825ecba20d6db50b93949c1b", "title": "Learning an English-Chinese Lexicon from a Parallel Corpus", "authors": ["Dekai Wu", "Xuanyin Xia"], "date": 1994, "abstract": "We report experiments on automatic learning of an English-Chinese translation lexicon, through statistical training on a large parallel corpus. The learned vocabulary size is nontrivial at 6,517 English words averaging 2.33 Chinese translations per entry, with a manuallyfiltered precision of 95.1% and a single-most-probable precision of 91.2%. We then introduce a significance filtering method that is fully automatic, yet still yields a weighted precision of 86.0%. Learning of translations is adaptive to the domain. To our knowledge, these are the first empirical results of the kind between an Indo-European and non-Indo-European language for any significant corpus size with a non-toy vocabulary.", "references": ["b83dbea361cd583f9fdfb134bb55c48f0335d297", "1f55d2bca810edbf9870934a41d956d06ae2d9cf", "a9b73e50903e731c66ac44d1c2b030470e4bf698", "a76563076016fb1cb813deba45db2409772a51da", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "8f3e1bbbdc1190ae6320c0520f539337a5ca5927", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "3491aa4a9a66ba3d1603230a70d82c7479666a7d", "ab7b5917515c460b90451e67852171a531671ab8", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214"], "page_rank": 0.0001231527093596059}, {"id": "8d021434c8da92634a49cead486826789af8ffff", "title": "Aligning Parallel Texts : Do Methods Developed for English-French Generalize to Asian Languages?", "authors": ["Kenneth Ward Church", "Ido Dagan", "William A. Gale", "Pascale Fung", "Jon Helfman", "Bala Satish"], "date": 1993, "abstract": "Parallel texts have recently received considerable attention in machine translation (e.g., Brown et al, 1990), bilingual lexicography (e.g., Klavans and Tzoukermann, 1990), and terminology research for human translators (e.g., Isabelle, 1992). We have been most interested in the terminology application. How would Microsoft, or some other software vendor, want the term \u2018\u2018dialog box\u2019\u2019 to be translated in their manuals? Technical terms such as \u2018\u2018dialog box\u2019\u2019 are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. Parallel texts could be used to help translators overcome their lack of domain expertise by providing them with the ability to search previously translated documents for examples of potentially difficult terminology and see how they were translated in the past. \u2018\u2018Existing translations contain more solutions to more translation problems than any other existing resource.\u2019\u2019 (Isabelle, 1992)", "references": ["a33e325f4871f0f1783b12bffad6df0570ce876f", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "23e1993ad65ef984bd11d58463d59f1e3cec1341", "1cf01798f07e1f13cd44daad6119f4978721c61d", "a76563076016fb1cb813deba45db2409772a51da", "1f55d2bca810edbf9870934a41d956d06ae2d9cf", "b1bf3d314ad996c394949f88c4091a4832ce0c9b", "0d8436f85424a450b15015610caf4d5fd4e4321b", "5417b3403bee25c2fce7418cb88624d62d39bf32", "eae275046b909dec7a062a35862376c750e60463"], "page_rank": 0.0001231527093596059}, {"id": "b83dbea361cd583f9fdfb134bb55c48f0335d297", "title": "Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria", "authors": ["Dekai Wu"], "date": 1994, "abstract": "We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.", "references": ["1f55d2bca810edbf9870934a41d956d06ae2d9cf", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "3491aa4a9a66ba3d1603230a70d82c7479666a7d", "a76563076016fb1cb813deba45db2409772a51da", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "23e1993ad65ef984bd11d58463d59f1e3cec1341", "1cf01798f07e1f13cd44daad6119f4978721c61d", "6d9a739c2bdbadf0d355f9c19d09ed89117a9589"], "page_rank": 0.0009031198686371098}, {"id": "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "title": "Steps toward Artificial Intelligence", "authors": ["Marvin Minsky"], "date": 1961, "abstract": "The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.", "references": ["e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "8def1503d588f2dc413e0b3e769be5e033d9e737", "ff433838bb2b178e1d6b1f045b0215385e1757f5", "fc436c311b9d6144297e92c0837e8349e0353d9e", "e7269ba40686ddc3c38ff5240fe26fd2e829831b", "0366fb9db0e086800f0c8189499275a39a8f0b02", "c3e12390899a4af23a9c40336518da5e7287cda4", "bc4e084831b9f987826b2cf184356e347ee155f8", "59fd78cc37b9aeee1490639b6d0fbaf09a393536", "0e9b804adcedfe82d5139b572c5c165fa161efb7"], "page_rank": 0.0006951286261631088}, {"id": "ec2f1ca803723c0fc09d4c10e045895cde526330", "title": "A model of a bi-directional transfer mechanism using rule combinations", "authors": ["Hideo Watanabe"], "date": 1995, "abstract": "This paper proposes a new type of transfer mechanism, called Rule Combination Transfer (or RCT), that produces an output structure by non-destructively combining target parts of translation rules, each of which consists of dependency structures in source and target languages and correspondences between them. This proposed mechanism employs more extended mapping of correspondences than one-to-one mapping used in conventional transfer systems, to allow expression of some peculiar or exceptional translation phenomena. Further, these translation rules can be used bi-directionally. Although, the proposed transfer mechanism is intended to be a foundation for an example-based transfer system by coupling it with a mechanism selecting translation rules based on the similarity with an input, it can be also used as a foundation for a conventional transfer system.", "references": ["9f50d2f07030efcbbb4c8c7562c5cdb1bcf5a911", "26fc956587b4605b4064a638a902680d765be4d2", "e9dc4b4f3f708facfbf89c633ae3eb8cc18ec92b", "ca485dca93256f2d469284e64ad241369010ce05", "b7e2f415bd6c5afb9d5b2b854eff2e09a024b017", "28e90d4dee3b0b51f6667ffdfdfc3c6a76b19731", "9277dade1fa22653811ca53fd5397c7ff2c99509", "bc43f6bccb18a5a4892daa8e66756e0a684e7f5c", "48cbc394fde40f3a994d7f2a609dbee6583d41cc", "988c86bb5314c4992e76ef12c73784fd7d47ca4f"], "page_rank": 0.0001231527093596059}, {"id": "a76563076016fb1cb813deba45db2409772a51da", "title": "Aligning Sentences in Parallel Corpora", "authors": ["Peter F. Brown", "Jennifer C. Lai", "Robert L. Mercer"], "date": 1991, "abstract": "In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried.", "references": ["4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "8b7e49e3ee16965d4bfdeeac03fbebb3308ebbec", "a1066659ec1afee9dce586f6f49b7d44527827e1", "539036ab9e8f038c8a948596e77cc0dfcfa91fb3", "983548e49ce6693103b68bb579d04cf61bcaac8c", "6d9a739c2bdbadf0d355f9c19d09ed89117a9589", "bdfb57141b2141095ed942b28be24808aeba8d54"], "page_rank": 0.0010399562123700053}, {"id": "92d5bbaa9981e33d9f09f32fcd14c80c02f9352f", "title": "Some conditions for consistency and uniform consistency of statistical procedures", "authors": ["Charles H. Kraft"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"Some conditions for consistency and uniform consistency of statistical procedures\" by Charles H. Kraft", "references": [], "page_rank": 0.0004926108374384236}, {"id": "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "title": "Recognition and Parsing of Context-Free Languages in Time n^3", "authors": ["Daniel H. Younger"], "date": 1967, "abstract": "A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the \u201ccube\u201d of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the \u201ccube\u201d factor of computational complexity, a parsing matrix\u2014a complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the \u201ccube\u201d of the test string length.", "references": ["dafabc60fe64f5fea4d20d464d453c262d2649b4", "242d7edee503e27a7236a4c7737c314712c3fbdd", "9c7376d1e65bf498aadf7c288b86a84ba2734d68", "f1a2c7757d110cc9e2b114de852f629dbaad4318", "a4d3de3ea747ffa5ae5173dad38e826bdc23f682", "1dc36e72f1b025dc0873da158a2e659e4ff01c7c"], "page_rank": 0.00047208538587848934}, {"id": "92d92fd3de1af6a46e8cd9ca841d5433e659179f", "title": "Mathematical Statistics: Basic Ideas and Selected Topics", "authors": ["Peter J. Bickel", "Kjell A. Doksum"], "date": 1977, "abstract": "(NOTE: Each chapter concludes with Problems and Complements, Notes, and References.) 1. Statistical Models, Goals, and Performance Criteria. Data, Models, Parameters, and Statistics. Bayesian Models. The Decision Theoretic Framework. Prediction. Sufficiency. Exponential Families. 2. Methods of Estimation. Basic Heuristics of Estimation. Minimum Contrast Estimates and Estimating Equations. Maximum Likelihood in Multiparameter Exponential Families. Algorithmic Issues. 3. Measures of Performance. Introduction. Bayes Procedures. Minimax Procedures. Unbiased Estimation and Risk Inequalities. Nondecision Theoretic Criteria. 4. Testing and Confidence Regions. Introduction. Choosing a Test Statistic: The Neyman-Pearson Lemma. Uniformly Most Powerful Tests and Monotone Likelihood Ratio Models. Confidence Bounds, Intervals and Regions. The Duality between Confidence Regions and Tests. Uniformly Most Accurate Confidence Bounds. Frequentist and Bayesian Formulations. Prediction Intervals. Likelihood Ratio Procedures. 5. Asymptotic Approximations. Introduction: The Meaning and Uses of Asymptotics. Consistency. First- and Higher-Order Asymptotics: The Delta Method with Applications. Asymptotic Theory in One Dimension. Asymptotic Behavior and Optimality of the Posterior Distribution. 6. Inference in the Multiparameter Case. Inference for Gaussian Linear Models. Asymptotic Estimation Theory in p Dimensions. Large Sample Tests and Confidence Regions. Large Sample Methods for Discrete Data. Generalized Linear Models. Robustness Properties and Semiparametric Models. Appendix A: A Review of Basic Probability Theory. The Basic Model. Elementary Properties of Probability Models. Discrete Probability Models. Conditional Probability and Independence. Compound Experiments. Bernoulli and Multinomial Trials, Sampling with and without Replacement. Probabilities on Euclidean Space. Random Variables and Vectors: Transformations. Independence of Random Variables and Vectors. The Expectation of a Random Variable. Moments. Moment and Cumulant Generating Functions. Some Classical Discrete and Continuous Distributions. Modes of Convergence of Random Variables and Limit Theorems. Further Limit Theorems and Inequalities. Poisson Process. Appendix B: Additional Topics in Probability and Analysis. Conditioning by a Random Variable or Vector. Distribution Theory for Transformations of Random Vectors. Distribution Theory for Samples from a Normal Population. The Bivariate Normal Distribution. Moments of Random Vectors and Matrices. The Multivariate Normal Distribution. Convergence for Random Vectors: Op and Op Notation. Multivariate Calculus. Convexity and Inequalities. Topics in Matrix Theory and Elementary Hilbert Space Theory. Appendix C: Tables. The Standard Normal Distribution. Auxiliary Table of the Standard Normal Distribution. t Distribution Critical Values. X 2 Distribution Critical Values. F Distribution Critical Values. Index.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "21e69bb8d3a36235a19da1408279290055b373bc", "title": "Machine Translation Divergences: A Formal Description and Proposed Solution", "authors": ["Bonnie J. Dorr"], "date": 1994, "abstract": "There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.", "references": ["64f275919407d768cb3ec93aaf238277d5012f49", "53ba7ea3fd7e6e081d049267e8864e9fd5f8f25b", "1d3320233ad03453523bb026e1062be8a14cb25a", "656c2bf0e40af142348273fd6887ba73149159c7", "28e90d4dee3b0b51f6667ffdfdfc3c6a76b19731", "99c1ef49842f72ad07b5283ec41b924ddc4e6959", "6c4322f8818b00d7da9bba1730ce2eca1896c5ac", "98a5b636ecea3f470f3c585df12757d73ce8da04", "a1cf0d1d11089cd6095001b6135aa36715c21497", "9277dade1fa22653811ca53fd5397c7ff2c99509"], "page_rank": 0.0001231527093596059}, {"id": "40dbb25a15b63af3faccb81c8e64a3f5d659e07e", "title": "The Theory Of Parsing, Translation, And Compiling", "authors": ["Alfred V. Aho", "Jeffrey D. Ullman"], "date": 1972, "abstract": "From volume 1 Preface (See Front Matter for full Preface) \n \nThis book is intended for a one or two semester course in compiling theory at the senior or graduate level. It is a theoretically oriented treatment of a practical subject. Our motivation for making it so is threefold. \n \n(1) In an area as rapidly changing as Computer Science, sound pedagogy demands that courses emphasize ideas, rather than implementation details. It is our hope that the algorithms and concepts presented in this book will survive the next generation of computers and programming languages, and that at least some of them will be applicable to fields other than compiler writing. \n \n(2) Compiler writing has progressed to the point where many portions of a compiler can be isolated and subjected to design optimization. It is important that appropriate mathematical tools be available to the person attempting this optimization. \n \n(3) Some of the most useful and most efficient compiler algorithms, e.g. LR(k) parsing, require a good deal of mathematical background for full understanding. We expect, therefore, that a good theoretical background will become essential for the compiler designer. \n \nWhile we have not omitted difficult theorems that are relevant to compiling, we have tried to make the book as readable as possible. Numerous examples are given, each based on a small grammar, rather than on the large grammars encountered in practice. It is hoped that these examples are sufficient to illustrate the basic ideas, even in cases where the theoretical developments are difficult to follow in isolation. \n \nFrom volume 2 Preface (See Front Matter for full Preface) \n \nCompiler design is one of the first major areas of systems programming for which a strong theoretical foundation is becoming available. Volume I of The Theory of Parsing, Translation, and Compiling developed the relevant parts of mathematics and language theory for this foundation and developed the principal methods of fast syntactic analysis. Volume II is a continuation of Volume I, but except for Chapters 7 and 8 it is oriented towards the nonsyntactic aspects of compiler design. \n \nThe treatment of the material in Volume II is much the same as in Volume I, although proofs have become a little more sketchy. We have tried to make the discussion as readable as possible by providing numerous examples, each illustrating one or two concepts. \n \nSince the text emphasizes concepts rather than language or machine details, a programming laboratory should accompany a course based on this book, so that a student can develop some facility in applying the concepts discussed to practical problems. The programming exercises appearing at the ends of sections can be used as recommended projects in such a laboratory. Part of the laboratory course should discuss the code to be generated for such programming language constructs as recursion, parameter passing, subroutine linkages, array references, loops, and so forth.", "references": [], "page_rank": 0.0005336617405582923}, {"id": "71c89de84e96a77db2accbdba04d6fda45bc044c", "title": "Modeling long range dependencies in languages", "authors": ["Rukmini Iyer", "Mari Ostendorf"], "date": 1996, "abstract": "Semantic Scholar extracted view of \"Modeling long range dependencies in languages\" by Rukmini Iyer et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "ae4614f758dfa344a04b33377c96abc10d5eeda7", "title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "authors": ["Chris Leggetter", "Philip C. Woodland"], "date": 1995, "abstract": "Abstract A method of speaker adaptation for continuous density hidden Markov models (HMMs) is presented. An initial speaker-independent system is adapted to improve the modelling of a new speaker by updating the HMM parameters. Statistics are gathered from the available adaptation data and used to calculate a linear regression-based transformation for the mean vectors. The transformation matrices are calculated to maximize the likelihood of the adaptation data and can be implemented using the forward\u2013backward algorithm. By tying the transformations among a number of distributions, adaptation can be performed for distributions which are not represented in the training data. An important feature of the method is that arbitrary adaptation data can be used\u2014no special enrolment sentences are needed. Experiments have been performed on the ARPA RM1 database using an HMM system with cross-word triphones and mixture Gaussian output distributions. Results show that adaptation can be performed using as little as 11 s of adaptation data, and that as more data is used the adaptation performance improves. For example, using 40 adaptation utterances, a 37% reduction in error from the speaker-independent system was achieved with supervised adaptation and a 32% reduction in unsupervised mode.", "references": ["b8334e18eff5e0420982a1f09838e77175d370e1", "778015de7b81dfde54367dd57fb76c86faa72be4", "490915864dc44c40c393229a591783746ff1ae81", "608d1b21f6027ddb2e0814087ed318d5a485449b", "df92b27030f16b387a9aee2540b3747625f6c83b", "74c691291de0ceedc0bd83a41e5c7d00b3666bd5", "618c54f2ee1ebffdb8dc8fc501166b01f8731496", "583d605b8c632d130e3779af7205066e2ca78d00", "664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "df6e700d469dc1d338a3184f93b263e0160a1d1d"], "page_rank": 0.00016420361247947453}, {"id": "3cedc790a215d8d2366125181f28f7a7545202c7", "title": "Efficient methods for multiple sequence alignment with guaranteed error bounds.", "authors": ["Dan Gusfield"], "date": 1992, "abstract": "Multiple string (sequence) alignment is a difficult and important problem in computational biology, where it is central in two related tasks: finding highly conserved subregions or embedded patterns of a set of biological sequences (strings of DNA, RNA or amino acids), and inferring the evolutionary history of a set of taxa from their associated biological sequences. Several precise measures have been proposed for evaluating the goodness of a multiple alignment, but no efficient methods are known which compute the optimal alignment for any of these measures in any but small cases. In this paper, we consider two previously proposed measures, and give two computationaly efficient multiple alignment methods (one for each measure) whose deviation from the optimal value is guaranteed to be less than a factor of two. This is the novel feature of thse methods. but the methods have additional virtues as well. For both methods, the guaranteed bounds are much smaller than two when the number of strings is small (1.33 for three strings of any length); for one of the methods we give a related randomized method which is much faster and which gives, with high probability, multiple alignments with fairly small error bounds; and for the other measure, the method given yields a non-obvious lower bound on the optimal alignment.", "references": ["c04e8d72013f0326f5679fe8bccf7a3045c0b95a", "d2204d2ccf6a1666bd123b6ff971cd6f0d07e4f6", "daceee6126bc54987b38df6d9fd953445232768d", "647541c53889d676c4d27552f73ac8b2704da3b8", "5021d9cc2fe2b425f27220fbf4f3456163376408", "f170646fce4970e6e9bb6cff134ecc24311f4ba4", "781f316c6f384383ad54c6dc3887a73d3d234e25", "1c67cd41ca0f2ea59adb264d3b4f181c9623031f", "847f863bccc72ed87977ca139e49cd231f46fa17"], "page_rank": 8.210180623973726e-05}, {"id": "d80000d84223e177d070a01a734dba56d5f5c069", "title": "SWITCHBOARD: telephone speech corpus for research and development", "authors": ["John J. Godfrey", "Edward Holliman", "Jan McDaniel"], "date": 1992, "abstract": "SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording.>", "references": [], "page_rank": 0.0002463054187192118}, {"id": "37c931cbaa9217b829596dd196520a838562a109", "title": "Generalized Iterative Scaling for Log-Linear Models", "authors": ["John N. Darroch", "Donald E. Ratcliff"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Generalized Iterative Scaling for Log-Linear Models\" by John N. Darroch et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "91e6bf81fbc0000e6ae62c11a46288049f7e828c", "title": "Neural-network based measures of confidence for word recognition", "authors": ["Mitch Weintraub", "Fran\u00e7oise Beaufays", "Ze{\\'e}v Rivlin", "Yochai Konig", "Andreas Stolcke"], "date": 1997, "abstract": "This paper proposes a probabilistic framework to define and evaluate confidence measures for word recognition. We describe a novel method to combine different knowledge sources and estimate the confidence in a word hypothesis, via a neural network. We also propose a measure of the joint performance of the recognition and confidence systems. The definitions and algorithms are illustrated with results on the Switchboard Corpus.", "references": ["f31705e3535c17c4e3a2da57ff89edc23052f572", "3fd51769d828ae04e0a82888c5bd5047efdf112a", "8a6d820385527df2183a36ae1615f426ba894c5d"], "page_rank": 8.210180623973726e-05}, {"id": "0861015b3c89d68749548c19c6f056eee34eafc6", "title": "A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)", "authors": ["Jonathan G. Fiscus"], "date": 1997, "abstract": "Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a \"voting\" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or \"voting\" process that selects the output sequence with the lowest score.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "feda127528eea5d05a30810c6869d13050b0a61e", "title": "Exploiting Nonlocal and Syntactic Word Relationships in Language Models", "authors": ["Radu Florian", "David Yarowsky"], "date": 1998, "abstract": "Semantic Scholar extracted view of \"Exploiting Nonlocal and Syntactic Word Relationships in Language Models\" by Radu Florian et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "91b52959c7731830f1518ed35e5ab1dabadf79ec", "title": "The computational analysis of English : a corpus-based approach", "authors": ["Roger Garside", "Geoffrey Leech", "Geoffrey Sampson"], "date": 1988, "abstract": "specifically on the difficulty of recognizing learning versus language difficulties, that is, how to identify a nonnative-speaking child's need for special education services. They propose a model that administrators can employ that minimizes bias. In a similar vein, the next chapter, by Jeffery Braden and Sandra Fradd, suggests ways that administrators can anticipate difficulties and intervene before such referrals are necessary. William Tikunoff, in the eighth chapter, focuses on instructional leadership. He discusses the characteristics of an effective principal and targets specific areas, such as effective time management. The final chapter, by Beatrice Ward, addresses the greatest resource of any educational institution: the teachers. She describes the clinical approach to teacher development and how it can be implemented. Overall, this book fills a need for basic, factual information about legal requirements, program types, and effective instructional and leadership strategies with respect to the LEP population. Furthermore, it provides guidance on the complex issue of special education for LEP students, particularly the referral process. An additional chapter exploring different models for assessment and program design for these students would have provided depth and balance. Although there is necessarily some overlap between the chapters, it is reinforcing, not repetitive. In addition to being extremely useful to administrators, this book would be of value to school personnel such as psychologists, special education consultants, LEP consultants, instructors-in short, for anyone committed to the design and delivery of effective instructional programs for LEP students.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "ffce6413805bfd1026a35a56655baa9fd13bb2bd", "title": "The theory of parsing", "authors": ["Alfred V. Aho", "Jeffrey D. Ullman"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"The theory of parsing\" by Alfred V. Aho et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "5c5e952f1414829947635b6a330dead4d6f4797e", "title": "LVCSR log-likelihood ratio scoring for keyword spotting", "authors": ["Mitchel Weintraub"], "date": 1995, "abstract": "A new scoring algorithm has been developed for generating wordspotting hypotheses and their associated scores. This technique uses a large-vocabulary continuous speech recognition (LVCSR) system to generate the N-best answers along with their Viterbi alignments. The score for a putative hit is computed by summing the likelihoods for all hypotheses that contain the keyword normalized by dividing by the sum of all hypothesis likelihoods in the N-best list. Using a test set of conversational speech from Switchboard Credit Card conversations, we achieved an 81% figure of merit (FOM). Our word recognition error rate on this same test set is 54.7%.", "references": ["219b0d4fefceae9e912ae9729914d04d98fefc05", "bee91152eba32244823da839209051af450e6f51", "3492842d6ec502cd9d314ccf1080b0defdb7955f", "3fd51769d828ae04e0a82888c5bd5047efdf112a", "df82162e95c1fd0506f4b7591bf579ec131228a9", "b31c0ce927fc2060066dd417532b8836e63118ce", "d6e79d8ebbd05e1e26ebb801f2efb34687e0d7ca", "446c62f8d434bab94737360f7be184c2fa7c1e5b"], "page_rank": 8.210180623973726e-05}, {"id": "9ba3565169b7d6d2d8ffa8c72237fc8ca6da70b7", "title": "A simple explanation of the induced size effect", "authors": ["Aries Arditi", "Lloyd Kaufman", "J. Anthony Movshon"], "date": 1981, "abstract": "Abstract When a meridional lens is oriented axis vertical before one eye, the horizontal magnification produces a tilting of visual space in the third dimension which is predictable from the geometry of binocular parallax. When, however, the lens is oriented axis horizontal so that the magnification is vertical, a distortion of space occurs which is similar to the tilting caused by the presence of a lens oriented axis vertical over the other eye. It has commonly been believed that this phenomenon, known as the induced size effect, has no geometric explanation, and is an anomalous stereoscopic response to vertical disparities. An explanation is presented which accounts for the induced effect in terms of actual horizontal disparities between vertically magnified oblique lines, and disparities of the two-dimensional spatial spectra of the two eyes.", "references": ["4bb1c5cccd130142ed8326cc94e979515ed6cd1f", "47b7b88999b9be8fe79ced6ccad2eb5fc4d78d46", "901ea420eebcba257e70f1c67bcf29b417ffbf97", "dce00bef3ebd5156891f7872e23cc6632f526630", "940fe6878ef0630c9efb3e409c9fb779c6efeb5a", "aeded5f080091715622a35f9e912746680421748", "1ea69db293c39c24f9c9e26a03c185cc4ba07988", "edf726d78e220e8feb187b69895e585b7f06127b", "0bd77a3b3f1809e90e93e6a2c9d7dd2e62cd8891", "d3676707aabe8398e901e44cd5e38fcaf8334ee3"], "page_rank": 0.0002463054187192118}, {"id": "5e07359ce25384c97a3695c708f913b470aa8abf", "title": "Programming Languages And Their Compilers", "authors": ["John Cocke"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Programming Languages And Their Compilers\" by John Cocke", "references": [], "page_rank": 0.0002463054187192118}, {"id": "603712db9533a60935d0dce32a8afe018e59c313", "title": "The contribution of the right parietal lobe to object recognition.", "authors": ["Elizabeth K. Warrington", "Ann M. Taylor"], "date": 1973, "abstract": "Summary Visual recognition of pictorial material was investigated in a group of 74 patients with localised cerebral lesions. Four tasks of visual perception, figure/ ground, fragmented drawings, enlarged drawings, and photographs of objects from an unconventional view, were administered. An unimpaired performance of the right posterior group on the figure/ground task contrasted with a marked deficit on the unconventional view objects task. It was demonstrated that there is a favoured view for efficiency of object recognition. The findings provide evidence that gestalt formation is intact whereas perceptual classification is impaired in patients with right posterior lesions. The implications of this interpretation of the data for theories of object recognition are discussed.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "810bd193898f9c58c93cfddd1c497eb5e2f342bc", "title": "The interpretation of a moving retinal image", "authors": ["Hugh Christopher Longuet-Higgins", "K. Prazdny"], "date": 1980, "abstract": "It is shown that from a monocular view of a rigid, textured, curved surface it is possible, in principle, to determine the gradient of the surface at any point, and the motion of the eye relative to it, from the velocity field of the changing retinal image, and its first and second spatial derivatives. The relevant equations are redundant, thus providing a test of the rigidity assumption. They involve, among other observable quantities, the components of shear of the retinal velocity field, suggesting that the visual system may possess specialized channels for computing these components.", "references": [], "page_rank": 0.00032840722495894905}, {"id": "3e643930ac21c71c46c7f74cebbc20b366f7a8c4", "title": "Packing Volumes by Spheres", "authors": ["Roger Mohr", "Ruzena Bajcsy"], "date": 1983, "abstract": "In this note we present an algorithm for packing spheres in an arbitrary shaped volume. This algorithm is similar to Blum's transform in that it fits spheres into a volume, but it is different in that it fits only tangential spheres, and thereby the data reduction is larger than by Blum's transform. The spheres are of variable radii, which enables us to achieve a hierarchy of intrinsic volume properties, i.e., from gross to more detailed. The result of this algorithm is a graph where the nodes are the centers of spheres and the arcs are the connections between two tangent spheres. Analysis of computational complexity and the time and error considerations are provided.", "references": ["4dd5ad87ccf9b8493625e38514e2f37ab9ce99cb", "7194f1c34cebfedd8b26f04f80c91af7ba735f3c", "a4230b89af8f05c593d3f0218add12fefd8a4bda", "6176a9ac8924f76b860478a858f65a18ab92ae2d", "f78ee9ee28087c14fe9096e178affa74f17ada7e", "0822170525a69e98e19d3371a828aa512d697e0f", "1c2312b4cce4a17af798046d7f317f5cf8e4c229", "b7e735ae958565568f4e8b01adc269fe62e11266", "313225cd9b6729c1a239a3af40d2dfeaf6946a52", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5"], "page_rank": 0.0002463054187192118}, {"id": "18d41a6590bb30fbeb3bafffdd8ab8c408fb0a7a", "title": "Determining The Instantaneous Direction Of Motion From Optical Flow Generated By A Curvilinearly Moving Observer", "authors": ["K. Prazdny"], "date": 1981, "abstract": "A method is described capable of decomposing the optical flow into its rotational and translational components. The translational component is extracted implicitly by locating the focus of expansion associated with the translational component of the relative motion. The method is simple, relying on minimizing an (error) function of 3 parameters. As such, it can also be applied, without modification, in the case of noisy input information. Unlike the previous attempts at interpreting optical flow to obtain elements, the method uses only relationships between quantities on the projection plane. No 3D geometry is involved. Also outlined is a possible use of the method for the extraction of that part of the optical flow containing information about relative depth directly from the image intensity values, without extracting the \"retinal\" velocity vectors.", "references": ["79e1995ded6994540dd5a03d8a41b8c9d0378238", "810bd193898f9c58c93cfddd1c497eb5e2f342bc", "67301c286439a7c24368300ea13e9785bd666aed", "94fdf4857d6a3b6593d10541bcd3e8c8df19c5e2", "22d262e5dfd45776f5616fe405cf153c8466e965", "6be8792af70f542b932b76f026f1d4699220a740", "598f181058e65251c25a74064f72e54b37b4eebe"], "page_rank": 0.0001231527093596059}, {"id": "8718bdea2c3aedc550f09d2709d2d75b25cbfab9", "title": "The Shape of Smooth Objects and the Way Contours End", "authors": ["Jan J. Koenderink", "Andrea J. van Doorn"], "date": 1982, "abstract": "It is shown, by elementary mathematical reasoning, that visual contours can only be concave at their endpoints. This simple natural fact is contrasted with general mannerisms of draftsmen: in the great majority of cases contours are drawn to have convex ends. It is argued that this results from our general concept of solid shapes: a general shape is conceived of as a conglomerate of convex (\u2018ovoid\u2019) elementary shapes, these shapes act as \u2018figure\u2019 and the way they are glued together is treated as the\u2014relatively unimportant\u2014\u2018ground\u2019. The hypothesis is supported through citations from academic-art literature. An attempt is made to give a geometrical specification of just what draftsmen draw if they disregard the contour.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "278c9a78d4505cfaf6b709df364dbd1206a017c1", "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "authors": ["Martin A. Fischler", "Robert C. Bolles"], "date": 1981, "abstract": "A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing", "references": [], "page_rank": 0.0001231527093596059}, {"id": "ad66f034fb63ce48524af09b7f162b397e44d364", "title": "Determining the movement of objects from a sequence of images", "authors": ["J. W. Roach"], "date": 1980, "abstract": "Discusses the problem of determining the three-dimensional model and movement of an object from a sequence of two-dimensional images. A solution to this problem depends on solving a system of nonlinear equations using a modified least-squared error method. Two views of six points or three views of four points are needed to provide an overdetermined set of equations when the images are noisy. It is shown, however, that this numerical method is not very accurate unless the images of considerably more points are used.", "references": ["b3a2168e356b4d2dc98968ba21cb20aca157191f", "698488b7d0d7bec7d016ff9da483350dc1a7d834", "9a1d3e7e3a46d6bf89adeddbd276ba188c1da6d8", "aff28892188c01bb0e17f82fd8938604edf9aff1", "36884e301358a4b11826c45d9778b72ca14a6fa1", "b81afd190adcd52190fa31af0841dd7b9695380e", "0cd7a87fe0f4ccc3b5c81018e515b0904c5bba62", "47922b31367ef2e58c516fff5ca2d77c3b80f70c", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5"], "page_rank": 0.0003694581280788177}, {"id": "a23d3a65df31e1523765c84972a062aea30171c3", "title": "On 3D Reconstruction from Two Perspective Views", "authors": ["Hans-Hellmut Nagel", "Bernd Neumann"], "date": 1981, "abstract": "A concise derivation is given for a compact nonlinear equation which specifies possible 3D rotations of rigid objects, compatible with measurements of five object points in two views. The insight provided by direct geometrical interpretation of this equation = which contains ULLMAN's polar equation as a special case - may be exploited for attempts to categorize the set of possible solutions.", "references": ["58352b95efab0631ce3b07dd4bc650fc9c8ac038", "ad66f034fb63ce48524af09b7f162b397e44d364", "ddb841734a7e09c54c1bb9a31e7de432039cef0e", "6ab12ae68cb1ac0147e25301369b0c7a277bf56f", "c2d2fefc1c61298059f9a160f190e6957587b74e", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5", "399ca0115749b4bfc9404537acb03876400bca1a"], "page_rank": 0.0001231527093596059}, {"id": "8f3e1bbbdc1190ae6320c0520f539337a5ca5927", "title": "Improving Chinese Tokenization With Linguistic Filters On Statistical Lexical Acquisition", "authors": ["Dekai Wu", "Pascale Fung"], "date": 1994, "abstract": "The first step in Chinese NLP is to tokenize or segment character sequences into words, since the text contains no word delimiters. Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang & Chen 1993; Chiang et al. 1992; Lin et al. 1993; Wu & Tseng 1993; Sproat et al. 1994).We present empirical evidence for four points concerning tokenization of Chinese text: (1) More rigorous \"blind\" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method. (2) The extent of the unknown-word problem is far more serious than generally thought, when tokenizing unrestricted texts in realistic domains. (3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%. (4) When augmenting the lexicon, linguistic constraints can provide simple inexpensive filters yielding significantly better precision, reducing error rates as much as 49.4%.", "references": ["8229ec71d7edb244d548218850cd97aa28b6274e", "6067d14c7d684ea68507218eb8bf071f833b867f", "f52b2abd79a3025ee1a2f4d2d7c0769e4f7b2277", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "b83dbea361cd583f9fdfb134bb55c48f0335d297", "f4f5a6bd31d1b1b11654bfe2437b69e8728b5183"], "page_rank": 0.0001231527093596059}, {"id": "eae275046b909dec7a062a35862376c750e60463", "title": "The awk programming language", "authors": ["Alfred V. Aho", "Brian W. Kernighan", "Peter J. Weinberger"], "date": 1988, "abstract": "Preface. 1. An AWK Tutorial. Getting Started. Simple Output. Fancier Output. Selection. Computing with AWK. Control-Flow Statements. Arrays. A Handful of Useful \"One-liners.\" What Next? 2. The AWK Lanaguage. Patterns. Actions. User-Defined Functions. Output. Input. Interaction with Other Programs. Summary. 3. Data Processing. Data Transformation and Reduction. Data Validation. Bundle and Unbundle. Multiline Records. Summary. 4. Reports and Databases. Generating Reports. Packaged Queries. A Relational Database System. Summary. 5. Processing Words. Random Text Generation. Interactive Text-Manipulation. Text Processing. Summary. 6. Little Languages. An Assembler and Interpreter. A Language for Drawing Graphs. A Sort Generator. A Reverse-Polish Calculator. An Infix Calculator. Recursive-Descent Parsing. Summary. 7. Experiments with Algorithms. Sorting. Profiling. Topological Sorting. Make: A File Updating Program. Summary. 8. Epilog. AWK as a Language. Performance. Conclusion. Appendix A: AWK Summary. Appendix B: Answers to Selected Exercises. Index.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "title": "A method for disambiguating word senses in a large corpus", "authors": ["William A. Gale", "Kenneth Ward Church", "David Yarowsky"], "date": 1992, "abstract": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics.", "references": ["84422ba04d0113a24033f49be772d586359200af", "a1f7769dc8bde42baa056ce7da37e825a747c603", "85b9eb556c211d954b31d9d58fed6891a07ab473", "806f47862549c51f12c5539451abce2f6573236d", "698c24202746bbd65c9a179b38a927bb4048790a", "2de1202303b1cf5972fb9837b083fd1bd17a1d56", "a76563076016fb1cb813deba45db2409772a51da", "bf0fc4a7a5ccf51aff40630c297db61d3fb0b50d", "4314c4f19eea6d4c3fd21fb37345198bea0d0559", "52b99d29c931d9aaf1b3d6f48b31577affef0208"], "page_rank": 0.0001231527093596059}, {"id": "59fd78cc37b9aeee1490639b6d0fbaf09a393536", "title": "Recognition of sloppy, hand-printed characters", "authors": ["Worthie Doyle"], "date": 1960, "abstract": "This report describes a pattern recognition scheme particularly intended to handle noisy and highly distorted data. The sample is subjected to a set of tests at the conclusion of which a single decision is made. The decision is made on the basis of experience obtained from prior processing of labelled samples. The method has been applied to hand-printed English capitals but is evidently general. Results are given for some trials made on the IBM 709.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "0e9b804adcedfe82d5139b572c5c165fa161efb7", "title": "How we know universals; the perception of auditory and visual forms.", "authors": ["Walter C. Pitts", "Warren S. McCulloch"], "date": 1947, "abstract": "Two neural mechanisms are described which exhibit recognition of forms. Both are independent of small perturbations at synapses of excitation, threshold, and synchrony, and are referred to partiular appropriate regions of the nervous system, thus suggesting experimental verification. The first mechanism averages an apparition over a group, and in the treatment of this mechanism it is suggested that scansion plays a significant part. The second mechanism reduces an apparition to a standard selected from among its many legitimate presentations. The former mechanism is exemplified by the recognition of chords regardless of pitch and shapes regardless of size. The latter is exemplified here only in the reflexive mechanism translating apparitions to the fovea. Both are extensions to contemporaneous functions of the knowing of universals heretofore treated by the authors only with respect to sequence in time.", "references": [], "page_rank": 0.0003166783954961295}, {"id": "983548e49ce6693103b68bb579d04cf61bcaac8c", "title": "Bilingual Concordancing and Bilingual Lexicography", "authors": ["Susan J. Warwick", "Graham Russell"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Bilingual Concordancing and Bilingual Lexicography\" by Susan J. Warwick et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "1dc36e72f1b025dc0873da158a2e659e4ff01c7c", "title": "On the relative effieiencies of context-free grammar recognizers", "authors": ["Thomas V. Griffiths", "Stanley R. Petrick"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"On the relative effieiencies of context-free grammar recognizers\" by Thomas V. Griffiths et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "64f275919407d768cb3ec93aaf238277d5012f49", "title": "Solving Thematic Divergences in Machine Translation", "authors": ["Bonnie J. Dorr"], "date": 1990, "abstract": "Though most translation systems have some mechanism for translating certain types of divergent predicate-argument structures, they do not provide a general procedure that takes advantage of the relationship between lexical-semantic structure and syntactic structure. A divergent predicate-argument structure is one in which the predicate (e.g., the main verb) or its arguments (e.g., the subject and object) do not have the same syntactic ordering properties for both the source and target language. To account for such ordering differences, a machine translator must consider language-specific syntactic idiosyncrasies that distinguish a target language from a source language, while making use of lexical-semantic uniformities that tie the two languages together. This paper describes the mechanisms used by the UNITRAN machine translation system for mapping an underlying lexical-conceptual structure to a syntactic structure (and vice versa), and it shows how these mechanisms coupled with a set of general linking routines solve the problem of thematic divergence in machine translation.", "references": ["a1cf0d1d11089cd6095001b6135aa36715c21497", "b10d344442dcbed16d98166a1d0ce0ab571cafcb", "0fe15ecef5b9ae00e6f69af794a6bb5a1cd50ecf", "4e466ecbe1aa130769b57decbe74f5f402a4e600", "be7ca40571b43108b281dd8cf6ba5be32b8c3125", "8ba4429283b265c2e42139d39680524e6180d847"], "page_rank": 0.00022577996715927748}, {"id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3", "title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process", "authors": ["Lawrence Baum"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process\" by Lawrence Baum", "references": [], "page_rank": 0.00016420361247947453}, {"id": "1f9a9153f15655ea8a07186d42307d6d43032d7d", "title": "Numerical Shape from Shading and Occluding Boundaries", "authors": ["Katsushi Ikeuchi", "Berthold K. P. Horn"], "date": 1981, "abstract": "An iterative method for computing shape from shading using occluding boundary information is proposed. Some applications of this method are shown. \n \nWe employ the stereographic plane to express the orientations of surface patches, rather than the more commonly used gradient space. Use of the stereographic plane makes it possible to incorporate occluding boundary information, but forces us to employ a smoothness constraint different from the one previously proposed. The new constraint follows directly from a particular definition of surface smoothness. \n \nWe solve the set of equations arising from the smoothness constraints and the image-irradiance equation iteratively, using occluding boundary information to supply boundary conditions. Good initial values are found at certain points to help reduce the number of iterations required to reach a reasonable solution. Numerical experiments show that the method is effective and robust. Finally, we analyze scanning electron microscope (SEM) pictures using this method. Other applications are also proposed.", "references": ["37a608c85cf95a8203d88c242d7fd0d9489d31d5", "6a39e4c99d2027df63f50ed16fbe3865951e1ed0", "1af25c587e3d7f664c9f14ed8494315418cae7f1", "c0c92209bce97ba5efcf4c77771c7dab2a8502f1", "e632f0c2e5e0b91588b3874889fa2d535ed1d3ca", "ac66c2b342ac7136f712cf3606aae576dd21c922", "5a6946d8e2c8934c24faad2bb8f125534183bd96", "6df96f40daec0c2a566c3529a8cba3f9db0615bd", "a750e60a696f934168a022c01dd41d6aecc1a8be", "cbe9176abf0c98c7abdfd136f53d58620f8acfec"], "page_rank": 0.0002463054187192118}, {"id": "a1cf0d1d11089cd6095001b6135aa36715c21497", "title": "Design of LMT: A Prolog-Based Machine Translation System", "authors": ["Michael C. McCord"], "date": 1989, "abstract": "LMT (logic-based machine translation) is an experimental English-to-German MT system, being developed in the framework of logic programming. The English analysis uses a logic grammar formalism, Modular Logic Grammar, which allows logic grammars to be more compact, and which has a modular treatment of syntax, lexicon, and semantics. The English grammar is written independently of the task of translation. LMT uses a syntax transfer method for translation, although the English syntactic analysis trees contain some results of semantic choices and show deep grammatical relations. Semantic type checking with Prolog inference is done during analysis and transfer. The transfer algorithm uses logical variables and unification to good advantage; transfer works in a simple left-to-right, top-down way. After transfer, the German syntactic generation component produces a surface structure tree by application of a system of tree transformations. These transformations use an augmentation of Prolog pattern matching. LMT has a single lexicon, containing both source and transfer information, as well as some idiosyncratic target morphological information. There is a compact external format for this lexicon, with a lexical preprocessing system that applies defaults and compiles it into an internal format convenient for the syntactic components. During lexical preprocessing, English morphological analysis can be coupled with rules that synthesize new transfer entries.", "references": ["488233501fde6e0ab4438fef62be80b5cfc4f30c", "b1fc8da10918888f8940606c933dcb81c1f69798", "ae931723114d0ad1fbee600a3dc1a7a1dcfe0e15", "fe32895baaea4b3759dd161fa4d879d64c60baa5", "1c042b6501f01315888f8ac54fefe82518e193e6", "aaccc51e3be14b74c000601395d8ee333f20c464", "d97ef8a04bf3bcf1f996953edbc16209ab52624d", "8acd0749710799a327d4d41ee8565f53b10569b5", "9c6e68a6d704d0d9518a807584a469f72a4c66c9", "547d483ed1e80066693af561f63daa30ffa8e9fa"], "page_rank": 0.0006568144499178981}, {"id": "98a5b636ecea3f470f3c585df12757d73ce8da04", "title": "A Parameterized Approach to Integrating Aspect with Lexical-Semanics for Machine Translation", "authors": ["Bonnie J. Dorr"], "date": 1992, "abstract": "This paper discusses how a two-level knowledge representation model for machine translation integrates aspectual information with lexical-semantic information by means of parameterization. The integration of aspect with lexical-semantics is especially critical in machine translation because of the lexical selection and aspectual realization processes that operate during the production of the target-language sentence: there are often a large number of lexical and aspectual possibilities to choose from in the production of a sentence from a lexical semantic representation. Aspectual information from the source-language sentence constrains the choice of target-language terms. In turn, the target-language terms limit the possibilities for generation of aspect. Thus, there is a two-way communication channel between the two processes. This paper will show that the selection/realization processes may be parameterized so that they operate uniformly across more than one language and it will describe how the parameter-based approach is currently being used as the basis for extraction of aspectual information from corpora.", "references": ["eb23f51f67a778d6e60f1e49cc3877230164eafe", "e036b125e3e2ff19d9f092fa9e2e3a3abbfbe8f5", "64f275919407d768cb3ec93aaf238277d5012f49", "ea85589dd69f986db65898d4e117423828908867", "271f36ba5338c7daab6141728d3db9b95de71451", "51da89ed9d701ebb66946dbfe03d726b8c23c5b3", "a6f698bb196b14839038d231de6f186983846c89", "d9f0f36365cfb8b203ed15879188109195c64e03", "195ae8bac8ae67cc1b0a5eed17c13e5d220822c2", "601ca2ccd8bb59ed2c314d3c4d767a5736067596"], "page_rank": 0.00016420361247947453}, {"id": "bdfb57141b2141095ed942b28be24808aeba8d54", "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm", "authors": ["A. D. Et Altri"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"Maximum Likelihood from Incomplete Data via the EM Algorithm\" by A. D. Et Altri", "references": [], "page_rank": 0.00016420361247947453}, {"id": "664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "title": "Maximum likelihood estimation for multivariate observations of Markov sources", "authors": ["Louis A. Liporace"], "date": 1982, "abstract": "Parameter estimation for multivariate functions of Markov chains, a class of versatile statistical models for vector random processes, is discussed. The model regards an ordered sequence of vectors as noisy multivariate observations of a Markov chain. Mixture distributions are a special case. The foundations of the theory presented here were established by Baum, Petrie, Soules, and Weiss. A powerful representation theorem by Fan is employed to generalize the analysis of Baum, {\\em et al.} to a larger class of distributions.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "df6e700d469dc1d338a3184f93b263e0160a1d1d", "title": "Spectral transformations through canonical correlation analysis for speaker adptation in ASR", "authors": ["Karim Choukri", "G{\\'e}rard Chollet", "Yves Grenier"], "date": 1986, "abstract": "This paper describes a technique of spectral transformation for improved adaptation of a knowledge data base or reference templates to new speakers in automatic speech recognition (ASR). Based on a statistical analysis tool (Canonical correlation analysis) the proposed method permits to improve speaker independance in Large vocabulary ASR. Application to an isolated word recognizer improved a 70% correct score to 87%.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "c04e8d72013f0326f5679fe8bccf7a3045c0b95a", "title": "Trees, stars, and multiple biological sequence alignment", "authors": ["Stephen F. Astschul", "David J. Lipman"], "date": 1989, "abstract": "One important problem in biological sequence comparison is how to simultaneously align several nucleic acid or protein sequences. A multiple alignment avoids possible inconsistencies among several pairwise alignments and can elucidate relationships not evident from pairwise comparisons. The basic dynamic programming algorithm for optimal multiple sequence alignment requires too much time to be practical for more than three sequences, the length of an average protein. Recently, Carrillo and Lipman (SIAMJ. Appl. Math., 48 (1988), pp. 1073\u20131082) have rendered feasible the optimal simultaneous alignment of as many as six sequences by showing that a consideration of minimal pairwise alignment costs can vastly decrease the number of cells a dynamic programming algorithm need consider. Their argument, however, requires the cost of a multiple alignment to be a weighted sum of the costs of its projected pairwise alignments.This paper presents an extension of Carrillo and Lipman's algorithm to the definition of mul...", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f31705e3535c17c4e3a2da57ff89edc23052f572", "title": "Detecting misrecognitions and out-of-vocabulary words", "authors": ["Sheryl R. Young"], "date": 1994, "abstract": "This paper describes and evaluates a new technique for evaluating confidence in word strings produced by a speech recognition system. It detects misrecognized and out-of-vocabulary words in spontaneous spoken dialogs. The system uses multiple, diverse knowledge sources including acoustics, semantics, pragmatics and discourse to determine if a word string is misrecognized. When likely misrecognitions are detected, a series of tests distinguishes out-of-vocabulary words from other error sources. The work is part of a larger effort to automatically recognize and understand new words when spoken in a spontaneous spoken dialog. The newly developed acoustic confidence metrics output independent probabilites that a word is recognized correctly and a measure of how reliably we can tell if it is wrong. At p >", "references": ["28c6a9b962ab9902ae1fd3b020d3db30679d90d8"], "page_rank": 0.0002463054187192118}, {"id": "1c67cd41ca0f2ea59adb264d3b4f181c9623031f", "title": "Simultaneous comparisons of three or more sequences related by a tree", "authors": ["David Sankoff", "R. J. Cedergren"], "date": 1983, "abstract": "A cable having an outer protective covering or sheath enclosing one or more insulated conductors and a cutting wire or rip cord intended for slitting the covering, the cutting wire being located under a longitudinal notch on the inner side of the covering with the position of the cutting wire being marked on the outer surface of the covering, the marking having the form of a longitudinal notch or two parallel ridges.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "847f863bccc72ed87977ca139e49cd231f46fa17", "title": "Evolution of protein molecules.", "authors": ["Thomas Hughes Jukes"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Evolution of protein molecules.\" by Thomas Hughes Jukes", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "781f316c6f384383ad54c6dc3887a73d3d234e25", "title": "Some biological sequence metrics", "authors": ["Michael S. Waterman", "Temple F. Smith", "W. A. Beyer"], "date": 1976, "abstract": "Abstract Some new metrics are introduced to measure the distance between biological sequences, such as amino acid sequences or nucleotide sequences. These metrics generalize a metric of Sellers, who considered only single deletions, mutations, and insertions. The present metrics allow, for example, multiple deletions and insertions and single mutations. They also allow computation of the distance among more than two sequences. Algorithms for computing the values of the metrics are given which also compute best alignments. The connection with the information theory approach of Reichert, Cohen, and Wong is discussed.", "references": ["d5df5523e3043089695f1b20cc888a4311271806", "0541ce163be8018f1c7d07c1ce95e63a29486a6c", "792a98c9d8e0c26f2b1750ecf2681cdb0c72b280", "c6aee6e8d9352bc906b5ea148b3e42c702f55c29", "967f32841955b72f358190436baa5510839d9ab3", "2211b817aecdcec91990c52e655aea47c579cc36", "70ff500942ccedcf0ae003eeae972260fb3d0f7f"], "page_rank": 8.210180623973726e-05}, {"id": "5021d9cc2fe2b425f27220fbf4f3456163376408", "title": "Sensitivity comparison of protein amino acid sequences.", "authors": ["Patrick Argos", "Martin Vingron"], "date": 1990, "abstract": "Publisher Summary This chapter describes a procedure for pair-wise sequence alignment for sensitivity comparison of protein amino acid sequences. Alignments considered \u2018standards of truth\u2019 can be had by spatial superposition of o-carbon backbone folds known from X-ray diffraction analysis. The distribution of residue match and conservation is often not uniform. A window length must be defined. When aligning every 25-residue span of one protein sequence with every such span in the other sequence, two scoring criteria are used. The sum of the Dayhoff mutation values for each of the residue pairs is determined. Weak homologies may become visible with simultaneous alignments of several related sequences. This is accomplished by a multiple sequence alignment algorithm. Though the iterative pair wise alignments and profile analyses are clever and useful, they are flawed. Different gap penalty choices gain result in different alignments, which, for weaker homologies, are difficult to choose from. The two clusters sharing the greatest number of dipeptides are joined by aligning each of their profiles. Two profiles are compared by once again calculating a search matrix where the columns correspond to the alignment positions of one sequence set and consist of their amino acid compositional distribution, and the rows are the similar values for the other sequence cluster.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f170646fce4970e6e9bb6cff134ecc24311f4ba4", "title": "A workbench for multiple alignment construction and analysis.", "authors": ["Gregory D. Schuler", "Stephen F. Altschul", "David J. Lipman"], "date": 1991, "abstract": "Multiple sequence alignment can be a useful technique for studying molecular evolution, as well as for analyzing relationships between structure or function and primary sequence. We have developed for this purpose an interactive program, MACAW (Multiple Alignment Construction and Analysis Workbench), that allows the user to construct multiple alignments by locating, analyzing, editing, and combining \"blocks\" of aligned sequence segments. MACAW incorporates several novel features. (1) Regions of local similarity are located by a new search algorithm that avoids many of the limitations of previous techniques. (2) The statistical significance of blocks of similarity is evaluated using a recently developed mathematical theory. (3) Candidate blocks may be evaluated for potential inclusion in a multiple alignment using a variety of visualization tools. (4) A user interface permits each block to be edited by moving its boundaries or by eliminating particular segments, and blocks may be linked to form a composite multiple alignment. No completely automatic program is likely to deal effectively with all the complexities of the multiple alignment problem; by combining a powerful similarity search algorithm with flexible editing, analysis and display tools, MACAW allows the alignment strategy to be tailored to the problem at hand.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "d3676707aabe8398e901e44cd5e38fcaf8334ee3", "title": "The fusion illusion", "authors": ["Lloyd Kaufman", "Aries Arditi"], "date": 1976, "abstract": "Abstract Four experiments were performed to test for the occurrence of perceived central fusion of vertically disparate stimuli. Two of these employed the method of Signal Detection Theory in order to measure the sensitivity of observers in discriminating disparate from non-disparate cyclofusional stimuli along the horizontal meridian. The other two experiments attempted to compare \u201cfusion\u201d thresholds with monocular control data. The fusion effect could not be differentiated from the limitations of monocular acuity. It was concluded that fusion is an illusion which may be attributed to the effects of suppression and failures of acuity.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "219b0d4fefceae9e912ae9729914d04d98fefc05", "title": "Keyword-spotting using SRI's DECIPHER large-vocabulary speech-recognition system", "authors": ["Michael Weintraub"], "date": 1993, "abstract": "The application of the speaker-independent large-vocabulary CSR (continuous speech recognition) system DECIPHER to the keyword-spotting task is described. A transcription is generated for the incoming spontaneous speech by using a CSR system, and any keywords that occur in the transcription are hypothesized. It is shown that the use of improved models of nonkeyword speech with a CSR system can yield significantly improved keyword spotting performance. The algorithm for computing the score of a keyword combines information from acoustics, language, and duration. One key limitation of this approach is that keywords are only hypothesized if they are included in the Viterbi backtrace. This does not allow the system builder to operate effectively at high false alarm levels if desired. Other algorithms are being considered for hypothesizing good score keywords that are on high scoring paths. An algorithm for smoothing language model probabilities was also introduced. This algorithm combines small task-specific language model training data with large task-independent language training data, and provided a 14% reduction in test set perplexity.>", "references": [], "page_rank": 0.0004926108374384236}, {"id": "0bd77a3b3f1809e90e93e6a2c9d7dd2e62cd8891", "title": "Stereopsis by harmonic analysis", "authors": ["Eugene Levinson", "Randolph Blake"], "date": 1979, "abstract": "Abstract Stereoscopically viewed vertical gratings whose cycle widths differ by 10% fuse into a single grating rotated in depth. The perceived tilt can be predicted on the basis of a barwise computation of geometrical disparities, or on the basis of a comparison in terms of harmonic content. We have found that monocular gratings of similar cycle width but different harmonic content cannot be fused, whereas gratings similar in harmonic content but not in cycle width give good depth sensations. Such observations support the idea of stereopsis by harmonic analysis.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "8a6d820385527df2183a36ae1615f426ba894c5d", "title": "Neural Network Classifiers Estimate Bayesian a posteriori Probabilities", "authors": ["Michael D. Richard", "Richard Lippmann"], "date": 1991, "abstract": "Many neural network classifiers provide outputs which estimate Bayesian a posteriori probabilities. When the estimation is accurate, network outputs can be treated as probabilities and sum to one. Simple proofs show that Bayesian probabilities are estimated when desired network outputs are 1 of M (one output unity, all others zero) and a squared-error or cross-entropy cost function is used. Results of Monte Carlo simulations performed using multilayer perceptron (MLP) networks trained with backpropagation, radial basis function (RBF) networks, and high-order polynomial networks graphically demonstrate that network outputs provide good estimates of Bayesian probabilities. Estimation accuracy depends on network complexity, the amount of training data, and the degree to which training data reflect true likelihood distributions and a priori class probabilities. Interpretation of network outputs as Bayesian probabilities allows outputs from multiple networks to be combined for higher level decision making, simplifies creation of rejection thresholds, makes it possible to compensate for differences between pattern class probabilities in training and test data, allows outputs to be used to minimize alternative risk functions, and suggests alternative measures of network performance.", "references": ["385f68e2d50236024a349aa0584282ec53a43112", "85241210389fbce403f5d12597b9bf32a5633dc2", "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "830ccb44084d9d6cdcb70d623df5012ae4835142", "adf81acbfb348c7ebacb97858beb3d193766bb2a", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "7c8293e7054230cc6cc6e3172f761d89d267f7a7", "656a33c1db546da8490d6eba259e2a849d73a001", "218f32b107e3d461781cad50888845a08486bb24", "b217788dd6d274ad391ee950e6f6a34033bd2fc7"], "page_rank": 0.0002463054187192118}, {"id": "edf726d78e220e8feb187b69895e585b7f06127b", "title": "Depth from spatial frequency difference: An old kind of stereopsis?", "authors": ["Christopher W. Tyler", "Erich E. Sutter"], "date": 1979, "abstract": "Abstract Perception of tilt in depth on the basis of spatial frequency difference between the two eyes was subjected to new tests to determine whether it could be explained by conventional binocular disparity mechanisms. When usual disparity cues are invalidated by rapidly changing displays and by using stimuli uncorrelated between the two eyes, perception of tilt remained for a great range of spatial frequency differences. It is suggested that the mechanism involved may be more primitive than the conventional disparity mechanism.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1ea69db293c39c24f9c9e26a03c185cc4ba07988", "title": "Binocular single vision and perceptual processing", "authors": ["Oliver Braddick"], "date": 1979, "abstract": "Stimuli with small binocular disparities are seen as single, despite their differing visual directions for the two eyes. Such stimuli also yield stereopsis, but stereopsis and single vision can be dissociated. The occurrence of binocular single vision depends not only on the disparities of individual stimulus elements, but also on the geometrical relation of different parts of the pattern presented to each eye. A pair of vertical bars with opposite binocular disparities is seen as single if the pair is moderately widely spaced but not if it is narrow. Vertical alignment and identity in length of such bars also increase the occurrence of double vision. It is argued that these effects reflect the extraction of features of the monocular patterns, with these detected monocular features determining the binocular percept. Single and double vision of bars differing in orientation can be similarly analysed. The occurrence of relatively elaborate processing of monocular signals does not exclude the possibility that binocular interaction can occur between signals that have not been so processed. Multiple sites or types of binocular interaction are likely.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "6be8792af70f542b932b76f026f1d4699220a740", "title": "Parallax and perspective during aircraft landings.", "authors": ["James Jerry Gibson", "Paul Olum", "Frank F. Rosenblatt"], "date": 1955, "abstract": "The performance of successfully landing an airplane is generally supposed to be dependent on the ability to perceive visual depth and distance, or more generally space. The classical explanation of human ability to see space is that the eye provides a set of cues to the distance of a given object, and likewise differential cues to the different distances of different objects. Such cues, include (a) binocular parallax or disparity, (b) linear perspective, including the apparent size of objects whose real size is known, (c) aerial perspective, superposition, shading, and (d) motion parallax, i.e. the apparent movement of physically stationary objects relative to one another and to the observer. The last depends upon motion of the observer, usually locomotion, and the strength of the effect is in proportion to the velocity of the observer. Consequently it ought to be of particular significance for spatial judgments during aircraft landings, in which locomotion occurs at a considerabe velocity. The aim of this report is: (1) to examine the existing definitions of motion parallax; (2) to show that they are not sufficiently general to cover the visual situation during aircraft landing; (3) to formulate geometrically a somewhat different principle (called motion perspective); and (4) to apply this general principle to the problem of perceiving the ground and one's relation to it during descent.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "598f181058e65251c25a74064f72e54b37b4eebe", "title": "Separable aftereffects of changing-size and motion-in-depth: Different neural mechanisms?", "authors": ["K. I. Beverley", "D. Regan"], "date": 1979, "abstract": "Abstract The following two aftereffects resulted from inspecting a square whose vertical edges moved towards each other with a ramping waveform: (1) a static test square appeared to be continously expanding along the horizontal direction, and (2) a static test square appeared to be moving in depth. We quantified each aftereffect by measuring the real rate of size change required to cancel it. Both aftereffects decayed exponentially. No. 1 rapidly with a time constant between 6.4 and 9.9 sec. and No. 2 more slowly with a time constant between 21 and 54 sec. Because of the different decay rates, the two aftereffects could be seen separately. We propose that the existence of these two aftereffects is consistent with a psychophysical model in which unidirectional motion filters feed a changing-size filter that in turn feeds a motion-in-depth stage. We tentatively suggest that this model might help to explain how the visual system resolves the ambiguity of a retinal image whose magnification is changing.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "94fdf4857d6a3b6593d10541bcd3e8c8df19c5e2", "title": "Optical Velocity Patterns, Velocity-Sensitive Neurons, and Space Perception: A Hypothesis", "authors": ["Ken Nakayama", "Jack M. Loomis"], "date": 1974, "abstract": "A hypothesis is put forward of how global patterns of optical flow, as discussed by Gibson, Johansson, and others, could be processed by relatively simple physiological mechanisms. It is suggested that there may exist motion-sensitive cells in the visual system which operate on the optical flow over the retina, and, in so doing, structure the visual field in terms of distinct surfaces that move and/or lie at varying distances from the observer. First, concepts of static and dynamic perspective relative to a sphere centered about the eye are developed, partly on the basis of the work of Gordon. It is pointed out that the velocity flow pattern has a very simple form making it amenable to analysis by relatively low-level mechanisms. Next a higher-order variable of optical flow, the \u2018convexity\u2019, is defined; under the assumption of a rigid environment, convexity is shown to be related to relative depth. It is then postulated that velocity-sensitive cells having center\u2014surround organization could be linked in such a way as to define a higher-order cell, the convexity cell, having functional properties that make it sensitive to the convexity function. The response profile of a layer of such cells would provide an efficient structuring of the visual field in terms of distinct optical surfaces. Relevant evidence is briefly discussed. Lastly, the optical flow patterns corresponding to rotations of the observer are considered. It is shown that the convexity cell is insensitive to rotations and in consequence responds in an invariant fashion to aspects of the optical flow which are related to the surrounding environment.", "references": ["97d7c8eda4b7de2a744656c32aca2688d80e4b88", "e11827caa9565f1b126f97865ebc1e28501dba01", "58ea2fa0580b2117618be6e1cc9658a5c9531dba", "126ca8fabed6dab3af077513baa57f433c9e27fd", "6d0198460198fdb49b89d1646049712b3a0683df", "97ba954cfc5a27cf69581c5d6ebfe9973c139508", "e14f8259095fdf96404e1edaaf57ca22276d15a8", "9f41a2e16630b1410b420bc55b7b4d25780117ec", "8ad778a1206c79b4cab487d0d36bcb739f9c7623", "0bbba72496c1390dbb6cb2d4eba661f182971fe7"], "page_rank": 8.210180623973726e-05}, {"id": "79e1995ded6994540dd5a03d8a41b8c9d0378238", "title": "Relative Depth And Local Surface Orientation From Image Motions", "authors": ["K. Prazdny"], "date": 1981, "abstract": "A simple mathematical formalism is presented suggesting a mechanism for computing relative depth of any two texture elements characterized by the same relative motion parameters. The method is based on a ratio of a function of the angular velocities of the projecting rays corresponding to the two texture elements. The angu-lar velocity of a ray cannot, however, be computed directly from the instantaneous characterization of motion of a \"retinal\" point. It is shown how it can be obtained from the (linear) velocity of the image element on the projection surface and the first time derivative of its direction vector. A similar analysis produces a set of equations which directly yield local surface orientation relative to a given visual direction. The variables involved are scalar quantities directly measurable on the projection surface but, unlike the case of relative depth, the direction of (instantaneous) motion has to be computed by different means before the method can be applied. The relative merits of the two for-malisms are briefly discussed.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "67301c286439a7c24368300ea13e9785bd666aed", "title": "Velocity determination in scenes containing several moving objects", "authors": ["Claude L. Fennema", "William B. Thompson"], "date": 1979, "abstract": "Abstract A method is described which quantifies the speed and direction of several moving objects in a sequence of digital images. A relationship between the time variation of intensity, the spatial gradient, and velocity has been developed which allows the determination of motion using clustering techniques. This paper describes these relationships, the clustering technique, and provides examples of the technique on real images containing several moving objects.", "references": ["bd98ad91d611a940f62b6b6d9cfa9303da6e75b6", "cd97ea8606db15d5c813fce33a7451ad674dea04", "698488b7d0d7bec7d016ff9da483350dc1a7d834", "3b4a713d67a0a4f7099bdc40d818311b8827b8b7", "e6b0088219fa42ec6a04560421540d2518ba004c", "fed5cca467782a36f6db299566fbeac2099c3e75", "2415fd60305739543105118739f7118493257af3", "de8e89291db99de704dc484398df9f980401adc3", "a2af0d367f13565cb5676aa70f49b1ee68fb69e3", "1a31bf7db362565900063c2aa485acfff461a907"], "page_rank": 8.210180623973726e-05}, {"id": "4314c4f19eea6d4c3fd21fb37345198bea0d0559", "title": "Automatic translation of languages", "authors": ["Antony F. R. Brown"], "date": 1964, "abstract": "This chapter focuses on automatic translation of languages. Most machine translation research projects show an obvious division into a linguistic section and a programming section. This reflects not only a certain natural division of skills between linguists and programmers but also the division between information about languages that ought to be valid without reference to any machine and techniques of giving the information to a particular machine for a particular purpose. Assuming the output language to be English, the synthesis involves the rearrangement of the items of the sentence into an English order, the inflection of English stems that were originally brought from the dictionary, the insertion and suppression of articles, the choice of English equivalents for polysemie words, especially prepositions, and the choice of English expressions for what the input language expresses by inflectional elements.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "f4f5a6bd31d1b1b11654bfe2437b69e8728b5183", "title": "A Preliminary Study On Unknown Word Problem In Chinese Word Segmentation", "authors": ["Ming-Yu Lin", "Tung-Hui Chiang", "Keh-Yih Su"], "date": 1993, "abstract": "Semantic Scholar extracted view of \"A Preliminary Study On Unknown Word Problem In Chinese Word Segmentation\" by Ming-Yu Lin et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "f52b2abd79a3025ee1a2f4d2d7c0769e4f7b2277", "title": "HMM-Based Part-of-Speech Tagging for Chinese Corpora", "authors": ["Chao-Huang Chang", "Cheng-Der Chen"], "date": 1993, "abstract": "Chinese part-of-speech tagging is more difficult than its English counterpart because it needs to be solved together wgh the problem of word identification. In this paper, we present our work on Chinese part-ofspeech tagging based on a first-order, fully-connected hsdden Markov model. Part of the 1991 United Daily corpus of approzimately 10 million Chinese characters zs used for training and testing. A news article is first segmented into clauses, then into words by a Viterbi-based word identification system. The (untagged} segmented corpus is then used to train the HMM for tagging using the Bantu. Welch reestimation procedure. We also adopt Kupiec's concept of word equivalence classes in the tagger. Modeling higher or. der local constraints, a pattern.driven tag corrector is designed to postprocess the tag output of the Vgerbi decoder based on ~rained HMM parameters. Experimental results for various testing conditions are re. ported: The system is able to correctly tag approzimately 96~ of all words in the testing data.", "references": ["68c2263ceef50530996f4807da8d9a0e835905e8", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "3a8de92b304729f15d9bd6c3d22a56ab9b31e212", "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "5a5ecd9cfcf8a33e2603f5aadd96e51172b303c3", "dcd39c42439280c7cbc59570495fc0a890304454"], "page_rank": 0.00016420361247947453}, {"id": "2de1202303b1cf5972fb9837b083fd1bd17a1d56", "title": "Two Languages Are More Informative Than One", "authors": ["Ido Dagan", "Alon Itai", "Ulrike Schwall"], "date": 1991, "abstract": "This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.", "references": ["bdaf232c561f1f50e88b1d24097e214890b37e8b", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "6c6a9548e18e372fd381794f18dcee518c9f506d", "bbf8f855f06345eb16d49315bd26a9a759c3d627", "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "6c99e76bad30aee43330aa8127f1ecdd5ea46ed0", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "ef5abd33b2e485609e388a03f1b7eb5061005cc2", "909273dc44ce4c4f2cf4adfe5d60c3d421635909", "8e92fcfb55781be650529d18364f0997bda1bfa4"], "page_rank": 5.473453749315818e-05}, {"id": "bf0fc4a7a5ccf51aff40630c297db61d3fb0b50d", "title": "Generic Text Processing: A Progress Report", "authors": ["Paul S. Jacobs", "George R. Krupka", "Susan W. McRoy", "Lisa F. Rau", "Norman K. Sondheimer", "Uri Zernik"], "date": 1990, "abstract": "A generic natural language system, without modification, can effectively analyze an arbitrary input at least to the level of word sense tagging. Considerable research has addressed the transportability of natural language systems, but not generic text processing capabilities. For example, previous DARPA-sponsored work [1, 2] produced transportable interfaces to database systems. Each new application of these interfaces generally required modifications to lexicons, new semantic knowledge bases, and other specialized features. The most that natural language text processing systems have accomplished has been the parsing of arbitrary text, without any real semantic analysis.", "references": ["8a903658bbf0db4d4ad3cd673ebea7c0d3391b6a", "2797a6f03a187e76cc5fca02253ed49ac179c462", "4f10f261b24185ba603a3b80f2ddd011a37914d1", "49d84aea1228ad029d809fc0700421aa4e1904a9", "e4bb51e9600c06d0da49e0e1aeddb8d856fd1558", "3300ff8aafff777fa2678ee0a3cc5d9552b30f1f", "26ae20a476bf4b79d001c8640e4c4e4580617776", "5c0d500f108199c8f0ec05c040f8bb9c0e5ead95"], "page_rank": 5.473453749315818e-05}, {"id": "806f47862549c51f12c5539451abce2f6573236d", "title": "An Experiment in Computational Discrimination of English Word Senses", "authors": ["Ezra Black"], "date": 1988, "abstract": "A number of researchers in text processing have independently observed that people can consistently determine in which of several given senses a word is being used in text, simply by examining the half dozen or so words just before and just after the word in focus. The question arises whether the same task can be accomplished by mechanical means. Experimental results are presented which suggest an affirmative answer to this query. Three separate methods of discriminating English word senses are compared information-theoretically. Findings include a strong indication of the power of domain-specific content analysis of text, as opposed to domain-general approaches.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "52b99d29c931d9aaf1b3d6f48b31577affef0208", "title": "Word and Object", "authors": ["Willard van Orman Quine"], "date": 1960, "abstract": "Willard Van Orman Quine begins this influential work by declaring, \"Language is a social art. In acquiring it we have to depend entirely on intersubjectively available cues as to what to say and when.\" As Patricia Smith Churchland notes in her foreword to this new edition, with Word and Object Quine challenged the tradition of conceptual analysis as a way of advancing knowledge. The book signaled twentieth-century philosophy's turn away from metaphysics and what Churchland calls the \"phony precision\" of conceptual analysis. In the course of his discussion of meaning and the linguistic mechanisms of objective reference, Quine considers the indeterminacy of translation, brings to light the anomalies and conflicts implicit in our language's referential apparatus, clarifies semantic problems connected with the imputation of existence, and marshals reasons for admitting or repudiating each of various categories of supposed objects. In addition to Churchland's foreword, this edition offers a new preface by Quine's student and colleague Dagfinn Follesdal that describes the never-realized plans for a second edition of Word and Object, in which Quine would offer a more unified treatment of the public nature of meaning, modalities, and propositional attitudes.", "references": [], "page_rank": 0.0002531472359058566}, {"id": "698c24202746bbd65c9a179b38a927bb4048790a", "title": "Semantic Interpretation and Ambiguity", "authors": ["Graeme Hirst"], "date": 1988, "abstract": "Abstract A new approach to semantic interpretation in natural language understanding is described, together with mechanisms for both lexical and structural disambiguation that work in concert with the semantic interpreter. ABSITY, the system described, is a Montague-inspired semantic interpreter. Like Montague formalisms, its semantics is compositional by design and is strongly typed, with semantic rules in one-to-one correspondence with the meaning-affecting rules of a Marcus parser. The Montague semantic objects\u2014functors and truth conditions\u2014are replaced with elements of the frame language FRAIL. ABSITY's partial results are always well-formed FRAIL objects. A semantic interpreter must be able to provide feedback to the parser to help it handle structural ambiguities. In ABSITY, this is done by the \u201cSemantic Enquiry Desk,\u201d a process that answers the parser's questions on semantic preferences. Disambiguation of word senses and of case slots is done by a set of procedures, one per word or slot, each of which determines the word or slot's correct sense, in cooperation with the other procedures. It is from the fact that partial results are always well-formed semantic objects that the system gains much of its power. This, in turn, comes from the strict correspondence between syntax and semantics in ABSITY. The result is a foundation for semantic interpretation superior to previous approaches.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "8acd0749710799a327d4d41ee8565f53b10569b5", "title": "Syntax, Preference, and Right Attachment", "authors": ["Yorick Wilks", "Xiuming Huang", "Dan Fass"], "date": 1985, "abstract": "The paper claims that the right attachment rules for phrases originally suggested by Frazier and Fodor are wrong, and that none of the subsequent patchings of the rules by syntactic methods have improved the situation. For each rule there are perfectly straightforward and indefinitely large classes of simple counterexamples. We then examine suggestions by Ford et al., Schubert and Hirst which are quasi-semantic in nature and which we consider ingenious but unsatisfactory. We offer a straightforward solution within the framework of preference semantics, and argue that the principal issue is not the type and nature of information required to get appropriate phrase attachments, but the issue of where to store the information and with what processes to apply it. We present a prolog implementation of a best first algorithm covering the data and contrast it with closely related ones, all of which are based on the preferences of nouns and prepositions, as well as verbs.", "references": ["b3fb85c1415ea198cfaad8913f096f6c023cae9f", "aa1bf0c2777c84ca10ceb252cd83b9e88699ff0b", "ed57a5b47a16ed1e0d6f0b3d061a4af24dd5675f", "3ea75343c1bba82c174f69a0e53c29b21480c1da", "e6d2b4301fd3bcab27078b5f9a4aa9b7d9a5e109", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "0785038d20b5f63437e8ebe04772984d5107e81e", "975672653d06a1976345776fd2d7170c32791bfe", "d1549f3502302ee8bdecefdfddd18581d237b0ab", "d1842e8a8f0a6bf7746762808ffbea32758a5fe6"], "page_rank": 0.0002463054187192118}, {"id": "9c6e68a6d704d0d9518a807584a469f72a4c66c9", "title": "Word Formation in Natural Language Processing Systems", "authors": ["Roy J. Byrd"], "date": 1983, "abstract": "Systems which process natural language require a reliable source of information about words. Not only must their lexical subsystems handle a large number of known words; they must also cope with coinages. The morphological principles underlying the notion \"possible word\" are under active study by linguists, and are articulated in the theory of word formation. This paper presents a technique for building lexical subsystems which embody these principles by emulating the behavior of word formation rules. These subsystems combine totally idiosyncratic lexical information, stored in a dictionary, with systematic information derived from word structure. Applications for lexical subsystems built along the lines described here will be discussed.", "references": ["85a033a8abca7587b6276179617d2dacd20a0d5c"], "page_rank": 0.00053835327234342}, {"id": "a1f7769dc8bde42baa056ce7da37e825a747c603", "title": "Disambiguation by short contexts", "authors": ["Yaacov Choueka", "Serge Lusignan"], "date": 1985, "abstract": "This paper describes a technique that we believe can be of great help in many text-processing situations, and reports on an experiment recently conducted to test its validity and scope. As a background we shall present in the following sections some fundamental clarifications and remarks on our specific view of lemmatization and disambiguation. Our starting point is the double assertion that we believe would be shared by many workers in applied computational linguistics and large text-processing projects, to wit: that on the one hand lemmatization is one of the most important and crucial steps in many non-trivial text-processing cycles, but on the other hand, no operational, reasonably general, fully automatic and high-quality context-sensitive text-lemmatization system nowadays is easily accessible for any natural language. Given these two premises, the problem is how to introduce a partial element (at least) of machineaided work in the process of text-lemmatization, so as to avoid the extremely laborious and frustrating task of a word-per-word manual lemmatization of large corpora as was done in the early days of automatic text-processing projects. (For a thorough report on mechanical lemmatization programs, see ref. 4.) In this paper we focus on the analysis and experimental testing of one idea that fits naturally into this framework, namely that of disambiguation by short contexts. (The somewhat unexpected shift from \"lemmatization\" to \"disambiguation\" will be justified in the sections to come.) Based on", "references": ["10622c78edbdc4f0853cb185139a952d9b9cbb22"], "page_rank": 5.473453749315818e-05}, {"id": "601ca2ccd8bb59ed2c314d3c4d767a5736067596", "title": "Aspect", "authors": ["J. M. Peirce"], "date": 1940, "abstract": "MR. LAUGHTON'S aspect is not only a felicitous word in relation to a plane, but it is susceptible of a wider application than that which he proposes for it, since it expresses a fundamental idea in the theory of surfaces. Every surface has at every point an aspect, which is the direction of a normal at that point. This may be regarded as the first property of surfaces, for if we define a surface as that form of extension which has at every part two and only two dimensions, we virtually say that, among all the directions in space that radiate from any point of the surface, there is one and only one perpendicular to all those (infinite in number) that lie within the surface at that point; in other words, that the surface has a normal at every point. A plane is then a continuous surface which has the same aspect throughout, the angle of two planes is the measure of their difference in respect of aspect; parallel planes (as Mr. Wilson points out) are those which have the same aspect, a plane tangent to a surface is one which contains a point of the surface, and has the aspect of the surface at that point, and a line tangent to a surface is one that contains a point of the surface, and has a direction which lies within the surface (or is perpendicular to the normal) at that point. Then a straight line tangent to a plane lies wholly in the plane, and if such a line, passing through any assumed point of a plane\u2014rotate about that point\u2014always remaining tangent to the plane, it must sweep every point of the plane, for it will generate a continuous and infinite surface coincident throughout its extent with the plane, and the plane, being continuous, can have no points without this surface. Therefore, a straight line which joins two points of a plane lies wholly in the plane, whence the propositions that a plane is determined by three points, and that the intersection of two planes is a straight line, together with the other elementary theorems of the geometry of space, are readily derived.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "85b9eb556c211d954b31d9d58fed6891a07ab473", "title": "Word-Sense Disambiguation Using Statistical Methods", "authors": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "date": 1991, "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.", "references": ["76e4e034c20bea86edcc6e71bbaddb47fafeecbc", "8a9b6828c5e4339025bb78af6b025d21b4830800", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "12666047cb4588405c0c111396c34ceaa0d0f3e0", "63d2bce4d0d1c1a61ea5d12ade750f558a57b8b6", "f2f56ab99ea90301af95c2a9565e110792f645dd"], "page_rank": 5.473453749315818e-05}, {"id": "a6f698bb196b14839038d231de6f186983846c89", "title": "Tense, Aspect, and Cognitive Representation of Time", "authors": ["Kenneth Man-kam Yip"], "date": 1985, "abstract": "This paper explores the relationships between a computational theory of temporal representation (as developed by James Allen) and a formal linguistic theory of tense (as developed by Norbert Hornstein) and aspect. It aims to provide explicit answers to four fundamental questions: (1) what is the computational justification for the primitives of a linguistic theory; (2) what is the computational explanation of the formal grammatical constraints: (3) what are the processing constraints imposed on the learnability and markedness of these theoretical constructs: and (4) what are the constraints that a linguistic theory imposes on. representations. We show that one can effectively exploit the interface between the language faculty and the cognitive faculties by using linguistic constraints to determine restructions on the cognitive representations and vice versa.Three main results are obtained: (1) We derive an explanation of an observed grammatical constraint on tense .. the Linear Order Constraint .. from the information monotonicity property of the constraint propagation algorithm of Allen's temporal system: (2) We formulate a principle of markedness for the basic tense structures based on the computational efficiency of the temporal representations: and (3) We show Allen's interval-based temporal system is not arbitrary. but it can be used to explair, independently motivated linguistic constraints on tense and aspect interpretations.We also claim that the methodology of research developed in this study .. \"cross-level\" investigation of independently motivated formal grammatical theory and computational models .. is a powerful paradigm with which to attack representational problems in basic cognitive domains, e.g., space. time, causality, etc.", "references": ["195ae8bac8ae67cc1b0a5eed17c13e5d220822c2", "2f9dd1cde9850a71484af4734946a2a09cfdcfd5", "4cc70c82e5fa7f5447ad33c5e50029dc848ee871"], "page_rank": 6.157635467980295e-05}, {"id": "28c6a9b962ab9902ae1fd3b020d3db30679d90d8", "title": "Using pragmatic and semantic knowledge to correct parsing of spoken language utterances", "authors": ["Sheryl Young", "Michael Matessa"], "date": 1991, "abstract": "This paper describes the structure and operation of SOUL, or Semantically-Oriented Understanding of Language. SOUL is a knowledge intensive reasoning system which is opportunistically used to provide a more thorough, fine grained analysis of an input utterance following its processing by a case-frame speech parser. The SOUL postprocessor relies upon extensive semantic and pragmatic knowledge to correct, reject and/or clarify the outputs of the CMU PHOENIX case-frame parser for speech and speech transcripts. We describe briefly both some of the linguistic phenomena which SOUL addresses and how SOUL works to correct inaccurate interpretations produced by the PHOENIX parser. Finally, we present the results six non-overlapping test sets, each evaluated for both speech and speech transcription processing. These test sets evaluate the systems ability to enhance performance in both highly restricted and completely unrestricted input data. Further, some test sets capitalize upon the unique linguistic features of spontaneous speech. These evaluations illustrate that the decrease in incorrect interpretations and total error rate resulting from SOUL''s postprocessing are most pronounced in unrestricted transcript data and all forms of speech data, as opposed to carefully constrained test sets. For example, in processing transcription data incorrect interpretations are reduced by 53% in constrained sets, as opposed to 81% in unrestricted sets. In other words, the more difficult the processing and/or interpretation, the more there was to be gained by using extensive reasoning abilities.", "references": ["28896778c55d8e023bb5f0aa95c245d9d521c31f", "37e2304f0e7d0bdb53e66a9e16905dbf9ed70c4e", "ec46b33e334814c42721d360fc36f64c30d80894"], "page_rank": 0.0004926108374384236}, {"id": "70ff500942ccedcf0ae003eeae972260fb3d0f7f", "title": "Matching code sequences utilizing context free quality measures", "authors": ["D N Cohen", "Thomas A. Reichert", "Andrew K. C. Wong"], "date": 1975, "abstract": "Abstract A method is described herein for discovering the optimal correspondence of a pair of code sequences under generalized quality measures. The limits of both this algorithm and that of Needleman and Wunsch are presented.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "195ae8bac8ae67cc1b0a5eed17c13e5d220822c2", "title": "Events, processes, and states", "authors": ["Alexander P. D. Mourelatos"], "date": 1978, "abstract": "The familiar Vendler-Kenny scheme of verb-types, viz., performances (further differentiated by Vedler into accomplishments and achievements), activities, and states, is too narrow in two important respects. First, it is narrow linguistically. It fails to take into account the phenomenon of verb aspect. The trichotomy is not one of verbs as lexical types but of predications. Second, the trichotomy is narrow ontologically. It is a specification in the context of human agency of the more fundamental, topic-neutral trichotomy, event-process-state.The central component in this ontological trichotomy, event, can be sharply differentiated from its two flanking components by adapting a suggestion by Geoffrey N. Leech and others that the contrast between perfective and imperfective aspect in verbs corresponds to the count/mass distinction in the domain of nouns. With the help of two distinctions, of \u201ccardinal count\u201d adverbials versus frequency adverbials, and of occurrence versus associated occasion, two interrelated criteria for event predication are developed. Accordingly, \u201cMary capsized the boat\u201d is an event predication because (a) it is equivalent to \u201cThere was at least one capsizing of the boat by Mary,\u201d or (b) because it admits cardinal count adverbials, e.g., \u201cat least once,\u201d \u201ctwice,\u201d \u201cthree times.\u201d Ontologically speaking, events are defined as those occurrences that are inherently countable.", "references": [], "page_rank": 0.0004720853858784893}, {"id": "d9f0f36365cfb8b203ed15879188109195c64e03", "title": "The syntax of event structure", "authors": ["James Pustejovsky"], "date": 1991, "abstract": "In this paper we examine the role of events within a theory of lexical semantics. We propose a configurational theory of event structure and examine how it contributes to a lexical semantic theory for natural language. In particular, we argue that an event structure can provide a distinct and useful level of representation for linguistic analysis involving the aspectual properties of verbs, adverbial scope, the role of argument structure, and the mapping from the lexicon to syntax.", "references": ["c847acb03f2855198b7f2a5053947ff8caa96f4b", "eef2b85546224cdb4ad61dea09f55d1a730dbfde", "47fbbe0c1ead3106e33b4eb9776d02faf2f02769", "7b6b312338faf7032d5674e64ff533fab9a7cce5", "f70390a71fc090bcc7589d024451b6564d23f907", "ed57a5b47a16ed1e0d6f0b3d061a4af24dd5675f", "37bb39e7e941e8ec0eaa7bdba81797cf05cb4198", "57926808d91fe0d3722a25f686512d3d415018cb", "2d552b71e6f91bc5d4573ecc8a85e921b390b680", "585296a980e01de12c741360b38b73a63a336e52"], "page_rank": 6.157635467980295e-05}, {"id": "ea85589dd69f986db65898d4e117423828908867", "title": "A Computational Model of the Semantics of Tense and Aspect", "authors": ["Rebecca J. Passonneau"], "date": 1988, "abstract": "The PUNDIT natural-language system processes references to situations and the intervals over which they hold using an algorithm that integrates the analysis of tense and aspect. For each tensed clause, PUNDIT processes the main verb and its grammatical categories of tense, perfect, and progressive in order to extract three complementary pieces of temporal information. The first is whether a situation has actual time associated with it. Secondly, for each situation that is presumed to take place in actual time, PUNDIT represents its temporal structure as one of three situation types: a state, process, or transition event. The temporal structures of each of these situation types consist of one or more intervals. The intervals are characterized by two features: kinesis, which pertains to their internal structure, and boundedness, which constrains the manner in which they get located in time. Thirdly, the computation of temporal location exploits the three temporal indices proposed in Reichenbach 1947: event time, speech time, and reference time. Here, however, event time is formulated as a single component of the full temporal structure of a situation in order to provide an integrated treatment of tense and aspect.", "references": ["195ae8bac8ae67cc1b0a5eed17c13e5d220822c2", "4012d98de6d47ed73f15b92161930dfb9dd584eb", "74e3c1360d8adba378e6f1ac9142a859a5749106", "f7cf8f499d881a98c2ffc8f8808e765bee0d3dfd", "2f9dd1cde9850a71484af4734946a2a09cfdcfd5", "96f8e185a95537aa3fc99c74ffb7755f5fad5884", "53f49cc56afdd85ea7217a796d50a29f810113b5", "527ac7437bf6f028f22d25bc48b92b8159b86581", "876e8b0c873d75ec516dfaf6e8027d4076eba78f", "d36afe59ad1b706e020f55b54740bc9cddf25dcd"], "page_rank": 6.157635467980295e-05}, {"id": "271f36ba5338c7daab6141728d3db9b95de71451", "title": "Aspectual Requirements of Temporal Connectives: Evidence for a Two-Level Approach to Semantics", "authors": ["Michael Herweg"], "date": 1991, "abstract": "This paper argues for a two-level theory of semantics as opposed to a one-level theory, based on the example of the system of temporal and durational connectives. Instead of identifying the semantics of lexical items and larger constituents with conceptual structures, the two-level theory assumes that there is an intra-linguistic level of abstract semantic representations distinct from the extra-linguistic level of conceptual representations. It is argued that the two-level theory can give a more appropriate treatment of polysemy as well as a better account of the conditions under which connectives may (more or less) be felicitously combined with complement expressions which do not respect the aspectual requirements of the connectives.", "references": ["e59d9d7ca7207abaddc8ce20b7e664050d6774ce", "fbbbc3548d7266712602148b5d5b1196e49656de", "c4b6c9fbfc2e63ee1c0350cc1819b6b5f47d07b0", "d61ef882fd9ccf444a11087d700e3a823a3e7b3d", "18db9b5e2e41ef0b64afb018d7643241385eaec1", "dd51413edb57a8c8267abd9745a9b226a39d536f", "4911cbf260a58e4f48a9a25e332b5aeff3b22557", "4323cc155dc0ca7c861ccc262b279b9a926e0d1f", "4226a9e0534a987712ad3df90daac829e86fd945"], "page_rank": 6.157635467980295e-05}, {"id": "51da89ed9d701ebb66946dbfe03d726b8c23c5b3", "title": "Temporal Ontology and Temporal Reference", "authors": ["Marc Moens", "Mark Steedman"], "date": 1988, "abstract": "A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a \"nucleus.\" A nucleus can be thought of as an association of a goal event, or \"culmination,\" with a \"preparatory process\" by which it is accomplished, and a \"consequent state,\" which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category. We claim that any manageable formalism for natural-language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language.", "references": ["4012d98de6d47ed73f15b92161930dfb9dd584eb", "b4cb210ae8d9c338a9f1939e11ef471ca6c5ad22", "3bb7eba7f496ed336c58cbe5290a83b411d4caad", "d36afe59ad1b706e020f55b54740bc9cddf25dcd", "350bb9552fac79fa2c3865ab93c498eeb1d9757b", "138de06d4c4a84ebce2b43cd2ac9852f6549b6f9", "07dd7b2061e3e4f775081c62dc29ce4e976fcce2", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "1bb8191ac3d1642828906c02366791964b1dd267", "7d90e153652401cfb690294210940412a56103e0"], "page_rank": 6.157635467980295e-05}, {"id": "2211b817aecdcec91990c52e655aea47c579cc36", "title": "An improved method of testing for evolutionary homology.", "authors": ["Walter M. Fitch"], "date": 1966, "abstract": "A more sensitive method of searching for a homologous relation between two proteins is presented. The method depends on determining the minimum number of nucleotides which must be altered to permit tho conversion of one sequence into the other. Results for \u03b1 - and \u03b2 -hemoglobin, for which the homology is already known, are presented and shown to be consistent.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "c6aee6e8d9352bc906b5ea148b3e42c702f55c29", "title": "Crossassociation: A method of comparing protein sequences", "authors": ["Michael J. Sackin"], "date": 1971, "abstract": "Crossassociation is a computer method of comparing protein sequences. It can help detect amino acid matches, deletions, insertions, and other similarities which would be hard to detect by eye. The method is to \u201cslide\u201d the sequences past each other one step at a time and to count the number of amino acids that match. At each overlap position, the program prints the percentage match and statistical significance measures of the matching. The null hypothesis for significance is the random arrangement of amino acids in the proportions found in the sequences under study. For most protein pairs, the expected proportion of matches is about 1/14. The method includes computation of three overall similarity measures between sequences which should have use in both evolutionary and taxonomic studies. The use of the method has been tested with actual and hypothetical sequences. Problems of recovering evolutionary relationships by this and related methods are discussed.", "references": ["fa24af669bc11711b18ca4f45e9abb85546ed36e", "a8444e7070cd372d77789f80aa737de2715fc648", "0c858efcdcf2b118e4412d89d394f11d4b82b0c4", "aab8e0b0b87147b894f1ac9e7f2e5a802063fc13", "792a98c9d8e0c26f2b1750ecf2681cdb0c72b280", "2b25c709f7d9bf6afff8ae68999c993337934da4", "e4d87b11dac549915e7eec45852c9d26951a9e70", "ba50675452af22a6f7651af92389155d5f567034", "58bebf5399025f1089769502329820045c7a8f07", "08ef6297631633402cbaeffa781a848140711ab3"], "page_rank": 9.852216748768472e-05}, {"id": "b217788dd6d274ad391ee950e6f6a34033bd2fc7", "title": "The multilayer perceptron as an approximation to a Bayes optimal discriminant function", "authors": ["Dennis W. Ruck", "Steven K. Rogers", "Matthew Kabrisky", "Mark E. Oxley", "Bruce W. Suter"], "date": 1990, "abstract": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function. The result is demonstrated for both the two-class problem and multiple classes. It is shown that the outputs of the multilayer perceptron approximate the a posteriori probability functions of the classes being trained. The proof applies to any number of layers and any type of unit activation function, linear or nonlinear.", "references": ["f8fab8de8a501263b2380438b642204f982c5285", "daa27a685cb1b9d29c967116bc16743f8d64bc28", "56623a496727d5c71491850e04512ddf4152b487"], "page_rank": 7.037297677691766e-05}, {"id": "792a98c9d8e0c26f2b1750ecf2681cdb0c72b280", "title": "Locating gaps in amino acid sequences to optimize the homology between two proteins", "authors": ["W. M. Fitch"], "date": 1969, "abstract": "A method for optimally locating gaps in the amino acid sequences of homologous proteins is presented. The method involves three steps: (1) demonstration that the sequences are indeed homologous, (2) location of regions where the homologous pairing is reasonably certain, and (3) location of gaps between these regions so as to minimize the total number of mutations required to account for the differences between the two sequences. The major virtues of this procedure are that the assertion of homology does not depend upon the prior introduction of gaps and that a genetic rather than a chemical test is the basis for asserting a genetic relationship.", "references": ["1c7457322e274926ad5b738df069049a8c64bbe1", "2c8259d341a9d98220ec0977150419d461340921", "be713ad77b230b2bc9e571ca84a3a6198bbe08d6"], "page_rank": 0.0002627257799671592}, {"id": "218f32b107e3d461781cad50888845a08486bb24", "title": "A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers", "authors": ["Kenney Ng", "R. P. Lippmann"], "date": 1990, "abstract": "Seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks. Two neural network (radial basis function and high order polynomial GMDH network) and five conventional classifiers (Gaussian mixture, linear tree, K nearest neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. Classifiers were chosen to be representative of different approaches to pattern classification and to complement and extend those evaluated in a previous study (Lee and Lippmann, 1989). This and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum error decision regions, when they are properly tuned, and when sufficient training data is available. Practical characteristics such as training time, classification time, and memory requirements, however, can differ by orders of magnitude. These results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate, but by practical considerations concerning memory usage, computational resources, ease of implementation, and restrictions on training and classification times.", "references": ["c82593ecb35e4590354b980234370de7e76c9116", "579d1134df253899a775c1b5857030d8c5023ac1", "4e08b316b07b62b74316db3079ba457d918e00db", "a7643bea8d749ca2a70b2277a50c061c3fc699a0"], "page_rank": 0.00019352568613652358}, {"id": "adf81acbfb348c7ebacb97858beb3d193766bb2a", "title": "A new error criterion for posterior probability estimation with neural nets", "authors": ["Amro El-Jaroudi", "John Makhoul"], "date": 1990, "abstract": "The authors introduce an error criterion for training which improves the performance of neural nets as posterior probability estimators, as compared to using least squares. The proposed criterion is similar to the Kullback-Leibler information measure and is simple to use. A straightforward iterative algorithm for the minimization of the error criterion which has been shown to have good convergence properties is described. The authors applied the proposed technique to some classification examples and showed it to produce better posterior probability estimates than least squares, especially for low probabilities", "references": ["7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "8da1dda34ecc96263102181448c94ec7d645d085"], "page_rank": 7.037297677691766e-05}, {"id": "967f32841955b72f358190436baa5510839d9ab3", "title": "A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins", "authors": ["CHRISTUS", "WUKSCH}"], "date": 1940, "abstract": "A computer adaptable method for finding similarities in the amino acid sequences of two proteins has been developed. From these findings it is possible to determine whether significant homology exists between the proteins. This information is used to trace their possible evolutionary development. The maximum match is a number dependent' upon the similarity of the sequences. One of its definitions is the largest number of amino acids of one protein that can be matched with those of a second protein allowing for all possible interruptions in either of the sequences. While the interruptions give rise to a very large number of comparisons, the method efficiently excludes from consideration those comparisons that cannot contribute to the maximum match. Comparisons are made from the smallest unit of significance, a pair of amino acids, one from each protein. All possible pairs are represented by a two-dimensional array, and all possible comparisons are representod by pathways through the array. For this maximum match only cerhain of the possible pathways must be evaluated. A numerical value, one in this case. is assigned to every cell in the array representing like amino acids. The maximum match is the largest number that would result from summing the cell values of every pathway. The amino acid sequences of a number of proteins have been compared to determine whether the relationships existing between them could have occurred by chance. Generally, these sequences are from proteins haring closely related functions and are so similar that simple visual comparisons can reveal sequence coincidence. Because the method of visual comparison is tedious and because the determination of the significance of a given result usually is left to intuitive rationalization, computer-based statistical approaches have been proposed (Fitch, 1966; Xeedleman 6 Blair, 1969). Direct comparison of two sequences, based on the presence in both of corresponding amino acids in a n identical array, is insuEcient to establish the full genetic relationships between the two proteins. Allowance for gaps (Braunitzer, 1965) greatly mnltiplies the number of comparisons t.hat can be made but int.roduces unnecessary and partial comparisons. The smallest unit of comparison is a pair of amino acids, one from each protein. The maximum match can be defined as the largest number of amino acids of one protein that can be matched with those of another protein while allowing for all possible deletions.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "7c8293e7054230cc6cc6e3172f761d89d267f7a7", "title": "Learning algorithms and probability distributions in feed-forward and feed-back networks.", "authors": ["John J. Hopfield"], "date": 1987, "abstract": "Learning algorithms have been used both on feed-forward deterministic networks and on feed-back statistical networks to capture input-output relations and do pattern classification. These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers. In simple but nontrivial networks the two learning rules are closely related. Under some circumstances the learning problem for the statistical networks can be solved without Monte Carlo procedures. The usual arbitrary learning goals of feed-forward networks can be given useful probabilistic meaning.", "references": ["f3d3244d62c38d653fd119e328eabe0bf53b53ff"], "page_rank": 7.037297677691766e-05}, {"id": "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "title": "A probabilistic approach to the understanding and training of neural network classifiers", "authors": ["Herbert Gish"], "date": 1990, "abstract": "It is shown that training a neural network using a mean-square-error criterion gives network outputs that approximate posterior class probabilities. Based on this probabilistic interpretation of the network operation, information-theoretic training criteria such as maximum mutual information and the Kullback-Liebler measure are investigated. It is shown that both of these criteria are equivalent to the maximum-likelihood estimation (MLE) of the network parameters. MLE of a network allows for the comparison of network models using the Akaike information criterion and the minimum-description length criterion.>", "references": ["ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33", "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5", "04955809d8b9472ce26bddaee7bf23340d6be97e"], "page_rank": 0.00039878020173586673}, {"id": "656a33c1db546da8490d6eba259e2a849d73a001", "title": "Learning in Artificial Neural Networks: A Statistical Perspective", "authors": ["Halbert White"], "date": 1989, "abstract": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks.", "references": ["72d761afbe35634213849419ff63fad5bc9fabeb", "b10440620da8a43a1b97e3da4b1ff13746306475", "1b1237609c9d95a0fb33dc95697f6b78cf7935ee", "212a4ab68c4489eca22984ecd297e986693e5200", "ee7f0bc85b339d781c2e0c7e6db8e339b6b9fec2", "8da1dda34ecc96263102181448c94ec7d645d085", "355dfdd45b4b644853250f531f66e72d917432b1", "386cbc45ceb59a7abb844b5078e5c944f17723b4", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "8e4bd5422c82009290a5cd71457388f0780530d6"], "page_rank": 7.037297677691766e-05}, {"id": "85241210389fbce403f5d12597b9bf32a5633dc2", "title": "Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function", "authors": ["John B. Hampshire", "Barak A. Pearlmutter"], "date": 1991, "abstract": "This paper presents a number of proofs that equate the outputs of a Multi-Layer Perceptron (MLP) classifier and the optimal Bayesian discriminant function for asymptotically large sets of statistically independent training samples. Two broad classes of objective functions are shown to yield Bayesian discriminant performance. The first class are \u201creasonable error measures,\u201d which achieve Bayesian discriminant performance by engendering classifier outputs that asymptotically equate to a posterioriprobabilities. This class includes the mean-squared error (MSE) objective function as well as a number of information theoretic objective functions. The second class are classification figures of merit (CFM mono ), which yield a qualified approximation to Bayesian discriminant performance by engendering classifier outputs that asymptotically identify the maximum a posteriori probability for a given input. Conditions and relationships for Bayesian discriminant functional equivalence are given for both classes of objective functions. Differences between the two classes are then discussed very briefly in the context of how they might affect MLP classifier generalization, given relatively small training sets.", "references": ["ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "4789ed5137d5384b33fc184b5a91151b5f6816e6", "4e08b316b07b62b74316db3079ba457d918e00db", "8da1dda34ecc96263102181448c94ec7d645d085", "73aa78f62329c01e60af4cf438645845aa9a803a", "a57c6d627ffc667ae3547073876c35d6420accff", "4c154b63659e656d0402cfc06e8aa8c24f804d08", "dfeb5d5c79f0dcdbddb789c00d940dd7034594ef"], "page_rank": 7.037297677691766e-05}, {"id": "1a31bf7db362565900063c2aa485acfff461a907", "title": "A Methodology for Real Time Scene Analysis", "authors": ["David C. Hogg"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"A Methodology for Real Time Scene Analysis\" by David C. Hogg", "references": [], "page_rank": 0.00016420361247947453}, {"id": "de8e89291db99de704dc484398df9f980401adc3", "title": "Automated Cloud Tracking Using Precisely Aligned Digital ATS Pictures", "authors": ["Eric A. Smith", "Dennis Gallegos Roxana Phillips"], "date": 1972, "abstract": "The measurement of winds to an accuracy of at least 2 knots (1 m/s) is a major requirement of the global atmospheric research program (GARP) [1]. Such accurate wind measurements will be needed over large regions presently lacking ground or rawinsonde observations. A man-computer system called WINDCO has been developed to measure cloud motion from ATS pictures. This system is capable of providing cloud motion measurements that are repeatable to within 6 knots over two independent time intervals in the majority of cases.", "references": ["47922b31367ef2e58c516fff5ca2d77c3b80f70c", "5e1eab8ffdc9de8ed466a7158f202837949b762f", "8ab49b2f920056c1d145fdebd3f077768bbb9a65", "a5d581182937c408f1ce615c8b4c3e2715b25803", "2bfd67677ec4e08f0a17bb44ef60a170be059b51"], "page_rank": 0.00016420361247947453}, {"id": "a2af0d367f13565cb5676aa70f49b1ee68fb69e3", "title": "Picture Processing by Computer", "authors": ["Azriel Rosenfeld"], "date": 1969, "abstract": "Abstract : The field of picture processing by computer is reviewed from a technique-oriented standpoint. Only the processing of given pictures (as opposed to computer-synthesized pictures) is considered. Specific areas covered include: (a) Pictures as information sources and their efficient encoding; (b) Approximation of pictures - sampling and quantization techniques; (c) Position-invariant operations on pictures and their implementation (digital, electro-optical, optical); applications to matched filtering (template matching), spatial frequency filtering and image restoration, measurement of image quality, and image enhancement ('smoothing' and 'sharpening'); (d) Picture properties (linear; local and 'textural'; random) useful for pictorial pattern recognition; (e) 'Figure extraction' from pictures; figure properties (topology, size, shape); (f) Picture description and 'picture languages.' (Author)", "references": [], "page_rank": 0.0002189381499726327}, {"id": "3300ff8aafff777fa2678ee0a3cc5d9552b30f1f", "title": "A transportable natural language interface for information retrieval", "authors": ["Matthew Bates", "Robert J. Bobrow"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"A transportable natural language interface for information retrieval\" by Matthew Bates et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e4bb51e9600c06d0da49e0e1aeddb8d856fd1558", "title": "Building A Large Thesaurus For Information Retrieval", "authors": ["Edward A. Fox", "Jane Terry Nutter", "Thomas Ahlswede", "Martha W. Evens", "Judith A. Markowitz"], "date": 1988, "abstract": "Information retrieval systems that support searching of large textual databases are typically accessed by trained search intermediaries who provide assistance to end users in bridging the gap between the languages of authors and inquirers. We are building a thesaurus in the form of a large semantic network to support interactive query expansion and search by end users. Our lexicon is being built by analyzing and merging data from several large English dictionaries; testing of its value for retrieval is with the SMART and CODER systems.", "references": ["2e6060b6deb83fc31d22e1e3e7ee456320b0f0de", "4398f181cb427458a7e49f64b3b8928d218fbf39", "b71990b9a715121ba6c8bdc00d6ae08ce2173bbf", "6c6a9548e18e372fd381794f18dcee518c9f506d", "9384f7812fec87a41938ae28fe19c8f65f555c97", "99f5d79da460a87b2fe802f3d70a83d58e4d0daf", "4244ac582d55b5109c28e231f0f7919c201a9ff9", "1aa9cf0242b66e26d3140cff56d75a1e58fbaf30", "b8d20c8b6a480c0f50fb9ef35280786321efdfad", "f0dea29ac3e2c7f8937b3f65cc51431b99b10943"], "page_rank": 0.00014367816091954023}, {"id": "5a5ecd9cfcf8a33e2603f5aadd96e51172b303c3", "title": "Constructing A Phrase Structure Grammar By Incorporating Linguistic Knowledge And Statistical Log-Likelihood Ratio", "authors": ["Keh-Yih Su", "Yu-Ling Hsu", "Claire Saillard"], "date": 1991, "abstract": "Semantic Scholar extracted view of \"Constructing A Phrase Structure Grammar By Incorporating Linguistic Knowledge And Statistical Log-Likelihood Ratio\" by Keh-Yih Su et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "26ae20a476bf4b79d001c8640e4c4e4580617776", "title": "Lexical acquisition and information retrieval", "authors": ["Robert Krovetz"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Lexical acquisition and information retrieval\" by Robert Krovetz", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "8a903658bbf0db4d4ad3cd673ebea7c0d3391b6a", "title": "The GE NLToolset: A Software Foundation for Intelligent Text Processing", "authors": ["Paul S. Jacobs", "Lisa F. Rau"], "date": 1990, "abstract": "Many obstacles stand in the way of computer programs that could read and digest volumes of natural language text. The foremost of these difficulties is the quantity and variety of knowledge about language and about the world that seems to be a prerequisite for any substantial language understanding. In its most general form, the robust text processing problem remains insurmountable; yet practical applications of text processing are realizable throngh a combination of knowledge representation and language analysis strategies. This project note describes the GE NLToo~s~,:T and its use in two text processing applications. In the first, dornain, the system selects and analyzes stories about corporate mergers and acquisitions as they come across a real-time news feed. In the second do~ main, the program uses naval operations messages to fill a 10--field template. In both cases, users can ask natural language questions about, the contents of the texts, and the system responds with direct answers along with the original text. The G E NLTooLsET is a software foundation for text processing. The NL'I'OOLS~?'r derives from a research effort aimed at preserving the capabilities of naturM language text processing across domains. The program achieves this transportabili ty by using a core knowledge base and lexicon that customizes easily to new applications, along with a flexible text processing strategy tolerant of gaps in the program's knowledge base. Developed over the last four years, it runs in real time on a SUN TM workstation in Common Lisp under UNIX TM. It performs the following t asks:", "references": ["29dfba100b567a6d9aaa3a66b83994f9925c49e1", "26ed78a6bbfe065b3f9876bc128e7867af394e4c", "4f10f261b24185ba603a3b80f2ddd011a37914d1", "9dd4169b8db81701ee68eb2f30c9e03bb4951310", "95ebc3f1886000e2f1077cd9b93b5f6ae66aadff", "5c0d500f108199c8f0ec05c040f8bb9c0e5ead95", "1746e016ecd239b27f6b476b001c060bf88a0a7a"], "page_rank": 6.157635467980295e-05}, {"id": "dcd39c42439280c7cbc59570495fc0a890304454", "title": "Discrimination Oriented Probabilistic Tagging", "authors": ["Yi-Chung Lin", "Tung-Hui Chiang", "Keh-Yih Su"], "date": 1992, "abstract": "Semantic Scholar extracted view of \"Discrimination Oriented Probabilistic Tagging\" by Yi-Chung Lin et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "49d84aea1228ad029d809fc0700421aa4e1904a9", "title": "TEAM: An Experiment in the Design of Transportable Natural-Language Interfaces", "authors": ["Barbara J. Grosz", "Douglas E. Appelt", "Paul A. Martin", "Fernando C Pereira"], "date": 1987, "abstract": "Abstract This article describes TEAM, a transportable natural-language interface system. TEAM was constructed to test the feasibility of building a natural-language system that could be adapted to interface with new databases by users who are not experts in natural-language processing. An overview of the system design is presented, emphasizing those choices that were imposed by the demands of transportability. Several general problems of natural-language processing that were faced in constructing the system are discussed, including quantifier scoping, various pragmatic issues, and verb acquisition. TEAM is compared with several other transportable systems; this comparison includes a discussion of the range of natural language handled by each as well as a description of the approach taken to achieving transportability in each system.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4f10f261b24185ba603a3b80f2ddd011a37914d1", "title": "Integrating Top-Down And Bottom-Up Strategies In A Text Processing System", "authors": ["Lisa F. Rau", "Paul S. Jacobs"], "date": 1988, "abstract": "The SCISOR system is a computer program designed to scan naturally occurring texts in constrained domains, extract information, and answer questions about that information. The system currently reads newspapers stories in the domain of corporate mergers and acquisitions. The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing. Four knowledge sources, including syntactic and semantic information and domain knowledge, interact in a flexible manner. This integration produces a more robust semantic analyzer designed to deal gracefully with gaps in lexical and syntactic knowledge, transports easily to new domains, and facilitates the extraction of information from texts.", "references": ["3cb80dedbf7328f86823b8bf6715a33a695061a9", "17db499f2d71faabd7719ecca51cba5854b31d91", "00c7b37ad903430676bd6afb825f7ce1a8d4cecf", "26ed78a6bbfe065b3f9876bc128e7867af394e4c", "82be702ac489915a05a97afc168fe1f9e5644d9a", "48d6ba1f1f359f44fe71180c4b28cb873e7aa49d", "c20bba13e0319039e76f08a40d5cff85a485c9dc", "b6c34bf9eaea1760ca88799fbc4a74358682f7c5", "0ff65ac698013cdd9d61326cab49a1d75404e001", "7e0fce0cfa5f076af3336cdef6099d4748169367"], "page_rank": 0.00014367816091954023}, {"id": "85a033a8abca7587b6276179617d2dacd20a0d5c", "title": "Computer Analysis of English Word Formation", "authors": ["Nick Cercone"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Computer Analysis of English Word Formation\" by Nick Cercone", "references": [], "page_rank": 0.0004926108374384236}, {"id": "f2f56ab99ea90301af95c2a9565e110792f645dd", "title": "Determination of lexical semantic relations for multi-lingual terminology structures", "authors": ["John S. White"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Determination of lexical semantic relations for multi-lingual terminology structures\" by John S. White", "references": [], "page_rank": 0.0002463054187192118}, {"id": "10622c78edbdc4f0853cb185139a952d9b9cbb22", "title": "Automatic retrieval of frequent idiomatic and collocational expressions in a large corpus", "authors": ["Yaacov Choueka", "Timothy M. Klein", "E. Neuwitz"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"Automatic retrieval of frequent idiomatic and collocational expressions in a large corpus\" by Yaacov Choueka et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "ec46b33e334814c42721d360fc36f64c30d80894", "title": "Coping with Extragrammaticality", "authors": ["Jaime G. Carbonell", "Philip J. Hayes"], "date": 1984, "abstract": "Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical and sentential levels, discussing recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammaticality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.", "references": ["b33e8093c332f185f8c7e074b73cd7c79838b16a", "d8d9f4a23dd35e0514a6b66cca3a1cd100e2b730", "0488fe2e6e94114aee15574782949a9984c7771a", "f3f09d77332979d8315c775c1e6654323ff661cd", "b3bab8a001fb3ded2e1d6cfa416ae9c6fbbdea0a", "2bdd74b2af14fb74571d6a31cefcce5753bcf5e8", "b9268d24c5adf344e8da18c9b20d7e690c46234e", "2e141dcaa084c257397a88e807dc2a0ea121550d", "76ab1651636880dcf38fe04a838f271b11d30d9a", "3887c408e6b49b2f5393a1fa2922195d02f37202"], "page_rank": 0.0004926108374384236}, {"id": "4cc70c82e5fa7f5447ad33c5e50029dc848ee871", "title": "The Philosophy of Grammar", "authors": ["Otto Jespersen}"], "date": 1940, "abstract": "1. Living grammar 2. Systematic grammar 3. Systematic grammar (continued) 4. Parts of speech 5. Substantive and adjectives 6. Parts of speech (concluded) 7. The three ranks 8. Junction and nexus 9. Various kinds of nexus 10. Nexus-substantives. Final words on nexus 11. Subject and predicate 12. Object. Active and passive 13. Case 14. Number 15. Number (concluded) 16. Person 17. Sex and Gender 18. Comparison 19. Time and tense 20. Time and tense (concluded 21. Direct and indirect speech 22. Classification of utterances 23. Moods 24. Negation 25. Conclusion", "references": [], "page_rank": 0.00016420361247947453}, {"id": "585296a980e01de12c741360b38b73a63a336e52", "title": "Lexical Knowledge Representation and Natural Language Processing", "authors": ["James Pustejovsky", "Branimir Boguraev"], "date": 1991, "abstract": "Pustejovsky, J. and B. Boguraev, Lexical knowledge representation and natural language processing, Artificial Intelligence 63 (1993) 193-223. Traditionally, semantic information in computational lexicons is limited to notions such as selectional restrictions or domain-specific constraints, encoded in a \"static\" representation. This information is typically used in natural language processing by a simple knowledge manipulation mechanism limited to the ability to match valences of structurally related words. The most advanced device for imposing structure on lexical information is that of inheritance, both at the object (lexical items) and meta (lexical concepts) levels of lexicon. In this paper we argue that this is an impoverished view of a computational lexicon and that, for all its advantages, simple inheritance lacks the descriptive power necessary for characterizing fine-grained distinctions in the lexical semantics of words. We describe a theory of lexical semantics making use of a knowledge representation framework that offers a richer, more expressive vocabulary for lexical information. In particular, by performing specialized inference over the ways in which aspects of knowledge structures of words in context can be composed, mutually compatible and contextually relevant lexical components of words and phrases are highlighted. We discuss the relevance of this view of the lexicon, as an explanatory device accounting for language creativity, as well as a mechanism underlying the implementation of open-ended natural language processing systems. In particular, we demonstrate how lexical ambiguity resolution--now an integral part of the same procedure that creates the semantic interpretation of a sentence itself--becomes a process not of selecting from a pre-determined set of senses, but of highlighting certain lexical properties brought forth by, and relevant to, the current context.", "references": ["8bd683d5aef2704207fca32a175f0c79f871e180", "98161f60a12c5a44b898f89282d2060808dc0aba", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "92ba24e7fbce40bd1d3e9275651b3a84f409a02e", "8cabcc75725e975d4433c176f9afa53fac984d83", "5dc32dde19502971279aa1699b7f6d5690c0eaa1", "e1df0ae12a634485f668f24ad683c238886c4d41", "d381e5621aff1410603ca35cc757a40a1ac82a90", "aff1b791866d308ea37cca86e2aa76e852f23604", "39ba9f7833c059ad2b165e4c38867d2043778667"], "page_rank": 0.0002463054187192118}, {"id": "63d2bce4d0d1c1a61ea5d12ade750f558a57b8b6", "title": "An iterative 'flip-flop' approximation of the most informative split in the construction of decision trees", "authors": ["A. Nadas", "David Nahamoo", "Michael A. Picheny", "J. Powell"], "date": 1991, "abstract": "The authors seek a fast algorithm for finding the best question to ask (i.e., best split of predictor values) about a predictor variable when predicting membership in more than two categories. They give a fast iterative algorithm for finding a suboptimal question in the N category problem by exploiting a fast algorithm for finding the optimal question in the two-category problem. The algorithm has been used in a number of speech recognition applications.>", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2d552b71e6f91bc5d4573ecc8a85e921b390b680", "title": "Operations on lexical forms : unaccusative rules in Germanic languages", "authors": ["L. Stefan Levin"], "date": 1985, "abstract": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 1986.", "references": ["0095fea57f5649ee17191b7c56f2d3d63e9971ea", "2baaaa54e45baa099df929ba1ba78628afe1d21c", "0f8e0586e2c60e3fc5dae3d88d9c0110e228aa96", "ee8482f582cecb344e0fd1800afa846ecf7caf30", "b32ee844f91f10ceff9bc6d37297029ec22efd85", "844ee351f389b02b159b5d09c7d4fa069fdadfa3", "c7078b69da231acd6c835b0b5dc0a9faf9938392", "4661df154827793820afcf62ad803cc57afe48df"], "page_rank": 0.0002463054187192118}, {"id": "4226a9e0534a987712ad3df90daac829e86fd945", "title": "Semantics of English Temporal Connectives", "authors": ["Osami Aki"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Semantics of English Temporal Connectives\" by Osami Aki", "references": [], "page_rank": 0.0002463054187192118}, {"id": "7d90e153652401cfb690294210940412a56103e0", "title": "A lattice theoretic approach to computation based on a calculus of partially ordered type structures (property inheritance, semantic nets, graph unification)", "authors": ["Hassan A{\\\"i}t-Kaci"], "date": 1984, "abstract": "The purpose of this thesis is twofold: (1) to define a formal lattice-theoretic calculus of partially ordered type structures where the ordering is meant to reflect subtyping; (2) to propose a model of computation which amounts to solving systems of simultaneous equations in a lattice of types. \nThe specific contributions which I believe to be original of the research presented here are: (1) An extrapolation of the syntactic properties of first-order terms to provide insight in formalizing record-like type structures; (2) A simple \"type-as-set\" semantics and a motivational discussion of what this entails for the operational use of partially ordered types in programming; (3) A lattice-theoretic calculus of type subsumption and a formal universal construction extending this calculus in the light of the foregoing discussion; (4) An efficient algorithm to compute greatest lower bounds of type structures; (5) The definition of a particular language (KBL) based on solving recursive equations in the lattice of types, and a fixed-point semantics study of its model of computation.", "references": [], "page_rank": 0.0006568144499178981}, {"id": "4323cc155dc0ca7c861ccc262b279b9a926e0d1f", "title": "Ans?tze zu einer semantischen Beschreibung topologischer Pr?positionen", "authors": ["Michael Herweg"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Ans?tze zu einer semantischen Beschreibung topologischer Pr?positionen\" by Michael Herweg", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2f9dd1cde9850a71484af4734946a2a09cfdcfd5", "title": "Tense and continuity", "authors": ["Barry Taylor"], "date": 1977, "abstract": "The paper proposes a formal account of Aristotle's trichotomy of verbs, in terms of properties of their continuous tensings, into S(\u2018state\u2019)-verbs, K(\u2018kinesis\u2019)-verbs, and E-(\u2018energeia\u2019)-verbs. Within a Fregean tense framework in which predicates are relativized to times, an account of the continuous tenses is presented and a preliminary account of the trichotomy devised, which permits an illuminating analogy to be drawn between the temporal properties of E- and K-verbs and the spatial properties of stuffs and substances. This analogy is drawn upon in constructing a sophisticated version of the preliminary theory accommodating more of the linguistic data.", "references": [], "page_rank": 0.0004105090311986863}, {"id": "08ef6297631633402cbaeffa781a848140711ab3", "title": "Homology of Pseudomonas cytochrome c-551 with eukaryotic c-cytochromes.", "authors": ["Saul B. Needleman", "Terence T. Blair"], "date": 1969, "abstract": "The homology of Pseudomonas cytochrome c-551 with eukaryotic cytochromes c is examined with a computer-based procedure devised to determine whether similarities exist between these proteins. One method is given by which the more recently evolved cytochromes c might have arisen from the Pseudomonas protein. This procedure involves only common genetic phenomena and accounts for most of the structural differences between the bacterial and mammalian cytochromes. A time-scale relationship between the c-cytochromes from several microorganisms, a mold, yeast, and the eukaryotic organisms is proposed.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "58bebf5399025f1089769502329820045c7a8f07", "title": "Selective Constraints on Amino-acid Substitutions during the Evolution of Proteins", "authors": ["Bryan Clarke"], "date": 1970, "abstract": "RECENTLY, several workers have suggested that during the evolution of proteins most amino-acid substitutions (in the sense of mutations that have spread to fixation) have been neutral in selective value1\u20134. These views have not received unanimous support5,6. It seems appropriate, therefore, to examine the pattern of substitutions in order to detect any possible influences of natural selection. I here analyse the relationship between the frequencies of particular substitutions and the chemical properties of the amino-acids concerned in them.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "be713ad77b230b2bc9e571ca84a3a6198bbe08d6", "title": "Evolutionary Similarities between Pancreatic Proteolytic Enzymes", "authors": ["B. S. Hartley", "J. R. Brown", "DOROTHY L. Kauffman", "L. B. Smillie"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Evolutionary Similarities between Pancreatic Proteolytic Enzymes\" by B. S. Hartley et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "a7643bea8d749ca2a70b2277a50c061c3fc699a0", "title": "Classifiers : adaptive modules in pattern recognition systems", "authors": ["Yuchun Lee"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Classifiers : adaptive modules in pattern recognition systems\" by Yuchun Lee", "references": [], "page_rank": 0.00016420361247947453}, {"id": "f3d3244d62c38d653fd119e328eabe0bf53b53ff", "title": "Information and Information Stability of Random Processes", "authors": ["Mark Semenovich Pinsker"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Information and Information Stability of Random Processes\" by Mark Semenovich Pinsker", "references": [], "page_rank": 0.0004926108374384236}, {"id": "8e4bd5422c82009290a5cd71457388f0780530d6", "title": "Construction of neural nets using the radon transform", "authors": ["Spencer Carroll", "Bradley W. Dickinson"], "date": 1989, "abstract": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.>", "references": [], "page_rank": 0.0002627257799671592}, {"id": "4e08b316b07b62b74316db3079ba457d918e00db", "title": "Pattern classification using neural networks", "authors": ["R. P. Lippmann"], "date": 1989, "abstract": "The author extends a previous review and focuses on feed-forward neural-net classifiers for static patterns with continuous-valued inputs. He provides a taxonomy of neural-net classifiers, examining probabilistic, hyperplane, kernel, and exemplar classifiers. He then discusses back-propagation and decision-tree classifiers; matching classifier complexity to training data; GMDH (generalized method of data handling) networks and high-order nets; K nearest-neighbor classifiers; the feature-map classifier; the learning vector quantizer; hypersphere classifiers; and radial-basis function classifiers.>", "references": ["d1382a29539b3de419d567f679b5f28cee459a49", "7419fe8bd7c427b36b656e30c78af38d9c3ee102", "9bc28ae97fa99fc2463b6e8a107c01ff84db9fdd", "58821c2fde1ec9f42feda075d5e034379870a7a7", "22c03aee416166a9aa9646b3adbcf1b28d695538", "69ea52636cd32d39088987fb3a77cc9cb0bc8da3", "a87953825b0bea2a5d52bfccf09d2518295c5053", "84dae6a2870c68005732b9db6890f375490f2d4e", "7b28610d2d681a11398eb614de0d70d7de41c20c", "b7ca6657a0b590f0130afa51fbe036a4f77cbfa6"], "page_rank": 0.0002463054187192118}, {"id": "579d1134df253899a775c1b5857030d8c5023ac1", "title": "Using Genetic Algorithms to Improve Pattern Classification Performance", "authors": ["Eric I. Chang", "Richard Lippmann"], "date": 1990, "abstract": "Genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classification tasks. For a complex speech recognition task, genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five (from 153 to 33 features). On a difficult artificial machine-vision task, genetic algorithms were able to create new features (polynomial functions of the original features) which reduced classification error rates from 19% to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were unable to provide such low error rates using only the original features. Genetic algorithms were also used to reduce the number of reference exemplar patterns for a KNN classifier. On a 338 training pattern vowel-recognition problem with 10 classes, genetic algorithms reduced the number of stored exemplars from 338 to 43 without significantly increasing classification error rate. In all applications, genetic algorithms were easy to apply and found good solutions in many fewer trials than would be required by exhaustive search. Run times were long, but not unreasonable. These results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed.", "references": ["8442af884e7b43ef2edfaf489a9a64af8ee44227", "218f32b107e3d461781cad50888845a08486bb24", "7be696497de01c57b300e19feeb51a43934985a4", "664ecf45799a760b9ef2dcd6c93da22734d2de0f", "81dc8ca76afa41d2c49865068c46114178bdf558"], "page_rank": 0.00016420361247947453}, {"id": "355dfdd45b4b644853250f531f66e72d917432b1", "title": "Dynamic Node Creation in Backpropagation Networks", "authors": ["Timur Ash"], "date": 1989, "abstract": "Abstract This paper introduces a new method called Dynamic Node Creation (DNC) which automatically grows BP networks until the target problem is solved. DNC sequentially adds nodes one at a time to the hidden layer(s) of the network until the desired approximation accuracy is achieved. Simulation results for parity, symmetry, binary addition, and the encoder problem are presented. The procedure was capable of finding known minimal topologies in many cases, and was always within three nodes of the minimum. Computational expense for finding the solutions was comparable to training normal BP networks with the same final topologies. Starting out with fewer nodes than needed to solve the problem actually seems to help find a solution. The method yielded a solution for every problem tried.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function", "authors": ["George Cybenko"], "date": 1989, "abstract": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.", "references": ["386cbc45ceb59a7abb844b5078e5c944f17723b4", "8e4bd5422c82009290a5cd71457388f0780530d6", "993455b7cdc0e84e65abb90a5ed570e7c51a5883", "b0f09280ba01ab2e2c60e9450bef332d183ba2f3", "c42c1305ce33c628ffc5401d5de2b0347f50ac78", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "b8778bb692cf105254fe767ef11a3a8afac4a068", "d1382a29539b3de419d567f679b5f28cee459a49", "10ddb646feddc12337b5a755c72e153e37088c02", "29b6251c84def0cbd35397c71fada0d22cd9409c"], "page_rank": 0.0004269293924466338}, {"id": "25406e6733a698bfc4ac836f8e74f458e75dad4f", "title": "What Size Net Gives Valid Generalization?", "authors": ["Eric B. Baum", "David Haussler"], "date": 1988, "abstract": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples.", "references": ["1d20eff70cb168111fb5cc320cb692a11f1adf62", "a1dace286582d91916fe470d08f30381cf453f20", "33fdc91c520b54e097f5e09fae1cfc94793fbfcf", "b83396caf4762c906530c9219a9e4dd0658232b0", "d0b3802718a395a425a5742d7b809c0ff3492251", "5e6dfb46ed298ff037e166291c128a465f90bfc0", "445ad69010658097fc317f7b83f1198179eebae8", "10ddb646feddc12337b5a755c72e153e37088c02", "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b", "e85a68602abf92fcc1efb8b7aa90d27d141a80c2"], "page_rank": 0.0003448275862068965}, {"id": "a5d581182937c408f1ce615c8b4c3e2715b25803", "title": "Mapping of geostationary satellite pictures : an operational experiment", "authors": ["Russell C. Doolittle", "Charles L. Bristor", "Levin Lauritson"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"Mapping of geostationary satellite pictures : an operational experiment\" by Russell C. Doolittle et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "2bfd67677ec4e08f0a17bb44ef60a170be059b51", "title": "a color view of planet Earth", "authors": ["Verner E. Suomi", "Robert J. Parent"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"a color view of planet Earth\" by Verner E. Suomi et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "386cbc45ceb59a7abb844b5078e5c944f17723b4", "title": "On the approximate realization of continuous mappings by neural networks", "authors": ["Ken-ichi Funahashi"], "date": 1989, "abstract": "Abstract In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.", "references": ["f1fb17dc0a4656aae5b0bb3f2c21cd5e5190f4f1", "bb73ff39bf5e42e03b5428ce03c43f451288d534", "090c5a5df345ab60c41d6de02b3e366e1a27cf43", "b8778bb692cf105254fe767ef11a3a8afac4a068", "d1382a29539b3de419d567f679b5f28cee459a49", "f258b49b4a65d5b30a9bd539067ded2a3b2c5531", "918aeead4adb3052bd0c437ac40939c116ba65db", "834b3738673dacc767563c2714239852a8a6d4b4", "de996c32045df6f7b404dda2a753b6a9becf3c08", "1339348aeef592802288d9d929a085cb3ae61c4b"], "page_rank": 0.0002627257799671592}, {"id": "5e1eab8ffdc9de8ed466a7158f202837949b762f", "title": "Formation and structure of equatorial anticyclones caused by large-scale cross- equatorial flows determined by ATS-I photographs.", "authors": ["Tetsuya Theodore. Fujita", "Kazuo Watanabe", "Tatsuo Izawa"], "date": 1969, "abstract": "Abstract Because of poor coverage by synoptic stations, the tropical circulation over the eastern Pacific has not been known too well. As a result of photographic experiments, using the geosynchronous ATS-I satellite, fields of cloud motion over the eastern equatorial Pacific were mapped in detail on a number of days in September 1967. It was found that a large-scale flow from the Southern Hemisphere recurves after crossing the equator to form an anticyclone centered around 10N. Dynamical characteristics of this type of anticyclone were investigated by estimating the vorticity dissipating force from computed values of divergence and vorticity of low-cloud velocities determined from successive ATS-I pictures. The vorticity dissipating force seems to be related closely to the sea-surface temperature which would reduce the frictional coupling between the low-level atmosphere and the underlying sea surface. It was found that the anticyclone in its development stage results in a discontinuity of the intertropi...", "references": [], "page_rank": 0.0001231527093596059}, {"id": "8ab49b2f920056c1d145fdebd3f077768bbb9a65", "title": "The Fast Fourier Transform", "authors": ["E. Oran Brigham", "C. K. Yuen"], "date": 1967, "abstract": "The fast Fourier transform (FFT), a computer algorithm that computes the discrete Fourier transform much faster than other algorithms, is explained. Examples and detailed procedures are provided to assist the reader in learning how to use the algorithm. The savings in computer time can be huge; for example, an N = 210-point transform can be computed with the FFT 100 times faster than with the use of a direct approach.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1746e016ecd239b27f6b476b001c060bf88a0a7a", "title": "Second message understanding conference (muck-ii) test report", "authors": ["B. Sudheim"], "date": 1990, "abstract": "Semantic Scholar extracted view of \"Second message understanding conference (muck-ii) test report\" by B. Sudheim", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "95ebc3f1886000e2f1077cd9b93b5f6ae66aadff", "title": "RUBRIC iII: An object-oriented expert system for information retrieval", "authors": ["Richard M. Tong", "Lee A. Appelbaum", "Victor N. Askman", "James F. Cunningham"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"RUBRIC iII: An object-oriented expert system for information retrieval\" by Richard M. Tong et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "1aa9cf0242b66e26d3140cff56d75a1e58fbaf30", "title": "Development of the coder system: A testbed for artificial intelligence methods in information retrieval", "authors": ["Edward A. Fox"], "date": 1987, "abstract": "Abstract The CODER ( Co mposite D ocument E xpert/Extended/Effective R etrieval) system is a testbed for investigating the application of artificial intelligence methods to increase the effectiveness of information retrieval systems. Particular attention is being given to analysis and representation of heterogeneous documents, such as electronic mail digests or messages, which vary widely in style, length, topic, and structure. Since handling passages of various types in these collections is difficult even for experimental systems like SMART, it is necessary to turn to other techniques being explored by information retrieval and artificial intelligence researchers. The CODER system architecture involves communities of experts around active blackboards, accessing knowledge bases that describe users, documents, and lexical items of various types. The initial lexical knowledge base construction work is now complete, and experts for search and time/date handling can perform a variety of processing tasks. User information and queries are being gathered, and a simple distributed skeletal system is operational. It appears that a number of artificial intelligence techniques are needed to best handle such common but complex document analysis and retrieval tasks.", "references": ["88a53fd20557a8b1124a43648bdabe6f62ab0f45", "bbea1610abdf34ba147b137e9408a63cafee24b7", "4398f181cb427458a7e49f64b3b8928d218fbf39", "3b274a7bf3a55be07fa18d45097054f87cd77ee1", "33ee6a52163cf3e1f549e73cbbb5906182596183", "1d9161b4109957c1915f0fceeb30812e5a3bb5d8", "a28bfe77909cbf5829f0497399a9b054081ef1c1", "96361880f59c8f032210fbcbb312ebfbb669b516", "7a4a6f9422190eeb42f7319ae39402d46cc0b991", "dff4f5f58d16f41c06d3d9dad8f5070327f6eaaf"], "page_rank": 0.0004926108374384236}, {"id": "29dfba100b567a6d9aaa3a66b83994f9925c49e1", "title": "Prediction and Substantiation: A New Approach to Natural Language Processing", "authors": ["Gerald DeJong"], "date": 1979, "abstract": "This paper describes a new approach to natural language processing which results in a very robust and efficient system. The approach taken is to integrate the parser with the rest of the system. This enables the parser to benefii from predictions that h e rest of the system makes in the course of its processing. These predictions can be invaluable as guides to the parser in such difficult problem areas as resolving referents and selecting meanings. of ambiguous words. A program, called FRUMP for Fast Reading Understanding and Memory Program, employs this approach to parsing. FRUMP skims articles rather than reading them for detail. The program works on the relatively unconstrained domain of news articles. It routinely understands stories it has never before seen. The program's suaess is largely due to its radically different approach to parsing.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "dfeb5d5c79f0dcdbddb789c00d940dd7034594ef", "title": "Probability and Statistics", "authors": ["WALTER L. Smith"], "date": 1960, "abstract": "IF k1(x) is any non-negative function in L1 (\u2212 \u221e, + \u221e), let us write: for n = 2, 3, \u2026 Then the function: is defined almost everywhere (although it is possibly infinite for some, or all, x).", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7e0fce0cfa5f076af3336cdef6099d4748169367", "title": "SCISOR: a system for effec-tive information retrieval from text", "authors": ["Lisa F. Rau"], "date": 1987, "abstract": "A low cost, light weight drum type optical encoder for producing a digital manifestation of the position of a shaft, such as the indicator needle shaft of a gauge or a weighing scale, comprises a small light weight dish with a strip of material, such as plastic or metal having coded optical transmissivity or reflectivity thereon, held to the flanged rim of the dish by a shrunken ring retainer. Light passing through a slit is transmitted by or reflected from the code drum in a coded fashion to be sensed by appropriately disposed photodetectors, the signals from which are used in any digital fashion.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "b6c34bf9eaea1760ca88799fbc4a74358682f7c5", "title": "Encoding and Acquiring Meanings for Figurative Phrases", "authors": ["Michael G. Dyer", "Uri Zernik"], "date": 1986, "abstract": "Here we address the problem of mapping phrase meanings into their conceptual representations. Figurative phrases are pervasive in human communication, yet they are difficult to explain theoretically. In fact, the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation. Due to the huge number of such phrases in the English language, phrase representation must be amenable to parsing, generation, and also to learning. In this paper we demonstrate a semantic representation which facilitates, for a wide variety of phrases, both learning and parsing.", "references": ["23e27224928a008c5be84a7ea1452e0c627fe380", "745eb34b5b341af868bbf1a87ea9dd03110c1cde", "6fae00c0401e66bbf6b2e4992864614b9102d521", "7721319d1a55632f20517a4f3b4d220b0be544c5", "38ec89a6e850f02befc9ccf42bb0423499423029", "8e6249fcacb5de3839d87ce6ac1ab0a67e9285e9", "5df03cd88486f119b01dbcfd9357f0d5c15443df"], "page_rank": 0.00016420361247947453}, {"id": "3887c408e6b49b2f5393a1fa2922195d02f37202", "title": "Multi-Strategy Construction-Specific Parsing for Flexible Data Base Query and Update", "authors": ["Philip J. Hayes", "Jaime G. Carbonell"], "date": 1981, "abstract": "The advantages of a multi-strategy, construction-specific approach to parsing in applied natural language processing are explained through an examination of two pilot parsers we have constructed. Our approach exploits domain semantics and prior knowledge of expected constructions, using multiple parsing strategies each optimized to recognize different construction types. It is shown that a multi strategy approach leads to robust, flexible, and efficient parsing of both grammatical and ungrammatical input in limited-domain, task oriented, natural language interfaces. We also describe plans to construct a single, practical, multi-strategy parsing system that combines the best aspects of the two simpler parsers already implemented into a more complex, embedded-constituent control structure. Finally, we discuss some issues in data base access and update, and show that a construction-specific approach, coupled with a case structured data base description, offers a promising approach to a unified, interactive data base query and update system.", "references": ["28896778c55d8e023bb5f0aa95c245d9d521c31f", "abeb1093836f9bb90d6723e5bdd125cce6f4a2b6", "d0d5db9d50ce25e680fe1aa0c2f9e4ef0e213660", "5679d998f43bfcf16d46406382ca660743a02d9f", "70be245e2efed68ae469c7460cd7e415c46ef941", "cf65e814b45aafa8bdcc7dfffd2f2ed128ea59ef", "09d350ebf108eae4564e0510ca5f7ffc9e8022fa", "50895529ef63c8dd40c186d07748639abae65f81", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "eca16c1c776406abd0d966653a705f945bd4b520"], "page_rank": 0.00016420361247947453}, {"id": "76ab1651636880dcf38fe04a838f271b11d30d9a", "title": "Knowledge Representation for Syntactic/Semantic Processing", "authors": ["Robert J. Bobrow", "Bonnie L. Webber"], "date": 1980, "abstract": "This paper describes the RUS framework for natural language processing, in which a parser incorporating a substantial ATN grammar for English interacts with a semantic interpreter to simultaneously parse and interpret input. The structure of that interaction is discussed, including the roles played by syntactic and semantic knowledge. Several implementations of the RUS framework are currently in use, sharing the same grammar, but differing in the form of their semantic component. One of these, the PSI-KLONE system, is based on a general object-centered knowledge representation system, called KL-ONE. The operation of PSI-KLONE is described, including its use of KL-ONE to support a general inference process called \"incremental description refinement.\" The last section of the paper discusses several important criteria for knowledge representation systems to be used in syntactic and semantic processing.", "references": ["2463d5321e2a7893e11de0c200a0f39f23d49b07"], "page_rank": 0.0008210180623973726}, {"id": "c7078b69da231acd6c835b0b5dc0a9faf9938392", "title": "On the middle and ergative constructions in English", "authors": ["Samuel Jay Keyser", "Thomas Roeper"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"On the middle and ergative constructions in English\" by Samuel Jay Keyser et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "b33e8093c332f185f8c7e074b73cd7c79838b16a", "title": "Recovery Strategies for Parsing Extragrammatical Language", "authors": ["Jaime G. Carbonell", "Philip J. Hayes"], "date": 1983, "abstract": "Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammaticality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.", "references": ["d8d9f4a23dd35e0514a6b66cca3a1cd100e2b730", "0488fe2e6e94114aee15574782949a9984c7771a", "2bdd74b2af14fb74571d6a31cefcce5753bcf5e8", "8846499b10b517ed583ceaa0bdf4620b976fcf66", "f3f09d77332979d8315c775c1e6654323ff661cd", "a3a81a69f5be7031bb57e4190226b42b3ac4f202", "b9268d24c5adf344e8da18c9b20d7e690c46234e", "2e141dcaa084c257397a88e807dc2a0ea121550d", "b3bab8a001fb3ded2e1d6cfa416ae9c6fbbdea0a", "76ab1651636880dcf38fe04a838f271b11d30d9a"], "page_rank": 0.00016420361247947453}, {"id": "26ed78a6bbfe065b3f9876bc128e7867af394e4c", "title": "A Knowledge Framework for Natural Language Analysis", "authors": ["Paul S. Jacobs"], "date": 1987, "abstract": "Recent research in language analysis and language generation has highlighted the role of knowledge representation in both processes. Certain knowledge representation foundations, such as structured inheritance networks and feature-based linguistic representations, have proved useful in a variety of language processing tasks. Augmentations to this common framework, however, are required to handle particular issues, such as the ROLE RELATIONSHIP problem: the task of determining how roles, or slots, of a given frame, are filled based on knowledge about other roles. Three knowledge structures are discussed that address this problem. The semantic interpreter of an analyzer called TRUMP (TRansportable Understanding Mechanism Package) uses these structures to determine the fillers of roles effectively without requiring excessive specialized information about each frame.", "references": ["b886f2c097b635ee9550ca29fff7dcbbb7727ff7", "a3b8146c7950597628689d14551e74d46cc3543d"], "page_rank": 0.0002463054187192118}, {"id": "4661df154827793820afcf62ad803cc57afe48df", "title": "Syntactic Representation, Syntactic Levels, and the Notion of Subject", "authors": ["David M. Perlmutter"], "date": 1982, "abstract": "Semantic Scholar extracted view of \"Syntactic Representation, Syntactic Levels, and the Notion of Subject\" by David M. Perlmutter", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "39ba9f7833c059ad2b165e4c38867d2043778667", "title": "An Application of Lexical Semantics to Knowledge Acquisition from Corpora", "authors": ["Peter G. Anick", "James Pustejovsky"], "date": 1990, "abstract": "In this paper, we describe a program of research designed to explore how a lexical semantic theory may be exploited for extracting information from corpora suitable for use in Information Retrieval applications. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about semantic relationships among words appearing in collocational systems. We illustrate the approach for the acquisition of lexical information for several classes of nominals.", "references": ["e026125272325299dcba6e59832982b6fa47cf4f", "88095a87def97e5920cc74759036c82a7559d75c", "f10a31579874878da57b9318fdfc6196598310d8", "407c9fc43eab508af8e189688de94c03306c5ad6", "819c80bc13dc40da5d0dd25f496c0cfa7dfcf832", "f7dee1a5afab457ea3aefa5f1a042601e50ebf14", "7b963e36a9a8354ec9cbb4710adfbb915a3177f3", "3d3e808f5ecadd1728b3c46da14bd02d37a3b45f", "2aa3668a5b01e1e0986f9352c4bfbd80c078326a", "a548eb2f55fd5a5ccdf5db66bef001d014568f72"], "page_rank": 0.0001231527093596059}, {"id": "844ee351f389b02b159b5d09c7d4fa069fdadfa3", "title": "Passive in lexical theory", "authors": ["Joan Bresnan"], "date": 1982, "abstract": "Semantic Scholar extracted view of \"Passive in lexical theory\" by Joan Bresnan", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "b32ee844f91f10ceff9bc6d37297029ec22efd85", "title": "Toward an Explanatory Semantic Representation", "authors": ["Ray Jackendoff"], "date": 1976, "abstract": "A field dressable inflatable packer which includes a resilient inflatable sleeve formed of a rubber tube having a plurality of spaced-apart cord members extending longitudinally over the entire length of the tube and clamp means for disconnectably connecting the resilient sleeve in operable position over an inner tubing section which clamp means are constrained in position on the tubing string during operation of the packer but which may be removed for repair or replacement of the resilient sleeve.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ee8482f582cecb344e0fd1800afa846ecf7caf30", "title": "Transitivity : grammatical relations in government-binding theory", "authors": ["Teun Hoekstra"], "date": 1984, "abstract": "A thermal switch for automatically opening a circuit when the ambient temperature is increased to a predetermined level. A pair of conductor wires are arranged in side by side fashion with extremities extending in the same direction. A spring portion is included in at least one of the conductor wires to provide stored energy when the spring is biased into electrical contact with each other. A localized contact region is included in the conductor wire in the region below the spring portion but spaced upwardly from the associated extremities to create a lever arm. The extremities, contact region, lever arm and spring being encapsulated with a heat fusible material which biases the conductor wires into contact with each other and which is coated with a rigid insulating material to retain the conductor wires electrically insulated from each other except at the contact region. The heat fusible material holding the conductors into electrical contact until the temperature level reaches the level at which the fusible material flows thereby allowing the contacts to be separated due to the energy stored by the spring.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0f8e0586e2c60e3fc5dae3d88d9c0110e228aa96", "title": "Studies in relational grammar", "authors": ["David M. Perlmutter", "Carol Z. Rosen", "Paul M. Postal", "Brian Joseph"], "date": 1984, "abstract": "This collection of nine original syntactic studies carried out within the framework for syntactic theory and description known as Relational Grammar provides a state-of-the-art survey of this and allied fields. In relational theory, grammatical relations such as subject, direct object, and predicate are taken to be theoretical primitives which permit the definition of formal objects called Arcs, the fundamental building blocks of syntactic structures. Edited by Paul M. Postal and Brian D. Joseph, this volume is the third in a series highlighting work in Relational Grammar. It extends the foundational studies of the first two volumes to refine and modify the insights, analyses, and theoretical devices developed in earlier connections, while at the same time providing support for some of the earlier constructs and claims. Of the nine papers, four treat various aspects of advancements to and demotions from indirect object; three deal with raising and clause union constructions, in which initial immediate constituents of one structure are nonimmediate constituents of another; and two are concerned with problems in the description and formalization of verbal agreement systems. The nine articles cover languages ranging from Chamorro to English, French, Georgian, Greek, Japanese, Kek'chi, Korean, Southern Tiwa, Spanish, and Tzotzil.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0095fea57f5649ee17191b7c56f2d3d63e9971ea", "title": "Aspects of Warlpiri morphology and syntax", "authors": ["Jane Helen Simpson"], "date": 1983, "abstract": "Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 1983.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "b7ca6657a0b590f0130afa51fbe036a4f77cbfa6", "title": "Adaptive learning networks: Development and Applications in the United States of algorithms related", "authors": ["Roger L. Barron", "Anthony N. Mucciardi", "F. F. Cook", "J. I. Craig", "Andrew R. Barron"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"Adaptive learning networks: Development and Applications in the United States of algorithms related\" by Roger L. Barron et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2baaaa54e45baa099df929ba1ba78628afe1d21c", "title": "Impersonal Passives and the Unaccusative Hypothesis", "authors": ["David M. Perlmutter"], "date": 1978, "abstract": "In this paper I give one argument in favor of the advancement analysis of impersonal passives over the demotion analysis. The argument is based on the interaction of this phenomenon with an independently motivated hypothesis about linguistic structure, the Unaccusative Hypothesis.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d381e5621aff1410603ca35cc757a40a1ac82a90", "title": "Anatomy of a Verb Entry: from Linguistic Theory to Lexicographic Practice", "authors": ["B. T. S. Atkins", "Judy Kegl", "Beth Levin"], "date": 1988, "abstract": "In the context of five learners\u2019 dictionaries, we examine the treatment of certain systematic relationships in the semantics and syntax of the English verb and find a lack of systematicity in this aspect of the lexicography. We present detailed evidence to support our criticism, including citations from a corpus of general English. We suggest that the discrepancies arise because of an inadequate representation of the native speaker\u2019s knowledge of the English verb system. Drawing on linguistic research into the verbal classification scheme of English, we construct a \u2018dictionary-neutral\u2019 summary of the semantic-syntactic relationships pertinent to the verb bake, designed to enable the lexicographer to make principled decisions about the content and presentation for a specific dictionary. On the basis of this summary we offer a revision of an entry for bake, showing that a theoretically motivated analysis, if clearly presented, will ease the lexicographer\u2019s task and improve the quality of the lexicography.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "e1df0ae12a634485f668f24ad683c238886c4d41", "title": "The ACQUILEX LKB: representation issues in semi-automatic acquisition of large lexicons", "authors": ["Ann A. Copestake"], "date": 1992, "abstract": "We describe the lexical knowledge base system (LKB) which has been designed and implemented as part of the ACQUILEX project1 to allow the representation of multilingual syntactic and semantic information extracted from machine readable dictionaries (MRDs), in such a way that it is usable by natural language processing (NLP) systems. The LKB's lexical representation language (LRL) augments typed graph-based unification with default inheritance, formalised in terms of default unification of feature structures. We evaluate how well the LRL meets the practical requirements arising from the semi-automatic construction of a large scale, multilingual lexicon. The system as described is fully implemented and is being used to represent substantial amounts of information automatically extracted from MRDs.", "references": ["0616b0f5e6edce01f153081e53bd0152c8d0a4bd", "f9593e30d8e5f8c47ccaed38233faf306dbbf0bb", "cf691762cc043fb3622ea098f997102ee4714973", "9f365cd5d3d677b0f739481444cf66b0ea271523", "f5d2fcac3dd05bf483a53bbaedbaf9d1682f6621", "fd3e7aee5a43f77d4b3871d0eb6d796e0fb8d0ca", "fe26bfffa3766b4521becb25468455e902064796", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "9a64c74f4866b62e7d4227c91fd423f8da3a4f41", "fd47db5a932cd9459178e08384ee1a0e1992de2a"], "page_rank": 0.0001231527093596059}, {"id": "81dc8ca76afa41d2c49865068c46114178bdf558", "title": "The GENITOR Algorithm and Selection Pressure: Why Rank-Based Allocation of Reproductive Trials is Best", "authors": ["L. Darrell Whitley"], "date": 1989, "abstract": "This paper reports work done over the past three years using rank-based allocation of reproductive trials. New evidence and arguments are presented which suggest that allocating reproductive trials according to rank is superior to tness proportionate reproduction. Ranking can not only be used to slow search speed, but also to increase search speed when appropriate. Furthermore, the use of ranking provides a degree of control over selective pressure that is not possible with tness proportionate reproduction. The use of rank-based allocation of reproductive trials is discussed in the context of 1) Holland's schema theorem, 2) DeJong's standard test suite, and 3) a set of neural net optimization problems that are larger than the problems in the standard test suite. The GENITOR algorithm is also discussed; this algorithm is speciically designed to allocate reproductive trials according to rank.", "references": ["ba68f78f73b812e75a5bd5e2630d510f8c50f74e"], "page_rank": 0.0001231527093596059}, {"id": "7b28610d2d681a11398eb614de0d70d7de41c20c", "title": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions", "authors": ["Merlin Stone"], "date": 1974, "abstract": "SUMMARY A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "aff1b791866d308ea37cca86e2aa76e852f23604", "title": "Information-based syntax and semantics", "authors": ["Carl Pollard", "Ivan A. Sag"], "date": 1987, "abstract": "A long-standing, near-universal, and erroneous practice of teaching syntax in a void exists, as if the communicative function of language had nothing to do with syntax. And semantics has customarily been taught in sequence after syntax, or else not at all. Based upon graduate courses taught at Stanford University, this work seeks to redress this situation by building up syntactic and semantic aspects of grammatical theory in an integrated way from the start, under the assumption that neither is of linguistic interest divorced from the other. The particular theory presented, head-driven phrase structure grammar (HPSG) - so-called because of its central notion of the grammatical head - is an information-based (or 'unification-based' theory that has its roots in a number of different research programs within linguistics and neighboring disciplines such as philosophy and computer science.Thus HPSG draws upon and attempts to synthesize insights and perspectives from several families of contemporary syntactic theories, such as categorial grammar, lexical-functional grammar, generalized phrase structure grammar, and government-binding theory; but many of its key ideas arise from semantic theories like situation semantics and discourse representation theory, and from computational work in such areas as knowledge representation, data type theory, and formalisms based upon the unification of partial information.", "references": ["aff1b791866d308ea37cca86e2aa76e852f23604"], "page_rank": 0.0006157635467980295}, {"id": "664ecf45799a760b9ef2dcd6c93da22734d2de0f", "title": "On Automatic Feature Selection", "authors": ["Wojciech W. Siedlecki", "Jack Sklansky"], "date": 1988, "abstract": "We review recent research on methods for selecting features for multidimensional pattern classification. These methods include nonmonotonicity-tolerant branch-and-bound search and beam search. We describe the potential benefits of Monte Carlo approaches such as simulated annealing and genetic algorithms. We compare these methods to facilitate the planning of future research on feature selection.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "d0b3802718a395a425a5742d7b809c0ff3492251", "title": "A 'Neural' Network that Learns to Play Backgammon", "authors": ["Gerald Tesauro", "Terrence J. Sejnowski"], "date": 1987, "abstract": "We describe a class of connectionist networks that have learned to play backgammon at an intermediate-to-advanced level. The networks were trained by a supervised learning procedure on a large set of sample positions evaluated by a human expert. In actual match play against humans and conventional computer programs, the networks demonstrate substantial ability to generalize on the basis of expert knowledge. Our study touches on some of the most important issues in network learning theory, including the development of efficient coding schemes and training procedures, scaling, generalization, the use of real-valued inputs and outputs, and techniques for escaping from local minima. Practical applications in games and other domains are also discussed.", "references": ["0b5dbf114ca38a9bc73bf575c5763275941ceddc", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "59f89c2628d52da0bb160c121d6c7e298c7f65d2", "63cd1cfd53aab03a36c25eaec9feb8eee8f9a683", "4e3cbd911b175d510a8cb3c65c79a5595915e42f"], "page_rank": 9.852216748768472e-05}, {"id": "7be696497de01c57b300e19feeb51a43934985a4", "title": "HMM Speech Recognition with Neural Net Discrimination", "authors": ["William Y. Huang", "Richard Lippmann"], "date": 1989, "abstract": "Two approaches were explored which integrate neural net classifiers with Hidden Markov Model (HMM) speech recognizers. Both attempt to improve speech pattern discrimination while retaining the temporal processing advantages of HMMs. One approach used neural nets to provide second-stage discrimination following an HMM recognizer. On a small vocabulary task, Radial Basis Function (RBF) and back-propagation neural nets reduced the error rate substantially (from 7.9% to 4.2% for the RBF classifier). In a larger vocabulary task, neural net classifiers did not reduce the error rate. They, however, outperformed Gaussian, Gaussian mixture, and k-nearest neighbor (KNN) classifiers. In another approach, neural nets functioned as low-level acoustic-phonetic feature extractors. When classifying phonemes based on single 10 msec. frames, discriminant RBF neural net classifiers outperformed Gaussian mixture classifiers. Performance, however, differed little when classifying phones by accumulating scores across all frames in phonetic segments using a single node HMM recognizer.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1339348aeef592802288d9d929a085cb3ae61c4b", "title": "A Theory of Adaptive Pattern Classifiers", "authors": ["Shun-ichi Amari"], "date": 1967, "abstract": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "445ad69010658097fc317f7b83f1198179eebae8", "title": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition", "authors": ["Thomas M. Cover"], "date": 1965, "abstract": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces.", "references": ["4c1632e3fbe569450978f901ec9c6fccecc0b76b", "fb9f07dc246ec7b3fa2d8491a1a8c4da7cc77985", "2920f6b916007e39aed3dffebebe4b4442e49f0d", "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93", "e8871e2641df536074612f73e26815bf324c0ba9", "7ce11916234117cf9e1d9bd7eae9a24fe68c8c01", "2934355c03faa1094eb413241bb9de6753b48988"], "page_rank": 9.852216748768472e-05}, {"id": "7a4a6f9422190eeb42f7319ae39402d46cc0b991", "title": "An intelligent system for document retrieval in distributed office environments", "authors": ["Uttam Mukhopadhyay", "Larry M. Stephens", "Michael N. Huhns", "Ronald D. Bonnell"], "date": 1986, "abstract": "MINDS (Multiple Intelligent Node Document Servers) is a distributed system of knowledge\u00b7based query engines for efficiently retrieving multimedia documents in an office environment of distributed workstations. By learning document distribution patterns, as well as user interests and preferences during system usage, it customizes document retrievals for each user. A two-layer learning system has been implemented for MINDS. The knowl\u00b7 edge base used by the query engine is learned at the lower level with the help of heuristics for assigning credit and recommending adjustments; these heuristics are incrementally refined at the upper level.", "references": ["07affac9f2cbfca1bb996462661575526885f01d", "cc2d05085b2e25563fbd83b7cb0846e79ee97ec1", "3fcaef02bc7634eaaea250e14969c7c7768ae971", "c2f4163c07b2aed53a639ef54e09f59d4414c1a1", "827411d2ebf389a5a8289abd6f17174ed25a278e"], "page_rank": 9.852216748768472e-05}, {"id": "918aeead4adb3052bd0c437ac40939c116ba65db", "title": "Phoneme Recognition: Neural Networks vs", "authors": ["Alex H. Waibel", "Hanazawa G. Hinton", "Ic Shikano Ic"], "date": 1988, "abstract": "neme recognition which is characterized by two important properties: 1.) Using a 3 layer arrangement of simple computing units, it can represent arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error back-propagatioii[l]. 2.) he time-delay arrangement enables the network to discover acoustichonetic features and the temporal relationships between them indeendent of position in time and hence not blurred by temporal shifts in the input. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task, i.e., the speakerdependent recognition of the phonemes \"B\", \"D\", and \"G\" extracted We show that the TDNN \"invented\" well-known acoustic-phonetic", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "a28bfe77909cbf5829f0497399a9b054081ef1c1", "title": "Knowledge-assisted document retrieval: I. The natural-language interface", "authors": ["Gautam Biswas", "James C. Bezdek", "Marisol Marques", "Viswanath Subramanian"], "date": 1987, "abstract": "In this article we describe the conceptual model and processing of (constrained) natural-language queries in information retrieval systems. A language interface based on fuzzy set techniques is proposed to handle the uncertainty inherent in natural-language semantics. The conceptual model is developed and exemplified in the context of document retrieval. Specifically, the user query is considered to be a triple, q = (qC, qv, q,,), where qC indicates the part of the query that deals with concepts and operators that link these concepts, qY identifies the publication period the user is interested in, and qn pertains to the number of documents to be retrieved. We describe query decomposition using an augmented transition network parser and the assignment of functions and relations needed by each portion of the query to represent uncertainties inherent in the natural language. The output of the natural-language interface is then passed to a knowledge-based retrieval mechanism that will be described in a companion article (Part II).", "references": ["49aff7e43aa864f0f62d8cb5289ccec9a9d3bf79", "bbea1610abdf34ba147b137e9408a63cafee24b7", "6f0aa57820d5f1700461b317faabad9b98d0f70d", "5107784c74f7f3a733e337a5190247cd04869cd6", "5679d998f43bfcf16d46406382ca660743a02d9f", "7135d4a1615638b723ce085b58c2e4709056f1aa", "09550accec47459a61fe1710a0a32c2ec22449bd", "bef96bdc528a1f50a23d3e70c3df5bcb6f4b763d", "86b1bafeb5371a7b4e6d8b16938e39c8eeb1a970", "de42b848775f9fa1e4bff758ae04a54099c0c381"], "page_rank": 9.852216748768472e-05}, {"id": "dff4f5f58d16f41c06d3d9dad8f5070327f6eaaf", "title": "Knowledge-assisted document retrieval: II. The retrieval process", "authors": ["Gautam Biswas", "James C. Bezdek", "Viswanath Subramanian", "Marisol Marques"], "date": 1987, "abstract": "This article presents our conceptual model of the retrieval process of a document-retrieval system. The retrieval mechanism input is an unambiguous intermediate form of a user query generated by the language processor using the method described previously. Our retrieval mechanism uses a two-step procedure. In the first step a list of documents pertinent to the query are obtained from the document database, and then an evidence-combination scheme is used to compute the degree of support between the query and individual documents. The second step uses a ranking procedure to obtain a final degree of support for each document chosen, as a function of individual degrees of support associated with one or more parts of the query. The end result is a set of document citations presented to the user in ranked order in response to the information request. Numerical examples are given to illustrate various facets of the overall system, which has been prototypically implemented in modular form to test system response to changes in model parameters. \u00a9 1987 John Wiley & Sons, Inc.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "834b3738673dacc767563c2714239852a8a6d4b4", "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models", "authors": ["Alex H. Waibel", "Toshiyuki Hanazawa", "Geoffrey E. Hinton", "Kiyohiro Shikano", "Kevin J. Lang"], "date": 1988, "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.>", "references": ["cd62c9976534a6a2096a38244f6cbb03635a127e", "c86590e947c28e8791d1e8bab8fc8ab53302341f", "b8fe93d3e5205a450fdd8a9fb94cea0ab73b067f", "b8778bb692cf105254fe767ef11a3a8afac4a068", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "1e32d64fd543b1e70012415fc6e47119257ae74a", "e69606729837aa1d0168c47f812cbccaba09dc83", "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "ff2c2e3e83d1e8828695484728393c76ee07a101"], "page_rank": 9.852216748768472e-05}, {"id": "96361880f59c8f032210fbcbb312ebfbb669b516", "title": "A text knowledge base from the AI handbook", "authors": ["Robert F. Simmons"], "date": 1987, "abstract": "Abstract This research aims at defining a consistent set of text representation conventions for organizing fifty pages of the AI handbook as an inferential knowledge base founded on a procedural logic system of general inference schemes for answering questions from it. As a result of research on the AI handbook project, we have developed a prototype, natural-language, text-knowledge system that includes a data base manager to compile the text knowledge and to make it available to navigational commands. The text is represented as logical propositions that form a set of text axioms to model its content. English questions and commands are translated to corresponding logical formulas and treated as theorems to be proved with respect to the text model. The logical form is that of semantic relations (SRs)\u2014logical predicates with varying numbers and ordering of arguments. To compute effectively with such a free form, a relaxed unification procedure was defined as the basis of the SR theorem prover. The use of procedural logic, augmented with fast compiled LISP functions, has shown that questions can be answered in times ranging from a few tenths of a second to minutes of CPU time on a DEC2060 system. The navigational capabilities of the data base manager make available larger contexts surrounding the text and offer the user complete freedom to explore the text and to extract any desired information from it.", "references": ["a5de6bc02c770c19331f70fefbe8e8d1094fc9f1", "85bb4912e8d58ae87ad89be4d284fe22cf26692d", "94f0acfa093c5f07cc21ab8c8c2b53d42c1d4232", "2e5ee3f2b1bf6525b300639478397c9e2607d4c6"], "page_rank": 9.852216748768472e-05}, {"id": "f258b49b4a65d5b30a9bd539067ded2a3b2c5531", "title": "Noise reduction using connectionist models", "authors": ["Shuichi Tamura", "Alex H. Waibel"], "date": 1988, "abstract": "Using a back propagation network learning algorithm, a four-layered feed-forward network is trained on learning samples to realize a mapping from the set of noisy signals a set of noise-free signals. Computer experiments were carried out on 12 kHz sampled Japanese speech data, using stationary and nonstationary noise. The experiments showed that the network can indeed learn to perform noise reduction. Even for noisy speech signals that had not been part of the training data, the network successfully produced noise-suppressed output signals.>", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "090c5a5df345ab60c41d6de02b3e366e1a27cf43", "title": "A logical calculus of the ideas immanent in nervous activity", "authors": ["Warren Sturgis McCulloch", "Walter Pitts"], "date": 1990, "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.", "references": ["04d0d7dd9d237f72e7d0c70f80a98593586971be", "b3ad74c4dfb674d7bdf0aa32d38cfd4364a70747"], "page_rank": 9.852216748768472e-05}, {"id": "1d9161b4109957c1915f0fceeb30812e5a3bb5d8", "title": "CALIN: a user interface based on a simple natural language", "authors": ["Patrick Bosc", "Mich{\\`e}le Courant", "Sophie Robin"], "date": 1986, "abstract": "In the framework of an application dealing with classified advertisement matching, a dedicated user interface has been designed and implemented. Its major originality relies on the user's language which is neither an artificial one, nor the usual natural language, but in fact the ad language. Beyond the language itself, the interface provides some facilities such as paraphrasing or explanations when needed. An expert system approach has been adopted and the interface is built up from the knowledge given by experts. They are in charge of describing what are acceptable ads, from both syntax and semantics points of view \u2026 Although designed in the context of ad matching, that interface may interestingly be adapted to other retrieving systems. We especially think that an adlike language is well-suited to ask questions since it is based on natural simple expressions. A given sentence involves terms that stand for elementary conditions applying to instances of a logical object contained inside the information system. This approach defines a complete interface, involving both a language and aiding capabilities. Moreover, the query language, although less powerful, represents a compromise between artificial languages and the usual natural language, with respect to ergonomics and analysis complexity.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "eca16c1c776406abd0d966653a705f945bd4b520", "title": "Ungrammaticality and Extra-Grammaticality in Natural Language Understanding Systems", "authors": ["Stanley C. Kwasny", "Norman K. Sondheimer"], "date": 1979, "abstract": "Among the components included in Natural Language Understanding (NLU) systems is a grammar which spec i f i es much o f the l i n g u i s t i c s t ruc tu re o f the ut terances tha t can be expected. However, i t is ce r ta in tha t inputs that are ill-formed with respect to the grammar will be received, both because people regularly form ungra=cmatical utterances and because there are a variety of forms that cannot be readily included in current grammatical models and are hence \"extra-grammatical\". These might be rejected, but as Wilks stresses, \"...understanding requires, at the very least, ... some attempt to interpret, rather than merely reject, what seem to be ill-formed utterances.\" [WIL76]", "references": ["b4d5633b96ea5e0f9e90e03df58e8ad0f6f3dee7", "8a8f1719e4b0f41ddc3d3c415d7a384e00c2549b", "ecb2be6f9352ae8c3bc95bf8103aa22e1f2e2f15", "b5a5306e8fb690859a2816ba07e603e47a3dcfc0"], "page_rank": 0.0001231527093596059}, {"id": "2463d5321e2a7893e11de0c200a0f39f23d49b07", "title": "Psi-klone: parsing and semantic interpretation in the bbn natural language understanding system", "authors": ["Robert J. Bobrow", "Bonnie Webber"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"Psi-klone: parsing and semantic interpretation in the bbn natural language understanding system\" by Robert J. Bobrow et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "title": "MULTIPLE REPRESENTATIONS OF KNOWLEDGE FOR TUTORIAL REASONING", "authors": ["John Seely Brown", "Richard R. Burton"], "date": 1975, "abstract": "Publisher Summary This chapter provides an overview of SOPHIE, an intelligent instructional system that reflects a major attempt to extend Carbonell's notion of mixed-initiative Computer Aided Instruction (CAI) for the purpose of encouraging a wider range of student initiatives. Unlike previous AI-CAI systems that attempt to mimic the roles of a human teacher, SOPHIE tries to create a reactive environment in which the student learns by trying out his ideas rather than by instruction. SOPHIE incorporates a strong model of its knowledge domain along with numerous heuristic strategies for answering a student's questions, providing him with critiques of his current solution paths, and generating alternative theories to his current hypotheses. In essence, SOPHIE enables a student to have a one-to-one relationship with an expert who helps the student create, experiment with, and debug his own ideas. SOPHIE's expertise is derived from an efficient and powerful inferencing scheme that uses multiple representations of knowledge including (1) simulation models of its microcosm, (2) procedural specialists that contain logical skills and heuristic strategies for using these models, and (3) semantic nets for encoding time-invariant factual knowledge.", "references": [], "page_rank": 0.000509031198686371}, {"id": "50895529ef63c8dd40c186d07748639abae65f81", "title": "Problems in Conceptual Analysis of Natural Language.", "authors": ["Lawrence Birnbaum", "Mallory Selfridge"], "date": 1979, "abstract": "Abstract : This paper reports on some recent developments in natural language analysis. We address such issues as the role of syntax in a semantics-oriented analyzer, achieving a flexible balance of top-down and bottom-up processing, and the role of short term memory. Our results have led to improved algorithms capable of analyzing the kinds of multi-clause inputs found in most text. (Author)", "references": [], "page_rank": 0.0001778872468527641}, {"id": "09d350ebf108eae4564e0510ca5f7ffc9e8022fa", "title": "Comprehension by computer : expectation-based analysis of sentences in context", "authors": ["Christopher Riesbeck", "Roger C. Schank"], "date": 1976, "abstract": "Abstract : ELI (English Language Interpreter) is a natural language parsing program currently used by several story understanding systems. ELI differs from most other parsers in that it: produces meaning representations (using Schank's Conceptual Dependency system) rather than syntactic structures; uses syntactic information only when the meaning can not be obtained directly; talks to other programs that make high level inferences that tie individual events into coherent episodes; uses context-based exceptions (conceptual and syntactic) to control its parsing routines. Examples of texts that ELI has understood, and details of how it works are given.", "references": [], "page_rank": 0.00017241379310344826}, {"id": "f7dee1a5afab457ea3aefa5f1a042601e50ebf14", "title": "Word-Meaning Selection in Multiprocess Language Understanding Programs", "authors": ["Richard E. Cullingford", "Michael J. Pazzani"], "date": 1984, "abstract": "An understander reading or listening to someone speak has to repeatedly solve the problem of word-meaning ambiguity, the selection of the intended meaning of a word from the set of its possible meanings. For example, the problem of pronominal reference can be considered as a choosing of the intended referent from the collection of entities which have already been mentioned or which can be inferred. Human understanders apply rules of syntax, surface semantics, general world knowledge, and various types of contextual knowledge to resolve word-sense or pronominal ambiguity as they process language. We describe a mechanism, called a cooperative word-meaning selector, which allows the computer to use various knowledge sources as it ``understands'' text. The word-meaning selector is part of a conceptual analyzer which forms the natural-language interface for a pair of multiprocess language processing systems. The first, called DSAM (distributable script applier mechanism), reads and summarizes newspaper articles making heavy reference to situational scripts. The second, ACE (academic counseling experiment), is a conversational program which automates certain parts of the academic counseling task. In each of these systems, a variety of knowledge sources, each managed by a distinct ``expert'' process, is brought to bear to enable the word-meaning selector to form the most plausible reading of a sentence containing ambiguous words.", "references": ["2382b146f49bd5015121d888902263c29c5eace7", "35f03721ecef2a7315a8d85d02bacaf00660a3fb", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda"], "page_rank": 5.473453749315818e-05}, {"id": "3d3e808f5ecadd1728b3c46da14bd02d37a3b45f", "title": "A direct manipulation interface for boolean information retrieval via natural language query", "authors": ["Peter G. Anick", "Jeffrey D. Brennan", "Rex A. Flynn", "David R. Hanssen", "Bryan Alvey", "Jeffrey M. Robbins"], "date": 1989, "abstract": "This paper describes the design of a direct manipulation user interface for Boolean information retrieval. Intended to overcome the difficulties of manipulating explicit Boolean queries as well as the \u201cblack box\u201d drawbacks of so-called natural language query systems, the interface presents a two-dimensional graphical representation of a user's natural language query which not only exposes heuristic query transformations performed by the system, but also supports query reformulation by the user via direct manipulation of the representation. The paper illustrates the operation of the interface as implemented in the AI-STARS full-text information retrieval system.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "f10a31579874878da57b9318fdfc6196598310d8", "title": "Full text indexing based on lexical relations an application: software libraries", "authors": ["Yoelle Maarek", "Frank Smadja"], "date": 1989, "abstract": "In contrast to other kinds of libraries, software libraries need to be conceptually organized. When looking for a component, the main concern of users is the functionality of the desired component; implementation details are secondary. Software reuse would be enhanced with conceptually organized large libraries of software components. In this paper, we present GURU, a tool that allows automatical building of such large software libraries from documented software components. We focus here on GURU's indexing component which extracts conceptual attributes from natural language documentation. This indexing method is based on words' co-occurrences. It first uses EXTRACT, a co-occurrence knowledge compiler for extracting potential attributes from textual documents. Conceptually relevant collocations are then selected according to their resolving power, which scales down the noise due to context words. This fully automated indexing tool thus goes further than keyword-based tools in the understanding of a document without the brittleness of knowledge based tools. The indexing component of GURU is fully implemented, and some results are given in the paper.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "7b963e36a9a8354ec9cbb4710adfbb915a3177f3", "title": "Support for Browsing in an Intelligent Text Retrieval System", "authors": ["R. H. Thompson", "W. Bruce Croft"], "date": 1989, "abstract": "Browsing is potentially an extremely important technique for retrieving text documents from large knowledge bases. The advantages of this technique are that users get immediate feedback from the structure of the knowledge base and exert complete control over the outcome of the search. The primary disadvantages are that it is easy to get lost in a complex network of nodes representing documents and concepts, and there is no guarantee that a browsing search will be as effective as a more conventional search. In this paper, we show how a browsing capability can be integrated into an intelligent text retrieval system. The disadvantages mentioned above are avoided by providing facilities for controlling the browsing and for using the information derived during browsing in more formal search strategies. The architecture of the text retrieval system is described and the browsing techniques are illustrated using an example session.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "407c9fc43eab508af8e189688de94c03306c5ad6", "title": "Machine tractable dictionaries as tools and resources for natural language processing", "authors": ["Yorick Wilks", "Dan Fass", "Cheng-ming Guo", "James E. McDonald", "Tony Plate", "Brian M. Slator"], "date": 1988, "abstract": "This paper discusses three different but related large-scale computational methods for the transformation of machine readable dictionaries (MRDs) into machine tractable dictionaries, i.e., MRDs converted into a format usable for natural language processing tasks. The MRD used is The Longman Dictionary of Contemporary English.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a548eb2f55fd5a5ccdf5db66bef001d014568f72", "title": "Word sense disambiguation using machine-readable dictionaries", "authors": ["Robert Krovetz", "W. Bruce Croft"], "date": 1989, "abstract": "A novel catalytic system for polymerizing olefins comprises: (a) a cocatalytic component containing an organometallic compound of Groups I-III of the periodic table, and (b) a catalytic component containing titanium obtained by: (i) copulverizing a substantially anhydrous magnesium compound containing halogen or manganese compound containing halogen with: a phenol, an organic polymer containing silicon, a titanium halide, and an electron-donor compound to produce a copulverized product, and (ii) reacting the copulverized product with a liquid titanium compound containing halogen. The invention also encompasses the novel catalytic component, processes for preparing the catalytic component containing titanium, and the use of the novel catalytic system for polyermizing olefins.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "819c80bc13dc40da5d0dd25f496c0cfa7dfcf832", "title": "Discovery Procedures for Sublanguage Selectional Patterns: Initial Experiments", "authors": ["Ralph Grishman", "Lynette Hirschman", "Ngo Thanh Nhan"], "date": 1986, "abstract": "Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure.", "references": ["a3a81a69f5be7031bb57e4190226b42b3ac4f202", "a651bb8c95f014e9528bc7f4b542c5eec2a78760", "8ddc18da615ab9e016ccbc29d4b6255054b37dac", "9a11469a8db6e143c3bc00d1e17b713f7c4885ba", "819b44e81eb2a204f4c59ecabb9ad09f12128a50", "4ef0d0c5cc556ec6be90748a64febe05c50b4a9d", "c957773352f6a6637dc1badac8663726ed1011b1", "aa63a22e127e89a004c64d589205e87bf69b1caa", "88d9d2d571d5fea932cb4cdffe1ff390d221ba50", "940f0042015afadebd246d91e390b4e071aa973d"], "page_rank": 5.473453749315818e-05}, {"id": "2aa3668a5b01e1e0986f9352c4bfbd80c078326a", "title": "An Overview of the KL-ONE Knowledge Representation System", "authors": ["Ronald J. Brachman", "James G. Schmolze"], "date": 1985, "abstract": "KL-ONE is o system for representing knowledge in Artificial Intelligence progroms. It has been developed and refined over o long period ond hos been used in both basic research and implemented knowledge-based systems in o number of places in the Al community. Here we present the kernel ideas of KL-ONE, emphasizing its ability to form complex structured descriptions. In oddition to detoiling oil of KL-ONE\u2019s description-forming structures, we discuss o bit of the philosophy underlying the system, highlight notions of taxonomy and clossificotion that ore central to it, ond include on extended example of the use of KL-ONE and its classifier in o recognition tosk.", "references": ["b56af9fc259f75855c9e0731117a728f1ce6bbdc", "ca83024fff6e580bfc681c94c20f56d6c8e7323b", "3a144d8a06287b697d69df8c94763fc922343cc4", "fba5b0877f68c147d387563843e1395e5a40e1b7", "e1a214d7efdaa03bfc593c02e6cf72fb1dd54859", "e369643dd7e4bde48a850b451baaec7d5fee42a0", "d9566ac89cd9b7fccc080b764aab5107430da28c", "d61d30b1e7f3a3cc3654a4a97a381456dd88e89c", "81cba5ea8397c7a32a775b95189e87f703ccc2d9", "14f498c4292f0f6b3f4170e31e2f53489b76701c"], "page_rank": 0.0002189381499726327}, {"id": "fd47db5a932cd9459178e08384ee1a0e1992de2a", "title": "Untangling definition structure into knowledge representation", "authors": ["Piek Vossen", "Ann A. Copestake"], "date": 1994, "abstract": "A portable vibratory compacting machine for compacting and flattening a poured floor covering. The machine includes a generally horizontal frame including a pair of depending flanges on its opposite sides. A plurality of transversely aligned rollers are rotatably mounted by the longitudinal flanges for contacting the floor and supporting the weight of the machine. A reversible handle is provided so that the machine can be pushed or pulled in either direction, and a vibratory motor is secured to the top of the frame for vibrating the rollers. A plurality of transverse strengthening channels are connected to the frame to evenly distribute the vibratory forces across all of the rollers.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "ba68f78f73b812e75a5bd5e2630d510f8c50f74e", "title": "Applying genetic algorithms to neural network learning", "authors": ["Darrell Whitley"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Applying genetic algorithms to neural network learning\" by Darrell Whitley", "references": [], "page_rank": 0.0004926108374384236}, {"id": "88095a87def97e5920cc74759036c82a7559d75c", "title": "An intelligent analyzer and understander of English", "authors": ["Yorick Wilks"], "date": 1975, "abstract": "The paper describes a working analysis and generation program for natural language, which handles paragraph length input. Its core is a system of preferential choice between deep semantic patterns, based on what we call \u201csemantic density.\u201d The system is contrasted:with syntax oriented linguistic approaches, and\n with theorem proving approaches to the understanding problem.\n", "references": ["bb20f121c979b535bbeade5ac06676d627d4ad7d", "a29ef700629e7196211765fb9f478684bac2c584", "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "f42d0737564e77f62ea1bf20298b10da192bb412"], "page_rank": 5.473453749315818e-05}, {"id": "b886f2c097b635ee9550ca29fff7dcbbb7727ff7", "title": "Artificial Intelligence", "authors": ["Nils J. Nilsson"], "date": 1974, "abstract": "This paper is a survey of Artifici'al Intelligence (AI). It divides the field into four cor~ topics (embodying the base fo\u00b7r a science of intelligence) and eight applications topics (in which research has been contributing to core ideas).. The paper discusses the history, the major landmarks, and some of the controversies in each of these twelve topics. Each topic is represented by a chart citing the major references. These references are contained in an extensive bibliography. The paper concludes with a discussion of some of the criticisms of 'AI and with some predictions about the course of future research.", "references": ["af6035d29f7825a7dc4e99c4c07b2bf4244d9f90", "472c734f091a097898cfc10ba3346bcce3c7aaf2", "489d1e39812f298622323b7c32a73fe403f7c45e", "4e895eed5fb7ded7baffd36f8aff455603787676", "2ad791a9b0a2965b2756bd67375a4d2e0108d30b", "9bfa558918eda54c0ff4f867d04e3d836b56b7af", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "78fc3b741828124d8a79667466516a544143bf68", "2edc8083073837564306943aab77d6dcc19d0cdc", "76885f51b47946cb1a611b691d6c60fb936e2215"], "page_rank": 0.0002463054187192118}, {"id": "9a64c74f4866b62e7d4227c91fd423f8da3a4f41", "title": "Current Issues In Computational Lexical Semantics", "authors": ["James Pustejovsky"], "date": 1989, "abstract": "In this talk I would like to address some issues of major importance in lexical semantics. In particular, I will discuss four topics relating to current research in the field: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Finally, I outline a theory of lexical semantics embodying a richer notion of compositionality, termed cocomposition, which aims to spread the semantic load more evenly throughout the lexicon.", "references": ["e026125272325299dcba6e59832982b6fa47cf4f", "6e4499450a6203c0c043a922f4140180ea6dc4df", "b884c8982258c0c20a86cccdbc30a03b44dcd2e8", "4dc2eea7948fb9f1f296cfc8fb00094b47d137fe", "78722fdd0f047baf0159edcd9493c531c05b95c4", "7aaf5f7e51509d27faf3d578f8dc635c73f169c0"], "page_rank": 0.00022577996715927748}, {"id": "9f365cd5d3d677b0f739481444cf66b0ea271523", "title": "An Approach to Building the Hierarchical Element of a Lexical Knowledge Base From a Machine Readable", "authors": ["Ann Copestake"], "date": 1990, "abstract": "This abstract describes an approach to extracting taxonomies from machine readable dictionaries and using them to structure a lexical knowledge base which incorporates default inheritance. Taxonomy construction is based on an intuitive notion of the organisation of the substantial quantities of data in machine readable dictionaries which were developed for quite independent purposes. Our intention is to investigate how this aaects, and is aaected by, the formal semantics of the knowledge representation for the lexical knowledge base which we are attempting to create, especially with respect to inheritance.", "references": ["de665704b908487341c707a564248095763697c7", "5c7527a41f62528c2c5d3535cac30a847932bae7", "aabafc421773c1785f61c84ce8b79eba72c6b2a4"], "page_rank": 0.00014367816091954023}, {"id": "fd3e7aee5a43f77d4b3871d0eb6d796e0fb8d0ca", "title": "Lexical Operations in a Unification-Based Framework", "authors": ["Ann A. Copestake", "Ted Briscoe"], "date": 1991, "abstract": "We consider lexical operations and their representation in a unification based lexicon and the role of lexical semantic information. We describe a unified treatment of the linguistic aspects of sense extension and derivational morphological processes which delimit the range of possible coercions between lexemes and give a preliminary account of how default interpretations may arise.", "references": ["9f365cd5d3d677b0f739481444cf66b0ea271523", "9f5246be9fb612b604779180b74de46a5b5f4137", "9a64c74f4866b62e7d4227c91fd423f8da3a4f41", "5dc32dde19502971279aa1699b7f6d5690c0eaa1", "0bbbee2bb93b033f7cf23d26c1a43337a9345da3", "2426795ca0024d7fe65b8a9a23690e0b5f2755c8", "268c976be45f3946978d779553c41892777be6da", "1b6dbd6c110ff16bc164e1a5827ef78bf149d2ce", "5c7527a41f62528c2c5d3535cac30a847932bae7"], "page_rank": 8.210180623973726e-05}, {"id": "f5d2fcac3dd05bf483a53bbaedbaf9d1682f6621", "title": "Lexical Issues in Natural Language Processing", "authors": ["Ted Briscoe"], "date": 1991, "abstract": "In this paper, I will briefly describe the role of the lexicon in natural language processing (NLP) applications and will go on to discuss a number of issues in lexical research and in the design and construction of lexicons for practical NLP applications. I will survey relevant research in Europe, America and Japan; however, in a paper of this length it is not possible to consider every instance of a particular approach, so neither the text nor references should be taken to be exhaustive.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "2934355c03faa1094eb413241bb9de6753b48988", "title": "A Problem in Geometric Probability.", "authors": ["Jonathan Wendel"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"A Problem in Geometric Probability.\" by Jonathan Wendel", "references": [], "page_rank": 0.00016420361247947453}, {"id": "fe26bfffa3766b4521becb25468455e902064796", "title": "Grammatical relations, thematic roles and verb semantics", "authors": ["Antonio Sanfilippo"], "date": 1991, "abstract": "Grammatical relations have always constituted a primary focus of attention in the study of language. Within the last three decades, the topicality of this trend has increasingly been determined by the quest for a universal characterization of the language faculty which has shaped the goals and directives of most current works in theoretical linguistics. Although the realization patterns and syntactic functionality of grammatical relations are subject to cross-linguistic variation, studies in comparative grammar have provided suggestive evidence that the range of variation found can often be contained within the limits fixed by a discrete set of parameters. The investigation of these parameters has broached the possibility of a universal specification of the nature of grammatical relations. This thesis proposes that such a specification should be achieved by establishing regularities in the syntax-semantics interface within a constraint-based approach to linguistic analysis that integrates a precise computational interpretation. In keeping with this objective, a unification-based categorial grammar framework is developed which incorporates the semantic insights of a Neo-Davidsonian approach to verb semantics and predicate-argument combination, where thematic roles are defined as clusters of entailments of verb meanings. This framework is extended with an integrated approach to argument selection and selection change. Properties of the resulting system are demonstrated with respect to a variety of natural language phenomena concerning grammatical function changing, unaccusativity and clitic dislocation.", "references": ["6fae00c0401e66bbf6b2e4992864614b9102d521", "de3c9fb765761551ddcac1689530d9df6303a2fc", "60c551652f4cba4db1555cccc472b9a7c704b5b8", "cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "2d29fa654812c44fd3285856d46fef18812bf23c", "24c56e382fc74b483214f949b0793284668a416b", "e40e4a3f4cffa628e98097107f16be69dcf81262", "03380e7083807d3264472871dc0582036cf79479", "f39638342e42959c6819695fa029aebf69340524", "b02af0721ab87071d954c0ad221cb2e49a65003b"], "page_rank": 8.210180623973726e-05}, {"id": "e8871e2641df536074612f73e26815bf324c0ba9", "title": "Gesammelte mathematische Abhandlungen", "authors": ["Hermann Amandus Schwarz}"], "date": 1940, "abstract": "Ersten Bande: Ueber die Minimalflache, deren Begrenzung als ein von vier Kanten eines regularen Tetraeders gebildetes raumliches Vierseit gegeben ist Bestimmung einer speciellen Minimalflache Bestimmung einer speciellen Minimalflache. Nachtrag Bestimmung einer speciellen Minimalflache. Anhang, enthaltend Anmerkungen und Zusatze Fortgesetzte Untersuchungen uber specielle Minimalflachen Ueber ein Modell eines Minimalflachenstuckes, welches langs seiner Begrenzung vier gegebene Ebenen rechtwinklig trifft Beitrag zur Untersuchung der zweiten Variation des Flacheninhalts von Minimalflachenstucken im Allgemeinen und von Theilen der Schraubenflache im Besonderen Miscellen aus dem Gebiete der Minimalflachen Ueber diejenigen Minimalflachen, welche von einer Schaar von Kegeln zweiten Grades eingehullt werden Ueber einige nicht algebraische Minimalflachen, welche eine Schaar algebraischer Curven enthalten Sur les surfaces a courbure moyenne nulle sur lesquelles on peut limiter une portion finie de la surface par quatre droites situees sur la surface Ueber ein die Flachen kleinsten Flacheninhalts betreffendes Problem der Variationsrechnung. Festschrift zum siebzigsten Geburtstage des Herrn Karl Weierstrass Ueber specielle zweifach zusammenhangende Flachenstucke, welche kleineren Flacheninhalt besitzen, als alle benachbarten, von denselben Randlinien begrenzten Flachenstucke Anmerkungen und Zusatze zum ersten Bande Zweiten Bande: Elementarer Beweis des Pohlkeschen Fundamentalsatzes der Axonometrie De superficiebus in planum explicabilibus primorum septem ordinum. Inauguraldissertation Ueber die geradlinigen Flachen funften Grades Ueber einige Abbildungsaufgaben Conforme Abbildung der Oberflache eines Tetraeders auf die Oberflache einer Kugel Notizia sulla rappresentazione conforme di un' area ellittica sopra un' area circolare Zur Theorie der Abbildung Ueber einen Grenzubergang durch alternirendes Verfahren Ueber die Integration der partiellen Differentialgleichung $\\frac{\\partial^{2}u}{\\partial x^2}+\\frac{\\partial^{2}u}{\\partial y^2}=0$ unter vorgeschriebenen Grenz- und Unstetigkeitsbedingungen Mittheilung uber diejenigen Falle, in welchen die Gaussische hypergeometrische Reihe $F(\\alpha, \\beta, \\gamma, x)$ eine algebraische Function ihres vierten Elementes darstellt Zur Integration der partiellen Differentialgleichung $\\frac{\\partial^{2}u}{\\partial x^2}+\\frac{\\partial^{2}u}{\\partial y^2}=0$ Ueber diejenigen Falle, in welchen die Gaussische hypergeometrische Reihe eine algebraische Function ihres vierten Elementes darstellt Ueber ebene algebraische Isothermen Beispiel einer stetigen nicht differentiirbaren Function Ueber ein vollstandiges System von einander unabhangiger Voraussetzungen zum Beweise des Satzes $\\frac{\\partial}{\\partial y}(\\frac{\\partial f(x, y)}{\\partial x})=\\frac{\\partial}{\\partial x}(\\frac{\\partial f(x, y)}{\\partial y})$ Ueber diejenigen algebraischen Gleichungen zwischen zwei veranderlichen Grossen, welche eine Schaar rationaler, eindeutig umkehrbarer Transformationen in sich selbst zulassen Essai d'une demonstration d'un theoreme de Geometrie, redige sur l'invitation de M. Charles Hermite Verallgemeinerung eines analytischen Fundamentalsatzes Auszug aus einem Briefe an Herrn F. Klein Demonstration elementaire d'une propriete fondamentale des fonctions interpolaires Sur une definition erronee de l'aire d'une surface courbe Bestimmung der scheinbaren Grosse eines Ellipsoids fur einen beliebigen Punkt des Raumes Zur conformen Abbildung der Flache eines Rechtecks auf die Flache einer Halbkugel Beweis des Satzes, dass die Kugel kleinere Oberflache besitzt, als jeder andere Korper gleichen Volumens Beweis eines fur die Theorie der trigonometrischen Reihen in Betracht kommenden Hulfssatzes Beweis des Satzes, dass unter allen einem spitzwinkligen Dreiecke eingeschriebenen Dreiecken das Dreieck der Hohenfusspunkte den kleinsten Umfang hat Bemerkung zu der Mittheilung des Herrn Weierstrass: Zur Theorie der aus $n$ Haupteinheiten gebildeten complexen Grossen Anmerkungen und Zusatze zum zweiten Bande.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "4e3cbd911b175d510a8cb3c65c79a5595915e42f", "title": "Escaping brittleness: the possibilities of general- purpose learning algorithms applied to parallel", "authors": ["John H. Holland"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"Escaping brittleness: the possibilities of general- purpose learning algorithms applied to parallel\" by John H. Holland", "references": [], "page_rank": 0.0004926108374384236}, {"id": "827411d2ebf389a5a8289abd6f17174ed25a278e", "title": "Models of Learning Systems.", "authors": ["Bruce G. Buchanan", "Tom Michael Mitchell", "Reid G. Smith", "C. Richard Johnson"], "date": 1979, "abstract": "Abstract : The terms adaptation, learning, concept-formation, induction, self-organization, and self-repair have all been used in the context of learning system (LS) research. In this article, three distinct approaches to machine learning and adaptation are considered: (i) the adaptive control approach, (ii) the pattern recognition approach, and (iii) the artificial intelligence approach. Progress in each of these areas is summarized in the first part of the article. In the next part a general model for learning systems is presented that allows characterization and comparison of individual algorithms and programs in all of these areas. The model details the functional components felt to be essential for any learning system, independent of the techniques used for its construction, and the specific environment in which it operates. Specific examples of learning systems are described in terms of the model. (Author)", "references": ["65c4e4b7c054721e7f452319b0f7d02a70acb2ff", "fb3ad4bfc20c3f4eacc8c58dffe6fb7bd1ddc5d0", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "19bfa432237dc1bd82113774727fe0307005e430", "693e791966b333ad5bec096999cea2cce122cd38", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "4cd8ac993c1f464ce9a4c8d3e4467ecd03b84f00", "bb377e6d5cb0c90396c2006880f1a9a861947d9a", "bbaac3ff0eb5b661faead2748313834c9cf771cf", "d057f13d7f9bc2ddc98482df6088e35ea83c9df5"], "page_rank": 0.0004926108374384236}, {"id": "7ce11916234117cf9e1d9bd7eae9a24fe68c8c01", "title": "A Nonlinear-Summation Threshold Device", "authors": ["Philip Kaszerman"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"A Nonlinear-Summation Threshold Device\" by Philip Kaszerman", "references": [], "page_rank": 0.00016420361247947453}, {"id": "bef96bdc528a1f50a23d3e70c3df5bcb6f4b763d", "title": "Quantitative fuzzy semantics", "authors": ["Lotfi A. Zadeh"], "date": 1971, "abstract": "The point of departure in this paper is the definition of a language, L, as a fuzzy relation from a set of terms, T = x, to a universe of discourse, U = y. As a fuzzy relation, L is characterized by its membership function @m\"L:T x U -> [0,1], which associates with each ordered pair (x,y) its grade of membership, @m\"L(x,y), in L. Given a particular x in T, the membership function @m\"L(x,y) defines a fuzzy set, M(x), in U whose membership function is given by @m\"M\"(\"x\")(y) = @m\"L(x,y). The fuzzy set M(x) is defined to be the meaning of the term x, with x playing the role of a name for M(x). If a term x in T is a concatenation of other terms in T, that is, x = x\"1 ... x\"n, x\"i @e T, i = 1,...,n, then the meaning of x can be expressed in terms of the meanings of x\"1,...,x\"n through the use of a lambda-expression or by solving a system of equations in the membership functions of the x\"i which are deduced from the syntax tree of x. The use of this approach is illustrated by examples.", "references": ["82b03ff061d8180a27ce3744860c82aabd01e93a", "a1db7c00de24ee240f45a94b2db272ba78ba178f", "52b99d29c931d9aaf1b3d6f48b31577affef0208", "caa65e007e44346d790ae888e30b2e02e4b59502", "6023269ef7c857619f8d10e59430f42497891a67", "90ee730542300008a3cef397215ad08616deb314", "281d9b56b02f8c4530433ec3b1d66425652803d1", "81b4b145f9b6e89e32cb399dba24384cb230f511"], "page_rank": 0.0002463054187192118}, {"id": "94f0acfa093c5f07cc21ab8c8c2b53d42c1d4232", "title": "Cooperative Responses From a Portable Natural Language Database Query System", "authors": ["Michael Brady", "Robert C. Berwick"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"Cooperative Responses From a Portable Natural Language Database Query System\" by Michael Brady et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "b3ad74c4dfb674d7bdf0aa32d38cfd4364a70747", "title": "Grundz\u00fcge der theoretischen Logik", "authors": ["Hans von Hornich"], "date": 1939, "abstract": "Semantic Scholar extracted view of \"Grundz\u00fcge der theoretischen Logik\" by Hans von Hornich", "references": ["03c1d33a92f21e143d3568b6f76a55c01c9ae794"], "page_rank": 0.0002463054187192118}, {"id": "de42b848775f9fa1e4bff758ae04a54099c0c381", "title": "ELIZA \u2014 a computer program for the study of natural language communication between man and machine", "authors": ["Joseph Weizenbaum"], "date": 1983, "abstract": "ELIZA is a program operating within the MAC time-sharing system of MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA \u201cscripts\u201d. A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "2e5ee3f2b1bf6525b300639478397c9e2607d4c6", "title": "Identification of Conceptualisations Underlying Natural Language", "authors": ["R. Schanck"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Identification of Conceptualisations Underlying Natural Language\" by R. Schanck", "references": [], "page_rank": 0.0001231527093596059}, {"id": "b5a5306e8fb690859a2816ba07e603e47a3dcfc0", "title": "The Structure of Language: Readings in the Philosophy of Language", "authors": ["L. Jonathan Cohen", "Jerry A. Fodor", "Jerrold J. Katz"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"The Structure of Language: Readings in the Philosophy of Language\" by L. Jonathan Cohen et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "85bb4912e8d58ae87ad89be4d284fe22cf26692d", "title": "Computations from the english", "authors": ["R. F. Simmons"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"Computations from the english\" by R. F. Simmons", "references": [], "page_rank": 0.0001231527093596059}, {"id": "04d0d7dd9d237f72e7d0c70f80a98593586971be", "title": "Logical Syntax of Language", "authors": ["Rudolf Carnap"], "date": 1937, "abstract": "Rudolf Carnap's entire theory of Language structure \"came to me,\" he reports, \"like a vision during a sleepless night in January 1931, when I was ill.\" This theory appeared in The Logical Syntax of Language (1934). Carnap argued that many philosophical controversies really depend upon whether a particular language form should be used. This leads him to his famous \"Principle of tolerance\" by which everyone is free to mix and match the rules of his language and therefore his logic in any way he wishes. In this way, philosophical issues become reduced to a discussion of syntactical properties, plus reasons of practical convenience for preferring one form of language to another. In a tour de force of precise reasoning, Carnap also indicated how two model languages could be constructed. This is one of three books which Open Court is making available in paperback reprint in its Open Court Classics series. The other two are Carnap's The Logical Structure of the World and Schlick's General Theory of Knowledge.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "ecb2be6f9352ae8c3bc95bf8103aa22e1f2e2f15", "title": "Degrees of Grammaticalness\u3068\u610f\u5473\u3068\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066", "authors": ["\u5229\u4fe1 \u67d3\u7530"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"Degrees of Grammaticalness\u3068\u610f\u5473\u3068\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\" by \u67d3\u7530 \u5229\u4fe1", "references": [], "page_rank": 0.0001231527093596059}, {"id": "ff2c2e3e83d1e8828695484728393c76ee07a101", "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations", "authors": ["David E. Rumelhart", "James L. McClelland"], "date": 1986, "abstract": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "35f03721ecef2a7315a8d85d02bacaf00660a3fb", "title": "The Metanovel: Writing Stories by Computer", "authors": ["James R. Meehan"], "date": 1976, "abstract": "Abstract : People draw on many diverse sources of real-world knowledge in order to make up stories, including the following: knowledge of the physical world; rules of social behavior and relationships; techniques for solving everyday problems such as transportation, acquisition of objects, and acquisition of information; knowledge about physical needs such as hunger and thirst; knowledge about stories their organization and contents; knowledge about planning behavior and the relationships between kinds of goals; and knowledge about expressing a story in a natural language. This thesis describes a computer program which uses all information to write stories. The areas of knowledge, called problem domains, are defined by a set of representational primitives, a set of problems expressed in terms of those primitives, and a set of procedures for solving those problems. These may vary from one domain to the next. All this specialized knowledge must be integrated in order to accomplish a task such as storytelling. The program, called TALE-SPIN, produces stories in English, interacting with the user, who specifies characters, personality characteristics, and relationships between characters. Operating in a different mode, the program can make those decisions in order to produce Aesop-like fables. (Author)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2382b146f49bd5015121d888902263c29c5eace7", "title": "Understanding Goal-Based Stories", "authors": ["Robert Wilensky"], "date": 1978, "abstract": "Abstract : Reading requires reasoning. A reader often needs to infer connections between the sentences of a text and must therefore be capable of reasoning about the situations to which the text refers. People can reason about situations because they posses a vast store of knowledge which they can use to infer implicit parts of a situation from those aspects of the situation explicitly described by a text. PAM (Plan Applier Mechanism) is a computer program that understands stories by reasoning about the situations they reference. PAM reads stories in English and produces representations for the stories that include the inferences needed to connect each story's events. To demonstrate that it has understood a story, PAM answers questions about the story and expresses the story from several points of view. PAM reasons about the motives of a story's characters. Many inferences needed for story understanding are concerned with finding explanations for events in the story. PAM has a great deal of knowledge about people's goals which it applies to find explanations for the actions taken by a story's characters in terms of that character's goals and plans.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a5de6bc02c770c19331f70fefbe8e8d1094fc9f1", "title": "A system of seven coherence relations for hierarchically organizing event concepts in text (nexus)", "authors": ["Richard Ethan Alterman"], "date": 1982, "abstract": "A theory of event concept coherence is developed. It is shown that the pieces of event description which appear in a body of text can be gathered together and hierarchically organized using a dictionary of event/state concepts. This theory has been implemented in a computer system, NEXUS. The representations it produces are constructed in terms of sevel coherence relations. The dictionary it uses was compiled from an analysis of ten folktales. \nThe seven coherence relations used are class/subclass, sequence/subsequence, coordinate, antecedent, precedent, consequent, and sequel. Class/subclass is a taxonomic relation. Sequence/subsequence and coordinate are two kinds of whole/part relations. The other four relations are temporal; antecedent and precedent concepts come before an event, consequent and sequel concepts come after an event. \nAssociations between concepts in the dictionary are also organized in terms of the seven coherence relations. Associated with the concepts in the dictionary are default values for its case related arguments. Relationships between concepts in the dictionary are refined by attaching to each relationship in the dictionary a set of constraints on matching case arguments. NEXUS uses the default values and the constraints to control the representation building process. \nThe flexibility of the representation scheme is demonstrated by applying it to several diverse examples of narrative text in the literature, including script and plan based stories. The feasability of NEXUS is shown by applying it to eight samples of text and discussing in detail the results. The utility of the hierarchical representation produced by NEXUS is validated by experiments in question answering and summarizing.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "8a8f1719e4b0f41ddc3d3c415d7a384e00c2549b", "title": "Black English and Black Attitudes.", "authors": ["David L. Shores"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Black English and Black Attitudes.\" by David L. Shores", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1e32d64fd543b1e70012415fc6e47119257ae74a", "title": "Some experiments with large-vocabulary isolated-word sentence recognition", "authors": ["Lalit R. Bahl", "Subrata K. Das", "Peter V. de Souza", "Frederick Jelinek", "Slava M. Katz", "Robert L. Mercer", "Michael Picheny"], "date": 1984, "abstract": "This paper deals with two experiments with a large vocabulary isolated word recognizer. The first compares word error rates for 1) meaningful sentences belonging to actual documents and 2) random word lists from the same vocabulary. The error rate is considerably lower for random word lists. The second experiment investigates the performance of the recognition system on sentences containing words outside the vocabulary of the recognizer. Sentences from a 5000 word vocabulary task are recognized with a recognizer limited to a 2000 word subvocabulary. The error rate is only slightly higher than it would be if recognition of the full 5000 word vocabulary was allowed.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "b8fe93d3e5205a450fdd8a9fb94cea0ab73b067f", "title": "BYBLOS: The BBN continuous speech recognition system", "authors": ["Yen-Lu Chow", "Mari O. Dunham", "Owen Kimball", "Michael A. Krasner", "Gregory Kubala", "John Makhoul", "P.. Price", "Salim Roucos", "Richard M. Schwartz"], "date": 1987, "abstract": "In this paper, we describe BYBLOS, the BBN continuous speech recognition system. The system, designed for large vocabulary applications, integrates acoustic, phonetic, lexical, and linguistic knowledge sources to achieve high recognition performance. The basic approach, as described in previous papers [1, 2], makes extensive use of robust context-dependent models of phonetic coarticulation using Hidden Markov Models (HMM). We describe the components of the BYBLOS system, including: signal processing frontend, dictionary, phonetic model training system, word model generator, grammar and decoder. In recognition experiments, we demonstrate consistently high word recognition performance on continuous speech across: speakers, task domains, and grammars of varying complexity. In speaker-dependent mode, where 15 minutes of speech is required for training to a speaker, 98.5% word accuracy has been achieved in continuous speech for a 350-word task, using grammars with perplexity ranging from 30 to 60. With only 15 seconds of training speech we demonstrate performance of 97% using a grammar.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "14f498c4292f0f6b3f4170e31e2f53489b76701c", "title": "An overview of NIKL, the new implementation of KL-ONE", "authors": ["Megan Moser"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"An overview of NIKL, the new implementation of KL-ONE\" by Megan Moser", "references": [], "page_rank": 0.0002189381499726327}, {"id": "c86590e947c28e8791d1e8bab8fc8ab53302341f", "title": "Learning the hidden structure of speech.", "authors": ["Jeffrey L. Elman", "David Zipser"], "date": 1988, "abstract": "In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95%. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.", "references": ["1cd4e2a1c4a503e18c910666fb5bf7f616f65ece", "bda4125a131c25f11c3e193de083718a7c6237c7", "ff8117dcddf1bdc6c4df7ae2daea0a91c74893b7", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "629a902def4fae97e47505fced2f800a583ad4ca", "2da1dd84c0679f0687a1f3e239f266c4396f4526", "6db399b4afd41d29c06bbb88c1de370a4b93f994"], "page_rank": 0.0001231527093596059}, {"id": "f42d0737564e77f62ea1bf20298b10da192bb412", "title": "Procedural Semantics for a Question-Answering Machine", "authors": ["W. Addison Woods}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Procedural Semantics for a Question-Answering Machine\" by W. Addison Woods", "references": [], "page_rank": 0.0002463054187192118}, {"id": "81cba5ea8397c7a32a775b95189e87f703ccc2d9", "title": "Reference and Definite Descriptions", "authors": ["Keith S. Donnellan"], "date": 1966, "abstract": "A deflector mountable on the saw blade guard of a portable rotary saw for deflecting sawdust, splinters and other cuttings, comprising a transparent semi-circular channel member adapted to be attached to the saw guard of the saw and effective to deflect the cuttings downwardly without interfering with the operator's view of the saw blade and workpiece at the cutting point.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "title": "Conceptual dependency: A theory of natural language understanding \u2606", "authors": ["Roger C. Schank"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Conceptual dependency: A theory of natural language understanding \u2606\" by Roger C. Schank", "references": [], "page_rank": 0.0004105090311986863}, {"id": "d61d30b1e7f3a3cc3654a4a97a381456dd88e89c", "title": "Naming and necessity.", "authors": ["J. E. J. Altham"], "date": 1981, "abstract": "Following your need to always fulfil the inspiration to obtain everybody is now simple. Connecting to the internet is one of the short cuts to do. There are so many sources that offer and connect us to other world condition. As one of the products to see in internet, this website becomes a very available place to look for countless naming and necessity sources. Yeah, sources about the books from countries in the world are provided.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "b4d5633b96ea5e0f9e90e03df58e8ad0f6f3dee7", "title": "Cohesion in English", "authors": ["Michael Halliday", "Ruqaiya Hasan"], "date": 1976, "abstract": "Cohesion in English is concerned with a relatively neglected part of the linguistic system: its resources for text construction, the range of meanings that are speciffically associated with relating what is being spoken or written to its semantic environment. A principal component of these resources is 'cohesion'. This book studies the cohesion that arises from semantic relations between sentences. Reference from one to the other, repetition of word meanings, the conjunctive force of but, so, then and the like are considered. Further, it describes a method for analysing and coding sentences, which is applied to specimen texts.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "78722fdd0f047baf0159edcd9493c531c05b95c4", "title": "Word Meaning and Montague Grammar", "authors": ["Terence Parsons", "David R. Dowty"], "date": 1979, "abstract": "A four position racking mechanism for a circuit breaker in a cubicle or housing providing Connected, Test, Disconnect and Out positions. The racking lead screw is mounted in the circuit breaker structure in its own support. The rear end of the racking screw is threaded to engage and move a travelling nut which rides in guide slots. The outer ends of the travelling nut engage a pair of connecting links which rotate a pivoted roller bracket. The roller bracket carries a racking roller which engages a cam slot in a member extending from the rear portion of the cubicle or housing. The shape of the cam slot plus the increasing lever arm of the connecting links around the pivot pin on the nut provide a variable racking force which is needed for primary disconnect contact engagement. The central positioning of the racking mechanism makes it suitable for any width of circuit breaker. The racking mechanism is interlocked with the closing mechanism; releasable locking means and interlock means are provided to limit the operation of the racking mechanism to conditions which are safe for the operator; thus, the unlocking of the racking mechanism after the breaker is tripped will hold the trip mechanism and hold it trip free as long as the racking mechanism is unlocked. An indexing element for the slide for the locking mechanism is provided for all the positions of the circuit breaker.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "4dc2eea7948fb9f1f296cfc8fb00094b47d137fe", "title": "The structure of appearance", "authors": ["NevilleM. Goodman"], "date": 1951, "abstract": "One/On The Theory Of Systems.- I. Constructional Definition.- 1. Extensional Identity.- 2. Substitution Criteria.- 3. Extensional Isomorphism.- 4. Consequences of Isomorphism as a Criterion.- 5. On Systems of Predicates of Individuals.- II. The General Apparatus.- 1. General Apparatus and Special Basis.- 2. The Question of Classes.- 3. Nominalism.- 4. The Calculus of Individuals.- 5. The Calculus in Systems.- III. Extralogical Bases.- 1. The Nature of Primitive Terms.- 2. The Choice of Basis.- 3. Simplicity.- 4. Reflexitivity and Complexity.- 5. Transitivity, Self-completeness, and Complexity.- 6. Symmetry and Complexity.- 7. Final Formulae for Primary Complexity.- 8. Secondary Complexity 75 9. Evaluation of Bases.- 10. Complexity of Other Primitives.- 11. Basic Individuals.- 12. Postulates.- Two I On Qualities and the Concrete.- IV. Approach to the Problems.- 1. Things.- 2. Properties.- 3. Qualia.- 4. Physicalistic and Phenomenalistic Systems.- 5. Realistic and Particularistic Systems.- 6. Introduction to the Problems of Abstraction and Concretion.- V. The System of the 'Aufbau'.- 1. Introduction.- 2. The Basic Units.- 3. Methods of Construction.- 4. The Choice of a Primitive.- 5. Definition of Qualities.- 6. Further Constructions.- 7. Conclusion.- VI. Foundations of a Realistic System.- 1. Qualia as Atoms.- 2. Atoms of the System.- 3. Togetherness.- 4. The Problem of Concretion.- 5. A Revision and its Consequences.- 6. Rectification of Particularism.- 7. Alternative Treatments of the Problem of Concretion.- VII. Concreta and Qualification.- 1. The Individuals of the System.- 2. Principles of Togetherness.- 3. Complexes.- 4. Concreta.- 5. Elementary Qualification.- 6. Compound Qualification.- 7. A Paradox and its Lesson.- 8. A Note on Abstract, Concrete, Universal, and Particular Individuals.- VIII. Size and Shape.- 1. The Problem.- 2. Size.- 3. Shape.- 4. Initial and Derivative Quality Terms.- Three/On Order, Measure, and Time.- IX. The Problem of Order.- 1. A New Problem.- 2. Choice of a Basic Predicate.- 3. Mapping and the Mapped.- 4. Reduction of Basis.- 5. Categories and Realms.- 6. Principles of Matching.- 7. A Rule of Order.- X. Topology of Quality.- 1. The Formal Problem.- 2. Betwixtness.- 3. Justification of the Definition of Betwixtness.- 4. Besideness.- 5. Just Noticeable Difference.- 6. Adjusted Linear Maps.- 7. Some Cartographical Conventions.- 8. Some Types of Nonlinear Array.- 9. Besideness in Square-Cell Networks.- 10. Nextness.- 11. Spurious Maps.- 12. Toward Shape and Measure.- 13. Ordinal Quasianalysis.- 14. Recent Developments.- 15. Note Added in Third Edition 257.- XI. Of Time and Eternity.- 1. Phenomenal Time.- 2. Time and Language.- 3. The Passage of Time.- 4. The Temporal Field.- 5. The Physical World.- Index to Special Symbols.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1b6dbd6c110ff16bc164e1a5827ef78bf149d2ce", "title": "Type Coercion and Selection", "authors": ["J. Pustejovsy"], "date": 1989, "abstract": "A graduated, grooved rule and/or graduated slotted and grooved rule member for use in combination with the subcombination head means of combination tool. For example, the subcombination head means of a combination square, the subcombination head means of a bevel protractor; combination head means of a builder's combination tool, square; subcombination head means of a double square; subcombination head means of a reliable try square; subcombination head means of a die maker's square and the like.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "268c976be45f3946978d779553c41892777be6da", "title": "The Mathematics of Inheritance Systems", "authors": ["David S. Touretzky"], "date": 1984, "abstract": "Virtually all AI and object-oriented programming languages include inheritance systems. Common examples are FRL, KRL, KLONE, NETL, Simula, Smalltalk, Flavors, and Ada. Systems that permit exceptions to inherited properties, which involves a form of nonmonotonic reasoning, remained unformalized until quite recently. The lack of a formal theory hid some defects in existing implementations. One defect was an incorrect treatment of networks with multiple grounded expansions. (A grounded expansion is the nonmonotonic equivalent of a \"theory\" in ordinary monotonic reasoning.) Another defect was a tendency to reason incorrectly when redundant statements are presented. \nWe present a formal mathematical theory of inheritance and show how the defects in existing systems can be corrected. Our formalism bears some relation to default and nonmonotonic logics, but includes an important hierarchical notion the other systems lack. We prove theorems about the consistency, existence, uniqueness, and constructibility of grounded expansions, and we provide a formal semantics for inheritance in terms of constructible lattices of predicates. \nThe formal theory of inheritance is then applied to the formal analysis of a massively parallel computer architecture known as a parallel marker propagation machine (PMPM), of which the best-known example is Fahlman's NETL Machine. This machine has been proposed as a high speed inference engine for AI. We show that a PMPM can perform correct inheritance reasoning only in certain limited cases. However, through a technique known as \"conditioning,\" the topology of a network can be altered to force the PMPM to produce correct results. We show that conditioning is always possible, and present a simple yet efficient conditioning algorithm. \nAn extension to the usual notion of inheritance allows us to infer, if Fred is a citizen and Dick a crook, that Fred dislikes Dick from the relation \"citizens dislike crooks.\" An exception to such a relation might be \"gullible citizens don't dislike elected crooks.\" We extend our earlier results by developing a formal theory of inheritable relations, including exceptions and their interaction with the IS-A \nhierarchy, and showing how a correct implementation on a PMPM may be achieved through conditioning. \n*This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 3597, monitored by the Air Force Avionics Laboratory Under Contract F33615-81-K-1539. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the US Government.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "2426795ca0024d7fe65b8a9a23690e0b5f2755c8", "title": "Book Reviews: Lecture on Contemporary Syntactic Theories: An Introduction to Unification-Based Approaches to Grammar", "authors": ["Neculai Curteanu"], "date": 1987, "abstract": "gerrymander, dummy, sample, foist, portray, belie, mirror, depict, typify, hippodrome, describe, describble, lament, paraphrase, blackbox, blazon and other words. Finding the words for this paragraph took an hour, and required looking at many pages of the book. Note that there is no common parent of represent and manifest. What about Wilkins? Represent is under \"Transcendental Relations of Action\" along with manifest (respectively under the subheadings comparate and simple transcendental relations). They are on facing pages and in the immediate vicinity of these two words are declare, show, exhibit, present, reveal, set forth, come to light,. render, demonstrate, and disclose. So I would rather have the older book, which also does not begin with dire warnings about what will happen to anyone who even reads a photocopy, much less makes one.", "references": ["cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "70785ff72f4ec20ddfc16399252ffc41c09f89d3", "503817d0b66bf379465c3cdccbcffc524a4fe4f5"], "page_rank": 6.157635467980295e-05}, {"id": "0bbbee2bb93b033f7cf23d26c1a43337a9345da3", "title": "Meaning, Use, and Interpretation of Language", "authors": ["Rainer B{\\\"a}uerle", "Christoph Schwarze", "Arnim von Stechow"], "date": 1983, "abstract": "Bibliothek M\u00fcnchen CIP-Kurztitelaufnahme der Deutschen Bibliothek Meaning, use and interpretation of language / ed. Cognitive impairments in aphasia : new results and new problems 30 Florian Coulmas Underdeterminacy and plausibility in word-formation 46 Maxwell J. Cresswell A highly impossible scene. The semantics of visual contradictions 62 Urs Egli The Stoic theory of arguments 79 Cathrine Fabricius-Hansen Wieder ein wieder? Zur Semantik von wieder 97 Jacques Fran\u00e7ois On the perspectival ordering of patient and causing event in the distribution of French and German verbs of change: a contrastive study 121 Harold Goodglass Disorders of lexical production and comprehension 134 Christopher Habel Inferences-the base of semantics? 147 VIII Contents Irene Heim File change semantics and the familiarity theory of definiteness , 164 Jaap Hoepelman Adjectives and nouns: a new calculus 190 Hans H\u00f6rmann The calculating listener, or : How many are einige, mehrere, and ein paar (some, several, and a few) ? 221 Karlheinz H\u00fclser The fragments on Stoic dialectic: a new collection 235 Hans Kamp and Christian Rohrer Tense in texts 250 Walther Kindt Neue modelltheoretische Ans\u00e4tze f\u00fcr die Semantik. The logical analysis of plurals and mass terms : a lattice-theoretical approach 302", "references": ["65ba5b234501960b5d0356d4c409edea38b66891", "d7bb69a83a0d36da6d34a0ab9f58748a5e77c6de", "65905515e212cf22657aabaddfeee5aa82cc7691"], "page_rank": 6.157635467980295e-05}, {"id": "81b4b145f9b6e89e32cb399dba24384cb230f511", "title": "Formal definition of programming languages : with an application to the difinition of algol 60", "authors": ["J. W. de Bakker"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Formal definition of programming languages : with an application to the difinition of algol 60\" by J. W. de Bakker", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "9f5246be9fb612b604779180b74de46a5b5f4137", "title": "Syllable-based Morphology", "authors": ["Lynne J. Cahill"], "date": 1990, "abstract": "This paper presents a language for the description of morphological alternations which is based on syllable structure. The justification for such an approach is discussed with reference to examples from a variety of languages and the approach is compared to Koskenniemi's two-level account of morphonology.", "references": ["b49931a2caf1dbfad15cc85e5ea0df99e976d017", "e11ce10eff56df1729626bda3ef6109566f779c9"], "page_rank": 6.157635467980295e-05}, {"id": "d057f13d7f9bc2ddc98482df6088e35ea83c9df5", "title": "Natural language learning by computer", "authors": ["Laurent Sikl{\\'o}ssy"], "date": 1968, "abstract": "Abstract : Learning a natural language is taken as an improvement in a system's ability to express situations in a natural language. This dissertation describes a computer program, called Zbie, written in IPL-V, which accepts the description of situations in a uniform, structured functional language and tries to express these situations in a natural language. Examples are given for German and, mostly, Russian. At run-time, Zbie builds simple memory structures. Patterns and sets are built on the functional language. The translation rules of the patterns and an in-context vocabulary provide the transition to the natural language. Zbie is a cautious learner, and avoids errors by several mechanisms. Zbie is capable of some evolutionary learning.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5c7527a41f62528c2c5d3535cac30a847932bae7", "title": "Enjoy the paper: lexical semantics via lexicology", "authors": ["Ted Briscoe", "Ann Copestake", "Branimir Boguraev"], "date": 1990, "abstract": "Current research being undertaken at both Cambridge and IBM is aimed at the construction of substantial lexicons containing lexical semantic information capable of use in automated natural language processing (NLP) applications. This work extends previous research on the semi-automatic extraction of lexical information from machine-readable versions of conventional dictionaries (MRDs) (see e.g. the papers and references in Boguraev & Briscoe, 1989; Walker et al., 1988). The motivation for this and previous research using MRDs is that entirely manual development of lexicons for practical NLP applications is infeasible, given the labour-intensive nature of lexicography (e.g. Atkins, 1988) and the resources likely to be allocated to NLP in the foreseeable future. In this paper, we motivate a particular approach to lexical semantics, briefly demonstrate its computational tractability, and explore the possibility of extracting the lexical information this approach requires from MRDs and, to some extent, textual corpora.", "references": ["9a64c74f4866b62e7d4227c91fd423f8da3a4f41", "9e7c09943444bc96f06ff92919c62cd79bb29066", "0b380f641e115ad426984079f504b21aea9636c5", "5dc32dde19502971279aa1699b7f6d5690c0eaa1", "7fd60f655c0a2c506a3071db86999444821b698d", "9806348927e1e642c362aa83065e7380a06c2282", "f461c5b2f209ddcadfb4eb79e3ce65eaae7ab5cf"], "page_rank": 0.00022577996715927748}, {"id": "aabafc421773c1785f61c84ce8b79eba72c6b2a4", "title": "The dictionary and the thesaurus can be combined", "authors": ["Nicoletta Calzolari"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"The dictionary and the thesaurus can be combined\" by Nicoletta Calzolari", "references": [], "page_rank": 0.00016420361247947453}, {"id": "281d9b56b02f8c4530433ec3b1d66425652803d1", "title": "A generalization of algol and its formal definition: parts 1 and 2", "authors": ["Niklaus Wirth", "Helmut Weber"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"A generalization of algol and its formal definition: parts 1 and 2\" by Niklaus Wirth et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "90ee730542300008a3cef397215ad08616deb314", "title": "Abstraction and pattern classification", "authors": ["Richard Bellman", "Robert E. Kalaba", "Lotfi A. Zadeh"], "date": 1966, "abstract": "Abstract : This is a preliminary paper in which the authors discuss a general framework for the treatment of pattern-recognition problems. They make precise the notion of a 'fuzzy' set. Then they show how this may be employed in a sequential experimental procedure to ascertain whether a symbol is a member of a particular set or not. The close relation between the problem of pattern recognition and interpolation is stressed. (Author)", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "a1db7c00de24ee240f45a94b2db272ba78ba178f", "title": "Note on fuzzy languages", "authors": ["E. T. Lee", "Lotfi A. Zadeh"], "date": 1969, "abstract": "Abstract A fuzzy language is defined to be a fuzzy subset of the set of strings over a finite alphabet. The notions of union, intersection, concatenation, Kleene closure, and grammar for such languages are defined as extensions of the corresponding notions in the theory of formal languages. An explicit expression for the membership function of the language L ( G ) generated by a fuzzy grammar G is given, and it is shown that any context-sensitive fuzzy grammar is recursive. For fuzzy context-free grammars, procedures for constructing the Chomsky and Greibach normal forms are outlined and illustrated by examples.", "references": ["eb7e14ee7a3f95afd6801f4356777d9dfd540f12"], "page_rank": 6.157635467980295e-05}, {"id": "03c1d33a92f21e143d3568b6f76a55c01c9ae794", "title": "Grundz\u00fcge der theoretischen Logik", "authors": ["Labajos Manzanares Mt"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Grundz\u00fcge der theoretischen Logik\" by Labajos Manzanares Mt", "references": [], "page_rank": 0.0004926108374384236}, {"id": "6023269ef7c857619f8d10e59430f42497891a67", "title": "EULER: a generalization of ALGOL, and its formal definition: Part II", "authors": ["Niklaus Wirth", "Helmut Weber"], "date": 1966, "abstract": "In this section the algorithmic language EULER is described first informally and then formally by its syntax and semantics. An attempt has been made to generalize and extend some of the concepts of ALGOL, thus creating a language which is simpler and yet more flexible than ALGOL 60. A second objective in developing this language was to show that a useful programming language which can be processed with reasonable efficiency can be defined in rigorous formality.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "caa65e007e44346d790ae888e30b2e02e4b59502", "title": "A syntax directed compiler for ALGOL 60", "authors": ["Edgar T. Irons"], "date": 1961, "abstract": "i Th(~ disposition of lhe parentheses is computed by numberbig the m ull;iplication signs consecutively. If n is divisible /)y 2 k but, not; by 2 kw, then (;he nt,h multiplication sign is 1-),'ecedcd by k right pat'entheses, and followed by k left parentheses. If the lasi, multiplication sign is numbered m, then the entire expression is surrounded by k parentheses, whore 2 k ~\" Ill. The extension go negative integral expohen Ls is obvious. The rewritLen expressions are compiled in the normal manner, the equivalent subexpressions being a.tttotnat ically recognized. At~ operational translator would require additional tests at sew;ml points to detect s.ymbol strings not allowed by lhc language. Such tests are omitted here for the sake of clariLy in {,he flow charts. A C K N O W L E D G M E N T The author is indebted to Arthur Anger, presently at Harvard University, for many helpful criticisms and suggestions, and for coding the algorithm on the UNiwxc 1105. REFERENCES 1. ERs[~ov: I)roqrammi~q Programme for the BESM Computer. Pergamon, 1959. 2. WI,ZSG'r~N, J. It. From formulas to computer oriented bmguage. Comm. ACM 2 (Mar. 1959), 6-8. 3. Am)i~:N, B., and (]m~m~M, R. On GAT and the construction of trtmslators. Comm. ACM 2 (July 1959), 24-26. 4. KANNER, H. An algebraic translator. Comm. ACM 2 (Oct. 1959), 19-22, 5. SAMELSO~', K., and BA*mR, F. L. Sequential formula translation. Comm. ACM 3 (Feb. 1960), 76-83.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "bb377e6d5cb0c90396c2006880f1a9a861947d9a", "title": "Learning Games through Pattern Recognition", "authors": ["Elliot B. Koffman"], "date": 1968, "abstract": "The objective of this research was to investigate a technique for machine learning that would be useful in solving problems involving forcing states. In games or control problems a forcing state is one from which the final goal can always be reached, regardless of what disturbances may arise. A program that learns forcing states in a class of games (in a game-independent format) by working backwards from a previous loss has been written. The class of positions that ultimately results in the opponent's win is learned by the program (using a specially designed description language) and stored in its memory together with the correct move to be made when this pattern reoccurs. These patterns are searched for during future plays of the game. If they are formed by the opponent, the learning program blocks them before the opponent's win sequence can begin. If it forms the patterns first, the learning program initiates the win sequence. The class of games for which the program is effective includes Qubic, Go-Moku, Hex, and the Shannon network games, including Bridge-it. The description language enables the learning program to generalize from one example of a forcing state to all other configurations that are strategically equivalent.", "references": ["492d70a8b53ea6ee98c4b84964fe0a859fb67bef"], "page_rank": 4.926108374384236e-05}, {"id": "bbaac3ff0eb5b661faead2748313834c9cf771cf", "title": "Outline for a Logical Theory of Adaptive Systems", "authors": ["John H. Holland"], "date": 1962, "abstract": "The purpose of this paper is to outline a theory of automata appropriate to the properties, requirements and questions of adaptation. The conditions that such a theory should satisfy come from not one but several fields: It should be possible to formulate, at least in an abstract version, some of the key hypotheses and problems from relevant parts of biology, particularly the areas concerned with molecular control and neurophysiology. The work in theoretical genetics initiated by R. A. Fisher [5] and Sewall Wright [24] should find a natural place in the theory. At the same time the rigorous methods of automata theory should be brought to bear (particularly those parts concerned with growing automata [1, 2, 3, 7, 8, 12, 15, 18, 23]). Finally the theory should include among its models abstract counterparts of artificial adaptive systems currently being studied, systems such as Newell-Shaw-Simon's \"General Problem Solver\" [13], Selfridge's \"Pandemonium\" [17], von Neumann's self-reproducing automata [22] and Turing's morphogenetic systems [19, 20]. The theory outlined here (which is intended as a theory and not the theory) is presented in four main parts. Section 2 discusses the study of adaptation via generation procedures and generated populations. Section 3 defines a continuum of generation procedures realizable in a reasonably direct fashion. Section 4 discusses the realization of generation procedures as populations of interacting programs in an iterative circuit computer. Section 5 discusses the process of adaptation in the context of the earlier sections. The paper concludes with a discussion of the nature of the theorems of this theory. Before entering upon the detailed discussion, one general feature of the theory should be noted. The interpretations or models of the theory divide into two broad categories: \"complete\" models and \"incomplete\" models. The \"complete\" models comprise the artificial systems--systems with properties and specifications completely delimited at the outset (cf. the rules of a game). One set of \"complete\" models for the theory consists of various programmed parallel computers. The \"incomplete\" models encompass natural systems. Any natural system involves an unlimited number of factors and, inevitably, the theory can handle only a selected few of these. Because there will always be variables which do not have explicit counterparts in the theory, the derived statements must be approximate relative to natural systems. For this reason it helps greatly that", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "693e791966b333ad5bec096999cea2cce122cd38", "title": "A pattern recognition program that generates, evaluates, and adjusts its own operators", "authors": ["Leonard Uhr", "Charles Vossler"], "date": 1961, "abstract": "This paper describes an attempt to make use of machine learning or self-organizing processes in the design of a pattern-recognition program. The program starts not only without any knowledge of specific patterns to be input, but also without any operators for processing inputs. Operators are generated and refined by the program itself as a function of the problem space and of its own successes and failures in dealing with the problem space. Not only does the program learn information about different patterns, it also learns or constructs, in part at least, a secondary code appropriate for the analysis of the particular set of patterns input to it.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "82b03ff061d8180a27ce3744860c82aabd01e93a", "title": "Similarity relations and fuzzy orderings", "authors": ["Lotfi A. Zadeh"], "date": 1971, "abstract": "The notion of ''similarity'' as defined in this paper is essentially a generalization of the notion of equivalence. In the same vein, a fuzzy ordering is a generalization of the concept of ordering. For example, the relation x @? y (x is much larger than y) is a fuzzy linear ordering in the set of real numbers. More concretely, a similarity relation, S, is a fuzzy relation which is reflexive, symmetric, and transitive. Thus, let x, y be elements of a set X and @m\"s(x,y) denote the grade of membership of the ordered pair (x,y) in S. Then S is a similarity relation in X if and only if, for all x, y, z in X, @m\"s(x,x) = 1 (reflexivity), @m\"s(x,y) = @m\"s(y,x) (symmetry), and @m\"s(x,z) >= @? (@m\"s(x,y) A @m\"s(y,z)) (transitivity), where @? and A denote max and min, respectively. ^y A fuzzy ordering is a fuzzy relation which is transitive. In particular, a fuzzy partial ordering, P, is a fuzzy ordering which is reflexive and antisymmetric, that is, (@m\"P(x,y) > 0 and x y) @? @m\"P(y,x) = 0. A fuzzy linear ordering is a fuzzy partial ordering in which x y @? @m\"s(x,y) > 0 or @m\"s(y,x) > 0. A fuzzy preordering is a fuzzy ordering which is reflexive. A fuzzy weak ordering is a fuzzy preordering in which x y @? @m\"s(x,y) > 0 or @m\"s(y,x) > 0. Various properties of similarity relations and fuzzy orderings are investigated and, as an illustration, an extended version of Szpilrajn's theorem is proved.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "8ef1568b4377fce96f9d350d6d46a619d71a1462", "title": "A model and a system for machine recognition of speech", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard B. Neely"], "date": 1973, "abstract": "This paper presents a model for machine recognition of connected speech and the details of a specific implementation of the model, the HEARSAY system. The model consists of a small set of cooperating independent parallel processes that are capable of helping in the decoding of a spoken utterance either individually or collectively. The processes use the \"hypothesize-and-test\" paradigm. The structure of HEARSAY is illustrated by considering its operation in a particular task situation: voice-chess. The task is to recognize a spoken move in a given board position. Procedures for determination of parameters, segmentation, and phonetic descriptions are outlined. The use of semantic, syntactic, lexical, and phonological sources of knowledge in the generation and verification of hypotheses is described. Preliminary results of recognition of some utterances are given.", "references": [], "page_rank": 0.00025177887246852763}, {"id": "19bfa432237dc1bd82113774727fe0307005e430", "title": "Version Spaces: A Candidate Elimination Approach to Rule Learning", "authors": ["Tom Michael Mitchell"], "date": 1977, "abstract": "An important research problem in artificial intelligence is the study of methods for learning general concepts or rules from a set of training instances. An approach to this problem is presented which is guaranteed to find, without backtracing, all rule versions consistent with a set of positive and negative training instances. The algorithm put forth uses a representation of the space of those rules consistent with the observed training data. This \"rule version space\" is modified in response to new training instances by eliminating candidate rule versions found to conflict with each new instance. The use of version spaces is discussed in the context of Meta-DENDRAL, a program which learns rules in the domain of chemical spectroscopy.", "references": ["b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "cf81287a8c654256fad258844cb58ab1d5afde4c", "6251ac542eb04037f657779037848e727701c338", "ba2995a5d24ed37103ca65b8a5196118b00bff2a", "673b86c4a81571365314f9aa4b925722a8eca459"], "page_rank": 4.926108374384236e-05}, {"id": "fb3ad4bfc20c3f4eacc8c58dffe6fb7bd1ddc5d0", "title": "Generalization Learning Techniques for Automating the Learning of Heuristics", "authors": ["Donald A. Waterman"], "date": 1970, "abstract": "This paper investigates the problem of implementing machine learning of heuristics. First, a method of representing heuristics as production rules is developed which facilitates dynamic manipulation of the heuristics by the program embodying them. Second, procedures are developed which permit a problem-solving program employing heuristics in production rule form to learn to improve its performance by evaluating and modifying existing heuristics and hypothesizing new ones, either during an explicit training process or during normal program operation. Third, the feasibility of these ideas in a complex problem-solving situation is demonstrated by using them in a program to make the bet decision in draw poker. Finally, problems which merit further investigation are discussed, including the problem of defining the task environment and the problem of adapting the system to board games.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "65c4e4b7c054721e7f452319b0f7d02a70acb2ff", "title": "Levels of Pattern Description in Learning", "authors": ["Elliot Soloway", "Edward M. Riseman"], "date": 1977, "abstract": "A learning system in a complex, real-world domain will require a significant amount of knowledge to be used in order to (1) deal with large numbers of features, most of which are irrelevant, and (2) find similarities between the concepts that are inferred from the observed data. Use of knowledge-free, syntactic approaches to generalization in complex environments will result in a combinatorial explosion in the number of possible generalizations. Moreover, the important semantic features are not \"in\" the data; rather they must be hypothesized using prior knowledge. \n \nThe learning system described in this paper uses a multi-level knowledge-directed approach in order to cope with these problems. This paradigm is explored in the action-oriented game of baseball. The system attempts to interpret observed activity in terms of general knowledge provided about competitive games. This approach to learning can be viewed as a type of recognition, where the level of initial knowledge is general and where the specific observations mold a particular structure from the general knowledge. The system is organized into multiple levels of pattern descriptions, processing, and knowledge, reflecting the logical structure of the problem. In moving through those levels of description, the system filters out irrelevant features, hypothesizes additional semantic features (goals and relationships) and forms a hierarchy of generalized classes that extract the similarities in the descriptions. Examples of learning by a working computer program are presented.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6db399b4afd41d29c06bbb88c1de370a4b93f994", "title": "Image compression by back propagation: A demonstration of extensional programming", "authors": ["Garrison W. Cottrell", "Paul W. Munro"], "date": 1987, "abstract": "Semantic Scholar extracted view of \"Image compression by back propagation: A demonstration of extensional programming\" by Garrison W. Cottrell et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f39638342e42959c6819695fa029aebf69340524", "title": "Toward a Universal Characterization of Passivization - eScholarship", "authors": ["David M. Perlmutter", "Paul Martin Postal"], "date": 1977, "abstract": "b Berkeley Linguistics Society Toward a Universal Characterization of Passivization Author(s): David M. Perlmutter and Paul M. Postal Proceedings of the 3rd Annual Meeting of the Berkeley Linguistics Society (1977), pp. 394-417 Please see \u201cHow to cite\u201d in the online sidebar for full citation information. Please contact BLS regarding any further use of this work. BLS retains copyright for both print and screen forms of the publication. BLS may be contacted Via http://linguisticsberkeley.edu/bls/. Tne Annual Proceedings 0//\ufb02e Berke/ey Linguistics Soczezjz is published online Via eLanguage, the Linguistic Society of America's digital publishing platform.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4cd8ac993c1f464ce9a4c8d3e4467ecd03b84f00", "title": "The Sciences of the Artificial", "authors": ["Alex C. Michalos", "Herbert A. Simon"], "date": 1969, "abstract": "Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools -- chaos, adaptive systems, genetic algorithms -- for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter \"Economic Reality\" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems.", "references": [], "page_rank": 0.00018062397372742197}, {"id": "1cd4e2a1c4a503e18c910666fb5bf7f616f65ece", "title": "The Role of Syllables in Speech Processing: Infant and Adult Data [and Discussion]", "authors": ["Jacques Mehler", "Ralph W. Hayes"], "date": 1981, "abstract": "An empirical account is offered of some of the constants that infants and adults appear to use in processing speech-like stimuli. From investigations carried out in recent years, it seems that syllable-like sequences act as minimal accessing devices in speech processing. Ss are aware in real time of syllabic structure in words and respond differently to words with the same initial three phonemes if the segmental one is CV/... and the other CVC/.... Likewise, infants seem to be aware that a 'good' syllable must have at least one alternation if it is composed of more than one phoneme. When the segment is only one phoneme long, its status is necessarily somewhere between that of the phoneme and the syllable. An important problem that arises with the syllable is that it is an unlikely device for speech acquisition. Indeed, there are a few thousand syllables and the attribution of a given token to a type is far from obvious. Even if physical invariants for syllables in contexts were to be found, the task facing the child still remains one of sorting thousands of types from many more tokens. Issues concerning acquisition versus stable performance will be addressed to further constrain possible models. In addition, I try to show that even though information processing models are useful tools for describing synchronic sections of organisms, the elements that can account for development will have to be uncovered in neighbouring branches.", "references": ["0b3cecf00134ef9e7a54c01e737b2978a5df02a8", "8ce8fdec05ee3c53717335e51adacf8214f5a5df", "005f89945bee078006a8ee77ac71fd2a40ea6aab", "f7840765a927533e423f10e0c47f56e2a6209150", "89c44be803674adf94f2cc1453e1c33d2dd5fc53", "5fbc4400067fcc81a0f3993ed8d87fc4b6a37f2c", "96affee023e1fd031b45cfef8b4ee6da61ac55fe", "c1b12377e84b0de7b51f72f3f5c0083e79a52e36", "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "8a146183193105f75e56200543ec549eb3947b52"], "page_rank": 8.210180623973726e-05}, {"id": "ff8117dcddf1bdc6c4df7ae2daea0a91c74893b7", "title": "The 1976 modular acoustic processor(MAP)", "authors": ["N. Rex Dixon", "Harvey F. Silverman"], "date": 1977, "abstract": "The modular acoustic processor (MAP), a complex experimental system for automatic derivation of phonemic string output for continuous speech, has stages dedicated to signal analysis, spectral classification, phonemic segmentation, phonemic (steady state) classification, phoneme boundary placement, dyadic (transitional) classification, and final phoneme string consolidation. This paper presents the concepts of and some details concerning these five stages. Results on a large body of continuous speech data, prepared by an automatic evaluation system, will also be presented.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b02af0721ab87071d954c0ad221cb2e49a65003b", "title": "Unaccusatives in Dutch and the Syntax-Semantics Interface", "authors": ["Annie Zaenen"], "date": 1988, "abstract": "Well tools comprising a go-devil actuated well safety valve, a locking assembly for releasably locking the safety valve at a desired depth in a well, running and pulling tools for installing and removing the safety valve, a go-devil ball for closing the safety valve, and apparatus for dropping the go-devil ball in a well under pressure and for retrieving the ball. The go-devil safety valve is mounted above the locking assembly and includes a trigger type latch which is released from above by the impact of the go-devil ball. The valve may be reset for reopening the valve without removal of the valve from the well bore by means of a special reset and pulling tool disclosed herein. The go-devil valve is installed in a well, preferably above a storm choke, to shut the well in under emergency conditions which releases the go-devil ball at the surface in response to hazardous conditions such as fire. The go-devil ball drops to the go-devil valve which closes in response to the impact of the ball.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "2da1dd84c0679f0687a1f3e239f266c4396f4526", "title": "Context-sensitive coding, associative memory, and serial order in (speech) behavior.", "authors": ["Wayne A. Wickelgran"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Context-sensitive coding, associative memory, and serial order in (speech) behavior.\" by Wayne A. Wickelgran", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "629a902def4fae97e47505fced2f800a583ad4ca", "title": "Word Formation in Generative Grammar", "authors": ["Mark Aronoff"], "date": 1976, "abstract": "Aronoff integrates an account of morphological structure into a general theory of generative grammar.", "references": [], "page_rank": 0.00015247478301665492}, {"id": "bda4125a131c25f11c3e193de083718a7c6237c7", "title": "On-line recognition of spoken words from a large vocabulary", "authors": ["Teuvo Kohonen", "Heikki Riittinen", "Erkki Reuhkala", "Seppo Haltsonen"], "date": 1984, "abstract": "Abstract It is demonstrated in this paper that a real-time, large-vocabulary, isolated-word speech recognition system can effectively be implemented using the following two-stage organization: 1. (1) conversion of the speech signal into phonemic transcriptions, 2. (2) recognition of phonemic transcriptions by advanced searching methods. A comparison of several alternatives for the first stage has indicated that the best accuracy is achieved by the learning-subspace method For the second stage the authors recommend fast string searching by redundant hash addressing combined with subsequent probabilistic analysis. The above system has been implemented in a minicomputer environment.", "references": ["901a86a283f20b0abe987751e5c5beed455a8e6e", "a0d51e0d812186dc583274847dad648e582bd0e7", "5c3ae1375219260a90d4e1a2158373463ad646f8", "8681ffde63e85aa1e204a6d86605dce82e8075ad", "2543dea11cbfce22c38fc2f57bd1dc3c502794b1", "163cf4b6095cf2933c2adfe046dded522e9f94df"], "page_rank": 8.210180623973726e-05}, {"id": "2d29fa654812c44fd3285856d46fef18812bf23c", "title": "On the definition of word", "authors": ["Sophia Ananiadou"], "date": 2004, "abstract": "In the four chapters of this monograph, the authors define four distinct notions of \u2018word\u2019: listeme, morphological object, syntactic atom and phonological word. The last is not discussed to any great extent. Chapter 1 defines the notions of \u201clistedness\u201d and \u201clisteme,\u201d The former is close to that of lexicalization. This implies that the lexicon is a collection of listed items which do not have predictable properties. Morphology and syntax both have lists too: the lists of primes, which are the words in syntax and the morphemes in morphology. Idioms are examples of listed objects in syntax. There is a hierarchy of listedness for the lexicon, as follows (p. 14):", "references": ["47fbbe0c1ead3106e33b4eb9776d02faf2f02769", "1367557ddd813617de6a4f8e002352f27e1ae384", "e15fd13dfccc59dee5c073eb3d28c22e1f20733c", "9bf778e4fafdbdab502bc5cf45982643f67914f4", "df9d5d2eefe8b73cb964a2bc168900282e02eaf4"], "page_rank": 6.157635467980295e-05}, {"id": "e40e4a3f4cffa628e98097107f16be69dcf81262", "title": "Generalized quantifiers and natural language", "authors": ["John Barwise", "Robin Cooper"], "date": 1981, "abstract": "In 1957, the Polish logician Andrej Mostowski pointed out that there are many mathematically interesting quantifiers that are not definable in terms of the first-order \u2200, \u2203 and initiated study of so-called generalized quantifiers (cf. Mostowski, 1957). Since then logicians have discovered and studied a large number of generalized quantifiers. At last count there were well over 200 research papers in this area. Most of this work has been directed toward cardinality quantifiers (e.g. Keisler, 1969) and topological quantifiers (e.g. Sgro, 1977) which are not particularly relevant to natural language, but even so, it has forced logicians to rethink the traditional theory of quantification.", "references": [], "page_rank": 0.00018472906403940885}, {"id": "24c56e382fc74b483214f949b0793284668a416b", "title": "Research on Interactive Acquisition and Use of Knowledge.", "authors": ["Mark E. Stickel"], "date": 1983, "abstract": "Abstract : SRI International is engaged in a long-term research effort under DARPA sponsorship to develop techniques for facilitating the acquisition of knowledge of computers. The ultimate goal is to build KLAUS(Knowledge-Learning and -Using System), a computer program that could acquire a model of a domain of interest by being instructed in English. The model could then be used for domain-related tasks. Systems based on the KLAUS concept would be useful for a variety of applications, including sophisticated interfaces to computer software, advanced database systems, and intelligent computer assistants. This report covers work done on the KLAUS project during the period July 1981 to January 1982. Section 2 describes DIALOGIC, the natural-language processing system used in MICROKLAUS, which is our current preliminary implementation of the KLAUS concept. The dialogic system translates English sentences into representation of their literal meaning in the context of an utterance. Section 3 describes a new scheme for syntax-directed translation of natural language into logical form. That scheme, comprising the basis of an English translation system called PATR, was used to specify a semantically interesting fragment of English that included such constructs as tense, aspect, modals, and various lexically controlled verb complement structures.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "title": "Generalized Phrase Structure Grammar", "authors": ["Gerald Gazdar", "Ewan Klein", "Geoffrey K. Pullum", "Ivan A. Sag"], "date": 1985, "abstract": "\"Generalized Phrase Structure Grammar\" provides the definitive exposition of the theory of grammar originally proposed by Gerald Gazdar and developed during half a dozen years' work with his colleagues Ewan Klein, Geoffrey Pullum, and Ivan Sag. This long-awaited book contains both detailed specifications of the theory and extensive illustrations of its power to describe large parts of English grammar. Experts who wish to evaluate the theory and students learning GPSP for the first time will find this book an invaluable guide.The initial chapters lay out the theoretical machinery of GPSP in a readily intelligible way. Combining informal discussion with precise formalization, the authors describe all major aspects of their grammatical system, including a complete theory of syntactic features, phrase structure rules, meta rules, and feature instantiation principles. The book then shows just what a GPSP analysis of English syntax can accomplish. Topics include the internal structure of phrases, unbounded dependency constructions of many varieties, and coordinate conjunction a construction long considered the sticking point for phrase structure approaches to syntax.The book concludes with a well developed proposal for a model theoretic semantic system to go along with GPSP syntax. Throughout, the authors maintain the highest standards of explicitness and rigor in developing and assessing their grammatical system. Their aim is to provide the best possible test of the hypothesis that syntactic description can be accomplished in a single-level system. And more generally, it is their intention to formulate a grammatical framework in which linguistic universals follow directly from the form of the system and therefore require no explicit statement. Their book sets new methodological standards for work in generative grammar while presenting a grammatical system of extraordinary scope.\"", "references": [], "page_rank": 0.00022577996715927748}, {"id": "d7bb69a83a0d36da6d34a0ab9f58748a5e77c6de", "title": "A propos des \u00e9nonc\u00e9s exclamatifs", "authors": ["Antoine Culioli"], "date": 1974, "abstract": "Culioli Antoine. A propos des enonces exclamatifs. In: Langue francaise, n\u00b022, 1974. Linguistique et enseignement du francais, sous la direction de Simone Delesalle et Helene Huot. pp. 6-15.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "65905515e212cf22657aabaddfeee5aa82cc7691", "title": "Pragmatics: Implicature, Presupposition, and Logical Form", "authors": ["Gerald Gazdar"], "date": 1978, "abstract": "Semantic Scholar extracted view of \"Pragmatics: Implicature, Presupposition, and Logical Form\" by Gerald Gazdar", "references": [], "page_rank": 0.00016420361247947453}, {"id": "b49931a2caf1dbfad15cc85e5ea0df99e976d017", "title": "CV Phonology: A Generative Theory of the Syllable", "authors": ["George N. Clements", "Samuel Jay Keyser"], "date": 1983, "abstract": "This work introduces a new approach to syllable representation. It proposes an additional level of phonological representation, the CV-tier; which defines functional positions within the syllable. The first three chapters provide an explanation of and support far this new approach from a typologically varied selection of languages, including English, Turkish, Finnish, French, Spanish, and Danish. The last two chapters are devoted to an in-depth application of the theory of Klamath, showing that a radical simplification of the phonological rules of that language is made possible in terms of this new framework. The book constitutes the first full-scale phonological justification for the CV-tier. George N. Clements is Associate Professor in the Linguistics Department at Cornell University and co-author, along with Morris Halle, of the recent MIT Press/ Bradford Books publication, \"Problem Book in Phonology. \"Samuel Jay Keyser is Head of the Department of Linguistics and Philosophy at MIT and editor of the Linguistic Inquiry Monograph Series.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "60c551652f4cba4db1555cccc472b9a7c704b5b8", "title": "Paradigmatic Morphology", "authors": ["Jonathan Calder"], "date": 1989, "abstract": "We present a notation for the declarative statement of morphological relationships and lexical rules, based on the traditional notion of Word and Paradigm (cf Hockett 1954). The phenomenon of blocking arises from a generalized version of Kiparsky's (1973) Elsewhere Condition, state in terms of ordering by subsumption over paradigms. Orthographic constraints on morphemic alternation are described by means of string equations (Siekmann 1975). We indicate some criticisms to be made of our approach from both linguistic and computational perspectives and relate our approach to others such as Finite-State Morphology (Koskenniemi 1983), DATR (Gazdar and Evans 1989) and object-oriented morphophonemics (de Smedt 1984, Daelemans 1988). Finally, we discuss the questions of whether a system involving string equations allows a reduction to finite-state techniques.", "references": ["5a5412833fc2cc1f4b7fdbda7d982688114e2fa6", "c9f347143e43ff7273f32e7695cf566e023f4b4f", "e11ce10eff56df1729626bda3ef6109566f779c9", "1dfae23c8825e5afcf1c422bbad950d4b9f6fbcd", "f10edd0e00c9ab10ca089c1ec9f23e81e48fb1af", "80e2eca3788df619da5061ca21d3dfe53dc65122", "f98a4e6ce57fab19b282141bbb4c4ab2ad10cba0", "40dbb25a15b63af3faccb81c8e64a3f5d659e07e", "6e11d278f28b82fd0012995724714fdc2c4b1b79", "39b30d0a5ac6894b503c9097f60eaa4a516590fe"], "page_rank": 6.157635467980295e-05}, {"id": "7fd60f655c0a2c506a3071db86999444821b698d", "title": "Course notes", "authors": ["John ffitch"], "date": 1975, "abstract": "Algebraic manipulation covers branches of software, particularly list processing, mathematics, notably logic and number theory, and applications largely in physics. The lectures will deal with all of these to a varying extent. The mathematical content will be kept to a minimum.", "references": ["79e3f152bbaf835286a90f4f257d39840549b22b"], "page_rank": 8.210180623973726e-05}, {"id": "f461c5b2f209ddcadfb4eb79e3ce65eaae7ab5cf", "title": "Expressing generalizations in unification-based grammar formalisms", "authors": ["Marc Moens", "Jonathan Calder", "Ewan Klein", "Mike Reape", "Henk Zeevat"], "date": 1989, "abstract": "This paper shows how higher levels of generalization can be introduced into unification grammars by exploiting methods for typing grammatical objects. We discuss the strategy of using global declarations to limit possible linguistic structures, and sketch a few unusual aspects of our type-checking algorithm. We also describe the sort system we use in our semantic representation language and illustrate the expressive power gained by being able to state global constraints over these sorts. Finally, we briefly illustrate the sort system by applying it to some agreement phenomena and to problems of adjunct resolution.", "references": ["d25e6f7b8f3a24434957c87c7e449beaffe3ce46", "2377d242b7beb20f12e7755ec3deeb7ee2dc9679", "f0dbb7a1ecd8c88babb797c6cfac9195785c5c6d", "5a43a6f89cc89b8d513f9b7b5f50b969400041b7", "6fae00c0401e66bbf6b2e4992864614b9102d521", "ad00212dec795217f299b671ef80f17867ba4a12", "281ac2d6d0138b6e1b9df544e005dbc45a984d73", "19960f19c8c26f59e12dd820c7e920caf251b4c1", "edd8d454a9fa8163f6bc6918e06c5ef93c784bfd"], "page_rank": 8.210180623973726e-05}, {"id": "e11ce10eff56df1729626bda3ef6109566f779c9", "title": "A General Computational Model For Word-Form Recognition And Production", "authors": ["Kimmo Koskenniemi"], "date": 1984, "abstract": "A language independent model for recognition and production of word forms is presented. This \"two-level model\" is based on a new way of describing morphological alternations. All rules describing the morphophonological variations are parallel and relatively independent of each other. Individual rules are implemented as finite state automata, as in an earlier model due to Martin Kay and Ron Kaplan. The two-level model has been implemented as an operational computer programs in several places. A number of operational two-level descriptions have been written or are in progress (Finnish, English, Japanese, Rumanian, French, Swedish, Old Church Slavonic, Greek, Lappish, Arabic, Icelandic). The model is bidirectional and it is capable of both analyzing and synthesizing word-forms.", "references": ["008da127af8fbdfdd9a3f7d36b23d7c3946e491b", "119ba277a0aeffc8171916756adbe9844b9ff71a", "ba817882de1a7093ee83bfa14a4005e5c8a5977e", "f4bbc00fdbcd8b96010d1d8db94f3bdf0e03ac48", "0271828a2b08a5fc990e78d0f6803fa2a6a290b1", "2bad5215293ee18cd251c924ffc4ae3280935102", "88b50f5be70b1c2fe40d7cc6bcf478a9d92307f3", "5bedc10d045b5594159cd7f2a1847a94d2e6a493"], "page_rank": 0.0003694581280788177}, {"id": "503817d0b66bf379465c3cdccbcffc524a4fe4f5", "title": "Natural Language Understanding and Logic Programming", "authors": ["Kepa Mirena Sarasola Gabiola", "Ana M. Garc{\\'i}a-Serrano"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Natural Language Understanding and Logic Programming\" by Kepa Mirena Sarasola Gabiola et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "9e7c09943444bc96f06ff92919c62cd79bb29066", "title": "Lexical semantics in review", "authors": ["Beth Levin"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Lexical semantics in review\" by Beth Levin", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "eb7e14ee7a3f95afd6801f4356777d9dfd540f12", "title": "On Stochastic Languages", "authors": ["Paavo Turakainen"], "date": 1968, "abstract": "Some properties of languages representable in finite probabilistic automata are investigated. It is shown that the cut-point can always be changed arbitrarily. The notion of a probabilistic automaton is generalized, and it is shown that this does not affect the family of representable languages. Generalized probabilistic automata are then used for two applications. Finally, some results concerning non-regular stochastic languages over a one-letter alphabet are mentioned.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "70785ff72f4ec20ddfc16399252ffc41c09f89d3", "title": "A government-binding parser for french", "authors": ["Eric Wehrli"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"A government-binding parser for french\" by Eric Wehrli", "references": [], "page_rank": 0.00016420361247947453}, {"id": "9806348927e1e642c362aa83065e7380a06c2282", "title": "Dictionaries, Dictionary Grammars and Dictionary Entry Parsing", "authors": ["Mary S. Neff", "Branimir Boguraev"], "date": 1989, "abstract": "We identify two complementary processes in the conversion of machine-readable dictionaries into lexical databases: recovery of the dictionary stucture from the typographical markings which persist on the dictionary distribution tapes and embody the publishers' notational conventions; followed by making explicit all of the codified and ellided information packed into individual entries. We discuss notational conventions and tape formats, outline structural properties of dictionaries, observe a range of representational phenomena particularly relevant to dictionary parsing, and derive a set of minimal requirements for a dictionary grammar formalism. We present a general purpose dictionary entry parser which uses a formal notation designed to describe the structure of entries and performs a mapping from the flat character stream on the tape to a highly structured and fully instantiated representation of the dictionary. We demonstrate the power of the formalism by drawing examples from a range of dictionary sources which have been processed and converted into lexical databases.", "references": ["32c280f291fa99dbdacc925b767cfecf8d2cbfce", "bb8b4e7c43459bf69ce8518797be216c7f9248c6", "1dfae23c8825e5afcf1c422bbad950d4b9f6fbcd", "4d58fbb388884caf78a0652a02ee279883d86f75", "e4bb51e9600c06d0da49e0e1aeddb8d856fd1558", "91e0a80cefb3fbfdb8c9b39780892db087f37516", "a1d58af78b248154ec5c3d7ce96a97d85d3b68bd"], "page_rank": 8.210180623973726e-05}, {"id": "65ba5b234501960b5d0356d4c409edea38b66891", "title": "Toward a Linguistic Theory of Speech Acts", "authors": ["Bernard Comrie", "Jerrold M. Sadock"], "date": 1975, "abstract": "Medicated compositions which contain as active ingredients meglumine complexes of fungicidal polyene macrolide antibiotics and treatment method utilizing these compositions is described. These compounds possess fungicidal and protistocidal activity and are useful in the treatment of candidoses and aspergilloses. Additionally, meglumine complexes of amphotericin B and mycoheptine can be used for treating most systemic mycoses by way of oral and inhalation administration as well as by instillation, and for treating leishmaniasis, schistosomiasis, lambliasis and trichomoniasis.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "0b380f641e115ad426984079f504b21aea9636c5", "title": "Ontological Promiscuity", "authors": ["Jerry R. Hobbs"], "date": 1985, "abstract": "To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems - opaque adverbials, the distinction between de re and de dicto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach.", "references": ["033ecd544c4e71b14a7d3ee0611a300a20b91232", "cf809cafdd31e9542cbf2393756c0d73626e026d", "6a6bfc2d27fe3c3a88d8fd5c21d9c3e2cc93ccaf"], "page_rank": 8.210180623973726e-05}, {"id": "492d70a8b53ea6ee98c4b84964fe0a859fb67bef", "title": "Dynamic programming and the calculus of variations", "authors": ["Stuart E. Dreyfus"], "date": 1960, "abstract": "Abstract Problems in the Calculus of Variations can be viewed as multistage decision problems of a continuous type. It follows that their solutions can be characterized by the functional equation technique of dynamic programming [1]. In this paper, it will be shown that the functional equation approach yields, in simple and intuitive fashion, formal derivations of such classical necessary conditions of the Calculus of Variations as the Euler-Lagrange equations, the Weierstrass and Legendre conditions, natural boundary conditions, a transversality condition and the Erdmann corner conditions. The more general \u201cproblem of Bolza\u201d in which the final time is defined implicitly and in which the expression to be extremized is the sum of an integral and a function evaluated at the end point is also considered. The principal necessary condition, usually called the \u201cmultiplier rule,\u201d is deduced. We shall also derive necessary conditions for the case where the decision variables are restricted by inequality constraints. Finally, it is shown that the functional equation characterization readily yields the Hamilton-Jacobi partial differential equation of classical mechanics.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "de3c9fb765761551ddcac1689530d9df6303a2fc", "title": "The role of agreement in clitic-doubled constructions", "authors": ["Margarita Su{\\~n}er"], "date": 1988, "abstract": "Summary and ConclusionWe began with the premise that doubling with DO- and IO-CLs requires that both elements of the chain agree in features. With IOs, doubling is universal because the features of the IO argument are practically irrelevant (section 1.1). However, since DO-CLs are inherently specific, this feature is crucial for DO-doubling to be grammatical. Animacy is the second feature necessary in most contexts. However, as pointed out in section 1.3.1, doubling may operate even in the absence of the latter. The same matching of features required of doubling constructions is required under extraction (a subset of doubling structures), hence, no special mechanisms are needed to reject ill-formed sentences. What causes ungrammaticality in extractions from CL-D DOs is a clash between the specific referential CL and a nonspecific interpreted object argument, a violation of the Matching Principle. Since lexical partitives and unagreement phenomena are productive ways to signal the specificity of the DO, extractions at LF and in the syntax proper can produce well-formed sentences (section 2.2). Moreover, the Matching Principle follows directly from the unique indexing peculiar to chain coindexing and Spec-head agreement.These results have two main consequences: (a) the hypothesis that CLs are agreement morphemes which do not absorb Case becomes viable. This claim is supported by the doubling of inanimates (section 1.3.1) and even of some animates (17) in the absence of a, by facts about Case with IOs (section 1.3.2), and by weak cross-over effects (section 3.1) where CLs-as-agreement serve to identify the relevant empty category. (b) Since extractions of both CL-D IOs (section 2.1) and CL-D DOs (section 2.2) are possible, it follows that CLs are not theta-role absorbers, because the doubled constituent must be in an argument position. The moral is that the absence of extraction does not itself show that the double is in an A' position. Rather one must look deeper to discover whether independent principles in the language can account for lack of extraction in such contexts.For extensions of our main hypothesis to other Spanish dialects see Su\u00f1er (1986).\nFinally, the assumption that datives in a CL-chain are NPs and not PPs (see Appendix) makes possible the generalization that contemporary Spanish CLs \u2014 whether direct or indirect \u2014 enter only into nominal chains, a generalization confirmed by their parallel behavior with respect to weak crossover and scope (sections 3.1\u20133.2). This conclusion brings the analysis of Spanish in line with those of French and Rumanian (Steriade 1980/81) where datives have been argued to be NPs, and it opens the door for the possibility that datives in all the Romance languages might be nominal in nature.Interestingly, datives are also NPs in Hebrew (Borer and Grodzinsky 1986).", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "de665704b908487341c707a564248095763697c7", "title": "Analysing the dictionary definitions", "authors": ["Hiyan Alshawi"], "date": 1989, "abstract": "Semantic Scholar extracted view of \"Analysing the dictionary definitions\" by Hiyan Alshawi", "references": [], "page_rank": 0.00016420361247947453}, {"id": "6e4499450a6203c0c043a922f4140180ea6dc4df", "title": "The structure of the merriam-webster pocket dictionary", "authors": ["Robert A. Amsler"], "date": 1980, "abstract": "The structure of a dictionary, in this case the Merriam-Webster Pocket Dictionary (1970 edition), has been revealed through a series of computational operations augmented with human interpretation. The following dissertation will attempt to describe the details of this investigation, its procedures, and conclusions with regard to lexical structure. \nThe structural description of the dictionary as a natural language artifact requires both an appreciation of the lexicogragher's design for the dictionary and the less apparent underlying organization which the dictionary suggests for the English lexicon. The dictionary is neither a purely formal description of the language, nor a casual assemblage of textual definitions. It has structure both as an artifact of the lexicographer's art, and more importantly as a result of the lexicographer's shared semantic and pragmatic knowledge of the world used in writing definitions. Because definitions posses a somewhat formal syntax they are capable of human analysis. Because there are tens of thousands of definitions involved it becomes necessary to use computational techniques to augment this human analysis. This might serve as an informal definition of computational lexicology, i.e. the application of computational techniques to facilitate human analysis of the structure of the dictionary. \nThe dictionary is shown to have a fundamentally taxonomic organization for nouns and verbs. Because the dictionary is a closed system, i.e. words used in definitions are themselves elsewhere defined in the dictionary, definitions naturally terminate in circular clusters. These clusters constitute the primitive concepts of the language and the exposition of their existence and members provides insights into Enlgish language semantics. The dictionary is an acyclic semi-lattice once allowance for these primitive terminal clusters is made. A maximal depth of less than 20 and widths of a few hundred senses are present. \nThe dictionary is seen as a profitable subject for future exploration and potentially useful in a multitude of tasks in computational linguistics, artificial intelligence and cognitive science. Techniques for providing detailed descriptions of semantic domains of nouns and verbs are described together with case studies of the verbs of motion and the vehicle nouns. The potential use of dictionary data for automatic disambiguation is discussed.", "references": [], "page_rank": 0.0003448275862068965}, {"id": "cf81287a8c654256fad258844cb58ab1d5afde4c", "title": "On generality and problem solving: a case study using the DENDRAL program", "authors": ["Edward A. Feigenbaum", "Bruce G. Buchanan", "Joshua Lederberg"], "date": 1970, "abstract": "Heuristic DENDRAL is a computer program written to solve problems of inductive inference in organic chemistry. This paper will use the design of Heuristic DENDRAL and its performance on different problems for a discussion of the following topics: 1. the design for generality; 2. the performance problems attendant upon too much generality; 3. the coupling of expertise to the general problem solving processes; 4. the symbiotic relationship between generality and expertness, and the implications of this symbiosis for the study and design of problem solving systems. We conclude the paper with a view of the design for a general problem solver that is a variant of the \"big switch\" theory of generality.", "references": ["77078f3a12baf36e7766f068695a97f90ab91089", "69ea38287cf31f3b8b2233365026d15ec007b778", "63ad3d9193c2734d0f12fbf2dbb6957d4ff044d4", "c38015704ff1ffc1e5708bcce82093f22e8637bc", "477c89e2d9ce60716a59f3bf439266a5ba4353b5", "e6f54b0faa9aab28d9ca1a96e451ecf9ae0a5b29"], "page_rank": 0.0002627257799671592}, {"id": "673b86c4a81571365314f9aa4b925722a8eca459", "title": "Problem solving and rule induction: a unified view", "authors": ["Gordon B. Lea", "Herbert A. Simon"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Problem solving and rule induction: a unified view\" by Gordon B. Lea et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "6251ac542eb04037f657779037848e727701c338", "title": "Model-directed learning of production rules", "authors": ["Bruce G. Buchanan", "Tom Michael Mitchell"], "date": 1977, "abstract": "The Meta-DENDRAL program is described in general terms that are intended to clarify the similarities and differences to other learning programs. Its approach of model-directed heuristic search through a comptex space of possible rules appears well suited to many induction tasks. The use of a strong model of the domain to direct time rule Eearch has been demons{rated for rule formation in two areas of chemistry. The high performance of programs which use the generated rules attests to the success of this learning strategy.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "b884c8982258c0c20a86cccdbc30a03b44dcd2e8", "title": "KNOWLEDGE RESOURCE TOOLS FOR ACCESSING LARGE TEXT FILES", "authors": ["Donald E. Walker"], "date": 2007, "abstract": "This paper provides an overview of a research program just being defined at Bellcore. The objective is to develop facilities for working with large document collections that provide more refined access to the information contained in these \"source\" materials than is possible through current information retrieval procedures. The tools being used for this purpose are machine-readable dictionaries, encyclopedias, and related \"resources\" that provide geographical, biographical, and other kinds of specialized knowledge. A major feature of the research program is the exploitation of the reciprocal relationships between sources and resources. These interactions between texts and tools are intended to support experts who organize and use information in a workstation environment. Two systems under development will be described to illustrate the approach: one providing capabilities for full-text subject assessment; the other for concept elaboration while reading text. Progress in the research depends critically on developments in artificial intelligence, computational linguistics, and information science to provide a scientific base, and on software engineering, database management, and distributed systems to provide the technology.", "references": ["c1ca849fc2c4dc984220c2263ff4293048a38acf", "7edc83b502981fcbca2ef0df12934477eb6e3c14", "6e4499450a6203c0c043a922f4140180ea6dc4df", "745eb34b5b341af868bbf1a87ea9dd03110c1cde", "dc1f402652bb68fea55ab3d4e3c23e2ae2f19f83", "557f25f3d74024d46ba55fb53c5fe6e29d7b26d9", "54d7ebff62332867fbece408b353a5c6367d52f3", "d8d2f856d9ded83f6d62687ad2544b401a5eeb59", "b7780f292c48e504e3a2724e54c205e6c6221932"], "page_rank": 0.0001231527093596059}, {"id": "ba2995a5d24ed37103ca65b8a5196118b00bff2a", "title": "Mycin: computer-based medical consultations", "authors": ["Edward H. Shortliffe"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Mycin: computer-based medical consultations\" by Edward H. Shortliffe", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "title": "The nonperceptual reality of the phoneme", "authors": ["Harris B. Savin", "Thomas G. Bever"], "date": 1970, "abstract": "Subjects responded as soon as they heard a preset target in a sequence of nonsense syllables. The target was a complete syllable (e.g., \u201cbaeb\u201d \u201csaeb\u201d) or a phoneme from that syllable, the syllable-initial consonant phoneme for some objects (e.g., \u201cb-\u201d or \u201cs-\u201d), and the medial vowel phoneme for other subjects (e.g., \u201c-ae-\u201d). Subjects responded more slowly to phoneme targets than to syllable targets (by 40 msec for /s-/, 70 msec for /b-/ and 250 msec for medial /ae/). These results indicate that phoneme identification is subsequent to the perception of larger phonological units. The reality of the phoneme is demonstrated independently of speech perception and production by the natural presence of alphabets, rhymes, spoonerisms, and interphonemic contextual constraints.", "references": [], "page_rank": 0.0007813355227148329}, {"id": "76885f51b47946cb1a611b691d6c60fb936e2215", "title": "Artificial intelligence : the heuristic programming approach", "authors": ["James R. Slagle"], "date": 1971, "abstract": "Give us 5 minutes and we will show you the best book to read today. This is it, the artificial intelligence the heuristic programming approach that will be your best choice for better reading book. Your five times will not spend wasted by reading this website. You can take the book as a source to make better concept. Referring the books that can be situated with your needs is sometime difficult. But here, this is so easy. You can find the best thing of book that you can read.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "8a146183193105f75e56200543ec549eb3947b52", "title": "Phonemic identification in a phoneme monitoring experiment: The variable role of uncertainty about vowel contexts", "authors": ["David A. Swinney", "Penny Prather"], "date": 1980, "abstract": "Subjects monitored for the syllable-initial phonemes /b/ and /s/, as well as for the syllables containing those phonemes, in lists of nonsense syllables. Time to detect /b/ was a function of the amount of uncertainty as to the identity of the vowel following the target consonant; when uncertainty was low, no difference existed between phoneme and syllable monitoring latencies, but when uncertainty was high, syllables were detected faster than phonemes. Time to detect /s/ was independent of uncertainty concerning the accompanying vowel and was always slower than syllable detection. The role of knowledge of contexts in a phoneme-monitoring task as well as the relative availability of phonemic information to the listener in this task are discussed.", "references": ["b96df0e70e7c9aa041c38996c63e57dbb200efc6", "5caf18bdbf00efa2e962dba79b059eaf8b91b66e", "5fbc4400067fcc81a0f3993ed8d87fc4b6a37f2c", "89c44be803674adf94f2cc1453e1c33d2dd5fc53", "730d385eb8b09a409da771650991c45188f79f18", "9b87d25efab3317955fc5cb1469ba0942b44ce26", "9dd596122b6762223ece10bfd7750b980da34211", "a6b42f24563b360b5537eef9ecd0cedaf92bfce4", "6e58cdf792cb09746361e65105d0866b88bb233a", "9def9de39fa3a2d887977fe5b1683e268e2210d7"], "page_rank": 0.0001231527093596059}, {"id": "c1b12377e84b0de7b51f72f3f5c0083e79a52e36", "title": "Some properties of auditory memory for rapid formant transitions", "authors": ["Peter Howell", "C. J. Darwin"], "date": 1977, "abstract": "In the three experiments reported here, subjects indicate whether two sequentially presented syllables, differing in the place of articulation of an initial stop consonant, are phonernically the same or not. The first experiment finds faster |ldsame\u201d responses to acoustically identical pairs than to pairs that are phonemically identical but acoustically distinct. provided that the second syllable is presented within 400 msec of the first. This is interpreted as indicating the persistence of a memory which preserves auditory information about within-category distinctions. The third experiment shows that this advantage remains when a tone is interposed between the two syllables, but is removed when a brief vowel is similarly interposed. The second experiment presents the second syllable of each pair dichotically with a noise burst, and shows that the size of the right-ear advantage for this reaction time task is reduced when the result of comparisons based on this auditory memory is compatible with the required phonemic decision, but that the right-ear advantage is increased when auditory comparisons would contradict the phonemic relationship.", "references": ["91cdee14248dec00916fcece3fd7828d734dd158", "d8caa2e0e9280023e75a6a1ae97490727fb4345e", "aa370f4a46436aff84044f9241324c29a4c4b18b", "0711bfd715844059c00574db4818597435d07b45", "694b448c1391a9c25ae43a0a744a5d189dffa6dc", "b493446485de82fe0a0e42d392add4b793b616d3", "aa648e3ad1c906d0ec33810ecabe1b4420c6bfff", "3f3a73f890a4e30f6fc981d037602f2c5db10424", "aaa99347d6c480b87b6b6391df7b7d65454c3d4a", "4823845b942fd6570d657650a21de01c240522b9"], "page_rank": 0.0001231527093596059}, {"id": "5c3ae1375219260a90d4e1a2158373463ad646f8", "title": "Improvement and comparison of three phonemic segmentation methods of speech", "authors": ["Seppo Haltsonen"], "date": 1981, "abstract": "In this paper phonemic segmentation methods of speech are studied in which the segmentation is performed by using the extrema of a time-varying signal. In most cases the number of extrema in the segmentation signal and thus also the number of characters in the phonemic transcription tend to be too large. A new method which utilizes the time behaviour of the segmentation signal and the phonemic labeling result is introduced to decrease the number of phonemic characters. In the practical experiments the performance of the proposed method is evaluated and a comparison of three segmentation methods is presented.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "8681ffde63e85aa1e204a6d86605dce82e8075ad", "title": "A comparison of several speech-spectra classification methods", "authors": ["Harvey F. Silverman", "N. Rex Dixon"], "date": 1976, "abstract": "An important consideration in speech processing involves classification of speech spectra. Several methods for performing this classification are discussed. A number of these were selected for comparative evaluation. Two measures of performance-accuracy and stability-were derived through the use of an automatic performance evaluation system. Over 3000 hand-labeled spectra were used. Of those evaluated, a linearly mean-corrected minimum distance measure, on a 40-point spectral representation with a square (or cube) norm was consistently superior to the other methods.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "df9d5d2eefe8b73cb964a2bc168900282e02eaf4", "title": "Towards a methodology for automatic term recognition. (volumes i and ii) (term banks)", "authors": ["Sofia Ananiadou"], "date": 1988, "abstract": "Semantic Scholar extracted view of \"Towards a methodology for automatic term recognition. (volumes i and ii) (term banks)\" by Sofia Ananiadou", "references": [], "page_rank": 0.0001231527093596059}, {"id": "a0d51e0d812186dc583274847dad648e582bd0e7", "title": "Syntactic Decision Rules for Recognition of Spoken Words and Phrases Using a Stochastic Automaton", "authors": ["Rangasami L. Kashyap"], "date": 1979, "abstract": "This study deals with the design of a syntactic decision rule for recognizing an unknown utterance from a set X. The decision rule is expressed as a function of the character string (CS) derived from the test utterance. To obtain the CS, the waveform of the utterance is divided into a large number of frames of roughly equal duration numbered 1, 2,...,n. The ith symbol in the CS is the phonemic symbol obtained by subjecting the ith frame of the waveform to a relatively simple phoneme decision rule, the number of symbols in the CS being n. All the available nonacoustic information such as the lexicon of words in the set X, the possibility of confusion between different phonemes as seen by the phoneme decision rule, etc. is used in the design of the decision rule. The syntactic decision rule can be implemented by a stochastic finite state automaton involving limited memory and computation. The decision rule can also be interpreted as yielding the phrase x which minimizes a distance measure D(x, z) between the phrase x X and the observed CS z. We wili compare this approach with the other approaches such as the Viterbi methods, the distance approaches involving various types of distances, etc.", "references": ["81ebb6805f4d30b2007241fa13d6d91a568b8f09", "c8adfa952cda07e2fb79a30ec398ac20765f5e57", "5f6070891aa179e1342dfa9d69868ad2c691211e", "901a86a283f20b0abe987751e5c5beed455a8e6e", "848bf8ed51d0dfbef7f7b06b7728d0aab2ff3dc5", "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "abf4d654c0150ca98f86fe175e097eed22de74fe", "ff451cd5f42206d5abc4c4422b7ce9b58a27cdba", "8377fc1fff7eb8073015c2227aba98bd0c741db2", "8010d8a379961b502ac14240604614606469ddbe"], "page_rank": 0.0001231527093596059}, {"id": "163cf4b6095cf2933c2adfe046dded522e9f94df", "title": "A Method for the Correction of Garbled Words Based on the Levenshtein Metric", "authors": ["Teruo Okuda", "Eiichi Tanaka", "Tamotsu Kasai"], "date": 1976, "abstract": "In this paper we propose a new method for correcting garbled words based on Levenshtein distance and weighted Levenshtein distance. We can correct not only substitution errors, but also insertion errors and deletion errors by this method. According to the results of simulation on nearly 1000 high occurrence English words, higher error correcting rates can be achieved by this method than any other method tried to date. Hardware realization of the method is possible, though it is rather complicated.", "references": ["5f6070891aa179e1342dfa9d69868ad2c691211e", "e28b31213c56249dfca833586d3ca1e975aa9f37"], "page_rank": 0.0001231527093596059}, {"id": "e15fd13dfccc59dee5c073eb3d28c22e1f20733c", "title": "Morphology and logical form", "authors": ["David Pesetsky"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Morphology and logical form\" by David Pesetsky", "references": [], "page_rank": 0.0001231527093596059}, {"id": "9bf778e4fafdbdab502bc5cf45982643f67914f4", "title": "On the notions \"lexically related\" and \"head of the word", "authors": ["Eric Williams"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"On the notions \"lexically related\" and \"head of the word\" by Eric Williams", "references": [], "page_rank": 0.0001231527093596059}, {"id": "96affee023e1fd031b45cfef8b4ee6da61ac55fe", "title": "The role of the word in phonological development", "authors": ["Shulamuth Chiat"], "date": 1979, "abstract": "The task facing the child in segmenting the speech chain is an important factor in phonological development, though it has long been overlooked by theories which attempt to account for the constraints operating on the child's phonological system. Before the child can begin to determine which phonetic features function to distinguish words, it must isolate at least some words from the stream of speech. This suggests that the child will attend first to those phonetic features which serve as cues to word boundaries. As its word store increases and the problem of identifying words in the syntagm declines, such syntagmatically relevant features may give way in importance to features which are paradigmatically contrastive. An analysis of the features which children's early word forms and children's word confusions typically share with their targets provides support for the hypothesis that different features of the word are prominent at different stages of development. Over the last ten or so years, a number of models of children's phonological systems have been proposed. These models all seek to account for the deviations from adult forms which have been observed in children's production of lexical items. They differ from one another in the explanations they offer, which may invoke limitations on perception, or production, or underlying representations, or on the form of the rules for realising underlying representations. But they have one feature in common: almost without exception, they focus on the emergence of paradigmatic contrasts in the child's phonological system, taking syntagmatic context into account only in so far as it affects paradigmatic contrasts in phonological processes like assimilation. Clearly, knowing the set of phonological features which serve to distinguish minimal pairs is an important component of the adult's phonological competence and hence an important goal in acquiring that Linguistics 17 (1979), 591-610. \u00a9 Mouton Publishers.", "references": ["faa7dad870210d065a636fee271f718330dc1b2c", "05736f2a05858207952a1d2b440824422a3678e5", "f4fc097e19c06fe68dd0819a6a14f558669d97ee", "feb1f7431e5e1b57a9c0cb80a669afaf48bc48e7", "decd6bccefcc0f8740ed20c36e0612ac8a631b7f", "7313c71eeb33c2c4ecf5ebf69824bc2677aafd5b", "0e274d14257b0ee043ae664c143a80c2d83f8c4c", "979abaf817c536072a73baf61674c5a2dd205d76", "9a777d55ccf1973cb84327231226690242e5bf37", "2132ada4c48bfde2ebaf67b59dd731d1dd3b8fe3"], "page_rank": 0.0001231527093596059}, {"id": "6e11d278f28b82fd0012995724714fdc2c4b1b79", "title": "The Complexity of the Vocabulary of Bambara", "authors": ["Christopher Culy"], "date": 1985, "abstract": "In this paper I look at the possibility of considering the vocabulary of a natural language as a sort of language itself. In particular, I study the weak generative capacity of the vocabulary of Bambara, and show that the vocabulary is not context free. This result has important ramifications for the theory of syntax of natural language.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "19960f19c8c26f59e12dd820c7e920caf251b4c1", "title": "A Mechanical Solution of Schubert's Steamroller by Many-Sorted Resolution", "authors": ["Christoph Walther"], "date": 1984, "abstract": "We demonstrate the advantage of using a many-sorted resolution calculus by a mechanical solution of a challenge problem. This problem known as \"Schubert's Steamroller\" had been unsolved by automated theorem provers until now. Our solution clearly demonstrates the power of a many-sorted resolution calculus. The proposed method is applicable to all resolution-based inference systems.", "references": ["97f34317b84064424bbd3868aba09f8afab9eddd", "885194cadd5989d077f5f25b3dc0c42d0d55ce40", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "6520dbde01d8cc2c84a0ca05a0e8e59e3215a213", "361c9f54e67c0fba17e373807195577f646d7bc3", "6e61c063eee7ffb76b93dbe635466d30f4969ebb", "05b44597834f6df07c1c1290fb33a979bdf99067", "ec3cf57a2029175930a9c040299f3c7c1b1d2ca3", "236c58d8ba3e9e9d5ed8562084690090752bf5df", "e9b525a1f00f3c7befc1815bfda90125d8c7a3e3"], "page_rank": 0.00016420361247947453}, {"id": "88b50f5be70b1c2fe40d7cc6bcf478a9d92307f3", "title": "A two-level morphological analysis of rumanian", "authors": ["Ronny Khan"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"A two-level morphological analysis of rumanian\" by Ronny Khan", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "5bedc10d045b5594159cd7f2a1847a94d2e6a493", "title": "A two-level morphological description of english", "authors": ["Lauri Karttunen", "Kay Wittenburg"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"A two-level morphological description of english\" by Lauri Karttunen et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "39b30d0a5ac6894b503c9097f60eaa4a516590fe", "title": "A prosodic theory of nonconcatenative morphology", "authors": ["John J. McCarthy"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"A prosodic theory of nonconcatenative morphology\" by John J. McCarthy", "references": ["884f3137884c40a42cae5398b4eb7037274105ca", "8a4b502973968752f0f9dcb05c5bcdf6ecc6154f", "57adcab61efd8e37bde2b06e12c418e4b2436b49", "f80bd79f67fb7111012e55f2448a7fea9de91755", "e6ceb5078efbbe60111377879b4a8d1c71825d0e", "b94e2bf584236ffad5b51ad0a8bf4ac949cf21b3", "41709a13e84c2c96120b37d8fae123b086b1ae40", "6fdffc6b73ad4d8813496489e9561a01688c01c5", "9bdb0278d1f6472e28849489539bfb32a958ec45"], "page_rank": 0.0001231527093596059}, {"id": "1367557ddd813617de6a4f8e002352f27e1ae384", "title": "Argument linking and compounds in English", "authors": ["Rochelle Lieber"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"Argument linking and compounds in English\" by Rochelle Lieber", "references": [], "page_rank": 0.0001231527093596059}, {"id": "0271828a2b08a5fc990e78d0f6803fa2a6a290b1", "title": "A two-level description of Old Church Slavonic morphology", "authors": ["Jouko Lindstedt"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"A two-level description of Old Church Slavonic morphology\" by Jouko Lindstedt", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "281ac2d6d0138b6e1b9df544e005dbc45a984d73", "title": "Order-Sorted Unification", "authors": ["Jos{\\'e} Meseguer", "Joseph A. Goguen"], "date": 1989, "abstract": "This paper studies unification for order-sorted equational logic. This logic generalizes unsorted equational logic by allowing a partially ordered set of sorts, with the ordering interpreted as set-theoretic containment in the models; it also allows overloading of function symbols, such as + for integer and rational number addition, with the overloaded functions of greater rank interpreted in the models as extensions of those of smaller rank. Our presentation emphasizes semantic aspects, and gives a categorical treatment of unification that has substantial advantages in this context over the usual treatment of unifiers as endomorphisms of a single free algebra. Given system @C of equations and a set E of axioms that is sort-preserving and does not impose restrictions on the sorts of its variables, the main results characterize when an order-sorted signature has a minimal (or finite, or most general when @C is solvable) family of order-sorted E-unifiers for @C. In addition, for unitary signatures, where each solvable system of equations has a most general unifier, we give a quasi-linear algorithm for syntactic unification (i.e., for E= ) a la Martelli-Montanari, that is more efficient than the unsorted one for failures.", "references": ["2e1fcbf65bb68cd7f302ba0ab581ab6802288124", "982793f01cc78d62257cc7dc31c7165fad791952", "e72d2597bd5584ebb55c6c9706c44b8d59dfb2de", "bfd857015f2bb05585018436121206bc84dfcacb", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "0da39f03345de151e30ca441eea5bac4fba684c8", "ecc8d716f6feb8db18e29fecd4ba144fce75a3bd", "eead5a062b8607df5e276bff34504531fbbfdd21", "5c3391bde2bb1b3d737913ee8caa01492a782732", "e4a7c4db86b8c578c86db223cc3c60c54d2fa8dd"], "page_rank": 0.00016420361247947453}, {"id": "f4bbc00fdbcd8b96010d1d8db94f3bdf0e03ac48", "title": "A two-level analysis of french", "authors": ["Lun Shan"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"A two-level analysis of french\" by Lun Shan", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "a1d58af78b248154ec5c3d7ce96a97d85d3b68bd", "title": "A STUDY OF THE EFFECTIVENESS OF A PROGRAMED LEARNING METHOD IN TEACHING THE USE OF \"WEBSTER'S SEVENTH NEW COLLEGIATE DICTIONARY.\".", "authors": ["William A. Stockdale"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"A STUDY OF THE EFFECTIVENESS OF A PROGRAMED LEARNING METHOD IN TEACHING THE USE OF \"WEBSTER'S SEVENTH NEW COLLEGIATE DICTIONARY.\".\" by William A. Stockdale", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "2bad5215293ee18cd251c924ffc4ae3280935102", "title": "A two-level morphological analysis of japanese", "authors": ["Yukiko Sasaki Alam"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"A two-level morphological analysis of japanese\" by Yukiko Sasaki Alam", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "cf809cafdd31e9542cbf2393756c0d73626e026d", "title": "Quantifiers and Propositional Attitudes", "authors": ["W. V. Quine"], "date": 1956, "abstract": "Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers, and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "91e0a80cefb3fbfdb8c9b39780892db087f37516", "title": "A DBMS prototype to support extended NF2 relations: an integrated view on flat tables and hierarchies", "authors": ["Peter Dadam", "Klaus K{\\\"u}spert", "F. Andersen", "Henk M. Blanken", "R. Erbe", "J{\\\"u}rgen G{\\\"u}nauer", "Vincent Y. Lum", "Peter Pistor", "Georg Walch"], "date": 1986, "abstract": "Recently, extensions for relational database management systems (DBMS) have been proposed to support also hierarchical structures (complex objects). These extensions have been mainly implemented on top of an existing DBMS. Such an approach leads to many disadvantages not only from the conceptual point of view but also from performance aspects. Thus paper reports on a 3-year effort to design and prototype a DBMS to support a generalized relational data model, called extended NF2 (Non First Normal Form) data model which treats flat relations, lists, and hierarchical structures in a uniform way. The logical data model, a language for this model, and alternatives for storage structures to implement generalized relations are presented and discussed.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "bb8b4e7c43459bf69ce8518797be216c7f9248c6", "title": "Tools and Methods for Computational Lexicology", "authors": ["Roy J. Byrd", "Nicoletta Calzolari", "Martin Chodorow", "Judith L. Klavans", "Mary S. Neff", "Omneya A. Rizk"], "date": 1987, "abstract": "This paper presents a set of tools and methods for acquiring, manipulating, and analyzing machine-readable dictionaries. We give several detailed examples of the use of these tools and methods for particular analyses. A novel aspect of our work is that it allows the combined processing of multiple machine-readable dictionaries. Our examples describe analyses of data from Webster's Seventh Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual dictionaries, the Collins Thesaurus, and the Zingarelli Italian dictionary. We describe existing facilities and results they have produced as well as planned enhancements to those facilities, particularly in the area of managing associations involving the senses of polysemous words. We show how these enhancements expand the ways in which we can exploit machine-readable dictionaries in the construction of large lexicons for natural language processing systems.", "references": ["6c6a9548e18e372fd381794f18dcee518c9f506d", "cada5a781ec0ed58f10e1ad30c0044e7421c44ab", "a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5", "e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "821cf1e5edc271a3206c4912c0a25b46a7dd6e11", "cf967bc6d2c2d7f9a7ba6b8e03cefb18e40b0e0f", "6e4499450a6203c0c043a922f4140180ea6dc4df", "9c6e68a6d704d0d9518a807584a469f72a4c66c9", "3d5eabf859c29670e3b91adf6f0193a8d7bc4981", "76e4e034c20bea86edcc6e71bbaddb47fafeecbc"], "page_rank": 0.00032840722495894905}, {"id": "119ba277a0aeffc8171916756adbe9844b9ff71a", "title": "A PROCESS MODEL OF MORPHOLOGY AND LEXICON", "authors": ["Fred KARLSSON", "and Kimmo Koskenniemi"], "date": 1985, "abstract": "The past 15 years have witnessed a steadily growing interest in morphology and lexicon. During the generative hey-days morphology was mostly reduced to phonology while the lexicon was seen just \u00e4s a minimal list of idiosyncratic properties of lexical entries. More recent views stress the \u00fcreducibility of morphology and attribute more structure and a more active role to the lexicon. Several comprehensive theories or more sepcific models have been proposed for describing the structure and interplay of morphology and lexicon. Many of these approaches are purely autonomous accounts of (parts of) the language System in the Saussurean or Chomskyan sense. Such are the natural,semiotically based morphology of MAYERTHALER (1981), DRESSLEB (1981), and WURZEL (1984), \u00e4s well \u00e4s the lexical morphology of KEPARSKY (1982) and others. By definition, these approaches pay little or no attention to properties of language use such \u00e4s the processing of word-form tokens or the import of frequency of occurrence. Other approaches are outspokenly behavioral. These stress the primacy of behavioral data over autonomous theorizing, at least \u00e4s a starting-point of psycholinguistics and psychology of language. Here belong e.g. several models of word-recognition such \u00e4s MORTON'S (1969) logogen model, FORSTER'S (1976) active search model, and the cohort model of MARSLEN\u2014WILSON and TYLER (e.g. 1980, 1981). These models are based on genuine experimental work and have little in common with autonomous morphology. Third, one may try to integrate autonomous analysis and performance data. A pert\u00fcient example is BYBEE and SLOBIN'S (1982) morphological Schemata which are based partly on autonomous analysis, partly on psycholinguistic evidence. The Schemata are claimed to be units used in accessing the lexicon.", "references": ["008da127af8fbdfdd9a3f7d36b23d7c3946e491b", "59e61b27b9af96b31357f588a2c6290f98b6fe89", "01a572d6e13d08bf0f8fd6664b9fc2c6dfc3787d", "e8a3782a85f76f9c0093ac7b1406486378cb5b06", "ebf13a6eb44fe3cc498404d0137d21fd13555690", "760af8ab0202189b9d545b8ba0c9c1845a1a7c48", "1759da91745f9faa941e945aa51cb0bbfa2684ce", "73c227f99c0b37e753c4477a6024fc560bc8f035", "92555e5f676d13a1a0093337fba8f0d420793e1c", "cb9f81f74f65475dc407bf393f1d03fbb11848fc"], "page_rank": 6.157635467980295e-05}, {"id": "4d58fbb388884caf78a0652a02ee279883d86f75", "title": "Extraction of semantic information from an ordinary English dictionary and its evaluation", "authors": ["Jun-ichi Nakamura", "Makoto Nagao"], "date": 1988, "abstract": "The automatic extraction of semantic information especially semantic relationships between words, from an odinary English dictionary is described. For the extraction, the magnetic tape version of LDOCE (Longman Dictionary of Contemporary English, 1978 edition) is loaded into a relational database system. Developed extraction programs analyze a definition sentence in LDOCE with a pattern matching based algorithm. Since this algorithm is not perfect, the result of the extraction has been compared with semantic information (semantic markers) which the magnetic tape version of LDOCE contains. The result of comparison is also discussed for evaluating the reliability of such an automatic extraction.", "references": ["a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5", "5d963629d0f601a71df76b54d63126836bf4fbe6", "75e6b2e12fda8730d358370c05eaf3ecd4b75a35"], "page_rank": 8.210180623973726e-05}, {"id": "ba817882de1a7093ee83bfa14a4005e5c8a5977e", "title": "A General Computational Model for Word-Form Recognition and Production", "authors": ["Kimmo Koskenniemi"], "date": 1984, "abstract": "Semantic Scholar extracted view of \"A General Computational Model for Word-Form Recognition and Production\" by Kimmo Koskenniemi", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e6f54b0faa9aab28d9ca1a96e451ecf9ae0a5b29", "title": "Applications of artificial intelligence for chemical inference. IV. Saturated amines diagnosed by their low resolution mass spectra and nuclear magnetic resonance spectra", "authors": ["Carl Djerassi", "Armand Buchs", "Alan M. Duffield", "Gustav Schroll", "Allan B. Delfino", "Bruce G. Buchanan", "Georgia L. Sutherland", "Edward A. Feigenbaum", "Joshua Lederberg"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"Applications of artificial intelligence for chemical inference. IV. Saturated amines diagnosed by their low resolution mass spectra and nuclear magnetic resonance spectra\" by Carl Djerassi et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "32c280f291fa99dbdacc925b767cfecf8d2cbfce", "title": "Acquisition of semantic information from an on-line dictionary", "authors": ["Nicoletta Calzolari", "Eugenio Picchi"], "date": 1988, "abstract": "After the first work on machine-readable dictionaries (MRDs) in the seventies, and with the recent development of the concept of a lexical database (LDB) in which interaction, flexibility and multidimensionality can be achieved, but everything must be explicitly stated in advance, a new possibility which is now emerging is that of a procedural exploitation of the full range of semantic information implicitly contained in MRDs. The dictionary is considered in this framework as a primary source of basic general knowledge. In the paper we describe a project to develop a system which has word-sense acquisition from information contained in computerized dictionaries and knowledge organization as its main objectives. The approach consists in a discovery procedure technique operating on natural language definitions, which is recursively applied and refined. We start from free-text definitions, in natural language linear form, analyzing and converting them into informationally equivalent structured forms. This new approach, which aims at reorganizing free text into elaborately structured information, could be called the Lexical Knowledge Base (LKB) approach.", "references": ["6c6a9548e18e372fd381794f18dcee518c9f506d", "ce0eedb0406268fdbb705702bcab012121f289c7", "a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5", "e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "9c6e68a6d704d0d9518a807584a469f72a4c66c9", "b6055cc5c4d70f2f66ff3d05b85113114cf1fa2b", "16d09e9b4a138525814ce9d14bad0c2cd988193f", "804d36ad793d55372890bc07c0c6c62d473199f6"], "page_rank": 8.210180623973726e-05}, {"id": "d8d2f856d9ded83f6d62687ad2544b401a5eeb59", "title": "Academic American Encyclopedia: A Review Essay", "authors": ["Gary D. Barber"], "date": 1982, "abstract": "The first edition of the Academic American Encyclopedia (AAE) appeared in July, 1980. The vigorous advertising campaign preceding its initial appearance has been maintained, both in the library press and elsewhere. All reviews of the set to date have been laudatory, although most reviewers have found a few minor points to criticize.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "008da127af8fbdfdd9a3f7d36b23d7c3946e491b", "title": "Two-Level Model for Morphological Analysis", "authors": ["Kimmo Koskenniemi"], "date": 1983, "abstract": "This paper presents a new linguistic, computationally implemented model for morphological analysis and synthesis. It is general in the sense that the same language independent algorithm and the same computer program can operate on a wide range of languages, including highly inflected ones such as Finnish, Russian or Sanskrit. The new model is unrestricted in scope and it is capable of handling the whole language system as well as ordinary running text. A full description for Finnish has been completed and tested, and the entries in the Dictionary of Modern Standard Finnish have been converted into a format compatible with it. \n \nThe model is based on a lexicon that defines the word roots, inflectional morphemes and certain nonphonological alternation patterns, and on a set of parallel rules that define phonologically oriented phenomena. The rules are implemented as parallel finite state automata, and the same description can be run both in the producing and in the analyzing direction.", "references": ["e7687c6c22bacedfdadf89ac9968f304ce420d90"], "page_rank": 0.00022577996715927748}, {"id": "b7780f292c48e504e3a2724e54c205e6c6221932", "title": "Computational analysis of present-day American English", "authors": ["Henry Kucera", "W. Nelson Francis", "William Freeman Twaddell", "Mary Lois Marckworth", "L. Michael Bell", "John Bissell Carroll"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Computational analysis of present-day American English\" by Henry Kucera et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "54d7ebff62332867fbece408b353a5c6367d52f3", "title": "The Advanced learner's dictionary of current English with Chinese Translation", "authors": ["Albert Sydney Hornby", "E. V. Gatenby", "A. H. Wakefield"], "date": 1963, "abstract": "The advanced Learner's dictionary of current english , The advanced Learner's dictionary of current english , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u062c\u0646\u062f\u06cc \u0634\u0627\u067e\u0648\u0631 \u0627\u0647\u0648\u0627\u0632", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "557f25f3d74024d46ba55fb53c5fe6e29d7b26d9", "title": "The Handbook of Artificial Intelligence", "authors": ["Avron Barr", "Edward A. Feigenbaum", "Paul R. Cohen"], "date": 1982, "abstract": "Similar to the first three volumes of this classic series, this volume contains a collection of articles by acclaimed experts representing the leading edge of knowledge about the field of AI.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4823845b942fd6570d657650a21de01c240522b9", "title": "Brief auditory storage: a modification of Sperling's paradigm applied to audition.", "authors": ["Michel Treisman", "A. B. Rostron"], "date": 1972, "abstract": "Abstract SPERLING (1960) demonstrated the existence of a peripheral visual information store, using a sampling procedure which he introduced. This procedure has been modified to facilitate its application to auditory stimulation. The results obtained are analysed in terms of a signal detectability and two \u2018loss of items\u2019 models. One of the latter is rejected. The other two indicate that there isan analogous brief auditory store whose contents are lost in about 1 sec.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "aaa99347d6c480b87b6b6391df7b7d65454c3d4a", "title": "Precategorical acoustic storage (PAS)", "authors": ["Robert G. Crowder", "John Magnus Morton"], "date": 1969, "abstract": "A system for precategorical storage of acoustic information is described. Material in this store is subject to overwriting and to decay with time. Precategorical Acoustic Storage (PAS) receives information only from the ears; it is not affected by silent rehearsal or by visual stimulation, and is explicitly distinguished from storage in terms of articulation. Two experiments are reported in which these properties of PAS are tested. Postulation of PAS permits an account of serial position functions for visual and auditory presentation in immediate memory, a distinction between \u201crecency\u201d and \u201cflnality\u201d effects, the differential effects of a redundant prefix and a redundant suffix, effects of vocalization at presentation and at recall, and the relation between memory confusions and speech perception. Implications for a general theory of human memory are discussed.", "references": ["87b00d2a3e84f6b66bc03b63374b281f0e277f29", "dc6f0681974712bf2cb6ed79e66cd601c3c1a89c", "b31be4820e6d46a3a90efc5e04b4a09353dd9df6", "f26ca8932847b6e43497cef0607b8f54cb4c83d8", "f1dd4a4b12b5cc0a836ad0bc84fa18fea72a1cf8", "cb7787ea9220b29980b997c80d2931de4be5c016", "e2c1a780fe4a9e4d9258ba8cf8b07067b90bdf1d", "56c16d9e2a5270ba6b1d83271e2c10916591968d", "0869f6b4b61cecb11bfabfcd90b15deb3b5b1b5d", "d471eed0a18376bc9dd3703843f8a5ef6e751424"], "page_rank": 0.00016009852216748767}, {"id": "3f3a73f890a4e30f6fc981d037602f2c5db10424", "title": "An auditory analogue of the Sperling partial report procedure: Evidence for brief auditory storage.", "authors": ["Charles Darwin", "Michael T. Turvey", "Robert G. Crowder"], "date": 1972, "abstract": "Abstract Three experiments are reported on the partial report of material presented auditorily over three spatially different channels. When partial report was required by spatial location, it was superior to whole report if the cue came less than four seconds after the end of the stimuli (Exp. I). When partial report was required by semantic category (letters/digits) the relation between it and whole report depended on whether the S was asked also to attribute each item to its correct spatial location. When location was required partial report was lower than whole report and showed no significant decay with delay of the partial report indicator (Exp. II), but when location was not required, partial report was superior to whole report for indicator delays of less than two seconds (Exp. III). This superiority was, however, much less than that found in Exp. I when partial report was required by spatial location. These results are compatible with a store which has a useful life of around two seconds and from which material may be retrieved more easily by spatial location than by semantic category.", "references": ["1fe856f63c04a2dde2a2f94b066d52e9aaeafc27", "74071c3a344489d66ebb73c4fc58d0dfc79ae7a0", "bac7597653508d0eafb84b293ff07fcc20441026", "460b19f7be6a600e74ad99cc2e729bad1bb6f7f1", "358bcc42e45cfb9e676b905e8307a0de3b23b0ae", "c8d5cf5d2cc936ae3a6f7a491fdfa2b126a43259", "e1bf4a5f294ab5df170eaebe744a23971ac6b57b", "fe843bf6a7ee65c58382267e1a0695006c189f1b", "4cf555148a3ad2ad736eae77f0d9ffa7eb0140f7", "aaa99347d6c480b87b6b6391df7b7d65454c3d4a"], "page_rank": 9.852216748768472e-05}, {"id": "8010d8a379961b502ac14240604614606469ddbe", "title": "A parametrically controlled spectral analysis system for speech", "authors": ["Harvey F. Silverman", "N. Rex Dixon"], "date": 1974, "abstract": "The parametrically controlled analyzer (PCA) is a large PL/I program which has been designed to perform spectral analysis of speech signals. PCA features parametric selection of several analysis methods, including discrete Fourier transformation and linear predictive coding. Also, selection may be made among various smoothing, normalization, and interpolation methods. PCA develops high-quality spectrographic representations of speech for standard line printers and CRT displays. The PCA is described and numerous examples of various parameter settings are presented and discussed.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8377fc1fff7eb8073015c2227aba98bd0c741db2", "title": "Matching sequences under deletion-insertion constraints.", "authors": ["David Sankoff"], "date": 1972, "abstract": "Given two finite sequences, we wish to find the longest common subsequences satisfying certain deletion/insertion constraints. Consider two successive terms in the desired subsequence. The distance between their positions must be the same in the two original sequences for all but a limited number of such pairs of successive terms. Needleman and Wunsch gave an algorithm for finding longest common subsequences without constraints. This is improved from the viewpoint of computational economy. An economical algorithm is then elaborated for finding subsequences satisfying deletion/insertion constraints. This result is useful in the study of genetic homology based on nucleotide or amino-acid sequences.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "aa648e3ad1c906d0ec33810ecabe1b4420c6bfff", "title": "Reaction times to comparisons within and across phonetic categories", "authors": ["David B. Pisoni", "Jeffrey Tash"], "date": 1974, "abstract": "Same-different reaction times (RTs) were obtained to pairs of synthetic speech sounds ranging perceptually from /ba/ through /pa/. Listeners responded \u201csame\u201d if both stimuli in a pair were the same phonetic segments (i.e., /ba/-/ba/ or /pa/-/pa/) or \u201cdifferent\u201d if both stimuli were different phonetic segments (i.e., /ba/-/pa/ or /pa/-/ba/). RT for \u201csame\u201d responses was faster to pairs of acoustically identical stimuli (A-A) than to pairs of acoustically different stimuli (A-a) belonging to the same phonetic category. RT for \u201cdifferent\u201d responses was faster for large acoustic differences across a phonetic boundary than for smaller acoustic differences across a phonetic boundary. The results suggest that acoustic information for stop consonants is available to listeners, although the retrieval of this information in discrimination will depend on the level of processing accessed by the particular information processing task.", "references": ["e9fa01e4b9be8052d45ab5be3c54761229575fb8", "aa370f4a46436aff84044f9241324c29a4c4b18b", "b493446485de82fe0a0e42d392add4b793b616d3", "101c7d3c3444f048ba7e6f794cd914099cad74be", "cb7852175e42497c7e8fa5f33114f867071d9266", "4cf555148a3ad2ad736eae77f0d9ffa7eb0140f7", "a546750bd366343354b745de9476d3cc52025a93", "318897a75afda96e62a8572834d3dbdb3d82e2ad", "8333c5478111681392343266e4165839b03e8b61"], "page_rank": 9.852216748768472e-05}, {"id": "ff451cd5f42206d5abc4c4422b7ce9b58a27cdba", "title": "The Viterbi algorithm as an aid in text recognition (Corresp.)", "authors": ["David L. Neuhoff"], "date": 1975, "abstract": "The results of an experiment are described in which contextual information is used to improve the performance of an optical character reader when reading English text. Specifically, English is modeled as a Markov source and the Viterbi algorithm is used to do maximum a posteriori sequence estimation on the output of an optical character reader (OCR).", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2132ada4c48bfde2ebaf67b59dd731d1dd3b8fe3", "title": "One idiosyncratic strategy in the acquisition of phonology", "authors": ["Tom Priestly"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"One idiosyncratic strategy in the acquisition of phonology\" by Tom Priestly", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "e28b31213c56249dfca833586d3ca1e975aa9f37", "title": "A Program for Correcting Spelling Errors", "authors": ["Charles R. Blair"], "date": 1960, "abstract": "A program using a simple, heuristic procedure for associating \u201csimilar\u201d spellings is able to correct misspelled words. Given only a vocabulary of properly spelled words, the computer can correct most (including unanticipated) misspellings without human assistance. Apart from practical applications, the process is interesting as an example of an unusual form of pattern recognition.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "9a777d55ccf1973cb84327231226690242e5bf37", "title": "English phonotactic structure and first-language acquisition", "authors": ["Lawrence Gaylord Jones"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"English phonotactic structure and first-language acquisition\" by Lawrence Gaylord Jones", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "b493446485de82fe0a0e42d392add4b793b616d3", "title": "Auditory and phonetic memory codes in the discrimination of consonants and vowels *", "authors": ["Bahri}"], "date": 1940, "abstract": "Recognition memory for consonants and v\u2019owels selected from within and between phonetic categories was examined in a delayed comparison d~scrimination task. Accuracy of discrimination for synthetic vowels selected from both within and between categories was inversely related to the magnitude of the comparison interval. In contrast, d~scrimination of synthetic stop consonants remained relatively stable both within and between categories. The results indicate that differences in discrimination between consonants and vowels are primarily due to the differential availability of auditory short-term memory for the acoustic cues distinguishing these t~vo classes of speech sounds. The findings provide evidence for distinct auditory a~d phonetic memory codes in speech perception.", "references": ["97217086c1ecafc3561b865ff1228a22320b1f6f", "d1489ef5b150196ba45b7ae61e5c68ac86293a5f", "101c7d3c3444f048ba7e6f794cd914099cad74be", "78a93bcb36b2e93d39d2b18d74277e95387a328b", "8b1d36854d72805bc5008f8dbc7909bb1ee23e5f"], "page_rank": 0.00016009852216748767}, {"id": "979abaf817c536072a73baf61674c5a2dd205d76", "title": "Stress and word position as determinants of imitation in first-language learners.", "authors": ["Richard Blasdell", "Peter E Jensen"], "date": 1970, "abstract": "This study investigated stress (prominence) and word position as decoding cues for first-language learners. Three levels of stress (defined perceptually and acoustically) were assigned systematical...", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "7313c71eeb33c2c4ecf5ebf69824bc2677aafd5b", "title": "Discrimination of linguistic stress in early infancy.", "authors": ["Dee Spring", "Philip S. Dale"], "date": 1977, "abstract": "The high-amplitude sucking (HAS) paradigm was used to evaluate the ability of one- to four-month-old infants to discriminate two artificially synthesized disyllables (/ba b\u00e1 and b\u00e1 ba/) which differed solely in the location of perceived stress. One hundred and twenty infants were tested in two experiments. A modification of the HAS paradigm was developed, in which both stimuli are alternated postshift. The results of the first experiment demonstrate that young infants are able to discriminate the acoustic correlates of stress location (fundamental frequency, intensity, and duration) and that the modified HAS paradigm produces significantly stronger evidence for this discrimination than does the standard paradigm. The second experiment determined that infants can discriminate durational differences alone, without concomitant variations in the naturally correlated parameters of fundamental frequency and intensity.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "6fdffc6b73ad4d8813496489e9561a01688c01c5", "title": "The interaction of morphological and phonological rules in Tagalog : a study in the relationship between rule components in grammar.", "authors": ["Jill Louise Carrier"], "date": 1979, "abstract": "Thesis. 1979. Ph.D.--Massachusetts Institute of Technology. Dept. of Linguistics and Philosophy.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "9bdb0278d1f6472e28849489539bfb32a958ec45", "title": "The syntax of phonology.", "authors": ["Joel Rotenberg"], "date": 1978, "abstract": "Thesis. 1978. Ph.D.--Massachusetts Institute of Technology. Dept. of Linguistics and Philosophy.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "e4a7c4db86b8c578c86db223cc3c60c54d2fa8dd", "title": "8th International Conference on Automated Deduction", "authors": ["J{\\\"o}rg H. Siekmann"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"8th International Conference on Automated Deduction\" by J\u00f6rg H. Siekmann", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "0e274d14257b0ee043ae664c143a80c2d83f8c4c", "title": "Elements of general phonetics", "authors": ["David Abercrombie"], "date": 1967, "abstract": "Fifty years - a memoire phonetics and phonology phoneme - the concept and the word segments hylomorphic taxonomy and William Holder Daniel Jones' teaching RP today - its position and its prospects the accents of standard English in Scotland some functions of silent stress stress and some other terms the indication of pronunciation in reference books phonetic iconicity in writing systems paralanguage encounter with aphasia.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "ecc8d716f6feb8db18e29fecd4ba144fce75a3bd", "title": "How to Prove Algebraic Inductive Hypotheses Without Induction", "authors": ["Joseph A. Goguen"], "date": 1980, "abstract": "This paper proves the correctness of algebraic methods for deciding the equivalence of expressions by applying rewrite rules, and for proving inductive equational hypotheses without using induction; it also shows that the equations true in the initial algebra are just those provable by structural induction. The major results generalize, simplify and rigorize Musser's method for proving inductive hypotheses with the Knuth-Bendix algorithm; our approach uses a very general result, that (under certain conditions) an equation is true iff it is consistent. Finally, we show how these results can be extended to proving the correctness of an implementation of one data abstraction by another.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "eead5a062b8607df5e276bff34504531fbbfdd21", "title": "Complete Sets of Unifiers and Matchers in Equational Theories", "authors": ["Fran\u00e7ois Fages", "G{\\'e}rard P. Huet"], "date": 1983, "abstract": "We propose an abstract framework to present unification and matching problems. We argue about the necessity of a somewhat complicated definition of basis of unifiers (resp. matchers). In particular we prove the non-existence of complete sets of minimal unifiers (resp. matchers) in some equational theories, even regular.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e72d2597bd5584ebb55c6c9706c44b8d59dfb2de", "title": "Inheritance Hierarchies: Semantics and Unification", "authors": ["Gert Smolka", "Hassan A{\\\"i}t-Kaci"], "date": 1989, "abstract": "Inheritance hierarchies are introduced as a means of representing taxonomicallyorganized data. The hierarchies are built up from so-called feature types that are ordered by subtyping and whose elements are records. Every feature type comes with a set of features prescribing fields of its record elements. So-called feature terms are available to denote subsets of feature types. Feature unification is introduced as an operation that decides whether two feature terms have a nonempty intersection and computes a feature term denoting the intersection. We model our inheritance hierarchies as algebraic specifications in ordersortedequational logic using initial algebra semantics. Our framework integrates feature types whose elements are obtained as records with constructor types whose elements are obtained by constructor application. Unification in these hierarchies combines record unification with order-sorted term unification and is presented as constraint solving. We specify a unitary unification algorithm by a set of simplification rules and prove its soundness and completeness with respect to the model-theoretic semantics.", "references": ["d0f551a3731c7ace2cd7569e63a756993464ac8d", "7d90e153652401cfb690294210940412a56103e0", "972ae67586483e51b44091fd80d41fb0a0ddcf0a", "dedc03e232bfb9dac8e21659ae4c155e4280679f", "b78dd760573e4c46b1208b3004cb393973b8b0c6", "bf11658c7223fe746c18c372acea619a6b9799e4", "982793f01cc78d62257cc7dc31c7165fad791952", "f487a24e7f7329da852d0638a130e00aae08a598", "8e78a74664a66f40cfd48010fba0b37460e62e62", "0da39f03345de151e30ca441eea5bac4fba684c8"], "page_rank": 0.00023457658925639217}, {"id": "821cf1e5edc271a3206c4912c0a25b46a7dd6e11", "title": "Creating And Querying Lexical Data Bases", "authors": ["Mary S. Neff", "Roy J. Byrd", "Omneya A. Rizk"], "date": 1988, "abstract": "Users of computerized dictionaries require powerful and flexible tools for analyzing and manipulating the information in them. This paper discusses a system for grammatically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information. It also describes a language for querying, formatting, and maintaining dictionaries and other lexical data stored with that representation.", "references": ["d6d84b46dbe786becb7e854dc195f210bc1410c3", "bb8b4e7c43459bf69ce8518797be216c7f9248c6", "cada5a781ec0ed58f10e1ad30c0044e7421c44ab", "6c6a9548e18e372fd381794f18dcee518c9f506d", "780c1ab269b718db16e96419b78052041d508200", "fbc04a1951003ba164303b2898fb7f3c6b4e9083", "9de9fd1835d7cac1b46685b3ffa93e3da49a3be4", "2975921ab8fe6b5d3e97afadff856ade06f92492", "def221cbd09aca90a70677643f69ea23adf23b3f", "de0a2f9ccf515771636976c55017eee7d55472df"], "page_rank": 0.0001231527093596059}, {"id": "0da39f03345de151e30ca441eea5bac4fba684c8", "title": "An Efficient Unification Algorithm", "authors": ["Alberto Martelli", "Ugo Montanari"], "date": 1982, "abstract": "The unification problem in f'mst-order predicate calculus is described in general terms as the solution of a system of equations, and a nondeterministic algorithm is given. A new unification algorithm, characterized by having the acyclicity test efficiently embedded into it, is derived from the nondeterministic one, and a PASCAL implementation is given. A comparison with other well-known unification algorithms shows that the algorithm described here performs well in all cases.", "references": ["d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "25d6b9e8938b30bbd64a043851f911eb6a9727cf", "10a463bb00b44bdd3a8620f2bedb9e1564bfcf32", "f0cfc904f2fe851a996231c769cb36872af49c02"], "page_rank": 0.00023457658925639217}, {"id": "cb9f81f74f65475dc407bf393f1d03fbb11848fc", "title": "Psychological reality in phonology", "authors": ["Per Linell"], "date": 1979, "abstract": "Semantic Scholar extracted view of \"Psychological reality in phonology\" by Per Linell", "references": [], "page_rank": 0.00016420361247947453}, {"id": "bfd857015f2bb05585018436121206bc84dfcacb", "title": "What Is Unification?: A Categorical View of Substitution, Equation and Solution", "authors": ["Joseph A. Goguen"], "date": 1989, "abstract": "Publisher Summary A substitution is a transformation from one space to another, an equation is a pair of such substitutions, and a solution to an equation is a substitution that yields the same value when composed with the substitutions that constitute the given equation. In some special cases, solutions are called unifiers. Unification has become important in computer science because of its connections to mechanical theorem proving and so-called logic programming and in linguistics because of its connections to so-called unification grammar. Other examples include Scott domain equations, linear programming, type inference, and differential equations. The intuition that the composition of substitutions should be associative when defined, and should have identities, motivates a general concept of substitution system based on category theory. Although certain simple ideas about substitutions, equations and solutions have been implicit in the categorical literature at least since Lawvere's thesis, it has been difficult for computer scientists, linguists, and even mathematicians to relate this work to their own concerns and appreciate how simple it really is.", "references": ["264dd9fdc41ea9707e7674d1dd01e2eb266824c6", "d80a39f0e0ad0ff9898beebd4c2895a7bd8ee9d8", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "9f2afacf046a3109b8bbc6ecdf9203f4d8498672", "f848231dba204552b01d5ec0e812220f8126d6ac", "a1a365b7bfe0182515a0ad144c336ff26627ac80", "e72d2597bd5584ebb55c6c9706c44b8d59dfb2de", "76b842cb3813741463f70556f5d904259b7bc829", "b78dd760573e4c46b1208b3004cb393973b8b0c6", "b93656ec69ec2ad9cd1de531596d5aa04a9796db"], "page_rank": 7.037297677691766e-05}, {"id": "75e6b2e12fda8730d358370c05eaf3ecd4b75a35", "title": "Approaches To Thesaurus Production", "authors": ["A. Michiels", "Jacques No{\\\"e}l"], "date": 1982, "abstract": "We contrast two approaches to thesaurus production: the traditional and intuitive one versus the Amsler-type procedure, which interactively generates filiations among the genus words in a computerized dictionary. We discuss the application of such a procedure to our lexical data base (LONGMAN DICTIONARY OF CONTEMPORARY ENGLISH).", "references": [], "page_rank": 0.0002463054187192118}, {"id": "5c3391bde2bb1b3d737913ee8caa01492a782732", "title": "WHO Technical Report", "authors": ["J. Wister Meigs"], "date": 1954, "abstract": "The Feather River Coordinated Resource Management Group (FR-CRM) has been restoring channel/ meadow/ floodplain systems in the Feather River watershed since 1985. Project and watershed-wide monitoring has shown multiple benefits of this type of work. With the concern over global climate change, the group wanted to measure the carbon sequestered in project areas. No protocol was found to measure carbon stores in native Sierra Nevada meadows. Plumas County funded the FR-CRM to conduct a pilot study to develop such a protocol. The sampling protocol included discrete sampling at consistent soil depths to determine the vertical distribution of carbon. A Technical Advisory Committee developed and refined a multi-project sampling protocol for three restored meadows and three un-restored meadows. Data from the un-restored meadows will also provide base-line data for before and after restoration comparisons. Initial data analysis indicates that restored meadows contain twice as much total carbon as degraded meadows; on average approximately 40 tonnes more carbon per acre. Virtually all of the additional carbon in restored meadows occurs in the soil, and is thus protected from loss via grazing, haying, wildfire, etc. Introduction In 1994 the Feather River Coordinated Resource Management (FR-CRM) group shifted its stream restoration approach from bank stabilization to landscape function. Called meadow re-watering, this approach entails returning the incised stream channel to the remnant channel(s) on the historic floodplain and eliminating the incised channel as a feature in the landscape. Historic channel incision resulted in significant land degradation as the adjacent groundwater levels dropped commensurate with the incising stream bed. Vegetation conversion rapidly follows as deep, densely rooted meadow plant communities convert to xeric shrubs and other plants. After a decade of meadow restoration, the FR-CRM recognized the possibility of a significant change in carbon stocks in these restored meadows and valleys. Plumas County has been a leader in advocating for investment in watershed ecosystem services such as water storage and filtering, and now, carbon sequestration. The county provided funding for the FR-CRM to conduct a pilot study of carbon in biomass and soils. Watershed Location and Characteristics The upper Feather River watershed is located in northeastern California encompassing 3,222 square miles that drains west from east of the Sierra crest into Oroville Reservoir and thence to the Sacramento River. Annual runoff produced from this watershed provides over 1,400 MW of hydroelectric power, and represents a significant component of the California State Water Project, annually providing 2.3 millionacre feet of water for urban, industrial and agricultural consumers downstream. The Feather River watershed is primarily comprised of two distinct geologies: the Sierra Nevada granitic batholith of the western third of the watershed; and Basin and Range fault-block meta-volcanics, metasedimentary and recent basalts in the eastern two-thirds. It is the Basin and Range zone (Diamond Mtns.) of the watershed that has been the primary area of restoration. This geologic m\u00e9lange of faulted and weathered rock has resulted in over 390 square miles of expansive meadows and valleys comprised of deep fine grained alluvium, shown as green and yellow in Figure 1. Figure 1. Upper Feather River Watershed Upper watershed meadows and valleys (shown as green/yellow in Figure 1), often dozens of miles in length, once supported a rich ecosystem of meadow and riparian habitats, for coldwater-loving trout, a diversity of wildlife, and indigenous peoples during the dry summers of California\u2019s Mediterranean climate. The densely rooted vegetation, cohesive soils and expansive floodplains all contributed to the sustainability of these meso-scale floodplain meadows, with associated alluvial fans. River system segments are often characterized simplistically as transport and depositional reaches. Depositional reaches feature lower gradients and a more expansive fluvial setting. These landscape attributes, in conjunction with the type and quantity of sediment, debris and nutrients, are what provide for the development and evolution of meso-scale \u201csinks\u201d or \u201cwarehouses\u201d, for the hydrologic products of the basin. Viewed as a macro-hyporheic corridor ( Harvey and Wagner, 2000; Boulton, et.al., 1998; Stanford and Ward, 1993) these features are crucial as a landscape zone of active mass and energy transfer as well as an active storage reservoir for water, sediment and nutrients. The long-term recruitment and evolution of these features involve physical, Figure 2. Typical Alluvial Features biological and chemical synthesis within the natural variability of fluvial processes. Euro-American settlement of the watershed began in 1850 with gold mining in the western portions of the watershed and, soon thereafter, agricultural production in meadows to support the mining communities. Dairy farming, horses (for cavalry mounts), sheep and beef cattle were some of the early intensive disturbances that led to localized channel incision. The resultant lowering of shallow groundwater elevations began to alter and weaken the vegetative structure of the system. Soon, near the burgeoning communities in the mid-elevation valleys, a permanent road system was established with frequent channel manipulation and relocation efforts to simplify drainage and minimize bridge construction, again leading to localized incision. In the early 1900\u2019s both an intercontinental, and numerous local, railroad systems were constructed throughout the watershed. The local railroad networks, for the purpose of both mining and logging, were routed through the long low-gradient valleys for ease of construction. These valleys were still relatively wet at that time so elevated grades were constructed using adjacent borrow ditches. By 1940, the severe morphological changes imposed by the railroad grades, in conjunction with the above referenced land use impacts resulted in rapid, severe systemic incision of many upper watershed meadow systems. In the mid 1980\u2019s numerous watershed stakeholders adopted a statutory authority that allowed for Coordinated Resource Management and Planning (CRMP). Twenty-four federal, state and local, public and private entities now form the Feather River Coordinated Resource Management (FRCRM) group to adopt, support and implement a watershed-wide restoration program. FR-CRM Restoration Approach & Background The FRCRM began an ongoing implementation program to address these watershed issues in 1990. Initially, these projects focused on geomorphic restoration techniques (Rosgen, 1996) to stabilize incised stream channels. While overall success was encouraging, the projects illustrated the concept that any restoration work in the incised channels was subject to elevated stresses even in moderate flood events (510 year return interval). Concurrently, the benefits from this approach were localized and limited to reduced erosion, and incremental improvement of aquatic habitats and water quality. Little overall improvement of watershed conditions was being realized (Wilcox, et al 2001). This led to re-evaluating restoration approach to encompass the entire historic fluvially-evolved valley bottom. Called meadow re-watering, this approach entails returning the incised stream channel to the remnant channel(s) on the historic floodplain and eliminating the incised channel as a water conveyance feature in the landscape (Figures 3 & 4 and photos 1a, 1b, 2a & 2b). Simultaneously, the FRCRM had received a project assistance request from the United States Forest Service, Plumas National Forest (PNF) to develop restoration alternatives for Cottonwood Creek in the Big Flat Meadow (Photos 2a & 2b). FRCRM staff, led by Jim Wilcox, began conducting surveys and data collection that included the entire relic meadow from hillslope to hillslope. This data collection effort quickly pointed to the nascent meadow re-watering technology as a likely restoration alternative. Figure 3. Typical cross-section, showing pre-project incision, post-project plug elevation, and the new channel. Photos 1a and 1b below show this same cross-section, however, the entire gully is not shown in the pre-project photo. Photo 1aClarks Creek Pre-project, July, 2001 Photo 1bClarks Creek Post project, July, 2006 The rocks in the background of photos 1a and 1b can be used for reference. Because the new channel is in a different location, the photo point also moved in order to show the channel in the preand postproject conditions. Figure 4. Typical cross-section, showing pre-project incision, post-project plug elevation, and the new channel. Implemented in 1995, this project quickly validated the fundamental soundness of this approach. The one mile long, 47 acre project produced elevated shallow groundwater levels, eliminated gully wall erosion, filtered sediments delivered from the upper watershed, extended and increased summer baseflows, and reversed the xeric vegetation trends resulting in improved terrestrial, avian and aquatic habitats. These benefits persisted despite withstanding a 100-year RI (return interval) flood in 1997. Photo 2aBig Flat Pre-project, Dec.,1993 Photo 2bBig Flat Post project, May, 2006 The success of this initial project led to the implementation of an additional 18 projects utilizing this technology (Table 1.). Varying in scale and watershed characteristics, these projects have restored another 20 miles of channel and 5,000 acres of meadow/floodplain. Carbon Sequestration Qualitatively, these projects appeared to significantly increase organic carbon stocks through the much increased root mass as well as increased surface growth, and, possibly, through the more effective hyporheic exchange throughout the meadow. The purpose of the following protocol is to quantitatively establish the effe", "references": ["eb0433978381e6abbec06d05ac05a141da7d918f"], "page_rank": 7.037297677691766e-05}, {"id": "cf967bc6d2c2d7f9a7ba6b8e03cefb18e40b0e0f", "title": "Computer Methods for Morphological Analysis", "authors": ["Roy J. Byrd", "Judith L. Klavans", "Mark Aronoff", "Frank Anshen"], "date": 1986, "abstract": "This paper describes our current research on the properties of derivational affixation in English. Our research arises from a more general research project, the Lexical Systems project at the IBM Thomas J. Watson Research laboratories, the goal for which is to build a variety of computerized dictionary systems for use both by people and by computer programs. An important sub-goal is to build reliable and robust word recognition mechanisms for these dictionaries. One of the more important issues in word recognition for all morphologically complex languages involves mechanisms for dealing with affixes.", "references": ["9c6e68a6d704d0d9518a807584a469f72a4c66c9", "2ff60c8f2df9a57b5dfba38e9be6d6401d88a1e9", "6c6a9548e18e372fd381794f18dcee518c9f506d", "8114586ed6d50eb2c74c11d52621c7cee780ee2d", "d5ffea285a1e5dc7e65e54d3078367817e31d2ef", "58558d4f31b8c95fb904ec4b31ff6aadc2fea9ff", "7b41ca22c20c5ad4ba32eacccfc215e6e1813429", "629a902def4fae97e47505fced2f800a583ad4ca"], "page_rank": 0.0001231527093596059}, {"id": "804d36ad793d55372890bc07c0c6c62d473199f6", "title": "A new computer format for Webster's seventh collegiate dictionary", "authors": ["Donald R. Sherman"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"A new computer format for Webster's seventh collegiate dictionary\" by Donald R. Sherman", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "16d09e9b4a138525814ce9d14bad0c2cd988193f", "title": "On the Thresholds of Knowledge", "authors": ["Douglas B. Lenat", "Edward A. Feigenbaum"], "date": 1987, "abstract": "We articulate the three major findings of AI to date: (1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates. (2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but farflung knowledge. (3) AI as Empirical Inquiry: we must test our ideas experimentally, on large problems. Each of these three hypotheses proposes a particular threshold to cross, which leads to a qualitative change in emergent intelligence. Together, they determine a direction for future AI research.", "references": ["f07c5ed1ce55f49a9a4d4109279aef80e52e8fdc", "1b115815f55d48ff59dc1208d7d410edeeefddff", "74dc088ad25c433739699aaf97b8444c9817d431", "7ab63b8e979744b58ff637458011152a9d4b677c", "37ab31f586cdc1efc3bf0dcc9ba52f644077e466", "568dbb56f62747d45bf5f595fb97c4acb9e80871", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "c648f156be45c4aeceb23fc379a2f49d8310356a", "f1feac581cf9ca99332d93835122ec22d9246302", "344dd41c8b69886e3bb89cdf0667d87e606dbcf6"], "page_rank": 9.852216748768472e-05}, {"id": "ce0eedb0406268fdbb705702bcab012121f289c7", "title": "A Semantic Expert Using an Online Standard Dictionary", "authors": ["Jean-Louis Binot", "Karen Jensen"], "date": 1987, "abstract": "A system has been developed to find the most likely attachment for prepositional phrases in English sentences in a fairly unrestricted way. The system receives as input a syntactic sentence parse provided by a general-purpose computational grammar called PEG (PLNLP English Grammar) The semantic decision that is necessary to make the right attachments is made (a) by parsing (also with PEG) the natural language definitions of an online standard dictionary, in this case Webster's Seventh New Collegiate Dictionary; (b) by relating words to other words in the dictionary; and (c) by reasoning heuristically about the comparative likelihood of different possible attachments. The basic assumption of this research is that natural language itself is a knowledge representation language that can be conveniently accessed and richly exploited. Techniques such as those presented here offer hope for eliminating the time-consuming and often incomplete hand coding of semantic information that has been conventional in natural language understanding systems.", "references": ["2f3ae222bd3ac630e2ea1e9e4601c62cc73014dc", "e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "00c7b37ad903430676bd6afb825f7ce1a8d4cecf", "8c742586839a091b504520e96c9c937bcba6eef2", "88d9d2d571d5fea932cb4cdffe1ff390d221ba50", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "65b4da712c649b2c4d0d91faa105f1f3a2638405", "ab5937191714bc2c41d8f8edebb3af54227b2fc8"], "page_rank": 9.852216748768472e-05}, {"id": "b6055cc5c4d70f2f66ff3d05b85113114cf1fa2b", "title": "Towards The Organization Of Lexical Definitions On A Database Structure", "authors": ["Nicoletta Calzolari"], "date": 1982, "abstract": "P r i n t e d d i c t i o n a r i e s a re g r e s t r e p o s i t o r i e s of i n f o r m a t i o n , and i t i s i m p o r t a n t t h a t they can be e x p l o i t e d as f u l l ~ as p o s s i b l e , w i t h r e g a r d to a l l the d i f f e r e n t t ype s of d a t a they c o n t a i n . This was one of the aims when o r g a n i z i n g the Machine D\u00a3otionax7 of the I t a l i a n l anguage on a d a t a b a s e s t r u c t u r e .", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "e7687c6c22bacedfdadf89ac9968f304ce420d90", "title": "Phonological rules and finite--state transducers", "authors": ["Ronald M. Kaplan", "Martin Kay"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"Phonological rules and finite--state transducers\" by Ronald M. Kaplan et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "5d963629d0f601a71df76b54d63126836bf4fbe6", "title": "An Attempt To Computerized Dictionary Data Bases", "authors": ["Makoto Nagao", "Jun'ichi Tsujii", "Yoshihiro Ueda", "M. Takiyama"], "date": 1980, "abstract": "Two dictionary data base systems developed at Kyoto University are presented in this paper. One is the system for a Japanese dictionary ( Shinmeikai Kokugojiten, published by Sansei-do) and the other is for an English-Japanese dictionary (New Concise English-Japanese Dictionary, also published by Sansei-do). Both are medium size dictionaries which contain about 60,000 lexical items. The topics discussed in this paper are divided into two sub-topics. The first topic is about data translation problem of large, unformatted linguistic data. Up to now, no serious attempts have been made to this problem, though several systems have been proposed to translate data in a certain format into another. A universal data translator/verifier, called DTV, has been developed and used for data translation of the two dictionaries. The detailed construction of DTV will be given. The other sub-topic is about the problem of data organization which is appropriate for dictionaries. It is emphasized that the distinction between 'external structures' and 'internal structures' is important in a dictionary system. Though the external structures can be easily managed by general DBMS's, the internal (or linguistic) structures cannot be well manipulated. Some additional, linguistic oriented operations should be incorprated in dictionary data base systems with universal DBMS operations. Some examples of applications of the dictionary systems will also be given.", "references": ["d200e5dc12592dcd0d229b37282d54822723dc9f", "d0c2e3aff79a7dedf8de54dc9b83f433abbc1148", "7d2404708290844881bddebec59af7e4e28ac58e", "09104ff087833074e6723bda4d79eb89387827d3", "3751b395f1280d497debf9070713008c7a803265"], "page_rank": 0.0002463054187192118}, {"id": "0869f6b4b61cecb11bfabfcd90b15deb3b5b1b5d", "title": "Phonic interference and the prefix effect", "authors": ["Robert G. Crowder"], "date": 1969, "abstract": "Forty-two S s recalled 90 series of 8 consonant letters, 30 under control conditions and 60 under each of two prefix conditions. The prefix letter was either V or K . In each 8-letter series half of the letters were phonically similar to V (from the subset BDGTPZ) and half were not (from the subset HJLNRX). An account of the prefix effect based on retroactive inhibition predicts that (a) V as prefix should disrupt recall more than K , and ( b ) the difference between prefix effects of V and K should be larger in performance on the BDGTPZ subset than on the JHLNRX subset. Both predictions were supported by the results.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "92555e5f676d13a1a0093337fba8f0d420793e1c", "title": "On the organization of the lexicon", "authors": ["Stanimir Raki{\\'c}"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"On the organization of the lexicon\" by Stanimir Raki\u0107", "references": [], "page_rank": 0.00016420361247947453}, {"id": "d471eed0a18376bc9dd3703843f8a5ef6e751424", "title": "Articulatory coding in short-term memory", "authors": ["Douglas L. Hintzman"], "date": 1967, "abstract": "Confusion errors in short-term memory for visually-presented nonsense syllables were analyzed to determine the influence of two articulatory features of consonants: voicing and place of articulation. Both were found to contribute to confusions. Results are interpreted as consistent with a hypothesis of mediation by kinesthetic cues arising from subvocal rehearsal, and inconsistent with a hypothesis of mediation by an auditory image.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8333c5478111681392343266e4165839b03e8b61", "title": "Abstraction and The Process of Recognition", "authors": ["Michael I. Posner"], "date": 1970, "abstract": "Publisher Summary This chapter discusses the abstraction and the process of recognition. It is concerned with the successive stages of processing that are involved in the encoding of simple stimuli and with the record that each stage produces. These issues lie within the areas of perception and memory, respectively. Because the recognition of stimuli is impossible without stored information, it will also be necessary to consider learning of trace systems applicable to the classification of patterns never before seen. The process of moving from the top to the bottom may be called abstraction. In psychological research, the term abstraction has been used in two different ways. One sense of abstraction involves the selection of certain portions or aspects of an experience. A second sense refers to the classification of a stimulus into a wider or more inclusive superordinate category. The second sense of abstraction has been used primarily with the investigation of object names. This sense of abstraction does not involve selection of any physical aspect of the stimulus, but rather a relationship between a particular stimulus name and another broader category name.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "318897a75afda96e62a8572834d3dbdb3d82e2ad", "title": "On the comparison between identification and discrimination tests in speech perception", "authors": ["Irwin Pollack", "David B. Pisoni"], "date": 1971, "abstract": "The conversion between identification and three forms of discrimination tests, based on the extreme assumption that discrimination performance in speech perception tests is determined solely by labeling in identification tests, is examined. Identical conversions are obtained for the two-interval same-different test, for the four-interval forced-choice test of pair similarity, and for the three-interval ABX test. Therefore, differences in discrimination performance among these tests are presumably due to their respective task requirements, not to differential prediction.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "a546750bd366343354b745de9476d3cc52025a93", "title": "The Information of Elementary Auditory Displays. II", "authors": ["Irwin Pollack"], "date": 1952, "abstract": "Whereas the ear's sensitivity for detecting a difference in frequency between two tones is remarkably acute, the ability of listeners to identify (and name) tones presented in isolation is relatively poor. When the frequency of a single tone (in the frequency range 100\u20138000 cps) is varied in equal logarithmic steps (and when the sound level is arbitrarily varied to reduce loudness cues), the amount of information transferred is about 2.3 bits. (The equivalent proficiency of response for an informational transfer of 2.3 bits is perfect identification among only 5 tones.) The amount of information that can be transferred is, within rather wide limits, independent of the number of tones and the range of frequencies employed.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "4cf555148a3ad2ad736eae77f0d9ffa7eb0140f7", "title": "Chronometric analysis of classification.", "authors": ["Michael I. Posner", "Richard Frank Mitchell"], "date": 1967, "abstract": "This series of studies represents an effort to extend the subtractive method of Bonders to the analysis of depth of processing in simple classification tasks. The stimuli are always pairs of items (letters, nonsense forms, digits) to which S must respond \"same\" or \"different\" as quickly as possible. Levels of instruction are physical identity (e.g., AA), name identity (e.g., Aa), and rule identity (e.g., both vowels). By use of the subtractive method, times for matches at each level are analyzed. The emphasis is not placed upon the times themselves but upon their relevance for understanding the operations and mechanisms involved in perceptual matching, naming, and classifying. Nearly 100 years ago the Dutch physiologist Donders presented a paper (Bonders, 1868) on the time for simple cognitive operations. This wellknown paper initiated the use of the subtractive method of latency analysis to measure the time for internal mental processes such as recognition and choice. Although the subtractive method has received a good deal of criticism (Boring, 1950), there is once again active interest in pursuing it. Recent work includes detailed analysis of successive stages in simple reaction time (McGill, 1963), separation of recognition from choice time (Taylor, 1966), effect of task variables such as S-R compatibility upon the component times (Broadbent & Gregory, 1965), and development of dynamic decision models to predict and explain various components of choice time (Fitts, 1966; Stone, 1960).", "references": [], "page_rank": 0.0001231527093596059}, {"id": "cb7852175e42497c7e8fa5f33114f867071d9266", "title": "Noncategorical perception of a voiced stop: A replication", "authors": ["J. Richard Barclay"], "date": 1972, "abstract": "Several studies have purported to show that perception to stop consonants is categorical, i.e., the stimuli are discriminated only slightly better than they are identified. However, all of these studies have employed successive rather than simultaneous discrimination tasks, confounding the effects of memory with those of immediate perception. The present experiment minimized the role of memory by requiring Ss to identify the same set of synthetic voiced stops twice, first using three response categories (b, d, g) and then using two (b, g). Results indicated that six out of seven Ss who appeared to be categorical perceivers on the basis of preliminary traditional measures later demonstrated noncategorical perception of /d/. A view of speech processing was suggested whereby consonants are perceived continuously but remembered categorically in terms of an articulatory code.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "56c16d9e2a5270ba6b1d83271e2c10916591968d", "title": "Human Memory: A Proposed System and its Control Processes", "authors": ["Richard C. Atkinson", "Richard M. Shiffrin"], "date": 1968, "abstract": "Publisher Summary This chapter presents a general theoretical framework of human memory and describes the results of a number of experiments designed to test specific models that can be derived from the overall theory. This general theoretical framework categorizes the memory system along two major dimensions. The first categorization distinguishes permanent, structural features of the system from control processes that can be readily modified or reprogrammed at the will of the subject. The second categorization divides memory into three structural components: the sensory register, the short-term store, and the long-term store. Incoming sensory information first enters the sensory register, where it resides for a very brief period of time, then decays and is lost. The short-term store is the subject's working memory; it receives selected inputs from the sensory register and also from long-term store. The chapter also discusses the control processes associated with the sensory register. The term control process refers to those processes that are not permanent features of memory, but are instead transient phenomena under the control of the subject; their appearance depends on several factors such as instructional set, the experimental task, and the past history of the subject.", "references": [], "page_rank": 0.00031746031746031746}, {"id": "8b1d36854d72805bc5008f8dbc7909bb1ee23e5f", "title": "Computer\u2010Controlled PCM System for Investigation of Dichotic Speech Perception", "authors": ["Franklin S. Cooper", "Ignatius G. Matingly"], "date": 1969, "abstract": "To facilitate study of dichotic speech perception, a computer\u2010controlled PCM system has been devised for preparation of dichotic tests from natural speech. A test is a series of paired utterances, presented one at each ear, simultaneously or with interaural delay. In compiling a test, each of several tape\u2010recorded utterances is digitized at a 8\u2010kHz rate (10 bits/sample) and read into computer memory. The stored utterance is played back, and its waveform simultaneously displayed on a storage oscilloscope. The experimenter defines the exact beginning and end of the utterance and adjusts its intensity. The edited utterance is saved with others on a disk file. According to the experimenter's test order, the two utterances of each pair are retrieved, converted with a specified relative delay, and recorded each on a separate audio tape track. The system is a useful tool for investigation of lateralization and fusion in speech perception. [Support from the Office of Naval Research and the National Institute of H...", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "d1489ef5b150196ba45b7ae61e5c68ac86293a5f", "title": "Auditory and Phonetic Processes in Speech Perception: Evidence from a Dichotic Study().", "authors": ["Michael Studdert-Kennedy", "Donald P. Shankweiler", "David B. Pisoni"], "date": 1972, "abstract": "The distinction between auditory and phonetic processes in speech perception was used in the design and analysis of an experiment. Earlier studies had shown that dichotically presented stop consonants are more often identified correctly when they share place of production (e.g., /ba-pa/) or voicing (e.g., /ba-da/) than when neither feature is shared (e.g., /ba-ta/). The present experiment was intended to determine whether the effect has an auditory or a phonetic basis. Increments in performance due to feature-sharing were compared for synthetic stop-vowel syllables in which formant transitions were the sole cues to place of production under two experimental conditions: (1) when the vowel was the same for both syllables in a dichotic pair, as in our earlier studies, and (2) when the vowels differed. Since the increment in performance due to sharing place was not diminished when vowels differed (i.e., when formant transitions did not coincide), it was concluded that the effect has a phonetic rather than an auditory basis. Right ear advantages were also measured and were found to interact with both place of production and vowel conditions. Taken together, the two sets of results suggest that inhibition of the ipsilateral signal in the perception of dichotically presented speech occurs during phonetic analysis.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "fe843bf6a7ee65c58382267e1a0695006c189f1b", "title": "Monitoring and storage of irrelevant messages in selective attention", "authors": ["Anne Treisman"], "date": 1964, "abstract": "Summary A series of experiments was designed to investigate the monitoring and storage of the irrelevant message during selective attention to one of two dichotic speech messages, and thus to throw light on the nature and level of selective \u201cfiltering\u201d (Broadbent, 1958) . The experiments extend Cherry's finding (1953) that Ss noticed if the rejected message was identical to the selected one but a few seconds out of step. It was shown that (a) the comparison between the messages must be made at a central level, and required the identification of words and meaning rather than a simple analysis of the sounds, since S s noticed the identity of the messages even when they were in different voices or different languages; (b) their identity is noticed at a 5-sec interval when a selected message of coherent prose leads the rejected one in time, but at 1 or 2 sec only when the order is reversed. The first result supports Treisman's suggestion (1960) that the filter acts by attenuating rather than blocking irrelevant signals. The second may give a measure of the different storage times for selected and rejected material. The critical delay interval for recognition also varied with the verbal content of the selected message: it decreased both when the information content of the words was high, and when the order of the items was crucial to the recognition. The relation of these to other findings on immediate memory is discussed.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "97217086c1ecafc3561b865ff1228a22320b1f6f", "title": "The sound of vowels and consonants in immediate memory", "authors": ["Robert G. Crowder"], "date": 1971, "abstract": "When auditory and visual presentation are compared in tests of immediate ordered recall of natural-language stimuli such as digits or letters, the typical finding is better performance on the auditory than on the visual lists. This difference is located in late portions of the serial position curve, and the advantage can be partially removed by presenting a redundant stimulus suffix. In the present research this type of comparison was extended to lists of consonant-vowel (CV) syllables varying either in the initial consonant or in the terminal vowel. The finding was that memory for vowels closely matched the results with digits or letters whereas memory for consonants showed neither the basic modality effect nor the suffix effect. Thus, the special memory system associated with auditory presentation may be said to \u201ccontain\u201d vowels but not consonants.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "78a93bcb36b2e93d39d2b18d74277e95387a328b", "title": "Retroactive interference in short-term recognition memory for pitch.", "authors": ["Dominic W. Massaro"], "date": 1970, "abstract": "Investigated the effects of different retroactive stimuli on pitch discrimination in a short-term recognition memory task. 4 female undergraduates served as Ss in Exp. I, and 50 in Exp. II. Tones, Gaussian noise, and \u201cblank\u201d stimuli were employed in the retroactive (interference) interval. The effects of different stimuli in the interference (I) interval are highly dependent on the strategies of S. Tones or noise in the I interval produce more forgetting than blank I intervals. Decrease in accuracy of perceptual memory over time with filled I intervals was attributed to interference rather than decay. A storage-forgetting model of perceptual memory described the quantitative results accurately. (16 ref.) (PsycINFO Database Record (c) 2006 APA, all rights reserved)", "references": ["4da05489252dc07844302a4c41dc6c1631fdc94d", "63d8274d869f7b350949b3a6fb1736987bba6b06", "01447dd22d9b5ba901532cfc84eade7e206b07f6", "795bca56a7ec3812c6c618fd54f8d21e00039b3d"], "page_rank": 9.852216748768472e-05}, {"id": "e9fa01e4b9be8052d45ab5be3c54761229575fb8", "title": "Categorical and noncategorical modes of speech perception along the voicing continuum.", "authors": ["David B. Pisoni", "Joan Lazarus"], "date": 1974, "abstract": "Native speakers of English identified and then discriminated between stimuli which varied in voice onset time (VOT). One group of listeners identified a randomized sequence of stimuli; another group identified an ordered sequence of stimuli, in which stimuli from the VOT continuum were presented in a consecutive order. Half of the Ss in each group then received one of two discrimination formats: the ABX discrimination test in which X was identified with A or with B, or 4IAX test of paired similarity in which two pairs of stimuli\u2014one pair always the same and one pair always different\u2014were presented on each trial. Noncategorical perception of the voicing distinction, reflected by an improvement in discrimination within phonetic categories, was obtained for the group of listeners who experienced both the sequential identification procedure and the 4IAX discrimination test. The results are interpreted as providing evidence for separate auditory and phonetic levels of discrimination in speech perception.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "101c7d3c3444f048ba7e6f794cd914099cad74be", "title": "The discrimination of speech sounds within and across phoneme boundaries.", "authors": ["Alvin M. Liberman", "Katherine Safford Harris", "Howard S. Hoffman", "Belver C. Griffith"], "date": 1957, "abstract": "In listening to speech, one typically reduces the number and variety of the many sounds with which he is bombarded by casting them into one or another of the phoneme categories that his language allows. Thus, a listener will identify as b, for example, quite a large number of acoustically different sounds. Although these differences are likely to be many and various, some of them will occur along an acoustic continuum that contains cues for a different phoneme, such as d. This is important for the present study because it provides a basis for the question to be examined here: whether or not, with similar acoustic differences, a listener can better discriminate between sounds that lie on opposite sides of a phoneme boundary than he can between sounds that fall within the same phoneme category. There are grounds for expecting an affirmative answer to this question. The most obvious, perhaps, are to be found in the common experience that in learning a new language one often", "references": ["e97817dc4d325f08231f333938920f094eadccf3", "23d55be51b73b482d2c1660cd53d427dceca7a63", "659e3fabb9ae054903b1b29aa5c50e877b7d3366", "8f9c8b1329a8975690e736c238df5a94a208bede"], "page_rank": 0.00016009852216748767}, {"id": "e1bf4a5f294ab5df170eaebe744a23971ac6b57b", "title": "Memory While Shadowing", "authors": ["D. A. Norman"], "date": 1969, "abstract": "Subjects were asked to repeat verbally (shadow) English words which were presented to one ear. They were then tested for their memory of two-digit numbers which were presented to their other ear. It is demonstrated that subjects have no memory for these digits if they must continue to shadow for 20 sec. before being tested on their memory for the digits. However, if tested immediately after the digit presentation, they do remember some digits. Hence, verbal material presented on non-attended channels gets into a short-term memory, but is not transferred into long-term memory.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "c8d5cf5d2cc936ae3a6f7a491fdfa2b126a43259", "title": "Short-term visual storage", "authors": ["Steven W. Keele", "William G. Chase"], "date": 1967, "abstract": "The delay between the offset of a briefly exposed array of letters and digits and the onset of an arrow pointing at one of the array positions was varied from 0 to 5000 msec. In addition, the luminance of the stimulus array was varied over three levels. The Ss reported the item in the position indicated by the arrow. Luminance, delay, and the luminance by delay interaction were all significant I Performance monotonically decreased from a delay of 0 msec to a delay of 250 msec, but the percent correct remained fairly constant from 250 msec to 5000 msec. With delays shorter than 250 msec, high luminance arrays showed better performance.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "358bcc42e45cfb9e676b905e8307a0de3b23b0ae", "title": "Memory for nonattended auditory material", "authors": ["Sam Glucksberg", "George N Cowen"], "date": 1970, "abstract": "Abstract Subjects performed a dichotic listening task. They shadowed prose in one ear and attempted, after delays ranging from 0.3 to 20.3 sec, to recall single digits that had been embedded in prose presented to the nonattended ear. Recall performance decreased from 0 to 5 sec in a typical negatively accelerated fashion. No memory for the nonattended material was apparent between 5 and 20.3 sec. The data indicate that verbal material presented auditorily to a nonattended channel persists briefly, but is not transferred into a long-term store.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "460b19f7be6a600e74ad99cc2e729bad1bb6f7f1", "title": "Retrieval from iconic memory with shape as the selection criterion", "authors": ["Michael T. Turvey", "Shlomo Kravetz"], "date": 1970, "abstract": "Selection of items from iconic memory on the basis of shape was studied by means of the Sperling (1960) partial report procedure. The results indicated that partial report by shape was superior to whole report, that partial report by location was superior to partial report by shape, and that the accuracy of report as a function of delay of indicator declined at the same rate for selection by shape and selection by location. The results were discussed in the framework of the flow of visual information within S.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "de0a2f9ccf515771636976c55017eee7d55472df", "title": "A Project for a Bilingual Lexicai Database System", "authors": ["Nicoletta Calzolari", "Eugenio Picchi"], "date": 1986, "abstract": "Semantic Scholar extracted view of \"A Project for a Bilingual Lexicai Database System\" by Nicoletta Calzolari et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "bac7597653508d0eafb84b293ff07fcc20441026", "title": "EXPERIMENTS ON THE FOUR-EARED MAN.", "authors": ["Neville Moray", "Angela N. Bates", "Thomas Dr. Barnett"], "date": 1965, "abstract": "Subjects were required to listen to messages consisting of 1, 2, 3, or 4 letters of the alphabet over each of 1, 2, 3, or 4 channels. It was found that increasing the number of channels above 2 had a markedly deleterious effect upon recall of the messages, 2 letters over each of four channels being less well recalled than 4 letters over each of two channels. In a second experiment, it was found that, providing only one channel was required in recall, and this one was indicated by a light immediately after the stimuli had been presented, then the decrement could be largely offset and the total number of signals stored held constant at about 8 regardless of how many channels were used. However, if the letters had to be recalled in exactly the correct order in which they arrived, performance dropped to almost zero. The results are discussed in terms of the channel capacity of the nervous system and in relation to corresponding findings in other sense modalities.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f487a24e7f7329da852d0638a130e00aae08a598", "title": "LOGIN: A Logic Programming Language with Built-In Inheritance", "authors": ["Hassan A{\\\"i}t-Kaci", "Roger Nasr"], "date": 1986, "abstract": "D An elaboration of the PROLOG language is described in which the notion of first-order term is replaced by a more general one. This extended form of terms allows the integration of inheritance-an IS-A taxonomy-directly into the unification process rather than indirectly through the resolutionbased inference mechanism of PROLOG. This results in more efficient computations and enhanced language expressiveness. The language thus obtained, called LOGIN, subsumes PROLOG, in the sense that conventional PROLOG programs are equally well executed by LOGIN. a", "references": [], "page_rank": 0.00016420361247947453}, {"id": "b93656ec69ec2ad9cd1de531596d5aa04a9796db", "title": "HOPE: An experimental applicative language", "authors": ["Rod M. Burstall", "David B. MacQueen", "Donald Sannella"], "date": 1980, "abstract": "An applicative language called HOPE is described and discussed. The underlying goal of the design and implementation effort was to produce a very simple programming language which encourages the construction of clear and manipulable programs. HOPE does not include an assignment statement; this is felt to be an important simplification. The user may freely define his own data types, without the need to devise a complicated encoding in terms of low-level types. The language is very strongly typed, and as implemented it incorporates a typechecker which handles polymorphic types and overloaded operators. Functions are defined by a set of recursion equations; the left-hand side of each equation includes a pattern used to determine which equation to use for a given argument. The availability of arbitrary higher-order types allows functions to be defined which 'package' recursion. Lazily-evaluated lists are provided, allowing the use of infinite lists which could be used to provide interactive input/output and concurrency. HOPE also includes a simple modularisation facility which may be used to protect the implementation of an abstract data type.", "references": ["0fb3b1a16b1d1db4a45b4b1d996ec04e684d073e", "3eb24a1d6094ff9ddf940ad22dc848533d62ff89", "3596e5121dd74f173400cd38eea4bef68a2b475d", "85c796b4df4cc325164a78166d8a2316f0ee50b1"], "page_rank": 0.00016420361247947453}, {"id": "7b41ca22c20c5ad4ba32eacccfc215e6e1813429", "title": "Topics in English morphology", "authors": ["D C Siegel"], "date": 1974, "abstract": "Massachusetts Institute of Technology. Dept. of Foreign Literatures and Linguistics. Thesis. 1974. Ph.D.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "58558d4f31b8c95fb904ec4b31ff6aadc2fea9ff", "title": "A grid-based theory of English meter", "authors": ["Bruce Hayes"], "date": 1983, "abstract": "In recent years, we have seen a real deepening of our understanding of the nature of poetic meter and the English metrical system. In part, this has resulted from the application of the formal methods oflinguistics to metrical study-the willingness to formulate metrical rules explicitly and to check out their consequences has naturally led to progress. However, the new results also depend on recent advances in notions of prosodic phonological structure, as an account of a metrical system can only be as adequate as the theory of prosodic structure on which it is based. In particular, the work of Kiparsky (1977) has shown that the metrical theory of stress, as developed in Liberman and Prince (1977) and other works, provides the basis for a more adequate and explanatory theory of English meter than the numerological stress representation of The Sound Pattern of English. Kiparsky's work has in turn repaid its debt to metrical stress theory-it is surely an argument in favor of Liberman and Prince's system that it can illuminate the facts of English meter so welL At the level ofdescriptive adequacy that has now been reached, it is fair to say that the study of meter can serve as a window into phonological structure. With this purpose in mind, I offer here what I believe to be an improvement on Kiparsky's analysis; one that simplifies the rules, accounts for the data more accurately.., and links up the English system with what is known about metrical systems in generaL These improvements are the result of basing the rules on a different representation of English stress, drawn again from the work of Liberman and Prince (1977). I argue that the aspects of stress that are relevant to meter are embodied not in metrical trees, as Kiparsky assumes, but rather in metrical grids, which Liberman and Prince develop at the end of their paper in a treatment of rhythmic stress clashes. My goals are to clarify the metrical system of English and to shed light on a current controversy concerning the proper representation of stress.", "references": ["f4b2c8e8a5e1f7ffc44ef817d8acd5a28335b003", "42481caf7319149d121dc2d13c13f2d8c23d5d40", "2ff60c8f2df9a57b5dfba38e9be6d6401d88a1e9", "b8ec853894551c0e7a822df50dc04eccd613d46f", "3e78edbe1416b24a233ec4de5d85c0fe00146e0a", "a7de414039c7f46f108368360bcd94f302b057c9", "f091ca1a36a04adecd2a5e7822c79046497dea76", "4f456830b40d58a422a2ef34a0c8267413a4c923"], "page_rank": 7.037297677691766e-05}, {"id": "76b842cb3813741463f70556f5d904259b7bc829", "title": "A Theory of Type Polymorphism in Programming", "authors": ["Robin Milner"], "date": 1978, "abstract": "The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm w which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot \u201cgo wrong\u201d and a Syntactic Soundness Theorem states that if fl accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on w is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system,", "references": ["9cf159a7e67143dcda48637391f96ee0d6f7ab36", "6c74e63edfd78486826a628ef40e09ab94de6380", "fc64117e5d5ed5947a0c85c55597e4116d6e55c6", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "8c8381de74ea919de58e51dfa0e5416af6c660b6", "cc3873996c805b77fc58f4994f840352d0ce2c78", "359eca57fe42d97cbb67f0b5591869abe5eb5421", "d7c3c95d5b9c3c4fd8b07863d072d59804f2506a", "b04c2feae5fd8ac4c00db47fecd6d52027a13021", "bea7a030fa0e1e63605899d73743307e2b5a7336"], "page_rank": 0.00016420361247947453}, {"id": "eb0433978381e6abbec06d05ac05a141da7d918f", "title": "BUREAU OF THE CENSUS", "authors": ["Robert Wilbur Burgess}"], "date": 1940, "abstract": "This series contains research reports, written by or in cooperation with staff members of the Statistical Research Division, whose content may be of interest to the general statistical research community. The views reflected in these reports are not necessarily those of the Census Bureau nor do they necessarily represent Census Bureau statistical policy or practice. Inquiries may be addressed to the author(s) or the SRD Report Series Coordinator, Statistical Research Division, Bureau of the Census, Washington, D.C. 20233.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "8114586ed6d50eb2c74c11d52621c7cee780ee2d", "title": "Synthesis of speech from unrestricted text", "authors": ["Jared Allen"], "date": 1976, "abstract": "For many applications, it is desirable to be able to convert arbitrary English text to natural and intelligible sounding speech. This transformation between two surface forms is facilitated by first obtaining the common underlying abstract linguistic representation which relates to both text and speech surface representations. Calculation of these abstract bases then permits proper selection of phonetic segments, lexical stress, juncture, and sentence-level stress and intonation. The resulting system serves as a model for the cognitive process of reading aloud, and also as a stable practical means for providing speech output in a broad class of computer-based systems.", "references": ["83f2cacc97a94b29fe9ffae324545b589d8a9beb", "0cc146de638d636bb912e9f08baafc97190a8536", "9bafa4a7836102c6f696c942d5576871b3eaae5a"], "page_rank": 7.037297677691766e-05}, {"id": "d5ffea285a1e5dc7e65e54d3078367817e31d2ef", "title": "The EPISTLE Text-Critiquing System", "authors": ["George E. Heidorn", "Karen Jensen", "Lance A. Miller", "Roy J. Byrd", "Martin Chodorow"], "date": 1982, "abstract": "The experimental EPISTLE system is intended to provide \"intelligent\" functions for processing business correspondence and other texts in an office environment. This paper focuses on the initial objectives of the system: critiquing written material on points of grammar and style. The overall system is described, with some details of the implementation, user interface, and the three levels of processing, especially the syntactic parsing of sentences with a computerized English grammar.", "references": ["74597585b796a23297c826ea5da3f9cf2e12fee3"], "page_rank": 7.037297677691766e-05}, {"id": "344dd41c8b69886e3bb89cdf0667d87e606dbcf6", "title": "Searching for Operational Concept Descriptions in BAR, MetaLEX, and EBG", "authors": ["Jack Mostow"], "date": 1987, "abstract": "Abstract The search control problem in concept learning consists of guiding the search for an operational description of a given target concept. We compare how three concept learning programs address this problem: BAR, MetaLEX, and the PROLOG-EBG implementation of explanation-based generalization. They use domain knowledge, theories of operationality, and training examples to constrain the search, but no one program fully exploits all these sources of constraint. Scaling up to richer knowledge bases and harder learning problems may require integrating these and other constraints into a comprehensive control strategy.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "3751b395f1280d497debf9070713008c7a803265", "title": "Data-Structure of a Large Japanese Dictionary and Morphological Analysis by Using It", "authors": ["Makoto Nagao", "Jun'ichi Tsujii", "Akira Yamagami", "Shuji Tatebe"], "date": 1978, "abstract": "Semantic Scholar extracted view of \"Data-Structure of a Large Japanese Dictionary and Morphological Analysis by Using It\" by Makoto Nagao et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "09104ff087833074e6723bda4d79eb89387827d3", "title": "A Study for English-Japanese Dic-tionary Data Base", "authors": ["Yasuyo Ueda"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"A Study for English-Japanese Dic-tionary Data Base\" by Yasuyo Ueda", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "d0c2e3aff79a7dedf8de54dc9b83f433abbc1148", "title": "A record oriented, grammar driven data translation model", "authors": ["Shaun Liu", "Jack Heller"], "date": 1974, "abstract": "The Data Translation problem is the problem of translating data from one computer stored form into another, either with or without changing its original logical structure. A lot of work have been established in this field, but most of them are geared to the data encountered in the commercial and business environment. Another kind of data, namely the self-defining data which is very common in the humanities data processing environment, has received very little consideration. This paper examines both and proposes an unifying approach \u2014a top down grammar driven method\u2014to the generalized data translation problem.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "d200e5dc12592dcd0d229b37282d54822723dc9f", "title": "An approach to Stored Data Definition and Translation", "authors": ["James P. Fry", "Diane C. P. Smith", "Robert W. Taylor"], "date": 1972, "abstract": "The CODASYL Stored Data Definition and Translation Task Group has for the past two years been investigating the major components necessary in a language for data translation. Data translation is defined as the process whereby data that can be processed on one computer (the source file) can be translated into a form (the target file) which can be used by the same or a different processing system on a possibly different computer.\n Two languages are necessary\u2014a Stored Data Definition Language to characterize the logical structure and physical realization of the source and target files, and a Translation Definition Language for defining how target file data instances are to be derived from source file data instances. This report discusses major components of each language and provides sample statements in terms of an example data translation from a COBOL file to a NIPS/360 file.", "references": ["7b5a4d4edaf8179c7730fb00be349bfe84234370", "16ab5841b0ca19c03ecb44f2edd496eba7c72cca"], "page_rank": 9.852216748768472e-05}, {"id": "7d2404708290844881bddebec59af7e4e28ac58e", "title": "A developmental model for data translation", "authors": ["James P. Fry", "Randall L. Frank", "Ernest A. Hershey"], "date": 1972, "abstract": "A model for generalized data translation is presented. Data translation is defined as \u201cthe process whereby data stored in a form that can be processed on one computer (the source file) can be translated into a form (target file) which can be used by the same or different processing systems on a possibly different computer.\u201d Inputs to the Data Translator are the source data and two descriptive languages which drive the translation process. A description of the source and target data is presented to the data translator in a Stored Data Definition Language (SDDL). This description includes both the logical (data structure) aspects of the data as well as the physical (storage structure) aspects. A Translation Definition Language (TDL) is used to define the source to target translation parameters.\n The data translation model includes several components - the source and target converters, which deal with the storage structure of the data, and a restructurer component which is concerned with changes in the logical structure of the data. A Normal Form of Data is introduced and used to allow the Restructurer to operate independently of the source and target conversion processes. The Normal Form of data provides a means of representing data which is independent of current data structuring dependencies.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "2ff60c8f2df9a57b5dfba38e9be6d6401d88a1e9", "title": "The Sound Pattern of English", "authors": ["Noam Chomsky", "Morris Halle"], "date": 1968, "abstract": "Since this classic work in phonology was published in 1968, there has been no other book that gives as broad a view of the subject, combining generally applicable theoretical contributions with analysis of the details of a single language. The theoretical issues raised in The Sound Pattern of English continue to be critical to current phonology, and in many instances the solutions proposed by Chomsky and Halle have yet to be improved upon.Noam Chomsky and Morris Halle are Institute Professors of Linguistics and Philosophy at MIT.", "references": ["4126239a4f3b89207c64a3d875fb41eb43d040b2", "7a126228141a3467f2ea13cefd362c4956101133", "982368726d6b27fbb8ffffda7f1a06ea7bd1610c", "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "100f9867f10f15274d0fb168b02bc0929245f817", "0e1f03b13cce8bd604ff76fcd325b548a26a5e40"], "page_rank": 0.0005629838142153413}, {"id": "ab5937191714bc2c41d8f8edebb3af54227b2fc8", "title": "Are There Preference Trade-offs in Attachment Decisions?", "authors": ["Lenhart K. Schubert"], "date": 1986, "abstract": "The paper argues for an affirmative answer to the question, against the view that correct attachment decisions can be made by a serial process that considers alternatives in some order and accepts the first \"satisfactory\" alternative. The pitfall in serial strategies is that they are apt to finalize their choice while \"the best is yet to come\".", "references": ["00d975899150b364784a7fc55560d789dd87aa25", "e6d2b4301fd3bcab27078b5f9a4aa9b7d9a5e109", "aa1bf0c2777c84ca10ceb252cd83b9e88699ff0b", "b78b881dec3334abf5b8c6390c40a2d93a177294", "bd459cc59b09e612eeec5327d0690d1508ffe362", "ed57a5b47a16ed1e0d6f0b3d061a4af24dd5675f", "37a5b98e31d8f1d0b6739c5f506df570d3c3536e", "988db1837c5323ff7e3f9d8631f9e74cda09b90f", "a19ea03ab4645dcce3e87719e4b7ffaf565bf526", "3ea75343c1bba82c174f69a0e53c29b21480c1da"], "page_rank": 9.852216748768472e-05}, {"id": "2f3ae222bd3ac630e2ea1e9e4601c62cc73014dc", "title": "Using Commonsense Knowledge to Disambiguate Prepositional Phrase Modifiers", "authors": ["Kathleen Dahlgren", "Joyce P. McDowell"], "date": 1986, "abstract": "This paper describes a method using commonsense knowledge for discarding spurious syntactic ambiguities introduced by post-verbal prepositional phrase attachment during parsing. A completely naive parser will generate three parses for sentences of the form NP-V-Det-N-PP. The prepositions alone are insufficiently precise in meaning to guide selection among competing parses. The method is imbedded in the Kind Types System (KT) which employs commonsense knowledge of concepts, including prototype and inherent features (generic information) and ontological classifications. The generic information is drawn from published psycholinguistic studies on how average people typically view the world. This method is employed in preference strategies which appeal to the meaning of the preposition combined with information about the verbs and nouns associated with it drawn from the text and from the generic and ontological databases. These determine which syntactic structures generated by a semantically naive parser are commonsensically plausible. The method was successful in 93% of cases tested.", "references": ["7476a53192144d3de768c54d4832acf759ea2654", "3709d6c71d1003f47fce2989134173bf477802ac", "ba3ac4cc3d15144bf856ae1ae72476e1a1f9fbdd", "83efd499e7223be947edbaf72d8ec59c31070ee4", "2c05d131576d01668ef01156ac73e52c3152384a", "0dd38898c35ba7364f10f10935e50ebb498ca91d", "9a66cd464abbebd99df2ba7e62f761a11696d18a", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "a5d795cc08672f117debad8edd94c4b7ca593301", "7c9a4334c1ca95e5d495c5d4e0e0a0edd4acc203"], "page_rank": 9.852216748768472e-05}, {"id": "65b4da712c649b2c4d0d91faa105f1f3a2638405", "title": "Understanding computers and cognition", "authors": ["Terry Winograd", "Fernando Flores"], "date": 1986, "abstract": "A new and distinct variety of nectarine tree which has a low winter chilling requirement of approximately 300 hours and the tree is of large size, vigorous, spreading growth and a regular and productive bearer of early maturing, large, yellow flesh, clingstone fruit having a high degree of red skin color. The new variety is also characterized by having fruit that colors two to three weeks before ripening and having more highly colored fruit than most early maturing, low chilling varieties.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "8c742586839a091b504520e96c9c937bcba6eef2", "title": "Implementing a Semantic Interpreter Using Conceptual Graphs", "authors": ["John F. Sowa", "Eileen C. Way"], "date": 1986, "abstract": "A parser applies grammar rules to generate a parse tree that shows the syntactic structure of a sentence. This paper describes a semantic interpreter that starts with a parse tree and generates conceptual graphs that represent the meaning of the sentence. To generate conceptual graphs, the interpreter joins canonical graphs associated with each word of input. The result is a larger graph that represents the entire sentence. During the interpretation, the parse tree serves as a guide to show how the graphs are joined. Both the front-end parser and the back-end semantic interpreter are written in the Programming Language for Natural Language Processing (PLNLP).", "references": ["5e52e74fa3dbeeffd47a079a53c90c1bda523592", "0f3edff08ee069948840610d9d58cc5a1225a118", "d2e32e2acf3cc906f69e8340497567bce1a852a7", "bc587390dcf5198671fb0fa9304f52dea5afdacb", "b5b01207c035b655fd764fb35d282d07467bb279", "172c6ad67c02b6d22a1f23a678583d547cffd4e8", "ac3b4d70f5cced40be6a07967c4cf8aa8a460ec6", "f8a2e2932af3b09ee9b3837319a32248de5498eb", "172c575792c078c54f2912664164b4b646755b21"], "page_rank": 9.852216748768472e-05}, {"id": "f1feac581cf9ca99332d93835122ec22d9246302", "title": "A Theory of Justified Reformulations", "authors": ["Devika Subramanian"], "date": 1989, "abstract": "Present day systems, intelligent or otherwise, are limited by the conceptualizations of the world given to them by their designers. This thesis explores issues in the construction of adaptive systems that can incrementally reformulate their conceptualizations to achieve computational efficiency or descriptional adequacy. A detailed account of a special case of the reformulation problem is presented: we reconceptualize a knowledge base in terms of new abstract objects and relations in order to make the computation of a given class of queries more efficient. \nAutomatic reformulation will not be possible unless a reformulator can justify a shift in conceptualization. We present a new class of meta-theoretical justifications for a reformulation, called irrelevance explanations. A logical irrelevance explanation proves that certain distinctions made in the formulation are not necessary for the computation of a given class of problems. A computational irrelevance explanation proves that some distinctions are not useful with respect to a given problem solver for a given class of problems. Inefficient formulations make irrelevant distinctions and the irrelevance principle logically minimizes a formulation by removing all facts and distinctions in it that are not needed for the specified goals. The automation of the irrelevance principle is demonstrated with the generation of abstractions from first principles. We also describe the implementation of an irrelevance reformulator and outline experimental results that confirm our theory.", "references": ["738b875c6e8237235b038960785bb32521a908c6", "33628b839766a4e071b0104a768143c29841f528"], "page_rank": 5.473453749315818e-05}, {"id": "c648f156be45c4aeceb23fc379a2f49d8310356a", "title": "Inference in Text Understanding", "authors": ["Peter Norvig"], "date": 1987, "abstract": "The problem of deciding what was implied by a written text, of \"reading between the lines\" is the problem of inference. To extract proper inferences from a text requires a great deal of general knowledge on the part of the reader. Past approaches have often postulated an algorithm tuned to process a particular kind of knowledge structure (such as a script, or a plan). An alternative, unified approach is proposed. The algorithm recognizes six very general classes of inference, classes that are not dependent on individual knowledge structures, but instead rely on patterns of connectivity between concepts. The complexity has been effectively shifted from the algorithm to the knowledge base; new kinds of knowledge structures can be added without modifying the algorithm.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d756c70a3cb6df2fdf52f0027942f10712ebde56", "title": "CYC: Using Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks", "authors": ["Douglas B. Lenat", "Mayank Prakash", "Mary Shepherd"], "date": 1986, "abstract": "The major limitations in building large software have always been (a) its brittleness when confronted by problems that were not foreseen by its builders, and (by the amount of manpower required. The recent history of expert systems, for example highlights how constricting the brittleness and knowledge acquisition bottlenecks are. Moreover, standard software methodology (e.g., working from a detailed \"spec\") has proven of little use in AI, a field which by definition tackles ill- structured problems. How can these bottlenecks be widened? Attractive, elegant answers have included machine learning, automatic programming, and natural language understanding. But decades of work on such systems have convinced us that each of these approaches has difficulty \"scaling up\" for want a substantial base of real world knowledge.", "references": ["c511446de2dbd77f5a5fd14809d17e6ce5fec16f", "2c95d7a9b6713eb0e92e22015274769e39129afe", "37ab31f586cdc1efc3bf0dcc9ba52f644077e466", "4cc4a5e1591a5a4e81f6ad52e05833b3e750f56e", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "e3945122239c960401555787a4128f54c127af97", "8482a60d94226f6734871761ec212ed3dd22a76c", "bc1393b5cd02cc4b866770675aef3ee3ae070410", "353c51bb0112af3537ed590019f5eabd58b6c08f"], "page_rank": 0.00020251778872468527}, {"id": "7ab63b8e979744b58ff637458011152a9d4b677c", "title": "RI: an Expert in the Computer Systems Domain", "authors": ["John P. McDermott"], "date": 1980, "abstract": "INTRODUCTION. Rl* is a rule-based system that has much in common with other domain-specific systems that have been developed over the past several years [l, 81. It differs from these systeins primarily in its use of Match rather than Generate-and-Test as its central problem solving method [2]; rather than exploring several hypotheses until an acceptable one is found, it exploits its knowledge of its task domain to generate a single acceptable solution. Rl\u2019s domain of expertise is configuring Digital Equipment Corporation\u2019s VAX-l l/780 systems. Its input is a customer\u2019s order and its output is a set of diagrams displaying the spatial relationships among the components on the order; these diagrams are used by the technician who physically assembles the system. Since an order frequently lacks one or more components required for system functionality, a major part of Rl\u2019s task is to notice what components are missing and add them to the order. Rl is currently being used on a regular basis by DEC\u2019s manufacturing organization.3", "references": ["ea0bc6190993e02ea017c57595df6d4a6edc0d20"], "page_rank": 0.00013683634373289543}, {"id": "568dbb56f62747d45bf5f595fb97c4acb9e80871", "title": "Knowledge Acquisition for Constructive Systems", "authors": ["Sandra Marcus", "John P. McDermott", "Tianran Wang"], "date": 1985, "abstract": "Over the past ten years, significant progress has been made in understanding how the knowledge acquisition process for classification systems can be automated. But during this period little attention has been paid to the problem of how to automate the knowledge acquisition process for systems that solve problems by constructing solutions. This paper describes SALT, a tool designed to assist with knowledge acquisition for configuration tasks. SALT assumes a problem-solving strategy involving stages of generate, test, backup, modify, and regenerate. It exploits this problem-solving strategy to guide its interrogation of domain experts and to represent the knowledge they provide in a way that insures it will be brought to bear whenever relevant.", "references": ["6b954272d8b0877b1565af172959fd476867a058", "2635fa6a2593745dcf12b6fe93335b923b398730", "8da7532c60e68aa98910a2cbf543b4b4dce51058", "c6d2d9deed5615fe98f061e91d3872ef73583e10", "c8e38cee27196aebba894bba680e74f23bf9c560"], "page_rank": 5.473453749315818e-05}, {"id": "74dc088ad25c433739699aaf97b8444c9817d431", "title": "BEINGS: Knowledge as Interacting Experts", "authors": ["Douglas B. Lenat"], "date": 1975, "abstract": "Knowledge may be organized as a community of intfiacting modules Each module is granted a complex stiucture, to simulate a particular expert in some small domain An extended analogy is drawn to a group of cooperating human specialists Based on this, an internal constraint is imposed on the modules. Then structure must be standard over the entire community Some advantages of a uniform formalism are thereby preserved. An experimental community was implemented for the task domain of automatic programming. It has managed to synthesize a few inductive inference LISP programs, nonformally. from specific restricted dialogues with a human user.", "references": [], "page_rank": 0.00013683634373289543}, {"id": "8f9c8b1329a8975690e736c238df5a94a208bede", "title": "The role of consonant-vowel transitions in the perception of the stop and nasal consonants.", "authors": ["Alvin M. Liberman", "Pierre Delattre", "Franklin S. Cooper", "Louis J. Gerstman"], "date": 1954, "abstract": "Direct fluorination of uracil and its derivatives, in the presence of an aqueous solvent, by fluorine gas to produce 5-fluorouracil and 5-fluorouracil derivatives is disclosed. Novel compounds produced by the reaction, such as 5,5-difluoro-6-hydroxy-5,6-dihydrouracil are also disclosed. The derivatives of 5-fluorouracil are useful as germicidal agents while 5-fluorouracil itself is a known cancer chemotherapy agent.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "37ab31f586cdc1efc3bf0dcc9ba52f644077e466", "title": "Why AM and Eurisko Appear to Work", "authors": ["Douglas B. Lenat", "John Seely Brown"], "date": 1983, "abstract": "Seven years ago, the AM program was constructed as an experiment in learning by discovery. Its source of power was a large body of heuristics, rules which guided it toward fruitful topics of investigation, toward profitable experiments to perform, toward plausible hypotheses and definitions. \"Other heuristics evaluated those discoveries for utility and \"interestingness\", and they were added to AM's vocabulary of concepts. AM's ultimate limitation apparently was due to its inability to discover new, powerful, domain-specific heuristics for the various new fields it uncovered. At that time, it seemed straight-forward to simply add Heuretics (the study of heuristics) as one more field in which to let AM explore, observe, define, and develop. That task - learning new heuristics by discovery - turned out to be much more difficult than was realized initially, and we have just now achieved some successes at it. Along the way, it became clearer why AM had succeeded in the first place, and why it was so difficult to use the same paradigm to discover new heuristics. This paper discusses those recent insights. They spawn questions about \"where the meaning really resides\" in the concepts discovered by AM. This leads to an appreciation of the crucial and unique role of representation in theory formation, a role involving the relationship between Form and Content.", "references": ["f07c5ed1ce55f49a9a4d4109279aef80e52e8fdc", "043acfaf749c456f76fe010022771ce39979f196", "2edc8083073837564306943aab77d6dcc19d0cdc", "4fe9b057f81ef69687e1ae82d5aab750cbb6ad92", "07a516fd6c7c09ebe5368201bfc4d2557a3be190", "74dc088ad25c433739699aaf97b8444c9817d431", "53e1760e89dd32a4a92b5b54230d38d53f658427", "1cf6628cabc2f899222db78249a3a25d75c9e872"], "page_rank": 0.00011631089217296114}, {"id": "659e3fabb9ae054903b1b29aa5c50e877b7d3366", "title": "Spectrum Analysis", "authors": [""], "date": 1940, "abstract": "THAT there is a real need for a fairly comprehensive text-book of spectrum analysis of moderate size will be acknowledged by all who have occasion to teach the principles of the subject, or to employ its methods for analytical purposes. To meet this need appears to be the object of the book before us, though it does not claim to be exhaustive. The treatment of the subject is historical throughout, and there is a great number of useful references to important memoirs, while particulars of the spectra of elements and compounds occupy nearly one-third of the volume. As a general text-book of the subject, apart from its astronomical applications, the book has much to recommend it; but it leaves a great deal to be desired as a guide to the practical details of spectroscopic work, which, as the translator remarks, \u201cfurnishes so many opportunities for an excellent training in accuracy of observation and manipulative skill.\u201d The lack of the practical touch is frequently indicated. In the table of magnesium lines (p. 143), for example, the lines of the arc and spark spectrum are grouped together, and the omission of the most striking line of the latter at \u03bb 4481, possibly for the reason that it is all but invisible in the arc, might cause much loss of time to a student who happened to observe this line and attempt to ascertain its origin. Again, there are no instructions for photographing spectra, although in the majority of cases this is by far the best method to adopt in practice. Least satisfactory of all, the whole subject of comparing observed with tabulated spectra is far too cursorily dealt with to be of real value to the practical student.Spectrum Analysis.By John Landauer Authorised English edition by J. B. Tingle, Ph.D., F.C.S. Pp. 239 + x. (New York: Wiley and Sons. London: Chapman and Hall, 1898.)", "references": [], "page_rank": 0.0001231527093596059}, {"id": "1b115815f55d48ff59dc1208d7d410edeeefddff", "title": "The Knowledge Level", "authors": ["Allen Newell"], "date": 1982, "abstract": "Abstract : This paper focuses on a basic substantive problem: the nature of knowledge and representation. There are ample indications that artificial intelligence is in need of substantial work in this area. The paper proposes a theory of the nature of knowledge, namely, that there is another computer system level immediately above the symbol (or program) level. The nature of computer system levels is reviewed, the new level proposed, and its definition is treated in detail. Knowledge itself is the processing medium at this level and the principle of rationality plays a central role. Some consequences of the existence of the knowledge level and some relations to other fields are discussed.", "references": ["6a6bfc2d27fe3c3a88d8fd5c21d9c3e2cc93ccaf", "a0fc0ffed960258d91db19e6c3a7c34997a9b205", "263620b8228b9aca595eb2e035c95e3f4e5e4747", "e2cc5c4174cb0e38b9bd261c3e9a2098a6b5474d", "e31ec55d4fab23a39a47763c3e64251c99db221f", "e7b31c998a0fddadd0ebb7f3a6b6f5b69c89fa13", "b3002260f6c9c45a6e95acf17ffad3ce6e9ede68", "9b86911ba530e2c4c465fea8dc17df1b8c7ad361", "59f89c2628d52da0bb160c121d6c7e298c7f65d2", "69fbd134f5a133cbe0e40fb8646b0f259dae5537"], "page_rank": 0.00010399562123700055}, {"id": "23d55be51b73b482d2c1660cd53d427dceca7a63", "title": "Acoustic Loci and Transitional Cues for Consonants", "authors": ["Pierre Delattre", "Alvin M. Liberman", "Franklin S. Cooper"], "date": 1954, "abstract": "Previous studies with synthetic speech have shown that second\u2010formant transitions are cues for the perception of the stop and nasal consonants. The results of those experiments can be simplified if it is assumed that each consonant has a characteristic and fixed frequency position, or locus, for the second formant, corresponding to the relatively fixed place of production of the consonant. On that basis, the transitions may be regarded as \u201cmovements\u201d from the locus to the steady state of the vowel.The experiments reported in this paper provide additional evidence concerning the existence and positions of these second\u2010formant loci for the voiced stops, b, d, and g. There appears to be a locus for d at 1800 cps and for b at 720 cps. A locus for g can be demonstrated only when the adjoining vowel has its second formant above about 1200 cps; below that level no g locus was found.The results of these experiments indicate that, for the voiced stops, the transition cannot begin at the locus and go from there to ...", "references": [], "page_rank": 0.0001231527093596059}, {"id": "e97817dc4d325f08231f333938920f094eadccf3", "title": "The interconversion of audible and visible patterns as a basis for research in the perception of speech.", "authors": ["F. Shewell Cooper", "Alvin M. Liberman", "J.G.G. Borst"], "date": 1951, "abstract": "of contexts, an investigator can arrive at a description of the acoustic features common to all of the samples, and in this way make progress toward defining the so-called invariants of speech, that is, the essential information-bearing sound elements on which the listener's identifications critically depend. The investigator can also take account of the variations among spectrograms, and by correlating these with the observed variations in pronunciation, he can begin to sort out the several acoustic features in relation to the several aspects of the perception. There are, however, many questions about the relation between acoustic stimulus and auditory perception which cannot be answered merely by an inspection of spectrograms, no matter how numerous and varied these may be. For any given unit characteristic of the auditory perception, such as the simple identification of a phoneme, the spectrogram will very often exhibit several features which are distinctive to the eye, and the information which can be obtained from the spectrogram is, accordingly, ambiguous. Even when only one feature or pattern is strikingly evident, one cannot be certain about its auditory significance, unless he assumes that those aspects of the spectrogram which appear most prominently on visual examination are, in fact, of greatest importance to the ear. That assumption, as we shall try to point out later in this paper, is itself extremely interesting, but it has not been directly tested, nor, indeed, has it always been made fully explicit. To validate conclusions drawn from visual examination of spectrograms, or, more generally, to determine the stimulus correlates of perceived speech, it will often be necessary to make controlled modifications in the spectrogram, and then to evaluate the effects of those modifications on the sound as heard. For these purposes, we have constructed an instrument, called", "references": [], "page_rank": 0.0001231527093596059}, {"id": "795bca56a7ec3812c6c618fd54f8d21e00039b3d", "title": "The decline of pitch discrimination with time.", "authors": ["Jerald D. Harris"], "date": 1952, "abstract": "Semantic Scholar extracted view of \"The decline of pitch discrimination with time.\" by Jerald D. Harris", "references": [], "page_rank": 0.0001231527093596059}, {"id": "01447dd22d9b5ba901532cfc84eade7e206b07f6", "title": "On the equivalence of two recognition measures of short-term memory.", "authors": ["D. M. Green", "Franklin L. Moses"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"On the equivalence of two recognition measures of short-term memory.\" by D. M. Green et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "63d8274d869f7b350949b3a6fb1736987bba6b06", "title": "Consolidation and retroactive interference in short-term recognition memory for pitch.", "authors": ["Wayne A. Wickelgren"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"Consolidation and retroactive interference in short-term recognition memory for pitch.\" by Wayne A. Wickelgren", "references": [], "page_rank": 0.0001231527093596059}, {"id": "4da05489252dc07844302a4c41dc6c1631fdc94d", "title": "A diffusion model of perceptual memory", "authors": ["Ronald A. Kinchla", "F. Smyzer"], "date": 1967, "abstract": "A model is presented of the perceptual process through which an observer compares two consecutively observed stimuli. Emphasis is placed on the marmer in which a memory of the first stimulus is maintained until the comparison stimulus is observed. It is argued that the role of this perceptual memory process provides the primary distinction between detection and recognition tasks. Two experiments are reported: an experiment in which the observer is asked to judge the similarity in position of two points of light presented serially in a dark room; and an experiment in which the observer judges the similarity in loudness of two serially presented tones. The visual experiment is discussed in relation to the analysis of autokinesis and involuntary eye movements, while the auditory experiment is shown to have special relevance to the issue of time-order errors.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "85c796b4df4cc325164a78166d8a2316f0ee50b1", "title": "A lazy evaluator", "authors": ["Peter Henderson", "James H. Morris"], "date": 1976, "abstract": "A different way to execute pure LISP programs is presented. It delays the evaluation of parameters and list structures without ever having to perform more evaluation steps than the usual method. Although the central idea can be found in earlier work this paper is of interest since it treats a rather well-known language and works out an algorithm which avoids full substitution. A partial correctness proof using Scott-Strachey semantics is sketched in a later section.", "references": ["e83c4eb6e8c83247e88ac343def89ae64fcdff39", "621f8ecb1f571039804b442c4287216424b42f3c", "8a45926a6e2669c68c05ce8e1fc154344359a692"], "page_rank": 0.00016420361247947453}, {"id": "0fb3b1a16b1d1db4a45b4b1d996ec04e684d073e", "title": "Programming by Refinement, as Exemplified by the SETL Representation Sublanguage", "authors": ["Robert B. K. Dewar", "Arthur Grand", "Ssu-Cheng Liu", "Jacob T. Schwartz", "Edmond Schonberg"], "date": 1979, "abstract": "\u201cPure\u201d SETL is a language of very high level allowing algorithms to be programmed rapidly and succintly. SETL's representation sublanguage adds a system of declarations which allow the user of the language to control the data structures that will be used to implement an algorithm which has already been written in pure SETL, so as to improve its efficiency. Ideally no rewriting of the algorithm should be necessary. The facilities provided by the representation sublanguage and the run-time data structures that it can generate are described; based on this a heuristic which uses some of the methods of global program analysis and which should be capable of selecting an acceptably efficient representation automatically is given.", "references": ["4860d2b5d44139d85b296b4cc782fa78addbc86f"], "page_rank": 0.00016420361247947453}, {"id": "3596e5121dd74f173400cd38eea4bef68a2b475d", "title": "Abstraction mechanisms in CLU", "authors": ["Barbara Liskov", "Alan Snyder", "Russell R. Atkinson", "Craig Schaffert"], "date": 1977, "abstract": "CLU is a new programming language designed to support the use of abstractions in program construction. Work in programming methodology has led to the realization that three kinds of abstractions, procedural, control, and especially data abstractions, are useful in the programming process. Of these, only the procedural abstraction is supported well by conventional languages, through the procedure or subroutine. CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and control abstractions. This paper provides an introduction to the abstraction mechanisms in CLU. By means of programming examples, we illustrate the utility of the three kinds of abstractions in program construction and show how CLU programs may be written to use and implement abstractions. We also discuss the CLU library, which permits incremental program development with complete type-checking performed at compile-time.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "fc64117e5d5ed5947a0c85c55597e4116d6e55c6", "title": "The Principal Type-Scheme of an Object in Combinatory Logic", "authors": ["R. Hindley"], "date": 1969, "abstract": "One of the aims of combinatory logic is to study the most basic properties of functions and other concepts, with as few restrictions as possible; hence in the simplest form of combinatory logic there is nothing to stop a function being applied to itself; thus XX is an ob for all obs X. However it is also natural to look at the consequences of introducing type-restrictions, formulated by assigning types to the obs according to certain rules, to be described later. Each type is a linguistic expression representing a set of functions, and for any type a the statement \" X has type a\" is intended to mean that the ob X represents a member of the set represented by a. Given types a and /, the set of all functions from the set a into the set 8 is represented by the type \" Fa/3\" (using Curry's notation). Now consider the ob l; if X has type a, then the ob I X must also have type a. Hence I represents a function from a into a, and so it must be given the type Faa, for each type a. Thus I has not just one type, but a whole class of types. This might seem strange, but it comes from the fact that I represents the abstract notion of \" identity-function\", rather than one particular identity-function for a particular set. The identity-function involves basically the same concept, no matter what we are applying it to. Similarly the other two basic combinators S and K (see later, or [1, ?5A]) represent certain simple operations which can be performed on almost any functions, and thus they too have an infinite class of types (see ?1 later for details, and ?5 for comment). To denote classes of types, we can use variables a, b, c; then the fact that I has every type of form Faa can be expressed by assigning to I the type-scheme", "references": ["d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "3eeb5747fbc56840dbf68fe4b659dd5765f4579e"], "page_rank": 5.473453749315818e-05}, {"id": "83f2cacc97a94b29fe9ffae324545b589d8a9beb", "title": "Automatic synthesis from ordinary english test", "authors": ["Cecil H. Coker", "Noriko Umeda", "C. P. Browman"], "date": 1973, "abstract": "We summarize work between 1969 and 1972 in a continuing project With two objectives: to produce acceptable synthetic speech directly from English text; and to demonstrate with speech synthesis a detailed model of human articulatory movements. Work in the four-year period has yielded moderately accurate rules for predicting the occurrence of pauses and lesser breaks in the sentence; rules for vowel duration in many conditions, not just primary stressed syllables immediately before a pause; rules for contextual variations of consonants; and rules for durational and other allophonic variations on consonants at word boundaries. Presently we are studying natural speech to quantify and add detail to these rules, and we are working to extend the vocal tract model to closer agreement with human articulation and vocal cord control.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "74597585b796a23297c826ea5da3f9cf2e12fee3", "title": "Text-critiquing with the EPISTLE system: an author's aid to better syntax", "authors": [""], "date": 1981, "abstract": "The experimental EPISTLE system is ultimately intended to provide office workers with intelligent applications for the processing of natural language text, particularly business correspondence. A variety of possible critiques of textual material are identified in this paper, but the discussion focuses on the system's capability to detect several classes of grammatical errors, such as disagreement in number between the subject and the verb. The system's error-detection performance relies critically on its parsing component which determines the syntactic structure of each sentence and the grammatical functions fulfilled by various phrases. Details of the system's operations are provided, and some of the future critiquing objectives are outlined.", "references": ["8ec1acd09ed2fe95640ce36a79d53ae306eff593", "32ca95ff4993a36f45526e85e5c1c3e69b00a8d7", "d6c8fbbba893f4aa16295ea8c1f0e162a256ce8e"], "page_rank": 0.0004926108374384236}, {"id": "9bafa4a7836102c6f696c942d5576871b3eaae5a", "title": "Functional diversity in language as seen from a consideration of modality and mood in English", "authors": ["Moira Halliday"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"Functional diversity in language as seen from a consideration of modality and mood in English\" by Moira Halliday", "references": [], "page_rank": 0.00016420361247947453}, {"id": "0cc146de638d636bb912e9f08baafc97190a8536", "title": "Development of a Dictionary for Speech Synthesis", "authors": ["Janice B. Allen"], "date": 1971, "abstract": "In order to synthesize any given English text, it is necessary to convert the text to a phonemic representation. In the procedure discribed, function words, prefixes, suffixes, \u201cfunctional roots\u201d (as \u201cpel\u201d in \u201cimpel, propel, repel\u201d), and exceptions form a base dictionary. A large corpus of words is then processed, shortest words first, by attempting to decompose each word into prefixes, roots, and suffixes which are in the base lexicon. If the decomposition fails, or is judged to be incorrect, then the word may be added to the lexicon. There is a tendency, however, for the short high\u2010frequency words of English to be more irregular from the letter\u2010to\u2010sound point of view than words of lower frequency. Hence content words are examined for possible conversion directly by a letter (s)\u2010to\u2010sound algorithm. In this way, the conversion of text to phonemes uses a dictionary that takes advantage of the morphemic structure of English words, and yet does not include those words that can be converted directly without t...", "references": [], "page_rank": 0.00016420361247947453}, {"id": "bea7a030fa0e1e63605899d73743307e2b5a7336", "title": "Models of LCF.", "authors": ["Robin Milner"], "date": 1973, "abstract": "LCF is a deductive system for computable functions proposed by D. Scott in 1969 in an unpublished memorandum. The purpose of the present paper is to demonstrate the soundness of the system with respect to certain models, which are partially ordered domains of continuous functions. This demonstration was supplied by Scott in his memorandum; the present paper is merely intended to make this work more accessible.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "7b5a4d4edaf8179c7730fb00be349bfe84234370", "title": "Informal definitions for the development of a storage structure definition language", "authors": ["William C. McGee"], "date": 1970, "abstract": "Intr oduc tion A storage structure definition can be defined informally as a description of a collection of data which is sufficient to enable a person or system other than the one creating the data to use the data in a meaningful way. Examples of meaningful use are the translation of the data from its original form to a form usable in another installation, and direct access to the data by a system not specifically designed to accept the data. A storage structure definition language can then be defined as a language for expressing storage structure definitions. The development of such a language is the goal of the SSDLTG. The approach presently being followed by the SSDLTG is to consider that a storage structure definition of a collection of data consists of three parts: I) a data structure definition, i.e. , a definition of the logical characteristics of the data collection; 2) a storage structure class definition, i.e., a definition of a class of physical data structures; and 3) a mapping definition, i.e. , a definition of the manner in which the data structure defined in (1) is mapped into the class of storage structures defined in (Z). The purpose of this paper is to elaborate on these components of a storage structure definition, and to provide some informal definitions and examples which we have found useful in developing a storage structure definition language. Specifically, the following are provided: * One of a set of papers presented by members of the Storage Structure Definition", "references": [], "page_rank": 0.0002463054187192118}, {"id": "16ab5841b0ca19c03ecb44f2edd496eba7c72cca", "title": "Generalized data base management system data structures and their mappingto physical storage", "authors": ["Robert W Taylor"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Generalized data base management system data structures and their mappingto physical storage\" by Robert W Taylor", "references": [], "page_rank": 0.0002463054187192118}, {"id": "d7c3c95d5b9c3c4fd8b07863d072d59804f2506a", "title": "Lambda-calculus models of programming languages.", "authors": ["James H. Morris"], "date": 1969, "abstract": "Massachusetts Institute of Technology, Alfred P. Sloan School of Management. Thesis. 1969. Ph.D.", "references": ["d4efc5c9aa688473beebf65f0937b91b12dd4d60", "9c6e618fe404c84ecb2fcca1dba505e040460f51", "cee5d4d123d6d289a14d41baffa73723dcd3e9e7", "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "f82211423d9cbf034d87598a93d9b4cae147ef34", "83f054294ba2726d02aa03e471da773c3383b146", "f07bc62734f02066f0d081d462398fb9ebfde3c4", "27dd189065bd8847a8ec8f27553282df67a42d3e", "d8ce4b5489ef14e8878c869101e30432d057599c", "9aed59ed036b5715706ac44ba53eb20eff0911ed"], "page_rank": 0.0005473453749315817}, {"id": "b04c2feae5fd8ac4c00db47fecd6d52027a13021", "title": "Revised Report on the Algorithmic Language ALGOL 68", "authors": ["A. van Wijngaarcien", "B. J. Mailloux", "J. E. L. Peck", "C. H. A. Kostcr", "Michel Sintzoff", "Charles H. Lindsey", "Lambert G. L. T. Meertens", "R. G. Fisker"], "date": 1977, "abstract": "The report gives a complete defining description of the international algorithmic language Algol 60. This is a language suitable for expressing a large class of numerical processes in a form suitably concise for direct automatic translation into the language of programmed automatic computers.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "8c8381de74ea919de58e51dfa0e5416af6c660b6", "title": "Abstraction and Verification in Alphard: Introduction to Language and Methodology", "authors": ["William A. Wulf", "Ralph L. London", "Mary Shaw"], "date": 1976, "abstract": "Alphard is a programming language whose goals include supporting both the development of well-structured programs and the formal verification of these programs. This paper attempts to capture the symbiotic influence of these two goals on the design of the language. To that end the language description is interleaved with the presentation of a proof technique and discussion of programming methodology. Examples to illustrate both the language and the verification technique are included.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "172c575792c078c54f2912664164b4b646755b21", "title": "Knowledge Representation for an Audit Office", "authors": ["Brian J. Garner", "Eric Tsui"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Knowledge Representation for an Audit Office\" by Brian J. Garner et al.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "cc3873996c805b77fc58f4994f840352d0ce2c78", "title": "A Metalanguage for interactive proof in LCF", "authors": ["Michael J. C. Gordon", "Robin Milner", "L. Morris", "Malcolm C. Newey", "Christopher P. Wadsworth"], "date": 1978, "abstract": "LCF (Icqic for Caqmtable Functions) is a prcof generating system mnsisting of an interactive programing language MG (MetaLmguage) for mnducting prcofs in PPA (Polynmrphic Predicate A-calculus) , a deductive calculus suitable for the formalisation of reasoning almut recursively defined functions, in particular about the syntax, semantics and iq?lementations of many prqrcmming languages. PPI is an enrichment (in respect of type structure and expressive pcwer) of an extended a-calculus due to Dana Scott and is fully discussed elsewhere [ 22 I . The puxposes of this paper are (a) to illustrate the features of ML which me it of general interest in language design quite independently of its use for machine assistd formal pxcof, (b) to illustrate ML applied to PPA, in encoding interesting prcof -finding-andperfonning procedures, and (c) to convey a methodology for controlled semiautomatic proof.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "359eca57fe42d97cbb67f0b5591869abe5eb5421", "title": "Data Types as Lattices", "authors": ["Dana S. Scott"], "date": 1976, "abstract": "The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of \u201cpartial\u201d objects. In this report these spaces are modeled in one universal domain ${\\bf P} \\omega $, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.", "references": ["9697b3978c639f9043acb29e4fd5b53aa9131fdd", "2769c203102a875c10bc11affc161891472176d1", "54b052229eb9ae76546c7674e1137c9ce140ebc8", "427dd6f76ec119aa185f1e2ac82f040082e7d2ff", "a5e82e5dba72ca4347597cb99ce6b9a1f9b9a812", "b762d1701e32feeff7a60a66ca2d019bd1415c65", "9f1fbbf387f2cfd6b01ce85f80e7a3a5fe5214a4", "bd598056b1b945b1a0076f520de1aa1b3c655b09", "67ca3ddd2c5067ce0f89bac7d88d50c360c3d59f", "c679deecae18fb0e9a7c6f38ca09a534979cd421"], "page_rank": 5.473453749315818e-05}, {"id": "f8a2e2932af3b09ee9b3837319a32248de5498eb", "title": "The case for case\" in universals in linguistic theory", "authors": ["Charles J. Fillmore"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"The case for case\" in universals in linguistic theory\" by Charles J. Fillmore", "references": [], "page_rank": 0.00013683634373289543}, {"id": "33628b839766a4e071b0104a768143c29841f528", "title": "An introduction to mathematical logic", "authors": ["H. E. Enderton"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"An introduction to mathematical logic\" by H. E. Enderton", "references": [], "page_rank": 0.0002463054187192118}, {"id": "738b875c6e8237235b038960785bb32521a908c6", "title": "Change in View", "authors": ["Ronald Prescott Loui", "Gilbert Harman"], "date": 1987, "abstract": "Change in view : , Change in view : , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u0648 \u0641\u0646 \u0622\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u062f\u0627\u0646\u0634\u06af\u0627\u0647 \u0627\u0645\u0627\u0645 \u0635\u0627\u062f\u0642(\u0639)", "references": [], "page_rank": 0.0002463054187192118}, {"id": "353c51bb0112af3537ed590019f5eabd58b6c08f", "title": "Metaphor and Common-Sense Reasoning", "authors": ["Jaime G. Carbonell", "Steven Minton"], "date": 1983, "abstract": "Abstract : Inferences based on metaphors appear to play a major role in human common sense reasoning. This paper identifies and analyzes general inference patterns based upon underlying metaphors, in particular the pervasive balance principle. Strategies for metaphor comprehension are explored, and analogical mapping structures are proposed as a means of representing metaphorical relationships between domains. In addition, a framework for a computational model embodying principles of metaphorical common sense reasoning is discussed.", "references": ["d8723388c2b838704b00e6cce886c132c2fcbd9d", "46d615ef27229e99493b5ea470d26816987afbf1", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "d9dd5f497c6831ee93f1515dfda728bb82f3ec87", "a044c62299ddb831c53c5a1a30a40ba043ae4b03", "c7b7662c811234c5f6692c4cb6b086e87335d7d4", "f2434dfa520705476fa48f9a368eb6a38ce2d626"], "page_rank": 6.157635467980295e-05}, {"id": "c8e38cee27196aebba894bba680e74f23bf9c560", "title": "Computer-based medical consultations, MYCIN", "authors": ["Edward H. Shortliffe"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Computer-based medical consultations, MYCIN\" by Edward H. Shortliffe", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "c6d2d9deed5615fe98f061e91d3872ef73583e10", "title": "Personal Construct Theory and the Transfer of Human Expertise", "authors": ["John H. Boose"], "date": 1984, "abstract": "The bottleneck in the process of building expert systems is retrieving the appropriate problem-solving knowledge from the human expert. Methods of knowledge elicitation and analysis from psychotherapy based on enhancements to George Kelly's Personal Construct Theory are applied to this process. The Expertise Transfer System is described which interviews a human expert and then constructs and analyzes the knowledge that the expert uses to solve his particular problem. The first version of the system elicits the initial knowledge needed to solve analysis problems without the intervention of a knowledge engineering team. Fast (two hour) initial prototyping of expert systems which run on KS-300tm* (an extended version of EMYCIN) and OPS5 is also performed. Conflicts in the problem-solving methods of the expert may also be enumerated and explored.", "references": ["7ab63b8e979744b58ff637458011152a9d4b677c", "fca44a0cd0b99e848b32d338f559c5ea879accad", "c7f6a0bd93fd6584945c2a03f2f8a058a9fbfbbd", "b1ee563ee3919825e58b53c7a4266ff352b74e91", "d5ae0954d93e13ba78b3244b6c3f574d256fa0df", "f782cc892db3b806bc73b9125a92b8b19b4e6781"], "page_rank": 9.852216748768472e-05}, {"id": "1cf6628cabc2f899222db78249a3a25d75c9e872", "title": "Foundations of Envisioning", "authors": ["Johan de Kleer", "John Seely Brown"], "date": 1982, "abstract": "This paper explores a particular kind of qualitative reasoning, called envisioning, that is capable of producing causal explanations for device behavior. It has been implemented in a computer program, ENVISION, which can analyze a wide variety of thermal, fluid, electrical, translational and rotational devices. Rather than present the technical details of the envisioning process, this paper examines the theoretical foundations upon which it is built. Many of these considerations are ones that any builder of qualitative reasoning systems must pay attention to. Two such considerations are explanation and robustness: What notion of causality is adequate for causal explanations of device behavior? How can there be any confidence in the analysis of a novel device?", "references": ["9b1a687d836ffc318ff1e125acd5a0dd7a21f3b5", "bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8"], "page_rank": 0.00016420361247947453}, {"id": "8da7532c60e68aa98910a2cbf543b4b4dce51058", "title": "The Advantages of Abstract Control Knowledge in Expert System Design", "authors": ["William J. Clancey"], "date": 1983, "abstract": "A poorly designed knowledge base can be as cryptic as an arbitrary program and just as difficult to maintain. Representing control knowledge abstractly, separately from domain facts and relations, makes the design more transparent and explainable. A body of abstract control knowledge provides a generic framework for constructing knowledge bases for related problems in other domains and also provides a useful starting point for studying the nature of strategies.", "references": ["eed84509d87fc40109845e3a2d8882583e6ed599", "f07c5ed1ce55f49a9a4d4109279aef80e52e8fdc", "a6be1b72e0dfbecbd4899df2e09aaa7b09215817", "596713f8858fa595989d32756ea6e29f6b5a64dc", "be69264e7ae850961c60f77ce18a84fec842991d", "e1dbeb8db14dc5aaa34b2fe589180bd39c90da72", "543395dcfd335fcad27d15aacacff26f1bed2ef3", "9fe41ba3d43012a28c04af59899aa68195d1f325", "f303a0eca1954bc66231402962a604fb2f94906f", "61160f647438826106380f76b45c93502948f547"], "page_rank": 0.00014778325123152708}, {"id": "2635fa6a2593745dcf12b6fe93335b923b398730", "title": "Classification Problem Solving", "authors": ["William J. Clancey"], "date": 1984, "abstract": "A broad range of heuristic programs\u2014embracing forms of diagnosis, catalog selection, and skeletal planning\u2014accomplish a kind of well-structured problem solving called classification. These programs have a characteristic inference structure that systematically relates data to a pre-enumerated set of solutions by abstraction, heuristic association, and refinement. This level of description specifies the knowledge needed to solve a problem, independent of its representation in a particular computer language. The classification problem-solving model provides a useful framework for recognizing and representing similar problems, for designing representation tools, and for understanding why non-classification problems require different problem-solving methods.", "references": ["00f7392c8c174df19041d280ccb3deb194a6be8d", "eed84509d87fc40109845e3a2d8882583e6ed599", "1b115815f55d48ff59dc1208d7d410edeeefddff", "8da7532c60e68aa98910a2cbf543b4b4dce51058", "750601d2b390cbd851773999b200a93cf80fa05d", "52b594b3ed821aac55c6068d8583994ad2c9f42b", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "16ef4cc3a80ee7ba8f59e0a55b2ef134c31e18b3", "b6797121011a32d7afdf9786b27cf5f6c1a966e9", "086e203b4c001107d31fbf976428f9b140979795"], "page_rank": 9.852216748768472e-05}, {"id": "6b954272d8b0877b1565af172959fd476867a058", "title": "Enhanced Maintenance and Explanation of Expert Systems Through Explicit Models of Their Development", "authors": ["Robert Neches", "William R. Swartout", "Johanna D. Moore"], "date": 1985, "abstract": "Principled development techniques could greatly enhance the understandability of expert systems for both users and system developers. Current systems have limited explanatory capabilities and present maintenance problems because of a failure to explicitly represent the knowledge and reasoning that went into their design. This paper describes a paradigm for constructing expert systems which attempts to identify that tacit knowledge, provide means for capturing it in the knowledge bases of expert systems, and, apply it towards more perspicuous machine-generated explanations and more consistent and maintainable system organization.", "references": ["eed84509d87fc40109845e3a2d8882583e6ed599", "9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "46d615ef27229e99493b5ea470d26816987afbf1", "e0318907c179df6a0a510a84e5709f6b59cc059f", "ff79d5d1282aea51195afcb7de898dca2c879a97", "927602c39f2924c10f7d91038fed5aa3b54dabd8", "1ac75c9b2b907b1ae285b316623f4c1b2b90ae5e", "14f498c4292f0f6b3f4170e31e2f53489b76701c", "d5ae0954d93e13ba78b3244b6c3f574d256fa0df"], "page_rank": 9.852216748768472e-05}, {"id": "53e1760e89dd32a4a92b5b54230d38d53f658427", "title": "Progress report on program-understanding systems.", "authors": ["Cordell Green", "Richard J. Waldinger", "David R. Barstow", "R. Elschlager", "Douglas B. Lenat", "B. P. McCune", "David E. Shaw", "Louis I. Steinberg"], "date": 1974, "abstract": "This progress report covers the first year and one half of work by our automatic-programming research group at the Stanford Artificial Intelligence Laboratory. Major emphasis has been placed on methods of program specification, codification of programming knowledge, and implementation of pilot systems for program writing and understanding. List processing has been used as the general problem domain for this work.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "69fbd134f5a133cbe0e40fb8646b0f259dae5537", "title": "AAAI president's message", "authors": ["Allen Newell"], "date": 1980, "abstract": "Births are always interesting affairs. According to some, births are always traumatic - a shock to come from the womb to the world. The birth we give witness to here is that of a new society, the American Association for Artificial Intelligence - AAAI. It has not seemed to me traumatic, but rather almost wholly benign. In a world where not much is benign at the moment, such an event is devoutly to be cherished.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "9b86911ba530e2c4c465fea8dc17df1b8c7ad361", "title": "New Programming Languages for Artificial Intelligence Research", "authors": ["Daniel G. Bobrow", "Bertram Raphael"], "date": 1974, "abstract": "New directions in Artificial Intelligence research have led tothe need for certainnovel features to be embedded in programming languages. This paper givesan overviewof the nature of these features, and their implementation in fourprincipal families of AI language*: SAILPLANNER/COXXIVER; QLISP/INTERLISP; and POPLER,POP-2. Th\u00ab-programming features described include :new data types and accessing meeh.vni.iins for stored expressions; moreflexible control structures, includingmultiple processes and backtracking; pattern matching to allow comparison of data item with atemplate, and extractionof labeled subexpressions; and deductive mechanisms which allow theprogramming system to carry out certain activities includingmodifying the data base and deciding which subroutines to run next using only constraints and guidelines set up by theprogrammer.", "references": ["36795df6bca1b601cd600d55a8b5485eab8c8335"], "page_rank": 6.157635467980295e-05}, {"id": "b3002260f6c9c45a6e95acf17ffad3ce6e9ede68", "title": "Ascribing Mental Qualities to Machines", "authors": ["John W. Mccarthy"], "date": 1979, "abstract": "Ascribing mental qualities like beliefs, intentions and wants to a machine is sometimes correct if done conservatively and is sometimes necessary to express what is known about its state. We propose some new definitional tools for this: definitions relative to an approximate theory and second order structural definitions.", "references": ["e2cc5c4174cb0e38b9bd261c3e9a2098a6b5474d", "42e2cc996b66929b434907dc5d100a5679ff26ef", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "2912b86d93ffa35d0fd665de99768aa313bfcf68", "98e84e80e7126805de225b263813bfb2cf596a26", "92667e72f39ee2a2a38ebf7bef89c117a8ad9f02"], "page_rank": 6.157635467980295e-05}, {"id": "8a45926a6e2669c68c05ce8e1fc154344359a692", "title": "Semantics and pragmatics of lambda-calculus", "authors": ["Christopher P. Wadsworth"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Semantics and pragmatics of lambda-calculus\" by Christopher P. Wadsworth", "references": [], "page_rank": 0.00016420361247947453}, {"id": "621f8ecb1f571039804b442c4287216424b42f3c", "title": "Behavioral semantics of nonrecursive control structures", "authors": ["Carl Hewitt", "Peter Boehler Bishop", "Richard Steiger", "Irene Greif", "Brian Cantwell Smith", "Todd Matson", "Roger Hale"], "date": 1974, "abstract": "Knowledge Based Programming is programming in an environment which has substantial knowledge of the semantic domain for which the programs are being written and of the purposes that the programs are supposed to satisfy. Actors are a semantic concept in which no active process is ever allowed to treat anything as an object; instead a polite request must be extended to accomplish what the activator desires. Using actors the PLANNER Project is constructing a Programming Apprentice to make it easier for expert programmers to do knowledge based programming. The apprentice is to aid in establishing and maintaining consistency of specifications, validating that modules meet their specifications, answering questions about behavioral dependencies between modules, and analyzing the implications of perturbations in modules and their specifications.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "e7b31c998a0fddadd0ebb7f3a6b6f5b69c89fa13", "title": "Psychology from an Empirical Standpoint", "authors": ["Franz Clemens Brentano}"], "date": 1940, "abstract": "Foreword to the Routledge Classics Edition Preface to the English Edition Foreword to the 1911 Edition, Franz Brentano Foreword to the 1874 Edition, Franz Brentano Book One: Psychology as a Science 1. The Concept and Purpose of Psychology 2. Psychological Method with Special Reference to its Experiential Basis 3. Further Investigations Concerning Psychological Method. Induction of the Fundamental Laws of Psychology 4. Further Investigations Concerning Psychological Method. The Inexact Character of its Highest Laws. Deduction and Verification Book Two : Mental Phenomena in General 1. The Distinction between Mental and Physical Phenomena 2. Inner Consciousness 3. Further Consideration Regarding Inner Consciousness 4. On the Unity of Consciousness 5. A Survey of the Principal Attempts to Classify Mental Phenomena 6. Classification of Mental Activities into Presentations, Judgements and Phenomena of Love and Hate 7. Presentation and Judgement: Two Different Fundamental Classes 8. Feeling and Will United into a Single Fundamental Class 8. Comparison of the Three Basic Classes with the Threefold Phenomena of Inner Consciousness. Determination of Their Natural Order. Appendix to The Classification of Mental Phenomena Additional Essays from Brentano's Nachlass Concerning Intuitions, Concepts and Objects of Reason Index", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e31ec55d4fab23a39a47763c3e64251c99db221f", "title": "The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty", "authors": ["Lee D. Erman", "Frederick Hayes-Roth", "Victor R. Lesser", "Raj Reddy"], "date": 1980, "abstract": "The Hearsay-II system, developed during the DARPA-sponsored five-year speech-understanding research program, represents both a specific solution to the speech-understanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behavior. As a computational problem, speech understanding reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, to the eventually resulting audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker's intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise. \n \nThe Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most important, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains, and it is anticipated that this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively. \n \nDiscussed in this paper are the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II's structure and those of other speech-understanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background.", "references": ["bbf2b0948ec73e21f6d5a67b22a31a20d503cc9e"], "page_rank": 6.157635467980295e-05}, {"id": "3eeb5747fbc56840dbf68fe4b659dd5765f4579e", "title": "Recursive objects in all finite types", "authors": ["Andrzej Grzegorczyk"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Recursive objects in all finite types\" by Andrzej Grzegorczyk", "references": [], "page_rank": 0.0004926108374384236}, {"id": "e83c4eb6e8c83247e88ac343def89ae64fcdff39", "title": "Correct and Optimal Implementations of Recursion in a Simple Programming Language", "authors": ["Jean Vuillemin"], "date": 1974, "abstract": "The object of this paper is to study the mechanism of recursion in a simple, LISP-like programming language, where the only means of iteration is through recursion. The theory of computation developed in Scott [6] provides the framework of our study. We show how the implementations of recursion which deserve to be called \u201ccorrect\u201d can be characterized semantically, and demonstrate a general criterion for the correctness of an implementation. We then describe an implementation of recursion which is both correct and optimal in a general class of sequential languages, and therefore constitutes an attractive alternative to both \u201ccall-by-name\u201d and \u201ccall-by-value\u201d.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "e2cc5c4174cb0e38b9bd261c3e9a2098a6b5474d", "title": "FIRST ORDER THEORIES OF INDIVIDUAL CONCEPTS AND PROPOSITIONS", "authors": ["John McCarthy"], "date": 1979, "abstract": "We discuss rst order theories in which individual concepts are admitted as mathematical objects along with the things that reify them. This allows very straightforward formalizations of knowledge, belief, wanting, and necessity in ordinary rst order logic without modal operators. Applications are given in philosophy and in articial intelligence. We do not treat general concepts, and we do not present any full axiomatizations but rather show how various facts can be expressed.", "references": [], "page_rank": 0.0001319493314567206}, {"id": "263620b8228b9aca595eb2e035c95e3f4e5e4747", "title": "Special issue on knowledge representation", "authors": ["Ronald J. Brachman", "Brian Christopher Smith"], "date": 1980, "abstract": "In the fall of 1978 we decided to produce a special issue of the SIGART Newsletter devoted to a survey of current knowledge representation research. We felt that there were twe useful functions such an issue could serve. First, we hoped to elicit a clear picture of how people working in this subdiscipline understand knowledge representation research, to illuminate the issues on which current research is focused, and to catalogue what approaches and techniques are currently being developed. Second -- and this is why we envisaged the issue as a survey of many different groups and projects -- we wanted to provide a document that would enable the reader to acquire at least an approximate sense of how each of the many different research endesvours around the world fit into the field as a whole.It would of course be impossible to produce a final or definitive document accomplishing these goals: rather, we hoped that this survey could initiate a continuing dialogue on issues in representation, a project for which this newsletter seems the ideal forum. It has been many months since our original decision was made, but we are finally able to present the results of that survey. Perhaps more than anything else, it has emerged as a testament to an astounding range and variety of opinions held by many different people in many different places.The following few pages are intended as an introduction to the survey as a whole, and to this issue of the newsletter. We will briefly summarize the form that the survey took, discuss the strategies we followed in analyzing and tabulating responses, briefly review the overall sense we received from the answers that were submitted, and discuss various criticisms which were submitted along with the responses. The remainder of the volume has been designed to be roughly self-explanatory at each point, so that one may dip into it at different places at will. Certain conventions, however, particularly regarding indexing and tabulating, will also be explained in the remainder of this introduction.As editors, we are enormously grateful to the many people who devoted substantial effort to responding to our survey. It is our hope that the material presented here will be interesting and helpful to our readers, and that fruitful discussion of these and other issues will continue energetically and enthusiastically into the future.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "32ca95ff4993a36f45526e85e5c1c3e69b00a8d7", "title": "Communicating through letters and reports", "authors": ["Clyde W. Wilkinson"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"Communicating through letters and reports\" by Clyde W. Wilkinson", "references": [], "page_rank": 0.00016420361247947453}, {"id": "d6c8fbbba893f4aa16295ea8c1f0e162a256ce8e", "title": "Parts--a system for assigning word classes to english text", "authors": ["Lorinda L. Cherry"], "date": 1978, "abstract": "Semantic Scholar extracted view of \"Parts--a system for assigning word classes to english text\" by Lorinda L. Cherry", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8ec1acd09ed2fe95640ce36a79d53ae306eff593", "title": "Writing tools - the style and diction programs", "authors": ["Lorinda L. Cherry", "William Vesterman"], "date": 1980, "abstract": "A device for the pressure fixing of a toner on a record carrier under ambient temperatures has a stand; a first pressure roll of relatively large diameter rotatably and stationarily supported in the stand; and a second pressure roll of relatively small diameter rotatably supported in the stand and cooperating with the first pressure roll to define therewith a roll gap. The second pressure roll is displaceably supported to be shiftable towards and away from the roll gap. The device further has a plurality of pressure rollers arranged in a series along the length dimension of the second pressure roll in an axially parallel orientation therewith and a support for individually displaceably supporting each pressure roller for movement towards and away from the roll gap. A separate spring urges each pressure roller into contact with the second pressure roll for exerting thereon a pressing force with which the first and second rolls compress a record carrier passing through the roll gap. With each pressure roller there is associated a setting mechanism for individually adjusting the pressing force exerted by each pressure roller.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "07a516fd6c7c09ebe5368201bfc4d2557a3be190", "title": "Competitive Argumentation in Computational Theories of Cognition.", "authors": ["Kurt VanLehn", "John Seely Brown", "James G. Greeno"], "date": 1982, "abstract": "Abstract : Computer science has given psychology a new way of expressing models of cognition that is much more detailed and precise than its predecessors. But unfortunately, the increased detail and precision in stating models has not been accompanied by correspondingly detailed and precise arguments analyzing and supporting them. Consequently, the new, richly detailed models of cognitive science often fail to meet the traditional criteria of scientific theories. This report discusses what kinds of tools are available or can be fashioned that will help cognitive scientists build computational theories of cognition that will meet some widely accepted standards that have so far proved difficult for such theories to meet. The prime tool of this discussion, actually a class of tools, is the competitive argument. (Author)", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a0fc0ffed960258d91db19e6c3a7c34997a9b205", "title": "Physical Symbol Systems", "authors": ["Allen Newell"], "date": 1980, "abstract": "On the occasion of a first conference on Cognitive Science, it seems appropriate to review the basis of common understanding between the various disciplines. In my estimate, the most fundamental contribution so far of artificial intelligence and computer science to the joint enterprise of cognitive science has been the notion of a physical symbol system, i.e., the concept of a broad class of systems capable of having and manipulating symbols, yet realizable in the physical universe. The notion of symbol so defined is internal to this concept, so it becomes a hypothesis that this notion of symbols includes the symbols that we humans use every day of our lives. In this paper we attempt systematically, but plainly, to lay out the nature of physical symbol systems. Such a review is in ways familiar, but not thereby useless. Restatement of fundamentals is an important exercise.The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency, or the U.S. Government.Herb Simon would be a co-author of this paper, except that he is giving his own paper at this conference. The key ideas are entirely joint, as the references indicate.", "references": ["37b0153259be7e305f258c85db90e41c6a00ad23", "d453e0a1022719f7a52155d0f71761ac756d0047", "b9f90d3a81df3b4f5433f9e33dd9cb4685b0955b", "affcf19551b01c4c8009d061750700d91c2f79e9", "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "979079ff368b2a32ce7546319c7837ef1793bf9b", "af465996da89a302fae95c2fe22e54d2b79e4ac3", "e97795382386ecd24300f3a6449ed5732b200bfa", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3", "5be4af732064ad777c697ee468e083f403d8056c"], "page_rank": 6.157635467980295e-05}, {"id": "9aed59ed036b5715706ac44ba53eb20eff0911ed", "title": "Correspondence between ALGOL 60 and Church's Lambda-notation: part I", "authors": ["Peter J. Landin"], "date": 1965, "abstract": "A method is provided for treating waste water containing solids of difficultly-soluble compounds of at least one heavy metal, the method comprising the steps of adding an amount of ferrous ions to said waste water corresponding on a molar basis to about 2 to 100 times the total molar amount of heavy metal present in said waste water, adding an alkaline substance to said waste water in an amount corresponding to about 0.9 to 1.2 equivalent of free acid present in said waste water, and stirring the waste water without aeration at a temperature of at least about 40 DEG C., whereby insoluble ferrite crystals are precipitated having said heavy metal incorporated therein.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f07bc62734f02066f0d081d462398fb9ebfde3c4", "title": "A \u03bb-CALCULUS APPROACH", "authors": ["P. J. Landin"], "date": 1966, "abstract": "Publisher Summary This chapter focuses on \u03bb-calculus approach. The chapter presents an introduction to a complementary activity is called semantic analysis and explores a mathematical basis for it. This technique consists of establishing a correspondence between the texts of the language to be analyzed and expressions of a structurally simpler language for which the problem of semantic description is less diffuse. This structurally simpler language comprises certain expressions called applicative expressions (AEs). The AEs are characterized in abstract terms, without reference to a particular written representation of them. When presenting specific AEs, one shall be compelled to adopt some conventions about how to write them, but one shall do so informally, and it will be convenient to use different conventions and even different representations of the same AE on different occasions.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f82211423d9cbf034d87598a93d9b4cae147ef34", "title": "Some aspects of Basel, the base language for an extensible language facility", "authors": ["P. Jorrand"], "date": 1968, "abstract": "Basel has been designed as the base language component for an extensible language facility called Elf. Elf will have several components: one for syntactic extension, one for the definition of communications with a given kind of environment, and some others. All these components will be able to perform modifications, additions, and deletions to Basel.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "83f054294ba2726d02aa03e471da773c3383b146", "title": "LISP 1.5 Programmer's Manual", "authors": ["John McCarthy"], "date": 1962, "abstract": "The LISP language is designed primarily for symbolic data processing used for symbolic calculations in differential and integral calculus, electrical circuit theory, mathematical logic, game playing, and other fields of artificial intelligence.The manual describes LISP, a formal mathematical language. LISP differs from most programming languages in three important ways. The first way is in the nature of the data. In the LISP language, all data are in the form of symbolic expressions usually referred to as S-expressions, of indefinite length, and which have a branching tree-type of structure, so that significant subexpressions can be readily isolated. In the LISP system, the bulk of the available memory is used for storing S-expressions in the form of list structures. The second distinction is that the LISP language is the source language itself which specifies in what way the S-expressions are to be processed. Third, LISP can interpret and execute programs written in the form of S-expressions. Thus, like machine language, and unlike most other high level languages, it can be used to generate programs for further executions.", "references": [], "page_rank": 0.0002914614121510673}, {"id": "d8ce4b5489ef14e8878c869101e30432d057599c", "title": "A Basis for a Mathematical Theory of Computation", "authors": ["John R. McCarthy"], "date": 1962, "abstract": "Publisher Summary This chapter discusses the mathematical theory of computation. Computation essentially explores how machines can be made to carry out intellectual processes. Any intellectual process that can be carried out mechanically can be performed by a general purpose digital computer. There are three established directions of mathematical research that are relevant to the science of computation\u2014namely, numerical analysis, theory of computability, and theory of finite automata. The chapter explores what practical results can be expected from a suitable mathematical theory. Further, the chapter presents several descriptive formalisms with a few examples of their use and theories that enable to prove the equivalence of computations expressed in these formalisms. A few mathematical results about the properties of the formalisms are also presented.", "references": [], "page_rank": 0.0003776683087027914}, {"id": "27dd189065bd8847a8ec8f27553282df67a42d3e", "title": "The next 700 programming languages", "authors": ["Peter J. Landin"], "date": 1966, "abstract": "A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.\nThe system is biased towards \u201cexpressions\u201d rather than \u201cstatements.\u201d It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c679deecae18fb0e9a7c6f38ca09a534979cd421", "title": "Implementation and applications of Scott's logic for computable functions", "authors": ["Robin Milner"], "date": 1972, "abstract": "The basis for this paper is a logic designed by Dana Scott [1] in 1969 for formalizing arguments about computable functions of higher type. This logic uses typed combinators, and we give a more or less direct translation into typed \u03bb-calculus, which is an easier formalism to use, though not so easy for the metatheory because of the presence of bound variables. We then describe, by example only, a proof-checker program which has been implemented for this logic; the program is fully described in [2]. We relate the induction rule which is central to the logic to two more familiar rules - Recursion Induction and Structural Induction - showing that the former is a theorem of the logic, and that for recursively defined structures the latter is a derived rule of the logic. Finally we show how the syntax and semantics of a simple programming language may be described completely in the logic, and we give an example of a theorem which relates syntactic and semantic properties of programs and which can be stated and proved within the logic.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "67ca3ddd2c5067ce0f89bac7d88d50c360c3d59f", "title": "Comparing Algebraic Structures up to Algorithmic Equivalence", "authors": ["Denis J. Kfoury"], "date": 1972, "abstract": "by Sara B. Quinn In mathematics, one often tries to classify some collection of objects up to isomor-phism. In mathematical logic, we can explore the complexity of that classification. A structure consists of a universe and an interpretation of a language, where the language has symbols representing constants, operations, and relations. We consider only structures whose universe is a subset of \u03c9, and we define a class as a collection of structures all with the same language and closed under isomorphism. One way that the complexity of the classification problem can be explored is by looking at the index set for a computable structure. We consider indices for computable structures, and write A e where \u03d5 e = \u03c7 D(A). The index set for A is the set of all indices for computable isomorphic copies of A. We write I(A) = {e : A e \u223c = A}. In the present work, the relationship between the complexity of the index set for a structure and the complexity of a sentence describing the structure (called a Scott sentence) is explored. We find an example of a structure for which there is not a match between the complexity of the index set and the complexity of a Scott sentence. We also examine the possible complexities of an index set, and give results characterizing when a particular class will have an index set that is m-complete at a certain complexity level. Another idea explored in the present work is that of comparing the complexity of the classification problem for various classes of structures, using the notion of a Turing computable embedding. Definition. A Turing computable embedding of K into K is an operator \u03a6 = \u03d5 e such that 1. for each A \u2208 K, there exists B \u2208 K such that \u03d5 D(A) e = \u03c7 D(B) , and 2. if A, A \u2208 K correspond, respectively, to B, B \u2208 K , then A \u223c = A if and only if B \u223c = B. The ordering of classes of structures that arises from this embedding allows us to compare the complexity of the classification problem for those classes. In the present work, we give characterizations for the classes of structures that embed into the class of equivalence structures, as well as into the class of reduced Abelian p-groups of various lengths.", "references": ["e272c33167aef37436ce6df64b428ebbdbefa742", "29e083f96c2dae37aba7c20ca240c29c3482649f", "55c858475e30d1c3f8739a937ec805ac3a908c76", "1be9dcfb94cbecd7abf1e45d7b4783f041ad33d9", "b6829b69831f783cca4cfc18d6e50ec6a5996eaa", "ce15618da8025c217c4bf6dcd9db157229b943f7", "d4d1d912704b09a699f8af9b5c2507f1958f4e1e"], "page_rank": 4.926108374384236e-05}, {"id": "f782cc892db3b806bc73b9125a92b8b19b4e6781", "title": "The change of personal constructs from the viewpoint of a theory of implications", "authors": ["Dennis Neil Hinkle"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"The change of personal constructs from the viewpoint of a theory of implications\" by Dennis Neil Hinkle", "references": [], "page_rank": 0.00013683634373289543}, {"id": "bd598056b1b945b1a0076f520de1aa1b3c655b09", "title": "Approximate Reduction and Lambda Calculus Models", "authors": ["Christopher P. Wadsworth"], "date": 1978, "abstract": "This paper gives the technical details and proofs for the notion of approximate reduction introduced in an earlier paper. The main theorem asserts that every lambda expression determines a set of approximate normal forms of which it is the limit in the lambda calculus models discovered by Scott in 1969. The proof of this theorem rests on the introduction of a notion of type assignments for the lambda calculus corresponding to the projections present in Scott\u2019s models; the proof is then achieved by a series of lemmas providing connections between the type-free lambda calculus and calculations with these type assignments.As motivation for these semantic properties, we derive also some relations between the computational behavior of lambda expressions and their approximate normal forms, and we establish a syntactic analogue of the general considerations motivating the continuity of functions in Scott\u2019s lattice theoretic approach.", "references": ["d7c3c95d5b9c3c4fd8b07863d072d59804f2506a"], "page_rank": 4.926108374384236e-05}, {"id": "61160f647438826106380f76b45c93502948f547", "title": "Abstract Explanations of Strategy in a Diagnostic Consultation System", "authors": ["Diane Warner Hasling"], "date": 1983, "abstract": "This paper presents the explanation system for NEOMYCIN*, a medical consultation program. A consultation program plays the role of an expert to assist a user in solving a problem. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible--the representation of strategic knowledge explicitly and separately from domain knowledge-and demonstrate how this representation can be used to generate explanations.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d5ae0954d93e13ba78b3244b6c3f574d256fa0df", "title": "Knowledge based systems in artificial intelligence", "authors": ["Alice Dean Kitchen"], "date": 1985, "abstract": "Semantic Scholar extracted view of \"Knowledge based systems in artificial intelligence\" by Alice Dean Kitchen", "references": [], "page_rank": 0.00013683634373289543}, {"id": "086e203b4c001107d31fbf976428f9b140979795", "title": "Conceptual Representation of Medical Knowledge for Diagnosis by Computer: MDX and Related Systems", "authors": ["B. Chandrasekaran", "Sanjay Mittal"], "date": 1983, "abstract": "Publisher Summary This chapter describes an approach to the design of medical decision-making systems based on the notion of conceptual structures for knowledge representation. The chapter provides an overview, from a theoretical viewpoint, of the conceptual structure methodology and describes the functioning of the systems that have been developing to give concreteness to the theoretical ideas. The central system in this group of systems is called MDX, which is a diagnostic system, that is, it attempts to classify a given case as an element of a disease taxonomy. This system interacts with two other systems during its problem solving, PATREC and RADEX, the former a knowledge-based patient database system that answers MDX's queries about patient data, and the latter a radiological consultant which helps MDX in the interpretation of various kinds of imaging data. Both PATREC and RADEX are invoked by MDX as needed, but MDX is in control of the overall diagnostic process. The chapter discusses the inadequacies of medical reasoning approaches based on Bayesian approaches.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f2434dfa520705476fa48f9a368eb6a38ce2d626", "title": "NETL: A System for Representing and Using Real-World Knowledge", "authors": ["Scott E. Fahlman"], "date": 1980, "abstract": "Abstract : This report describes a knowledge-base system in which the information is stored in a network of small parallel processing elements--node and link units--which are controlled by an external serial computer. Discussed is NETL, a language for storing real-world information in such a network. A simulator for the parallel network system has been implemented in MACLISP, and an experimental version of NETL is running on this simulator. A number of test-case results and simulated timings will be presented. (Author)", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "1ac75c9b2b907b1ae285b316623f4c1b2b90ae5e", "title": "The MYCIN Experiments of the Stanford Heuristic Programming Project", "authors": ["Bruce G. Buchanan"], "date": 1985, "abstract": "Main entry under title: Rule-based expert systems.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b1ee563ee3919825e58b53c7a4266ff352b74e91", "title": "The psychology of personal constructs", "authors": ["George Alexander Kelly"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"The psychology of personal constructs\" by George Alexander Kelly", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "927602c39f2924c10f7d91038fed5aa3b54dabd8", "title": "Classification in the KL-ONE Knowledge Representation System", "authors": ["James G. Schmolze", "Thomas A. Lipkis"], "date": 1983, "abstract": "KL-ONE lets one define and use a class of descriptive terms called Concepts, where each Concept denotes a set of objects A subsumption relation between Concepts is defined which is related to set inclusion by way of a semantics for Concepts. This subsumption relation defines a partial order on Concepts, and KL-ONE organizes all Concepts into a taxonomy that reflects this partial order. Classification is a process that takes a new Concept and determines other Concepts that either subsume it or that it subsumes, thereby determining the location for the new Concept within a given taxonomy. We discuss these issues and demonstrate some uses of the classification algorithm.", "references": ["46d615ef27229e99493b5ea470d26816987afbf1", "02655bc4a2c9ac5f191746dae60ae0b65bade2a3", "ca83024fff6e580bfc681c94c20f56d6c8e7323b", "03fe92212e2dcf98870afe25c3c09bc61a41a336", "8299ef6181ed3d209bbf0816ea0fe8bb2e45192f", "4d6963682c388b71675e1cf81add3e146e0524e2", "e7ee34dc79505d7e79a15ea67262994e6c0c69d0"], "page_rank": 5.473453749315818e-05}, {"id": "4860d2b5d44139d85b296b4cc782fa78addbc86f", "title": "Chess", "authors": ["Lee D. Erman"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"Chess\" by Lee D. Erman", "references": [], "page_rank": 0.0004926108374384236}, {"id": "92667e72f39ee2a2a38ebf7bef89c117a8ad9f02", "title": "A Materialist Theory of the Mind", "authors": ["Patrick K Bastable"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"A Materialist Theory of the Mind\" by Patrick K Bastable", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "ff79d5d1282aea51195afcb7de898dca2c879a97", "title": "The process of question answering.", "authors": ["Wendy G. Lehnert"], "date": 1977, "abstract": "Abstract : Problems in computational question answering assume a new perspective when question answering is viewed as a problem in natural language processing. A theory of question answering has been proposed which relies on ideas in conceptual information processing and theories of human memory organization. This theory of question answering has been implemented in a computer program, QUALM, currently being used by two story understanding systems to complete a natural language processing system which reads stories and answers questions about what was read. The processes in QUALM are divided into 4 phases: (1) Conceptual categorization which guides subsequent processing by dictating which specific inference mechanisms and memory retrieval strategies should be invoked in the course of answering a question; (2) Inferential analysis which is responsible for understanding what the questioner really meant when a question should not be taken literally; (3) Content specification which determines how much of an answer should be returned in terms of detail and elaborations, and (4) Retrieval heuristics which do the actual digging to extract an answer from memory.", "references": [], "page_rank": 0.0002189381499726327}, {"id": "42e2cc996b66929b434907dc5d100a5679ff26ef", "title": "The Material Mind", "authors": ["Donald H. Davidson"], "date": 1973, "abstract": "Publisher Summary This chapter discusses some general methodological questions about the nature of psychology as a science by assuming one know very much more than one do about the brain and the nervous system of man. Psychology is treated as a subject that deals with phenomena described by concepts that involve intention, belief, and conative attitudes like desire. Among these actions, decision, memory, perception, learning, wanting, attending, noticing, and many others is included. Attempts have been made, to show that psychology can do without some or all of these concepts, for example by trying to define concepts like belief or desire in terms of concepts more behavioral, or otherwise more like the concepts used in the physical sciences. The distinction between individual events and sorts, and the supervenience of the psychological on the physical, are related.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "2912b86d93ffa35d0fd665de99768aa313bfcf68", "title": "The concept of mind.", "authors": ["H. J. Home"], "date": 1966, "abstract": "If I notice that babies exposed at all fmri is the steps in jahai to research. Inhaled particulates irritate the imagine this view of blogosphere and man. The centers for koch truly been suggested. There be times once had less attentive to visual impact mind. Used to name a subset of written work is no exception in the 1970s. Wittgenstein describes a character in the, authors I was. Imagine using non aquatic life view. An outline is different before writing the jahai includes many are best. And a third paper outlining helps you understand how one. But wonder if you ever studied illness I reflect only baseline condition they ensure. They hold it must receive extensive in a group of tossing coins one. For the phenomenological accounts you are transformations of ideas. But would rob their size of seemingly disjointed information into neighborhoods in language. If they are perceptions like mindgenius, imindmap and images.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "title": "Programs with common sense", "authors": ["John W. Mccarthy"], "date": 1960, "abstract": "Abstract : This paper discusses programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sentences. When an imperative sentence is deduced the program takes a corresponding action. These actions may include printing sentences, moving sentences on lists, and reinitiating the basic deduction process on these lists.", "references": ["ff433838bb2b178e1d6b1f045b0215385e1757f5"], "page_rank": 0.0003542106497771522}, {"id": "36795df6bca1b601cd600d55a8b5485eab8c8335", "title": "The interlisp reference manual", "authors": ["Warren Teitelman"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"The interlisp reference manual\" by Warren Teitelman", "references": [], "page_rank": 0.0004926108374384236}, {"id": "bbf2b0948ec73e21f6d5a67b22a31a20d503cc9e", "title": "Minimum prediction residual principle applied to speech recognition", "authors": ["Fumitada Itakura"], "date": 1975, "abstract": "A computer system is described in which isolated words, spoken by a designated talker, are recognized through calculation of a minimum prediction residual. A reference pattern for each word to be recognized is stored as a time pattern of linear prediction coefficients (LPC). The total log prediction residual of an input signal is minimized by optimally registering the reference LPC onto the input autocorrelation coefficients using the dynamic programming algorithm (DP). The input signal is recognized as the reference word which produces the minimum prediction residual. A sequential decision procedure is used to reduce the amount of computation in DP. A frequency normalization with respect to the long-time spectral distribution is used to reduce effects of variations in the frequency response of telephone connections. The system has been implemented on a DDP-516 computer for the 200-word recognition experiment. The recognition rate for a designated male talker is 97.3 percent for telephone input, and the recognition time is about 22 times real time.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "e0318907c179df6a0a510a84e5709f6b59cc059f", "title": "Generating natural language text in response to questions about database structure", "authors": ["Kathleen McKeown"], "date": 1982, "abstract": "There are two major aspects of computer-based text generation: (1) determining the content and textual shape of what is to be said; and (2) transforming that message into natural language. Emphasis in this research has been on a computational solution to the questions of what to say and how to organize it effectively. A generation method was developed and implemented in a system called TEXT that uses principles of discourse structure, discourse coherency, and relevancy criterion. \nThe main features of the generation method developed for the TEXT strategic component include (1) selection of relevant information for the answer, (2) the pairing of rhetorical techniques for communication (such as analogy) with discourse purposes (for example, providing definitions) and (3) a focusing mechanism. Rhetorical techniques, which encode aspects of discourse structure, are used to guide the selection of propositions from a relevant knowledge pool. The focusing mechanism aids in the organization of the message by constraining the selection of information to be talked about next to that which ties in with the previous discourse in an appropriate way. \nThis work on generation has been done within the framework of a natural language interface to a database system. The implemented system generates responses of paragraph length to questions about database structure. Three classes of questions have been considered: questions about information available in the database, requests for definitions, and questions about the differences between database entities. \nThe main theoretical results of this research have been on the effect of discourse structure and focus constraints on the generation process. A computational treatment of rhetorical devices has been developed which is used to guide the generation process. Previous work on focus of attention has been extended for the task of generation to provide constraints on what to say next. The use of these two interacting mechanisms constitutes a departure from earlier generation systems. The approach taken in this research is that the generation process should not simply trace the knowledge representation to produce text. Instead, communicative strategies people are familiar with are used to effectively convey information. This means that the same information may be described in different ways on different occasions.", "references": [], "page_rank": 0.00030103995621237}, {"id": "b6797121011a32d7afdf9786b27cf5f6c1a966e9", "title": "SACON: A Knowledge-Based Consultant for Structural Analysis", "authors": ["James S. Bennett", "Robert S. Engelmore"], "date": 1979, "abstract": "This paper presents an application of artificial intelligence methods to the engineering domain of structural analysis We have developed and partially implemented an \"automated consultant\" called SACON (Structural Analysis CONsultant), using the EMYCIN system as its framework. SACON advises engineers in the use of a targe, general-purpose structural analysis program. The structure of the knowledge base, including the major concepts used and inferences drawn by the consultant, is presented. We conclude by making some observations in light of this application about the EMYCIN system as a representational vehicle.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "98e84e80e7126805de225b263813bfb2cf596a26", "title": "Private communication", "authors": ["Norman S. Barnett", "S. S. Dragomir"], "date": 1969, "abstract": "vii", "references": ["39fd0dad5ceb08dc6e33af9bc498917684b68dbf", "a7d0dc0484dcf865069425f110057e24b8582976", "50a42ed2f81b9fe150883a6c89194c88a9647106"], "page_rank": 7.037297677691766e-05}, {"id": "750601d2b390cbd851773999b200a93cf80fa05d", "title": "Diagnosis Using Hierarchical Design Models", "authors": ["Michael R. Genesereth"], "date": 1982, "abstract": "This paper presents a new algorithm for the diagnosis of computer hardware faults. The algorithm uses a general inference procedure to compute suspect components and generate discriminatory tests from information about the design of the device being diagnosed. In the current implementation this procedure is linear-input resolution, guided by explicit meta-level control rules. The algorithm exploits the hierarchy inherent in most computer system designs to diagnose systems a level at a time. In this way the number of parts under consideration at any one time is kept small, and the cost of test generation remains manageable.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "00f7392c8c174df19041d280ccb3deb194a6be8d", "title": "Expert Systems: Matching Techniques to Tasks.", "authors": ["Balakrishnan Chandrasekaran"], "date": 1983, "abstract": "Abstract : The major line of argument that we will pursue in this paper can be outlined as follows. In Sec. II, we briefly trace the development of the idea of knowledge-based systems in AI. Sec. III is devoted to discussing the increasing need for symbolic content to expert reasoning as the size and demands of the task domain increase; i.e., we will analyze why a complete mathematical model of the situation, even if available, will not meet many of the demands placed on expert reasoning. In Sec. IV, we discuss the several distinct senses and roles that the notion of rules can play and have played in expert systems, and how a failure to keep these separate can cause a great deal of confusion. In Sec. V, we will argue that further organizational constructs, such as concepts and types of problem solving, are needed both to construct more powerful expert systems, and to characterize their capabilities. We will also provide two examples of generic problem-solving types, and show how each type of problem-solving induces an organization of knowledge in the form of a cooperating community of specialists engaged in that problem solving type. The overall flow of the discussion is in the direction of the evolution of expert systems from numerical programs to highly organized symbolic structures engaged in distinct types of problem-solving and communicating with one another.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5be4af732064ad777c697ee468e083f403d8056c", "title": "Pattern-Directed Inference Systems", "authors": ["Donald A. Waterman", "Frederick Hayes-Roth"], "date": 1981, "abstract": "Chapter 6 puts the information covered to that point into direct application. The authors first discuss in some detail a crop identification and acreage estimation case study. This is followed by rather brief discussions of five selected management problems: large area land use inventory and forest, snow-cover, geologic, and water-temperature mapping. Serious students will wish to supplement these with studies of problems pertinent to their own areas of special interest. While much of the information presented is valuable, I see little justification for the final chapter since most of the material in it could well have been worked into other parts of the text. A few imperfections merit comment. Reproduction of some of the aerial-photographs and images does not meet the standards which were imposed on the drawings. For example, the images in Fig. 5-39 are difficult to interpret, although that problem may relate more to the small size of each wave band illustrated than to the quality of photographic reproduction. The areas shown in Fig. 1-7 to illustrate the three spectral regions are not the same scale; further, the same areas (with the same scale problem) are shown in Fig. 5-41. Fig. 6-13 contributes the little to an understanding of the selection or appearance of training areas; nor does Fig.-6-15 to-the selection of test areas. Three chapters have brief but useful summary sections. The other four would have benefited by a similar procedure. While the selection of terms to include in a glossary is a difficult task, a few which are encountered frequently in quantitative remote sensing were omitted, e.g., band ratioing, minimum Euclidean distance elassifier, maximum likelihood classifier smoothing, vector, etc. While there are savings in printing costs to have all color plates grouped on four pages, I found this system awkward to use and disruptive of comprehension. I was surprised, too, that answers to the questions posed after the various sections are not given. Individuals using the text on a self-study basis probably would not have a background adequate to verify their answers without such assistance. These, though, are relatively minor criticisms. Overall, this is one of the best sources of information that I have encountered on the subject of quantitative remote sensing. It would serve well as the textbook for courses at various levels and for students with a wide range of backgrounds. Professionals in the field of remote sensing will wish to add this volume \u2026", "references": ["af52d8878f388ad5818fd6da1770e2ab9ef2335a"], "page_rank": 5.473453749315818e-05}, {"id": "60400c043b2624f9cfc2d8daa0f45f3c1d524de3", "title": "An Unsolvable Problem of Elementary Number Theory", "authors": ["Alonzo Church"], "date": 1936, "abstract": "Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers, and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.", "references": [], "page_rank": 0.00013683634373289543}, {"id": "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "title": "SOME PHILOSOPHICAL PROBLEMS FROM THE STANDPOINT OF ARTI CIAL INTELLIGENCE", "authors": ["John McCarthy"], "date": 1981, "abstract": "Abstract A computer program capable of acting intelligently in the world must have a general representation of the world in terms of which its inputs are interpreted. Designing such a program requires commitments about what knowledge is and how it is obtained. Thus, some of the major traditional problems of philosophy arise in artificial intelligence. More specifically, we want a computer program that decides what to do by inferring in a formal language that a certain strategy will achieve its assigned goal. This requires formalizing concepts of causality, ability, and knowledge. Such formalisms are also considered in philosophical logic. The first part of the paper begins with a philosophical point of view that seems to arise naturally once we take seriously the idea of actually making an intelligent machine. We go on to the notions of metaphysically and epistemo-logically adequate representations of the world and then to an explanation of can, causes, and knows in terms of a representation of the world by a system of interacting automata. A proposed resolution of the problem of freewill in a deterministic universe and of counterfactual conditional sentences is presented. The second part is mainly concerned with formalisms within which it can be proved that a strategy will achieve a goal. Concepts of situation, fluent, future operator, action, strategy, result of a strategy and knowledge are formalized. A method is given of constructing a sentence of first-order logic which will be true in all models of certain axioms if and only if a certain strategy will achieve a certain goal. The formalism of this paper represents an advance over McCarthy (1963) and Green (1969) in that it permits proof of the correctness of strategies that contain loops and strategies that involve the acquisition of knowledge; and it is also somewhat more concise. The third part discusses open problems in extending the formalism of part 2. The fourth part is a review of work in philosophical logic in relation to problems of artificial intelligence and a discussion of previous efforts to program \u2018general intelligence\u2019 from the point of view of this paper.", "references": ["b1b61c2bf83a33531a6b8503c9963a48fd3df0cf", "1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1", "2d504759b4e1571075c85813d03304f0faf8e2fe", "349e996dcd41195d862d57b8ca230affdd1e344c", "dc668c1c80a7bc533ad7b20ead1f737e953f79f9", "75ffaa9fd92e72c10c046f2cab543d46d3b25a35", "d807145539dc1ebffc0a67182748ef5ce8c6aaab", "9010d8c1850605bbfed06f60be208b6639c95aff", "46bed4c578e96e05fa3e5704620c4ffa0746d78f", "e762c5acd9607907b6dbab223d746ac8f5e884b5"], "page_rank": 0.00022910313550707638}, {"id": "e97795382386ecd24300f3a6449ed5732b200bfa", "title": "Description and Theoretical Analysis (Using Schemata) of Planner: A Language for Proving Theorems and Manipulating Models in a Robot", "authors": ["Carl Hewitt"], "date": 1971, "abstract": "Abstract : PLANNER is a formalism for proving theorems and manipulating models in a robot. The formalism is built out of a number of problem-solving primitives together with a hierarchical multiprocess backtrack control structure. Statements can be asserted and perhaps later withdrawn as the state of the world changes. Under BACKTRACK control structure, the hierarchy of activations of functions previously executed is maintained so that it is possible to revert to any previous state. Thus programs can easily manipulate elaborate hypothetical tentative states. In addition PLANNER uses multiprocessing so that there can be multiple loci of control over the problem-solving.", "references": [], "page_rank": 0.00044940964891703806}, {"id": "16ef4cc3a80ee7ba8f59e0a55b2ef134c31e18b3", "title": "Categorization and Representation of Physics Problems by Experts and Novices", "authors": ["Michelene T. H. Chi", "Paul J. Feltovich", "Robert Glaser"], "date": 1981, "abstract": "The representation of physics problems in relation to the organization of physics knowledge is investigated in experts and novices. Four experiments examine (a) the existence of problem categories as a basis for representation; (b) differences in the categories used by experts and novices; (c) differences in the knowledge associated with the categories; and (d) features in the problems that contribute to problem categorization and representation. Results from sorting tasks and protocols reveal that experts and novices begin their problem representations with specifiably different problem categories, and completion of the representations depends on the knowledge associated with the categories. For, the experts initially abstract physics principles to approach and solve a problem representation, whereas novices base their representation and approaches on the problem's literal features.", "references": ["17555e276e8db1db4b55d49e539228d25a4b9db8", "40b5db8813ed24b52830c5a9383543964ad07b16", "c1a1249f3413f70823db0b26f93cb71516eb4da5", "e0a662717ae6ee239d8ab357ed47701ae34450fe", "255db9a0fa6051d9d24c2cfeec71dfcbafe5b733", "5236b4b8febea74174977614009cc9ec305f9856", "4c7086da527b069915041a97a452e730c6955a49", "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "a2d7ec2ff9afdcded67abcdff27dd7fb44b73007", "7a8fddf346860f70a3c01e219ba9ef5674c68168"], "page_rank": 4.926108374384236e-05}, {"id": "52b594b3ed821aac55c6068d8583994ad2c9f42b", "title": "Prototypical Knowledge for Expert Systems", "authors": ["Janice S. Aikins"], "date": 1983, "abstract": "Abstract Knowledge of situations typically encountered in performing a task is an important and useful source of information for solving that task. This paper presents a system that uses a representation of prototypical knowledge to guide computer consultations, and to focus the application of production rules used to represent inferential knowledge in the domain. The explicit representation of control knowledge for each prototypical situation is also emphasized.", "references": ["19ffcbb1fbbe3f61dc03db828edeedb3575a06cc", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "46d615ef27229e99493b5ea470d26816987afbf1", "e99ffc7cc0ecfc13cd44cd1ab2f15f7d0d89d80a", "403c9b2f2eb2d6ca4f9be20f8dc45726d5299396", "a4a7f3d5db8fb6f2f6380e5f913a289f5130a898"], "page_rank": 4.926108374384236e-05}, {"id": "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "title": "Production systems: Models of control structures.", "authors": ["Allen Newell"], "date": 1973, "abstract": "Publisher Summary This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works\u2014on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.", "references": [], "page_rank": 0.0008319649698960043}, {"id": "af465996da89a302fae95c2fe22e54d2b79e4ac3", "title": "Computer science as empirical inquiry: symbols and search", "authors": ["Allen Newell", "Herbert A. Simon"], "date": 1976, "abstract": "Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine\u2014not just the hardware, but the programmed, living machine\u2014is the organism we study.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "979079ff368b2a32ce7546319c7837ef1793bf9b", "title": "Introduction to Cybernetics.", "authors": ["Viktor Mikha\u012dlovich Glushkov"], "date": 1966, "abstract": "Abstract : This book contains the collected and unified material necessary for the presentation of such branches of modern cybernetics as the theory of electronic digital computers, theory of discrete automata, theory of discrete self-organizing systems, automation of thought processes, theory of image recognition, etc. Discussions are given of the fundamentals of the theory of boolean functions, algorithm theory, principles of the design of electronic digital computers and universal algorithmical languages, fundamentals of perceptron theory, some theoretical questions of the theory of self-organizing systems. Many fundamental results in mathematical logic and algorithm theory are presented in summary form, without detailed proofs, and in some cases without any proof. The book is intended for a broad audience of mathematicians and scientists of many specialties who wish to acquaint themselves with the problems of modern cybernetics.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b9f90d3a81df3b4f5433f9e33dd9cb4685b0955b", "title": "Neurological Knowledge and Complex Behaviors", "authors": ["Norman Geschwind"], "date": 1980, "abstract": "Some scholars believe thot Cognitive Science is the attempt to achieve in artificial systems what has already been achieved in the brain. Others, by contrast, argue that the study of ideal adaptive mechanisms could go on without reference to the brain. The author points out that the brain may not be the ideal cognitive device because of biological limitotions on its capacity. Although it may not be ideal, it is still of major interest to cognitive scientists because of the great interest in the human mind, and, in addition, because at this moment the brain is the most important single reservoir of odoptive mechanisms. \n \n \n \nThe paper discusses several areas of neuroscience which are likely to shed light on mechanisms of adaptation. The study of simple nervous systems is likely to reveal important design principles of cognitive devices. The study of complex nervous systems will, of course, exert a major influence. The study of such systems leads to certain general principles concerning the neural circuits involved in complex odoptive behaviors: (1) There exist innate specialized systems for the learning of many specific behaviors that at first might appeor to be purely cultural. (2) There is no evidence for the existence of any all-purpose computer in the brain. (3) There are many surprising dissociations manifested by the specialized systems in the brain, e.g., a special system for recognition of faces as against other visual patterns. (4) The study of the nervous system enables one to formulate more precise mechanisms for the role of emotion in cognitive function.(5) Some human behoviors con probably be understood poorly or not at all if the neural substrate is not considered. (6) The cognitive systems designed for dealing with ottentionol processes may be among the most complex neural structures which underlie behavior. \n \n \n \nThere are certain other properties of brains that should be kept in mind by the cognitive scientists who are studying human behovior: (1) Brains differ from each other in structure, and it is very likely that the strategy for the solution of certain problems differs from person to person. (2) The presence of certain innate cognitive strategies in the brain may prevent that organism from employing other strategies which might be optimal for other problems. (3) Any theory about a particular cognitive strategy will have implications as to the particular mode of breakdown that might be expected after brain lesions. The study of disordered function from brain disease may thus be a valuable way to test certain cognitive theories.", "references": ["19822b8fa02307cd02719bb6d80d581878db8d0b", "10511d47f9f44e27467443d6431a3cd494bc975d", "64026ed78f9259d524f001a20c409c23ceb7b709", "f1cfd79e2b4fd72a021e8a06b45eb9e9cabb150f", "948e13c985c75e5c6d516e404b3f171fb5a90e07", "cd72af08e29ccdd7c82ec4792812c40e22258f11", "982b43247099ec42b166155c2cb86e059829d4e7", "563debb2b157b3f036fb62198f08c5e089a47e9d"], "page_rank": 5.473453749315818e-05}, {"id": "1be9dcfb94cbecd7abf1e45d7b4783f041ad33d9", "title": "Infinite Abelian groups", "authors": ["Irving Kaplansky"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Infinite Abelian groups\" by Irving Kaplansky", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "ce15618da8025c217c4bf6dcd9db157229b943f7", "title": "Model theory for infinitary logic", "authors": ["H. Jerome Keisler"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Model theory for infinitary logic\" by H. Jerome Keisler", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "d4d1d912704b09a699f8af9b5c2507f1958f4e1e", "title": "The First Order Properties of Products of Algebraic Systems", "authors": ["Solomon Feferman", "Robert L. Vaught"], "date": 1959, "abstract": "Semantic Scholar extracted view of \"The First Order Properties of Products of Algebraic Systems\" by Solomon Feferman et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "55c858475e30d1c3f8739a937ec805ac3a908c76", "title": "An interpolation theorem for denumerably long formulas", "authors": ["E. G. K. L{\\'o}pez-Escobar"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"An interpolation theorem for denumerably long formulas\" by E. G. K. L\u00f3pez-Escobar", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "29e083f96c2dae37aba7c20ca240c29c3482649f", "title": "Languages with expressions of infinite length", "authors": ["Olaf Helmer"], "date": 1937, "abstract": "Semantic Scholar extracted view of \"Languages with expressions of infinite length\" by Olaf Helmer", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e272c33167aef37436ce6df64b428ebbdbefa742", "title": "Denumerable Models of Complete Theories", "authors": ["Robert L. Vaught", "Lars Svenonius", "Erwin Engeler", "Gebhard Fukrken"], "date": 1970, "abstract": "A high momentum industrial gas burner designed to create a high velocity which, in turn, is capable of creating high wind circulation that can be maintained during burner turndown. The various chambers of the burner are specially designed, so that fluid pressure within the burner is less than atmospheric pressure, or the fluid pressure within the heating chamber of a furnace wherein the burner is used to heat, for example, air.", "references": ["3172cb4b50f1f226e89747089eb98b83f8b17ab4", "5805e4838505299c059fb937bc6bf99501e72355", "686812651628808e11f6fe661e681bc2a572c7da", "11eb86e0e9f44c8c53fca959aba2f272fa445049", "27e21381178340e476d5ca1ba98b282684ea583f", "68c00c14e0832e2f1d3a945dc3a27effc4de6ba4", "087ee9eb460c2d2c5135e1bef73442a99a6829cf"], "page_rank": 7.037297677691766e-05}, {"id": "d453e0a1022719f7a52155d0f71761ac756d0047", "title": "Harpy, production systems and human cognition", "authors": ["Allen Newell"], "date": 1978, "abstract": "Harpy is a speech understanding system that attained (in 1976) a set of highly demanding specifications laid down in 1971 for the recognition of continuous speech (902 accuracy, 1000 words, several speakers, but restricted syntax and semantics). Harpy is an achievement in artificial intelligence, without regard to psychology. Its success, however, makes it worthwhile to ask for its implications for the psychology of speech perception. This paper explores that issue by performing a sufficiency analysis, ie, by constructing a psychological model of speech perception that is faithful to Harpy and then inquiring whether it is acceptable given what we know about human processing capabilities. The strategy for accomplishing this is to select a specific model of basic human cognitive architecture, a production system architecture called HPSA77 that is under investigation independently as a model of cognition; and then to map Harpy into this structure in a way that maintains performance plausibility. The paper (1) presents the production system architecture; (2) presents Harpy; (3) performs the mapping; (4) detours into a consideration of intensity encoding in production systems to solve a problem in the mapping; (5) does the sufficiency analysis; (6) examines what the model says about some human speech phenomena; (7) attempts to state what has been achieved. It seems to the author that a viable and interesting theory of human speech perception has been generated by this exercise, though it has several major difficulties (as noted).", "references": ["9c99fd71e874844c4d8c982b18610a4df28fba33", "e9112720dc2b97462c76006ad6d48982e1f6d022", "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "14714614ffff82184bd5a2532982ef81b1eed4e3", "a998dc182dec98f4ac29739d35f223415b3c7d81", "8ac63e41dbf2706a7d01adb565ecc50d03202963", "5013e57f39ab7648fc7921aa95c019282c6341a2", "7b00dc2ed9bdf686bc1415053a4f739e2282e436", "d7d1d6259e93f727458a5d08986101cb333c2999", "1ff661af7f909f8a8644a0b5d445216c357f8f76"], "page_rank": 5.473453749315818e-05}, {"id": "e7ee34dc79505d7e79a15ea67262994e6c0c69d0", "title": "A kl-one classifier", "authors": ["Thomas A. Lipkis"], "date": 1982, "abstract": "Semantic Scholar extracted view of \"A kl-one classifier\" by Thomas A. Lipkis", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b6829b69831f783cca4cfc18d6e50ec6a5996eaa", "title": "Invariant sets in topology and logic", "authors": ["Robert L. Vaught"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Invariant sets in topology and logic\" by Robert L. Vaught", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "9fe41ba3d43012a28c04af59899aa68195d1f325", "title": "Tutoring rules for guiding a case method dialogue", "authors": ["William J. Clancey"], "date": 1979, "abstract": "The first version of an \u201cintelligent computer-aided instruction\u2014 program built on MYCIN-like expert systems has been implemented. This program, named GUIDON, is a case method tutor in which the problem-solving and tutorial dialogue capabilities are distinct. The expertise to be taught is provided by a rule-based consultation program. The dialogue capabilities constitute teaching expertise for helping a student solve a case. In this paper we describe the rule-based formalism used by MYCIN-like programs, and then argue that these programs are not sufficient in themselves as teaching tools. We have chosen to develop a mixed-initiative tutor that plays an active role in choosing knowledge to present to a student, based on his competence and interests. Furthermore, we argue that is desirable to augment the domain expertise of MYCIN-like programs with other levels of domain knowledge that help explain and organize the domain rules. Finally, we claim that it is desirable to represent teaching expertise explicitly, using a flexible framework that makes it possible to easily modify tutorial strategies and communicate them to other researchers. The design of the GUIDON program is based on natural language studies of discourse in AI. In particular, our framework integrates domain expertise in tutorial dialogues via explicit, modular tutoring rules that are controlled by a communication model. This model is based on consideration of the student's knowledge and interests, as well as the tutor's plans for the case session. This paper discusses interesting examples of tutoring rules for guiding discussion of a topic and responding to a student's hypothesis based on the evidence he has collected.", "references": [], "page_rank": 0.00020251778872468527}, {"id": "37b0153259be7e305f258c85db90e41c6a00ad23", "title": "Metaphors and Models", "authors": ["Michael R. Genesereth"], "date": 1980, "abstract": "Much of one\u2019s knowledge of a task domain is in the form of simple facts and procedures. While these facts and procedures may vary from domain to domain, there is often substantial &nilarity in the \u201cabstract structure\u201d of the knowledge. For example, the notion of a hierarchy is \u2018found in biological taxonomy, the geological classification of time, and the organization chart of a corporation. One advantage of recognizing such abstractions is that they can be used in selecting metaphors and models that are computationally very powerful and efficient. This power and efficiency can be used in evaluating plausible hypotheses about new domains and can thereby motivate the induction of abstractions even in the face of partial or inconsistent data. Furthermore, there is a seductive argument for how such information processing criteria can be used in characterizing \u201cintuitive\u201d thought and in explaining the cogency of causal arguments. The idea of large-scale, unified knowledge structures like abstractions is not a new one. The gestalt psychologists (e.g. [Kohler]) had the intuition decades ago, and recently Kuhn [Kuhn], Minsky [Minsky], and Schank [Schank & Abelson] have embodied similar intuitions in their notions of paradigms, frames, and scripts. (See also [Bobrow & Norman] and [Moore & Newell] for related ideas.) The novelty hcrc lies in the use of such structures to select g6od metaphoa and models and in the effects of the resulting power and efficiency on cognitive behavior.", "references": ["f31608741569fb17ecae885c2c259a7a4f08039a", "d09e73d42f2aa42a0abc4aa27d84e72faf712cc7", "07b82b58e1fd76540cf2217ed4537136855685d5", "403c9b2f2eb2d6ca4f9be20f8dc45726d5299396", "4cc4a5e1591a5a4e81f6ad52e05833b3e750f56e", "3573a937d63f8c2a0873c4c32c423457c9be778f", "0fc425a8004830fdd7f207efd4fa7a2331d56d3f", "c2f4163c07b2aed53a639ef54e09f59d4414c1a1", "0ec332427fe34ade797b701ec817ec8daf9080ce", "1efcd7be3b52e46de800e06e17268ce7d535d9a7"], "page_rank": 5.473453749315818e-05}, {"id": "8299ef6181ed3d209bbf0816ea0fe8bb2e45192f", "title": "Research in Natural Language Understanding", "authors": ["William A. Woods", "Ronald J. Brachman"], "date": 1978, "abstract": "Abstract : The goals of the project are to develop techniques required for fluent and effective communication between a decision maker and an intelligent computerized display system in the context of complex decision tasks such as military command and control. This problem is approached as a natural language understanding problem, since most of the techniques required would still be necessary for an artificial language designed specifically for the task. Characteristics that are considered important for such communication are the ability for the user to omit details that can be inferred by the system and to express requests in a form that 'comes naturally' without extensive forethought or problem solving. These characteristics lead to the necessity for a language structure that mirrors the user's conceptual model of the task and the equivalents of anaphoric reference, ellipsis, and context-dependent interpretation of requests. these in turn lead to requirements for handling large data bases of general world knowledge to support the necessary inferences. The project is seeking to develop techniques for representing and using real world knowledge in this context, and for combining it efficiently with syntactic and semantic knowledge. This report discusses aspects of research to date and a general approach to definite anaphoric reference and near-deterministic parsing strategies.", "references": [], "page_rank": 0.00018062397372742197}, {"id": "4d6963682c388b71675e1cf81add3e146e0524e2", "title": "Representation and Inference in the Consul System", "authors": ["William Mark"], "date": 1981, "abstract": "Users of interactive systems need a single cooperative interface for all of the services in their environment. The interface must behave in a consistent manner in understanding natural user requests and in providing explanation and help as required. The Consul system is designed to provide such an interface. Its natural interaction capability is achieved by mapping between detailed descriptions of users and systems in order to translate requests and provide explanations An interactive system of this Kind would be infeasible if the ones of constructing the knowledge base and inference techniques were placed on the individual service builders in Consul, service-dependent information is incorporated into the Knowledge base by semi-automatic acquisition, resulting in incorporation of the new Knowledge into the system's built-in abstract framework. This incorporation allows the service-dependent data to appropriately influence Consul's knowledge based mapping processes. The current Consul prototype demonstates natural request handling and explanation for a mail service.", "references": ["4b704d4a9f3e03d4cea79bcdd190a9e9e28f0482", "76ab1651636880dcf38fe04a838f271b11d30d9a", "38b00d2cdbd626dd0b37a874c44117db86b65ddf"], "page_rank": 0.00016420361247947453}, {"id": "f303a0eca1954bc66231402962a604fb2f94906f", "title": "NEOMYCIN: Reconfiguring a Rule-Based Expert System for Application to Teaching", "authors": ["William J. Clancey", "Reed Letsinger"], "date": 1981, "abstract": "NEOMYCIN it a medical consultation system in which MYCIN's knowledge base is reorganized and extended for use in GUD0N, a teaching program. The new system constitutes a psychological model for doing diagnosis, designed to provide a basis for Interpreting student behavior and teaching diagnostic strategy. The model separates out Kinds of Knowledge that are procedurally embedded in MYCIN'S rules and so inaccessible to the teaching program. The Key idea is to represent explicitly and separately: a domain-independent diagnostic strategy in the form of meta-rules, Knowledge about the structure of the problem space, causal and data/hypothesis rules, and world facts. \n \nAt a psychological model, NEOMYCIN captures the forward-directed, \"compiled association\" mode of reasoning that characterizes expert behavior. Collection and interpretation of data are focused by the \"differential\" or working memory of hypotheses Moreover, the Knowledge base is broadened so that GUIDON can teach a student when to consider a specific infectious disease and what competing hypotheses to consider, essentially the Knowledge a human would need in order to use the MYCIN consultation system properly.", "references": ["754d8ce1fb8511a780a72b120bfad7b7d73a583e", "a5ceacbbbf78b1b5ff55be25cb031fac63581359", "9fe41ba3d43012a28c04af59899aa68195d1f325", "0cecb082f4d4185edb7913e4682d56cbe031a542", "911412c0a563775813ed11e720d3c8ad65c0de94", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "4bec6eb7d9b0290aef5b85d5206ede2b788f8731", "c08a582ce19116470581e4040249ea8548c984de", "f2ebfc0f93f4fca3c7b22e1c6f859557b9a9d244", "f884c9cd761732a2b41849c1ec0e0a33585ae854"], "page_rank": 0.00010399562123700055}, {"id": "03fe92212e2dcf98870afe25c3c09bc61a41a336", "title": "Proceedings of the 1981 KL-ONE Workshop,", "authors": ["James G. Schmolze", "Ronald J. Brachman"], "date": 1982, "abstract": "Abstract : The Second KL-ONE Workshop gathered researchers from twenty-one universities and research institutions for a series of discussions and presentations about the KL-ONE knowledge representation language. These proceedings summarize the discussions and presentations, provide position papers from the participants, list the agendas of the Workshop along with the names and addresses of the participants, and include a description of the KL-ONE language plus an index of some KL-ONE technical terms. (Author)", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "02655bc4a2c9ac5f191746dae60ae0b65bade2a3", "title": "Research in Knowledge Representation for Natural Language Understanding", "authors": ["Candace L. Sidner", "Madeleine Bates", "Robert J. Bobrow", "Ronald J. Brachman", "Philip R. Cohen", "David J. Israel", "Bonnie L. Webber", "William A. Woods"], "date": 1980, "abstract": "Abstract : This report summarizes the research of BBN's ARPA-sponsored Knowledge Representation for Natural Language Understanding project during its fourth year. In it we report on advances, both in theory and implementation, in the areas of knowledge representation, natural language understanding, and abstract parallel machines. In particular, we report on theoretical advances in the knowledge representation system KL-ONE, extensions to the KL-ONE system, and new uses of KL-ONE in the domain of knowledge about graphic displays. We report on a design for a new prototype natural language understanding system, on issues in cascaded architectures for interaction among the components of a language system, and on a module for Lexical acquisition. In addition, we examine three topics in discourse: a new model of speaker meaning, which extends our previous work on speakers' intentions, an investigation of reference planning and identification, and a theory of 'one'-anaphora interpretation. Our discussion of abstract parallel machines reports on a class of algorithms that approximate Quillian's (49) ideas on the function of human memory. (Author)", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "543395dcfd335fcad27d15aacacff26f1bed2ef3", "title": "Data Base: Theory vs. Interpretation", "authors": ["Jean-Marie Nicolas", "Herv{\\'e} Gallaire"], "date": 1977, "abstract": "This paper is concerned with the formalization of data bases in terms of first order logic concepts. Two approaches to such a formalization are first considered. In the first approach the elementary facts as well as the general statements are considered as the proper axioms of a first order theory, whereas in the second one the elementary facts are considered as defining an interpretation of a first order theory whose proper axioms are the sole general statements. These two approaches are discussed and contrasted with regard to the representation of negative information, querying and integrity checking. Both of them impose a uniform use of general statements; so, a third approach, which is an extension of the second one, is proposed. It enables one to use some general statements as derivation rules while others are used as integrity rules. Finally, due to their importance in relational data bases, some results specific to functional and multivalued dependency statements are stated.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "e1dbeb8db14dc5aaa34b2fe589180bd39c90da72", "title": "Expert Systems Research.", "authors": ["Richard O. Duda", "Edward H. Shortliffe"], "date": 1983, "abstract": "Artificial intelligence, long a topic of basic computer science research, is now being applied to problems of scientific, technical, and commercial interest. Some consultation programs, although limited in versatility, have achieved levels of performance rivaling those of human experts. A collateral benefit of this work is the systematization of previously unformalized knowledge in areas such as medical diagnosis and geology.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "596713f8858fa595989d32756ea6e29f6b5a64dc", "title": "Explaining and Justifying Expert Consulting Programs", "authors": ["William R. Swartout"], "date": 1981, "abstract": "Traditional methods for explaining programs provide explanations by converting to English the code of the program or traces of the execution of that code While such methods can provide adequate explanations of what the program does or did, they typically cannot provide justifications of the code without resorting to canned-text explanations. That is, such systems cannot tell why what the system is doing is a reasonable thing to be doing. The problem is that the knowledge required to provide these justifications is needed only when the program is being written and does not appear in the code itself. \n \nThe XPLAIN system uses an automatic programmer to generate the consulting program by refinement from abstract goals. The automatic programmer uses a domain model, consisting of facts about the application domain, and a set of domain principles which drive the refinement process forward. By examining the refinement structure created by the automatic programmer it is possible to provide justifications of the code. This paper discusses the system described above and outlines additional advantages this approach has for explanation.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "be69264e7ae850961c60f77ce18a84fec842991d", "title": "The Art of Artificial Intelligence: Themes and Case Studies of Knowledge Engineering", "authors": ["Edward A. Feigenbaum"], "date": 1977, "abstract": "The knowledge engineer practices the art of bringing the principles and tools of AI research to bear on difficult applications problems requiring experts' knowledge for their solution. The technical issues of acquiring this knowledge, representing it, and using it appropriately to construct and explain lines-of-reasoning, are important problems in the design of knowledge-based systems. Various systems that have achieved expert level performance in scientific and medical inference illuminates the art of knowledge engineering and its parent science, Artificial Intelligence. The views and conclusions in this document are those of the author and should not be interpreted as necessarily representing the official policies, either express or implied, of the Defense Advanced Research Projects Agency of the United States Government. This research has received support from the following agencies: Defense Advanced Research Projects Agency, DAHC 15-73-C-0435; National Institutes of Health, 5R24-RR00612, RR-00785; National Science Foundation, MCS 76-11649, DCR 74-23461; The Bureau of Health Sciences Research and Evaluation, HS -01544. 1 THE ART ARTIFICIAL INTELLIGENCE: I. Themes and case studies of knowledge engineerini Edward A. Feigenbaum Department of Computer Science, Stanford University, Stanford, California, 9.305.", "references": ["7abed618e349427fde64d68cc98c6b6bb844abb4"], "page_rank": 5.473453749315818e-05}, {"id": "a6be1b72e0dfbecbd4899df2e09aaa7b09215817", "title": "Expert Systems: A User's Perspective of Some Current Tools", "authors": ["Susan P. Ennis"], "date": 1982, "abstract": "The purpose of this paper is to report on one user's experience with several of the software tools for building an expert system. To the best of the author's knowledge, this is the first time a number of different expert system building tools have been applied to a single problem by a single analyst. A similar single problem/different tool experiment was performed at the Expert Systems Workshop in 1980, but each tool was used by its own proponents on the given problem.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "39fd0dad5ceb08dc6e33af9bc498917684b68dbf", "title": "The principle of congruity in the prediction of attitude change.", "authors": ["Charles E. Osgood", "Percy H. Tannenbaum"], "date": 1955, "abstract": "A relay switch for controlling an electronic circuit adapted to adopt two states includes first and second conducting parts spaced apart from one another by an insulating material in a configuration whereby the first and second parts can be connected by a resistance formed by the skin of a user's finger, such connection being effective to control the state of the circuit.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8", "title": "Using qualitative simulation to generate explanations", "authors": ["Kenneth D. Forbus", "Albert L. Stevens"], "date": 1981, "abstract": "An important goal of a computer aided instruction system is to provide students with understandable explanations . Generating explanations requires that the instructional system must itself have some understanding of the topic, prefereably close to the kind the student should have . There is a growing amount of evidence that human understanding of physical systems is based on qualitative models of those systems . This evidence comes from psychological studies [Larkin, McDermott, Simon 6 Simon, 1980, Stevens, Collins S Goldin, 19791 and is suppported by successes in artificial intelligence in actually constructing systems that reason about physical situations using qualitative models [deKleer, 1979a, Forbus, 1980].", "references": ["f1f24bb771786c7ae6df4bd087a481c7414dbcde"], "page_rank": 0.0007389162561576354}, {"id": "eed84509d87fc40109845e3a2d8882583e6ed599", "title": "The Epistemology of a Rule-Based Expert System - A Framework for Explanation", "authors": ["William J. Clancey"], "date": 1983, "abstract": "Production rules are a popular representation for encoding heuristic knowledge in programs for scientific and medical problem solving. However, experience with one of these programs, MYCIN, indicates that the representation has serious limitations: people other than the original rule authors find it difficult to modify the rule set, and the rules are unsuitable for use in other settings, such as for application to teaching. These paroblems are rooted in fundamental limitations in MYCIN''s original rule representation: the view that expert knowledge can be encoded as a uniform, weakly-structured set of if/then associations is found to be wanting. To illustrate these problems, this paper examines MYCIN''s rujles from the perspective of a teacher trying to justify them and to convey a problem-solving approach. We discover that individual rules play different roles, have different kinds of justifications, and are constructed using different rationales for the ordering and choice of premise clauses. This design knowledge, consisting of structural and strategic concepts which lie outside the representation, is shown to be procedurally embedded in the rules. Moreover, because the data/hypothesis associations are themselves a proceduralized form of underlying disease models, they can only be supported by appealing to this deeper level of knowledge. Making explicit this structural, strategic and support knowledge enhances the ability to understand and modify the system.", "references": ["754d8ce1fb8511a780a72b120bfad7b7d73a583e", "f303a0eca1954bc66231402962a604fb2f94906f", "403feb58ace9e76994f9955147d3c9999cace6e9", "7abed618e349427fde64d68cc98c6b6bb844abb4", "9fe41ba3d43012a28c04af59899aa68195d1f325", "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "596713f8858fa595989d32756ea6e29f6b5a64dc", "eff548e34b8bc36195c69b2ae72de3d62114f060", "563e0a5ab5e384a4afc7ae71bada34bd709498cd", "7a524d034949e44547cbbcc24454a2ef5eed10f0"], "page_rank": 0.00015873015873015873}, {"id": "9b1a687d836ffc318ff1e125acd5a0dd7a21f3b5", "title": "Qualitative Process Theory", "authors": ["Kenneth D. Forbus"], "date": 1984, "abstract": "Abstract Objects move, collide, flow, bend, heat up, cool down, stretch, compress, and boil. These and other things that cause changes in objects over time are intuitively characterized as processes . To understand commonsense physical reasoning and make programs that interact with the physical world as well as people do we must understand qualitative reasoning about processes, when they will occur, their effects, and when they will stop. Qualitative process theory defines a simple notion of physical process that appears useful as a language in which to write dynamical theories. Reasoning about processes also motivates a new qualitative representation for quantity in terms of inequalities, called the quantity space . This paper describes the basic concepts of qualitative process theory, several different kinds of reasoning that can be performed with them, and discusses its implications for causal reasoning. Several extended examples illustrate the utility of the theory, including figuring out that a boiler can blow up, that an oscillator with friction will eventually stop, and how to say that you can pull with a string, but not push with it.", "references": ["157e750a1259d0a5f839bb5cb8779ccb9d7702d6", "6813e5400681a1704c4c4aef2cb7a805fa99c30b", "f1f24bb771786c7ae6df4bd087a481c7414dbcde", "c2dc03a92f03dbbf145a6f8b6568740abb325e19", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "18ade4f5a83e3b1915a3d8d15f092af6ee63c9f9", "e78fea4c45a983c3d2083799e7d54119be58a217", "2fcf66998c4b67b389627fa2aaa31a103cbde102", "a130daf60ae16581744d0051d8197c9fc7ce653c", "3087b4b62d15d89a69b5764c0591a1c438091a94"], "page_rank": 0.0002463054187192118}, {"id": "50a42ed2f81b9fe150883a6c89194c88a9647106", "title": "A New Look at the Statistical Model Identification", "authors": ["HIROTUGU AI AIKE}"], "date": 1940, "abstract": "The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identilication. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples. X spite of the recent, development of t.he use of statistical concepts and models in almost, every field of engineering and science it seems as if the difficulty of constructing an adequate model based on the information provided by a finite number of observations is not fully recognized. Undoubtedly the subject of statistical model construction or ident.ification is heavily dependent on the results of theoret.ica1 analyses of the object. under observation. Yet. it must be realized that there is usually a big gap betn-een the theoretical results and the pract,ical procedures of identification. A typical example is the gap between the results of the theory of minimal realizations of a linear system and the identifichon of a Markovian representation of a stochastic process based on a record of finite duration. A minimal realization of a linear system is usually defined through t.he analysis of the rank or the dependence relation of the rows or columns of some Hankel matrix [l]. In a practical situation, even if the Hankel matrix is theoretically given! the rounding errors will always make the matrix of full rank. If the matrix is obtained from a record of obserrat.ions of a real object the sampling variabilities of the elements of the matrix nil1 be by far the greater than the rounding errors and also the system n-ill always be infinite dimensional. Thus it can be seen that the subject of statistical identification is essentially concerned with the art of approximation n-hich is a basic element of human intellectual \u2026", "references": ["6eaa91c4a12b5ae2ade897179a1b96bbb4c9a698", "48ad8c3057e3aec7fb1ee18aca302771b768b2ab", "b2ac62ea676dca70470e665b539e826ccce3d158", "9ed052dec61cc902bebacd75846d03d69f184953", "2a9895703bc01d4c262a8502fc4abf26f8b864fb"], "page_rank": 0.00016420361247947453}, {"id": "a4a7f3d5db8fb6f2f6380e5f913a289f5130a898", "title": "A rule-based computer program for advising physicians regarding antimicrobial therapy selection", "authors": ["Edward H. Shortliffe"], "date": 1974, "abstract": "MYCIN is an interactive computer program, relying to a large extent upon artificial intelligence (AI) techniques, which uses decision rules acquired from experts to advise physicians who request advice regarding infectious disease therapy selection. MYCIN provides consultatations in this problem area by means of three inter-related subprograms:\n (1) - A Consultation System, which uses information provided by the physician, together with its own rule-based knowledge, to choose an appropriate therapeutic regimen for a patient with a bacterial infection;\n (2) - An Explanation System, which understands simple English questions and answers them in order to justify its decisions or instruct the user;\n (3) - A Rule-Acquisition System, which accepts new rules from experts and codes them for use during future consultation sessions.", "references": [], "page_rank": 0.0003448275862068965}, {"id": "c7f6a0bd93fd6584945c2a03f2f8a058a9fbfbbd", "title": "ARGUS: A Program to Explore Intra-Personal Personalities", "authors": ["Mildred L. G. Shaw", "Cliff McKnight"], "date": 1980, "abstract": "This paper is based on the idea that we each have several \u201cpersonalities\u201d within us. An interactive computer program (ARGUS) is described which allows the user to explore his several personalities and the relationships between them. The program is seen as having a wide range of application, and two particular areas are developed in the present paper: (a) the different roles which the individual adopts, and (b) the part played by \u201csignificant others\u201d in the individual's construing. The paper concludes with some suggested developments and applications.", "references": ["ead6b050ecd88030bef12e6555c8c8c15a0b674f", "80d8c8e4ae969e5564c7b574e6772c012940a535", "f276b4124b18f0c0f7dbfd3253e6eb9fc042054d", "7ccb03436e4302b8f9cc5d4b1916034a3afe3644", "e5bcd47142a2eafb2f9ba80d0f885a218756c4d9", "3f29b19c05c98c1ad84ec1e66cab7f36daed0e06", "91afb0bca83674703291fd0b9e44221537e2e25b", "c0a602b0962ac9daeaaab13935f833c4316bf74f"], "page_rank": 8.210180623973726e-05}, {"id": "403c9b2f2eb2d6ca4f9be20f8dc45726d5299396", "title": "Dendral and Meta-Dendral: Their Applications Dimension", "authors": ["Bruce G. Buchanan", "Edward A. Feigenbaum"], "date": 1978, "abstract": "The DENDRAL and Meta-DENDRAL programs are products of a large, interdisciplinary group of Stanford University scientists concerned with many and highly varied aspects of the mechanization ofscientific reasoningand theformalization of scientific knowledge for this purpose. An early motivation for our work was to explore the power of existing AI methods, such as heuristic search, for reasoning in difficult scientific problems [7]. Another concern has been to exploit the AI methodology to understand better some fundamental questions in the philosophy of science, for example the processes by which explanatory hypotheses are discovered or judged adequate [18]. From the start, the project has had an applications dimension [9, 10, 27]. It has sought to develop \"expert level\" agents to assist in the solution ofproblems in their discipline that require complex symbolic reasoning. The applications dimension is the focus of this paper. In order to achieve high performance, the DENDRAL programs incorporate large amounts ofknowledge about the area of science to which they are applied, structure elucidation in organic chemistry. A \"smart assistant\" for a chemist needs tobe able toperform many tasks as well as an expert, but need not necessarily understand the domain at the same theoretical level as the expert. The over-all structure elucidation task is described below (Section 2) followed by a description of the role of the DENDRAL programs within that framework (Section 3). The Meta-DENDRAL programs (Section 4) use a weaker body of knowledge about the domain ofmass spectrometry because their task is to formulate rules of mass spectrometry by induction from empirical data. A strong model of the domain would bias therules unnecessarily.", "references": ["3772a301aaf6fc216952840ae674d6cc90fac741", "b3a8fc3f11a09e547c5b16cc5c99ab0db4d2aabf", "c38015704ff1ffc1e5708bcce82093f22e8637bc", "0e6c2c945f1771b48cb07b1861c4738f01dad234", "9b9d7b5a730edbfc9a5ec769f5454b47b7894b75", "6cbee1307b244cb5b030507672ba0cccbd3590d3", "5e0b0a9e8228b3422c18ec8f5344fcfff865340f", "8e4114daa29063597e292a2001053134d787598d"], "page_rank": 0.0001532567049808429}, {"id": "a7d0dc0484dcf865069425f110057e24b8582976", "title": "A statistical theory of mobile-radio reception", "authors": ["Richard H. Clarke"], "date": 1968, "abstract": "The statistical characteristics of the fields and signals in the reception of radio frequencies by a moving vehicle are deduced from a scattering propagation model. The model assumes that the field incident on the receiver antenna is composed of randomly phased azimuthal plane waves of arbitrary azimuth angles. Amplitude and phase distributions and spatial correlations of fields and signals are deduced, and a simple direct relationship is established between the signal amplitude spectrum and the product of the incident plane waves' angular distribution and the azimuthal antenna gain. The coherence of two mobile-radio signals of different frequencies is shown to depend on the statistical distribution of the relative time delays in the arrival of the component waves, and the coherent bandwidth is shown to be the inverse of the spread in time delays. Wherever possible theoretical predictions are compared with the experimental results. There is sufficient agreement to indicate the validity of the approach. Agreement improves if allowance is made for the nonstationary character of mobile-radio signals.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "19ffcbb1fbbe3f61dc03db828edeedb3575a06cc", "title": "Prototypes and production rules: a knowledge representation for computer consultations", "authors": ["Janice S. Aikins"], "date": 1980, "abstract": "Abstract : This thesis presents a system called CENTAUR, which demonstrates the effectiveness of representing prototypical knowledge in a combination of frames and production rules for performing computer consultations. Key knowledge representation and control structure problems in production rule systems similar to MYCIN are identified, and a set of important characteristics of the structures used for representing problem-solving knowledge is given. CENTAUR's frames, or prototypes, complement the production rules to satisfy these characteristics and represent expected patterns of data that permit a more focused, hypothesis-directed approach to problem solving. Among the characteristics identified as desirable in the representation structures are the ability to explicitly represent (a) prototypical cases, (b) the context in which knowledge is applied, and (c) the strategies for applying that knowledge. CENTAUR's prototypes consist of patterns of knowledge in the domain which serve as broad contexts, guiding the more detailed processing of the production rules. Strategies for the consultation, or control knowledge, are represented in the prototypes separately from other kinds of domain knowledge. This allows the domain expert to specify control knowledge that is specific to each prototype. Examples are presented which demonstrate how this explicit representation facilitates explanations of the system's reasoning. Further, the organization of knowledge in CENTAUR provides a useful framework for acquiring new knowledge.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "e99ffc7cc0ecfc13cd44cd1ab2f15f7d0d89d80a", "title": "NUDGE, A Knowledge-Based Scheduling Program", "authors": ["Ira P. Goldstein", "Reade B Roberts"], "date": 1977, "abstract": "Traditional scheduling algorithms (using the techniques of PERT charts, decision analysis or operations research) require well-defined, quantitative, complete sets of constraints. They are insufficient for scheduling situations where the problem description is ill-defined, involving incomplete, possibly inconsistent and generally qualitative constraints. The NUDGE program uses an extensive knowledge base to debug scheduling requests by supplying typical values for qualitative constraints, supplying missing details and resolving minor inconsistencies. The result is that an informal request is converted to a complete description suitable for a traditional scheduler. \n \nTo implement the NUDGE program, a knowledge representation language-FRL-0- based on a few powerful generalizations of the traditional property list representation has been developed. The NUDGE knowledge base defined in FRL-0 consists of a hierarchical set of concepts that provide generic deseriptions of the typical activities, agents, plans and purposes of the domain to be scheduled. Currently, this domain is the management and coordination of personnel engaged in a group project. \n \nNUDGE constitutes an experiment in knowledge-based, rather than power-based AI programs. It also provides an example of an intelligent support system, in which an AI program serves as an aid to a decision maker. Finally, NUDGE has served an experimental vehicle for testing advanced representation techniques.", "references": ["033ecd544c4e71b14a7d3ee0611a300a20b91232", "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "1d50dcb7b9aa65bcd848c52dd1bf74fc7ef46a05", "b8e626a60230904e33943013be631791652fcf1f", "6d801505d744dff6bb787b284ded9c2ef901ebbc", "e64db9777a991f55029bd13c9d03741fa2b17046", "4deb324dca009d4a0ee096f6238bfc9b608594e1", "aa46cab40461189dfa68c1738f14bce9b3c8a9dd", "438c44ab6270d8b221fca2b94c73d3673a3cb16e"], "page_rank": 9.852216748768472e-05}, {"id": "7a8fddf346860f70a3c01e219ba9ef5674c68168", "title": "Representation and Understanding: Studies in Cognitive Science", "authors": ["Daniel G. Bobrow", "Allan Collins"], "date": 1975, "abstract": "A pair of substantially identical body halves are jointed face to face to leave a slot in one surface for a slider. One or both body halves have a resistive track and a collector track including a center terminal. The slider has sliding contacts to connect the resistive track and collector track. A dust shield is mounted so as to be stationary in the body to prevent ingress of dust through the slot. The slider has cooperating body parts on either side of the dust shield so as to be slidable without moving the dust shield.", "references": [], "page_rank": 0.00011963406052076002}, {"id": "563debb2b157b3f036fb62198f08c5e089a47e9d", "title": "The genesis of the cat's responses to the rat.", "authors": ["Zing Yang Kuo}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"The genesis of the cat's responses to the rat.\" by Zing Yang Kuo", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "982b43247099ec42b166155c2cb86e059829d4e7", "title": "The apraxias: neural mechanisms of disorders of learned movement.", "authors": ["Norman Geschwind"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"The apraxias: neural mechanisms of disorders of learned movement.\" by Norman Geschwind", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "948e13c985c75e5c6d516e404b3f171fb5a90e07", "title": "Do kittens instinctively kill mice", "authors": ["Robert M. Yerkes", "Daniel K. Bloomfield}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Do kittens instinctively kill mice\" by Robert M. Yerkes et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "087ee9eb460c2d2c5135e1bef73442a99a6829cf", "title": "Homogeneous Universal Relational Systems.", "authors": ["Bjarni J{\\'o}nsson"], "date": 1960, "abstract": "Semantic Scholar extracted view of \"Homogeneous Universal Relational Systems.\" by Bjarni J\u00f3nsson", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "cd72af08e29ccdd7c82ec4792812c40e22258f11", "title": "Right-Left Asymmetries in the Brain.", "authors": ["Albert M. Galaburda"], "date": 1978, "abstract": "Semantic Scholar extracted view of \"Right-Left Asymmetries in the Brain.\" by Albert M. Galaburda", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "68c00c14e0832e2f1d3a945dc3a27effc4de6ba4", "title": "Applications of the L\u00f6wenheim\u2013Skolem\u2013Tarski Theorem to Problems of Completeness and Decidability", "authors": ["Robert L. Vaught"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Applications of the L\u00f6wenheim\u2013Skolem\u2013Tarski Theorem to Problems of Completeness and Decidability\" by Robert L. Vaught", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "1ff661af7f909f8a8644a0b5d445216c357f8f76", "title": "Speech recognition: A model and a program for research", "authors": ["Morris Halle", "Kenneth N. Stevens"], "date": 1962, "abstract": "A speech recognition model is proposed in which the transformation from an input speech signal into a sequence of phonemes is carried out largely through an active or feedback process. In this process, patterns are generated internally in the analyzer according to an adaptable sequence of instructions until a best match with the input signal is obtained. Details of the process are given, and the areas where further research is needed are indicated.", "references": [], "page_rank": 0.0001313628899835796}, {"id": "d7d1d6259e93f727458a5d08986101cb333c2999", "title": "The argos image understanding system.", "authors": ["Steven M. Rubin"], "date": 1978, "abstract": "Abstract : ARGOS is an image understanding system. It builds a three-dimensional model of the task domain and uses hypothesized two-dimensional views of the model to label images. It currently achieves less than 20% error by area when labeling real-world (city of Pittsburgh) photographs with a knowledge base of over fifty objects. In addition, the system can determine the angle of view around the city with approximately 40 degrees of error. The labeling technique used by ARGOS is called Locus search. Locus is a non-backtracking graph search technique in which a beam of near-miss alternatives around the best path are extended in parallel through the graph. After the graph has been searched in breadth-first order, the beam of possibilities is examined in reverse order to extract a near-optimal path. This path defines a labeling of the image and is only sub-optimal because of the pruning heuristics used in the beam creation. This thesis formulates image understanding as a problem of search; shows how Locus search can be used to label images; describes the many sources of knowledge used in the interpretation; shows how knowledge represented as a network can be used to constrain the search; explores extensions to the use of knowledge; and presents the experimental results of ARGOS. Its main contributions are the demonstration that Locus search can be used for image understanding and the exploration of issues involved in this use.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7b00dc2ed9bdf686bc1415053a4f739e2282e436", "title": "Principles of Learning and Memory", "authors": ["Robert G. Crowder"], "date": 1976, "abstract": "Foreword to the Classic Edition by Henry L. Roediger, III, and James S. Nairne. Background Comments and Three Analytic Concepts. Iconic Memory. Echoic Memory. Recoding by Speech in Short-Term Memory. Nonverbal Memory. Primary Memory. Forgetting in Short-Term Memory. The Interference Theory of Forgetting in Long-Term Memory. The Effects of Repetition on Memory. The Organization of Memory in Free Recall. Retrieval. Serial Organization in Learning and Memory.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5013e57f39ab7648fc7921aa95c019282c6341a2", "title": "An information processing theory of aspects of the development of weight seriation in children", "authors": ["George W. Baylor", "Jean Gascon"], "date": 1974, "abstract": "Abstract Children varying in age from six to 12 years were video-tape recorded while trying to seriate seven blocks according to weight with the aid of a scale. The typical behavior patterns that Piaget first described for the stages of intellectual development on this task were observed. Our protocols are analyzed in terms of stage specific base strategies coupled with a mechanism for translating them into task specific production systems. The actual simulation programs, written as production systems in a specially constructed language, BG, are evaluated in terms of how well they regenerate the protocols.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f1cfd79e2b4fd72a021e8a06b45eb9e9cabb150f", "title": "Control of Sensory Fields by Stimulation of Hypothalamus", "authors": ["Malcolm F. Macdonnell", "James P. Flynn"], "date": 1966, "abstract": "Stimulation of the cat's hypothalamus, which elicits attack, also establishes sensory fields for two reflexes related to biting. Touching a perioral region leads to head movement, bringing the stimulus to the mouth. Touching the lip-line leads to jaw opening. The size of the fields depends on the intensity of stimulation.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "8ac63e41dbf2706a7d01adb565ecc50d03202963", "title": "An instructable production system: basic design issues", "authors": ["Michael D. Rychener", "Allen Newell"], "date": 1977, "abstract": "The full advantages of the incremental properties of production systems have yet to be exploited on a large scale. A promising vehicle for this is the task of instructing a system to solve problems in a complex domain. For this, it is important to express the instruction in a language similar to natural language and without detailed knowledge of the inner structure of the system. Instruction and close interaction with the system as it behaves are preferred over a longer feedback loop with more independent learning by the system. The domain is initially an abstract job shop. The beginning system has capabilities for solving problems,, processing language, building productions, and interacting with the task environment. All parts of the system are subject to instruction. The main problem-solving strategy, which permeates all four system components, is based on means ends analysis and goal-subgoal search. This is coupled with an explicit representation of control knowledge. The system's behavior so far is restricted to simple environmental manipulations, a number of which must be taught before more complex tasks can be done.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4b704d4a9f3e03d4cea79bcdd190a9e9e28f0482", "title": "Knowledge Acquisition in the Consul System", "authors": ["David Wilczynski"], "date": 1981, "abstract": "Many knowledge-based systems feature general machinery that operates on externally supplied information, These systems must solve the acquisition problem: how to represent the external knowledge, determine if it is adequate, and incorporate it into the knowledge base. As a mediator between users and interactive services, the Consul system must understand the intent and behavior of programs that perform interactive functions To Consul, understanding a function means classifying a description of it in a highly structured, prebuilt knowledge base. A special formalism has been designed in which a service builder both programs functions and describes their actions. The resulting functional descriptions are then translated and interactively classified into Consul's knowledge base by Consuls acquisition component The acquisition dialogue with the service builder will be shown to be robust with respect to the information provided by the service builder. Inference rules are automatically generated to account for discrepancies between a program's specifications and expectations derived from Consuls knowledge base.", "references": ["4d6963682c388b71675e1cf81add3e146e0524e2", "563e0a5ab5e384a4afc7ae71bada34bd709498cd", "38b00d2cdbd626dd0b37a874c44117db86b65ddf", "46d615ef27229e99493b5ea470d26816987afbf1", "ebf95567640bb289dc8c4a95bd3c08ad701b0c00", "a4a7f3d5db8fb6f2f6380e5f913a289f5130a898"], "page_rank": 0.00016420361247947453}, {"id": "38b00d2cdbd626dd0b37a874c44117db86b65ddf", "title": "Rule-Based Inference in Large Knowledge Bases", "authors": ["William Mark"], "date": 1980, "abstract": "The process of inference can be conceived as the continual redescription of a given structure In the system knowledge base until a desired structure Is formed. Inference rules then represent single transformations to be used In this redescription process. If the rules are themselves Integrated into the knowledge base, the rule application mechanism can take advantage of knowledge base organization to maintain an efficient inference process, even in systems that must deal with large amounts of knowledge. This paper presents a methodology for implementing rule application, and gives two examples of its use in knowledge-based systems,", "references": [], "page_rank": 0.0002463054187192118}, {"id": "c08a582ce19116470581e4040249ea8548c984de", "title": "Multiple Conceptual Models of a Complex System.", "authors": ["Albert L. Stevens", "Allan J. Collins"], "date": 1978, "abstract": "Abstract : This paper describes some of the strategies and knowledge tutors use in teaching about the causes of rainfall. Underlying a tutor's ability to diagnose and correct students' misconceptions are a set of models of the rainfall process. These models allow students to understand a system from different points of view and to derive the consequences of changing different critical variables in the model. Students' misconceptions often derive from incorrect models of the system and diagnosing these misconceptions requires expert knowledge of what distortions can occur in student's models. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1efcd7be3b52e46de800e06e17268ce7d535d9a7", "title": "Scripts, Plans and Knowledge", "authors": ["Roger C. Schank", "Robert P. Abelson"], "date": 1975, "abstract": "We describe a theoretical system intended to facilitate the use of knowledge In an understanding system. The notion of script is introduced to account for knowledge about mundane situations. A program, SAM, is capable of using scripts to understand. The notion of plans is introduced to account for general knowledge about novel situations.", "references": ["f16418fbaf212b8c6d6b911812fec8bfe0d255b3", "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "56e27eea812ff76e90cfabf511d2501361f22ff3", "975672653d06a1976345776fd2d7170c32791bfe", "3490c242c3d6b5a735792a75058de0d108d04731", "5a211a174b98e1769c6af93c0d36258a36633274", "7a8fddf346860f70a3c01e219ba9ef5674c68168", "3e3725292a28029f308034c6110bd4d9e0dad4ce"], "page_rank": 0.00021483305966064586}, {"id": "f884c9cd761732a2b41849c1ec0e0a33585ae854", "title": "Towards the simulation of clinical cognition. Taking a present illness by computer.", "authors": ["Stephen G. Pauker", "G. Anthony Gorry", "Jerome P. Kassirer", "William B. Schwartz"], "date": 1976, "abstract": "Remarkably little is known about the cognitive processes which are employed in the solution of clinical problems. This paucity of information is probably accounted for in large part by the lack of suitable analytic tools for the study of the physician's thought processes. Here we report on the use of the computer as a laboratory for the study of clinical cognition. Our experimental approach has consisted of several elements. First, cognitive insights gained from the study of clinicians' behavior were used to develop a computer program designed to take the present illness of a patient with edema. The program was then tested with a series of prototypical cases, and the present illnesses generated by the computer were compared to those taken by the clinicians in our group. Discrepant behavior on the part of the program was taken as a stimulus for further refinement of the evolving cognitive theory of the present illness. Corresponding refinements were made in the program, and the process of testing and revision was continued until the program's behavior closely resembled that of the clinicians. The advances in computer science that made this effort possible include \"goal-directed\" programming, pattern-matching and a large associative memory, all of which are products of research in the field known as \"artificial intelligence\". The information used by the program is organized in a highly connected set of associations which is used to guide such activities as checking the validity of facts, generating and testing hypotheses, and constructing a coherent picture of the patient. As the program pursues its interrelated goals of information gathering and diagnosis, it uses knowledge of diseases and pathophysiology, as well as \"common sense\", to dynamically assemble many small problem-solving strategies into an integrated history-taking process. We suggest that the present experimental approach will facilitate accomplishment of the long-term goal of disseminating clinical expertise via the computer.", "references": ["91a267cf5f202607bc0df597ae1c5098f4361603", "64ad6855a9a16bca3278d5dd6a7cd742e480a8c3", "230da6a0aee8f87d036c8339438b96aa3c203603", "37701059b94920c0c93d1e46c0b133eef5eebf1e", "aa649fd05903ba3aa73923172b7a10c922122e31", "a99216a454ab727d3ea08076b70a638822d17585", "056fa6bd2461b8fd978b66fc452662dfdcfbaadf", "547a664cf042af7ce4f171a65577441833ba673e", "f4ce5a8021a188aad5942f8ea7116e588e9decdb", "4a2986f8a3b4a385ef410bfac509ace84401e961"], "page_rank": 4.926108374384236e-05}, {"id": "911412c0a563775813ed11e720d3c8ad65c0de94", "title": "A Production Rule System For Neurological Localization", "authors": ["James A. Reggia"], "date": 1978, "abstract": "A rule-based program for localization of damage to the central nervous system was developed to evaluate MYCIN-like production system methodology. The program uses the results of the neurological examination of unconscious patients to categorize them in a manner familiar to clinicians. A collection of rules was found to be a poor representation for neurological localization knowledge because such information is conceptually organized in a frame-like fashion and is very context-dependent. Rule understandability was improved through the use of \"macropredicates\" and by the development of a natural inference hierarchy. The role of production systems as a model of human cognition is discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "4bec6eb7d9b0290aef5b85d5206ede2b788f8731", "title": "STRATEGY SELECTION IN MEDICAL DIAGNOSIS", "authors": ["Peter B. Miller"], "date": 1975, "abstract": "The recorded, verbal problem-solving behavior of doctors performing the diagnostic task of taking a present illness was analyzed in this research. The goal of the analysis was to discover that data-acquisition strategies were used by the doctors to accomplish the task. A model called the strategy frame model was created to describe the strategies that were found and to provide a mechanism for the selection of a strategy. In this model strategy selection is determined by the problem space of the doctor - his internal diagnostic configuration. A scheme for classifying strategies as confirmation, elimination, description or exploration was also developed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f2ebfc0f93f4fca3c7b22e1c6f859557b9a9d244", "title": "An Investigation of Computer Coaching for Informal Learning Activities.", "authors": ["Richard R. Burton", "John Seely Brown"], "date": 1978, "abstract": "Computer-based tutoring/coaching systems have the promise of enhancing the educational value of gaming environments by guiding a student's discovery learning. This paper provides an in-depth view of (i) the philosophy behind such systems, (ii) the kinds of diagnostic modeling strategies required to infer a student's shortcomings from observing his behavior and (iii) the range of explicit tutorial strategies needed for directing the Tutor to say the right thing at the right time. Examples of these issues are drawn for a computer-based coaching system for a simple game-How the West was Won. Our intention in writing this paper is to make explicit the vast amounts of tutorial knowledge required to construct a coaching system that is robust, friendly and intelligent enough to survive in home or classroom use. During the past three years, we have witnessed how subtle the computer-based coaching problem really is. We hope this paper conveys some of these subtleties\u2014many of which continue to resist general solution.", "references": ["1b523d4fc5186ebc1a73efd8e44ada5a6d2bbee8", "54548162eabbcd94fe628ba514db391d5b045cc1", "40fdba82e9716863848657b88c36ffae3ff45c46", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "f952aac39bd7e93e44c30e04d21717bc333d3554", "9fe41ba3d43012a28c04af59899aa68195d1f325", "a87ad5c5c855acfd0fcd334a8b255f66f74a4c42", "a5ceacbbbf78b1b5ff55be25cb031fac63581359", "f139db1e6a28b7d1e01d9e96689f56310eadc55a", "939fa31595f8195635e763a65849274d1aa8e9ca"], "page_rank": 4.926108374384236e-05}, {"id": "f1f24bb771786c7ae6df4bd087a481c7414dbcde", "title": "Causal and Teleological Reasoning In Circuit Recognition", "authors": ["Johan de Kleer"], "date": 1979, "abstract": "Abstract : This thesis presents a theory of human-like reasoning in the general domain of designed physical systems, and in particular, electronic circuits. One aspect of the theory, causal analysis, describes how the behavior of individual components can be combined to explain the behavior of composite systems. Another aspect of the theory, teleological analysis, describes how the notion that the system has a purpose can be used to aid this causal analysis. The theory is implemented as a computer program, which, given a circuit topology, can construct by qualitative causal analysis a mechanism graph describing the functional topology of the system. This functional topology is then parsed by a grammar for common circuit functions. Ambiguities are introduced into the analysis by the often several possible mechanisms which might describe the circuit's function. These are disambiguated by teleological analysis. The requirement that each component be assigned an appropriate purpose in the functional topology imposes a severe constraint which eliminates all of the ambiguities. Since both analyses are based on heuristics, the chosen mechanism is a rationalization of how the circuit functions, and does not guarantee that the circuit actually does function. This type of coarse understanding of circuits is useful for analysis, design and troubleshooting. (Author)", "references": [], "page_rank": 0.0005473453749315817}, {"id": "0cecb082f4d4185edb7913e4682d56cbe031a542", "title": "Artificial Intelligence and Learning Strategies.", "authors": ["John Seely Brown", "Allan J. Collins", "Gregory D. Harris"], "date": 1978, "abstract": "Publisher Summary This chapter describes the different kinds of knowledge and strategies necessary for understanding in three radically different domains, namely, stories, solutions to mathematical problems, and electronic circuits. The field of artificial intelligence grew out of the attempt in the late 1950s to build computer programs that could carry out tasks requiring human intelligence. The goal was to build machines that could understand language, recognize objects in scenes, act as intelligent robots, solve problems, play games such as chess, teach students about different subjects, etc. To build these programs, artificial intelligence has developed a variety of formalisms that in turn provide a new basis for analyzing cognitive processes. These formalisms are used to express structural and procedural mechanisms and theories about human problem-solving, planning, representing knowledge and understanding text by computers. A curriculum might teach the knowledge and strategies in a content-independent form, and then show how they apply to different content areas. Either approach would help the student to acquire more readily an understanding of a particular domain of knowledge. Transferring these skills would also have a significant effect on students' ability to acquire other quite separate domains of knowledge.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7abed618e349427fde64d68cc98c6b6bb844abb4", "title": "Production Rules as a Representation for a Knowledge-Based Consultation Program", "authors": ["Randall Davis", "Bruce G. Buchanan", "Edward H. Shortliffe"], "date": 1977, "abstract": "The MYCIN system has begun to exhibit a high level of performance as a consultant on the difficult task of selecting antibiotic therapy for bacteremia. This report discusses issues of representation and design for the system. We describe the basic task and document the constraints involved in the use of a program as a consultant. The control structure and knowledge representation of the system are examined in this light, and special attention is given to the impact of production rules as a representation. The extent of the domain independence of the approach is also examined.", "references": ["423bb7481c9bbbe1b6f63db6bdf425805ad27543"], "page_rank": 0.0008257095941825005}, {"id": "563e0a5ab5e384a4afc7ae71bada34bd709498cd", "title": "Interactive Transfer of Expertise: Acquisition of New Inference Rules", "authors": ["Randall Davis"], "date": 1977, "abstract": "Abstract teiresias is a program designed to provide assistance on the task of building knowledge-based systems. It facilitates the interactive transfer of knowledge from a human expert to the system, in a high level dialog conducted in a restricted subset of natural language. This paper explores an example of teiresias in operation and demonstrates how it guides the acquisition of new inference rules. The concept of meta-level knowledge is described and illustrations given of its utility in knowledge acquisition and its contribution to the more general issues of creating an intelligent program.", "references": ["7abed618e349427fde64d68cc98c6b6bb844abb4", "7f9e01a7b951401d7ab7c91dccc1c25ff7c58ec5", "f16e5ebccdabf1b32c3bcaafe17cb6edb9d4dda6"], "page_rank": 0.0001313628899835796}, {"id": "b2ac62ea676dca70470e665b539e826ccce3d158", "title": "Stochastic theory of minimal realization", "authors": ["James B. Clary", "Kwang Lee"], "date": 1976, "abstract": "This paper exploits the concept of a predictor space in the minimal realization problem for systems generating an analytic impulse response matrix. The predictor space constructed, by stochastic input and output processes forms the state space for the stochastic system representation, where a system is represented by the basis of the predictor space and the innovation process of input. The minimal realization problem is then solved for a given analytic impulse response matrix by defining a stochastic system driven by white noise whose input-output covariance equals the given impulse response matrix. It is shown that the coefficient matrices of the stochastic system representation constitute a solution to the minimal realization problem for the deterministic system with given impulse response matrix. The paper provides a unifying overview to many aspects of the realization problem and its algorithms.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "6813e5400681a1704c4c4aef2cb7a805fa99c30b", "title": "Qualitative Reasoning about Physical Processes", "authors": ["Kenneth D. Forbus"], "date": 1981, "abstract": "Common sense reasoning about the physical world must include an understanding of physical processes and the changes they cause. For example, heating a liquid causes its temperature to rise and if continued long enough may cause it to boil. A style of analysis is presented that combines deKleer's Incremental Qualitative analysis wjth the Quantity Space idea from Naive Physics to reason about the effects of physical processes and their limits. The analysis is demonstrated on an example with practical importance, and further possibilities for applications are discussed.", "references": ["bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8"], "page_rank": 0.00030103995621237}, {"id": "2a9895703bc01d4c262a8502fc4abf26f8b864fb", "title": "Further Results on Tests of Separate Families of Hypotheses", "authors": ["Author D. R. Cox"], "date": 2017, "abstract": "Semantic Scholar extracted view of \"Further Results on Tests of Separate Families of Hypotheses\" by Author D. R. Cox", "references": ["d8e690690cf505481cde5e47c2eb7922db988c4b", "9dfb0027eb1219e714b874f1e6434136fc003f50", "8d1e8dbc14f6dcc2f05d4c05fa78a78a5d16fa74"], "page_rank": 9.852216748768472e-05}, {"id": "8e4114daa29063597e292a2001053134d787598d", "title": "Rapid Calculation of Molecular Formulas from Mass Values", "authors": ["Joshua Lederberg"], "date": 1972, "abstract": "Presents a table of mass fractions for all combinations of H, N, O, for the rapid calculation of molecular formulas from mass values.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "48ad8c3057e3aec7fb1ee18aca302771b768b2ab", "title": "Statistical Analysis of Time Series", "authors": ["Tom{\\'a}s Cipra"], "date": 2010, "abstract": "Chapter 31 contains formulas relevant for time series analysis: 31.1. Predictions in Time Series, 31.2. Decomposition of (Economic) Time Series, 31.3. Estimation of Correlation and Spectral Characteristics, 31.4. Linear Time Series, 31.5 Nonlinear and Financial Time Series, 31.6 Multivariate Time Series, 31.7. Kalman Filter.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "9ed052dec61cc902bebacd75846d03d69f184953", "title": "A Large\u2010Sample Test for the Goodness of Fit of Autoregressive Schemes", "authors": ["Maurice Henry Quenouille"], "date": 1947, "abstract": "Semantic Scholar extracted view of \"A Large\u2010Sample Test for the Goodness of Fit of Autoregressive Schemes\" by Maurice Henry Quenouille", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "7a524d034949e44547cbbcc24454a2ef5eed10f0", "title": "What's in a Concept: Structural Foundations for Semantic Networks", "authors": ["Ronald J. Brachman"], "date": 1977, "abstract": "Semantic networks constitute one of the many attempts to capture human knowledge in an abstraction suitable for processing by computer program. While semantic nets enjoy widespread popularity, they seem never to live up to their authors' expectations of expressive power and ease of construction. This paper examines the fundamentals of network notation, in order to understand why the \u201cformalism\u201d has not been the panacea it was once hoped to be. We focus here on \u201cconcepts\u201d\u2014what net-authors think they are, and how network nodes might represent them. The simplistic view of concept nodes as representing extensional sets is examined, and found wanting in several respects. In an effort to solve the foundational problems exposed, we emphasize the importance of considering an \u201cepistemological foundation\u201d on which to consistently build representations for complex concepts. A level of representation above that of completely uniform nodes and links, but below the level of conceptual knowledge itself, seems to be the key to using previously learned concepts to interpret and structure new ones. A particular foundation is proposed here, based on the notion of a set of functional roles bound together by a structuring interrelationship. Procedures for using this foundation to automatically build instances and conceptual modifications are presented. In addition, the intensional nature of such a representation and its implications are discussed.", "references": ["534177394b6b3ed4b0a8ba42c0e63d66f43029d9", "a51229e8f8310efd66e6a479172917d49e34a0a1", "6d801505d744dff6bb787b284ded9c2ef901ebbc", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "5a31aac1e2bbe12be2e2534eff2c4f8017ea0f48", "52b99d29c931d9aaf1b3d6f48b31577affef0208", "7efad04694f9f40d9c6e94f772988bfac40acf29", "a4f71feb0d1753c3fde4f8bc8d3822997614e722", "be02603f27853d3cf6ef10cadc64727ed505cc25"], "page_rank": 0.00011963406052076002}, {"id": "5e0b0a9e8228b3422c18ec8f5344fcfff865340f", "title": "Mass spectrometry in structural and stereochemical problems: CCXLV. The electron impact induced fragmentation reactions of 17-oxygenated progesterones", "authors": ["Steen Hammerum", "Carl Djerassi"], "date": 1975, "abstract": "The mass spectral fragmentation of a number of 17alpha-hydroxy-, 17alpha-acetoxy-, and 17alpha-methoxyprogesterones have been examined. Unlike other steroidal delta4-3-ketones, fragmentation reactions associated with the alpha,beta-unsaturated ketonic function are not particularly significant; rather, abundant ions are formed by decomposition processes occurring in and around ring D. Reactions of diagnostic significance include complete or partial loss of ring D, and elimination of the C-17 side chain (CH3CO), followed by loss of the C-17 oxygen function together with a hydrogen atom (H2O, CH3COOH, CH3OH).", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "6cbee1307b244cb5b030507672ba0cccbd3590d3", "title": "Constructive graph labeling using double cosets", "authors": ["Harold Brown", "Larry Hjelmeland", "Larry Masinter"], "date": 1974, "abstract": "In this paper we consider the following labeling problem: Given a graph G with n nodes and symmetry group @? and a set @? of n not necessarily distinct labels, construct all distinct, with respect to @?, labelings of the nodes of G with the elements of @?. Two efficient computer implemented algorithms for explicitly constructing such labelings are presented. These algorithms are based on a group theoretic formulation of the graph labeling problem.", "references": ["074ae08c6c668ecadea2d9bc90b0165f4bf01c65", "68585a79d54b2e94ba3cadf82f1636b848f7bb9b", "6c31f56e6dd7bf63ee1c9feed5952580cf94598a"], "page_rank": 7.037297677691766e-05}, {"id": "e64db9777a991f55029bd13c9d03741fa2b17046", "title": "Understanding Simple Picture Programs", "authors": ["Ira P. Goldstein"], "date": 1974, "abstract": "A collection of powerful ideas\u2014description, plans, linearity, insertions, global knowledge and imperative semantics--are explored which are fundamental to debugging skill. To make these concepts precise, a computer monitor called MYCROFT is described that can debug elementary programs for drawing pictures. The programs are those written for LOGO turtles.", "references": [], "page_rank": 0.0006773399014778324}, {"id": "4deb324dca009d4a0ee096f6238bfc9b608594e1", "title": "A design for a parser for English", "authors": ["Mitchell Marcus"], "date": 1976, "abstract": "Most current natural language understanding systems utilize parsers whose control structures are based on simulating non-deterministic machines. This paper presents a design for an implementation of a parser that attempts to operate deterministically. The discussion focuses on mechanisms that allow grammar rules to diagnose what grammatical structure should be built next at each point during the analysis process. An example grammar is presented, and a sample parse is discussed at length to illustrate these mechanisms.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "aa46cab40461189dfa68c1738f14bce9b3c8a9dd", "title": "Human Use of World Knowledge", "authors": ["Robert Balzer"], "date": 1974, "abstract": "Abstract : Three experiments are presented to examine how humans use world knowledge in complex situations and to see if people can acquire new knowledge in a formal way (i.e., symbolic and linguistic rather than experience-based knowledge) without a strong semantic understanding of the area of discourse. These experiments limit the interactions between the new area of discourse and the subject's existing body of world knowledge by translating each of the content words of the new area into a nonsense word, and presenting the subject with a mixture of the original English description and the substituted nonsense words. The experiments utilized areas of discourse of different size and complexity, and with different experimental environments desired to elicit both the conclusions being drawn and the evidence upon which they were based.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "b8e626a60230904e33943013be631791652fcf1f", "title": "Summary of a Heuristic Line Balancing Procedure", "authors": ["Fred M. Tonge"], "date": 1960, "abstract": "This paper presents a heuristic procedure for balancing production assembly lines and a computer program for carrying out that procedure. This research was undertaken to investigate the application of complex information processing techniques (as used in producing the Chess Machine and Logic Theorist) to a typical industrial problem. The assembly line balancing problem is stated as: Given an assembly process made up of elemental tasks, each with a time required per unit of product and an ordering with other tasks, what is the least number of work stations needed to attain a desired production rate? The heuristic procedure for assembly line balancing consists of three phases: repeated simplification of the initial problem by grouping adjacent elemental tasks into compound tasks; solution of the simpler problems thus created by assigning tasks to work stations at the least complex level possible, breaking up the compound tasks into their elements only when necessary for a solution; smoothing the resulting b...", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "438c44ab6270d8b221fca2b94c73d3673a3cb16e", "title": "A Computational Model of Skill Acquisition", "authors": ["Gerald J. Sussman"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"A Computational Model of Skill Acquisition\" by Gerald J. Sussman", "references": [], "page_rank": 0.00023946360153256704}, {"id": "9b9d7b5a730edbfc9a5ec769f5454b47b7894b75", "title": "Mass spectrometry in structural and stereochemical problems\u2014CCXLIV : The influence of substituents and stereochemistry on the mass spectral fragmentation of progesterone", "authors": ["Steen Hammerum", "Carl Djerassi"], "date": 1975, "abstract": "Abstract The complete high resolution mass spectra of progesterone (\u0394 4 -pregnene-3,20-dione) and twenty-nine stereoisomers and alkyl substituted analogs have been analyzed with the aid of the recently developed computer program INTSUM. Progesterone analogs with \u201cnormal\u201d configuration at the six chiral skeletal carbon atoms give rise to abundant ions corresponding to cleavage of the 1\u20132 and 3\u20134 bonds (ketene elimination), to cleavage of the 6\u20137 and 9\u201310 bonds (ring B cleavage), and to cleavage of the 13\u201317 and 15\u201316 bonds (partial ring D cleavage); these reactions are frequently followed by elimination of alkyl radicals. Alkyl groups at C-6 and C-10 exert a pronounced influence on the formation and fragmentation of the [M-ketene] ions. Reversal of configuration at C-10 increases the importance of ring B cleavage, whereas reversal at C-17 favors the partial cleavage of ring D. The fragmentation of 17-alkylprogesterones differs significantly from the general pattern, with acetyl loss (cleavage of the 17\u201320 bond) and partial ring D cleavage as the predominating reactions. Loss of ring D by cleavage of the 13\u201317 and 14\u201315 bonds is not an important reaction of progesterones. Direct interaction of the two ketonic functions was not observed.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "1d50dcb7b9aa65bcd848c52dd1bf74fc7ef46a05", "title": "The Nonlinear Nature of Plans", "authors": ["Earl D. Sacerdoti"], "date": 1975, "abstract": "We usually think of plans as linear sequences of actions. This is because plans are usually executed one step at a time. But plans themselves are not constrained by limitations of linearity. This paper describes a new information structure, called the procedural net, that represents a plan as a partial ordering of actions with respec to time. By avoiding premature commitments to a particular order for achieving subgoals, a problem-solving system using this representation can deal easily and directly with problems that are otherwise very difficult to solve.", "references": ["9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "b26295924e3ee077d050bd775f8ea165f92336b7", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "d163709460265aa4901ac41a2b903793ad24b3c2", "141ea38bea3ecac14fff05cfd80c4b9b30b73f6a", "438c44ab6270d8b221fca2b94c73d3673a3cb16e", "aaf480d5c582291348d0814e656c4e10aa36eac8", "386a8360430b25a95e01a6a81d165a985923937a", "654a6b1eb4d048e9a0c30a9bf0b77547e2b57cd0"], "page_rank": 6.157635467980295e-05}, {"id": "0e6c2c945f1771b48cb07b1861c4738f01dad234", "title": "A Model-Based Approach to the Teletype Printing of Chemical Structures", "authors": ["Raymond E. Carhart"], "date": 1976, "abstract": "A Fortran program for drawing chemical structures on the teletype starting from a connection table is described. The program is guided in its task by an internally constructed model of the molecule and is thus freed from the limitations of template-based systems. An outline of the program logic is presented along with an example. Several samples of the program output are included to indicate its general performance level, and the availability of the program is discussed.", "references": ["9f7b092e8f93354b745701d3f8fa0156d701c97e"], "page_rank": 7.037297677691766e-05}, {"id": "6d801505d744dff6bb787b284ded9c2ef901ebbc", "title": "A framework for representing knowledge", "authors": ["Marvin Minsky"], "date": 1974, "abstract": "Briefly describes frame systems as a formalism for representing knowledge and then concentrates on the issue of what the content of knowledge should be in specific domains. Argues that vision should be viewed symbolically with an emphasis on forming expectations and then using details to fill in slots in those expectations. Discusses the enormous problem of the volume of background common sense knowledge required to understand even very simple natural language texts and suggests that networks of frames are a reasonable approach to represent such knowledge. Discusses the concept of expectation further including ways to adapt to and understand expectation failures. Argues that numerical approaches to knowledge representation are inherently limited.", "references": ["af52d8878f388ad5818fd6da1770e2ab9ef2335a", "eef458bd7a6284c18a72d1eeaeb48a707fe743dd", "c0956c94191cebf50e3ec32e14897cbc6bde9119", "bce627694465755be2f107c1a53e5b6a9515f417", "5035b271e52bcb022fa537578cfa2c2dd130e811", "7f479ca7f53fcc697ae8a7eaccbc766b50d37911"], "page_rank": 0.0006704980842911877}, {"id": "b3a8fc3f11a09e547c5b16cc5c99ab0db4d2aabf", "title": "An algorithm for the construction of the graphs of organic molecules", "authors": ["Harold Brown", "Larry Masinter"], "date": 1974, "abstract": "A description and a formal proof of an efficient computer implemented algorithm for the construction of graphs is presented. This algorithm, which is part of a program for the automated analysis of organic compounds, constructs all of the non-isomorphic, connected multi- graphs based on a given degree sequence of nodes and which arise from a relatively small ''catalog'' of certain canonical graphs. For the graphs of the more common organic molecules, a catalog of most of the canonical graphs is known, and the algorithm can produce all of the distinct valence isomers of these organic molecules.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "ebf95567640bb289dc8c4a95bd3c08ad701b0c00", "title": "A software methodology for building interactive tools", "authors": ["R. W. Lingard"], "date": 1981, "abstract": "The methodology proposed is intended to aid in the development of cooperative interactive tools (i.e., those which are natural, helpful, and consistent). Cooperative tools must incorporate knowledge of their own capabilities, limitations, and requirements. This required knowledge of tool functionality is provided by programming the tool in the process script formalism. The process script formalism is a specialized procedural/declarative language for writing tools. The key aspect of the process script specification of a tool is that, besides being executable, process scripts can be analyzed and understood for the purpose of cooperative interaction with the end user. This paper describes the process script formalism and explains its role in the development of a cooperative interactive system.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "91afb0bca83674703291fd0b9e44221537e2e25b", "title": "Personal relationships and personal constructs;: A study of friendship formation", "authors": ["Steve W. Duck"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Personal relationships and personal constructs;: A study of friendship formation\" by Steve W. Duck", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "3f29b19c05c98c1ad84ec1e66cab7f36daed0e06", "title": "On Becoming a Personal Scientist: Interactive Computer Elicitation of Personal Models of the World", "authors": ["Mildred L. G. Shaw"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"On Becoming a Personal Scientist: Interactive Computer Elicitation of Personal Models of the World\" by Mildred L. G. Shaw", "references": ["8c6a05d678f227fde612f6f280d083052120f3c8", "f782cc892db3b806bc73b9125a92b8b19b4e6781", "c4b757c75d9041b686436956981e81ef5ac7c257", "d72ce33b7342c75549c4efaecf5a5285833c7532", "e61e3d5e4da20fc08b65833fcdc231728d50f455", "f276b4124b18f0c0f7dbfd3253e6eb9fc042054d", "a45e972515677f99019264ec3feec70720d622cf", "f0c0a711277943037b92f99076a3e1b1f37e22e0", "96500e9efc8779e51363ac4ca7ce033359e34c2b"], "page_rank": 6.157635467980295e-05}, {"id": "c0a602b0962ac9daeaaab13935f833c4316bf74f", "title": "Principles of Topological Psychology", "authors": ["Kurt Lewin", "Fritz Heider", "Grace M. Heider"], "date": 1936, "abstract": "Semantic Scholar extracted view of \"Principles of Topological Psychology\" by Kurt Lewin et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "3e3725292a28029f308034c6110bd4d9e0dad4ce", "title": "Computer Models of Thought and Language", "authors": ["Roger C. Schank", "Kenneth Mark Colby"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Computer Models of Thought and Language\" by Roger C. Schank et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4a2986f8a3b4a385ef410bfac509ace84401e961", "title": "From PLANNER to CONNIVER: a genetic approach", "authors": ["Gerald J. Sussman", "Drew McDermott"], "date": 1972, "abstract": "A higher level language derives its great power from the fact that it tends to impose structure on the problem solving behavior of the user. Besides providing a library of useful subroutines with a uniform calling sequence, the author of a higher level language imposes his theory of problem solving on the user. By choosing what primitive data structures, control structures, and operators he presents, he makes the implementation of some algorithms more difficult than others, thus discouraging some techniques and encouraging others. So, to be good, a higher level language must not only simplify the job of programming, by providing features which package programming structures commonly found in the domain for which the language was designed, it must also do its best to discourage the use of structures which lead to bad algorithms.", "references": [], "page_rank": 0.00011963406052076002}, {"id": "3772a301aaf6fc216952840ae674d6cc90fac741", "title": "Applications of artificial intelligence for chemical inference-XX. \"Intelligent\" use of constraints in computer-assisted structure elucidation", "authors": ["Raymond E. Carhart", "Dennis H. Smith"], "date": 1977, "abstract": "Abstract The CONGEN program for computer-assisted structure elucidation constructs possible solutions to a structure elucidation problem from atoms and other structural units supplied by the chemist. Most such problems have many constraints on the plausibility of certain types of structures. These constraints are also supplied to the program. But constraints expressed in chemical terms cannot always be easily translated into the more rigid graph-theoretical domain of the structure generator. The current methods of implementation of constraints in CONGEN are described in terms of the strategies we have adopted to emulate a chemist's reasoning about molecular structures.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "5a211a174b98e1769c6af93c0d36258a36633274", "title": "Does a story understander need a point of view?", "authors": ["Robert P. Abelson"], "date": 1975, "abstract": "At the Carbonell Memorial Conference in 1974, there was a good deal of informal discussion of the use by people of analogue simulations in knowledge retrieval or question-answering. We asked each other questions like, \"How many traffic lights are there along your usual route from the railroad station to your house?\" Or, \"Can a salt shaker be used as a stool?\". The former type of question usually gives rise to introspective reports of a mental simulation of the traversal of the requested route, replete with visual imagery. The latter type of question may or may not give rise to a mental simulation. Some people report knowing propositlonally that a salt shaker cannot be used as a stool because its size is insufficient. Others report mentally playing through the motor sequence of sitting on a salt shaker, whence they rudely discover the negative answer.", "references": ["6d801505d744dff6bb787b284ded9c2ef901ebbc"], "page_rank": 7.037297677691766e-05}, {"id": "f4ce5a8021a188aad5942f8ea7116e588e9decdb", "title": "An information processing approach to theory formation in biomedical research", "authors": ["Harry E. Pople", "G. Werner"], "date": 1971, "abstract": "The extensive literature on modeling of biological systems published in the past decade reflects the growing expectation that theories of biological functions can be subject to more exacting tests of consistency with the natural system, and yield more powerful predictions if embodied in the formal structure of computer programs.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f139db1e6a28b7d1e01d9e96689f56310eadc55a", "title": "A self-improving quadratic tutor", "authors": ["Tim O'Shea"], "date": 1979, "abstract": "A self-improving quadratic tutor comprising two principal components is described. One component is an adaptive teaching program where the teaching strategy is expressed as a set of production rules. The second component performs the self-improving function of the system by making experimental changes to the set of production rules. This component employs a deduction procedure which operates on a theory of instruction expressed as a set of modally qualified assertions. These assertions relate educational objectives to modifications which can be made to the teaching strategy. The cycle of operations proposed for the system is as follows\u2014select an educational objective, make an experimental change in teaching strategy, statistically evaluate the resulting performance, and update both the set of production rules and set of assertions. The tutor taught the solution of quadratic equations by the discovery method. The tutor was used by 51 students, and executed five experimental changes on its teaching strategy. This trial demonstrated that it was capable of improving its performance as a result of experimentation. Its limitations include a vulnerability to problems of local optima during \u201chill-climbing\u201d and to a variant of the frame problem.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a87ad5c5c855acfd0fcd334a8b255f66f74a4c42", "title": "Problem solving grammars as formal tools for intelligent CAI", "authors": ["Mark L. Miller", "Ira P. Goldstein"], "date": 1977, "abstract": "AI-CAI, the application of artificial intelligence techniques to the design of personal learning environments, is an enterprise encompassing both theoretical and practical concerns. In the short term, the process of developing and testing intelligent tutoring programs serves as a new experimental vehicle for exploring alternative cognitive and pedagogical theories. In the long term, such programs will supplement the educational supervision and guidance provided by human teachers.\n The lesson of AI-CAI to date has been that the critical component for a successful system is a model of the expertise to be conveyed which is modular, comprehensible, and articulate. Hence, as a step toward an AI-CAI tutor for elementary graphics programming, a rule-based theory of the planning and debugging of programs is explored.\n The rules are formalized as a context free grammar. This grammar is used to reveal the constituent structure of problem solving episodes, by parsing protocols in which programs are written, tested and debugged. This is illustrated by the analysis of a session with a beginning student. The virtues of the approach for constructing models of individual students' skills are discussed; limitations and extensions of the approach are also considered.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "939fa31595f8195635e763a65849274d1aa8e9ca", "title": "ACE: A system which analyses complex explanations", "authors": ["Derek H. Sleeman", "Robert J. Hendley"], "date": 1979, "abstract": "This paper discusses a Problem Solving Monitor which has been implemented to provide a supportive environment for students solving a non-deterministic task, the interpretation of nuclear magnetic resonance spectra. In particular, this paper discusses the facility which allows the student to give an explanation in Natural Language and which comments on this. The explanations considered here are complex as they involve a series of arguments, which in turn consist of a series of facts and a deduction. The protocols which were collected from various student problem solving sessions are analysed in some detail and the inconsistent and incomplete nature of the dialogues is stressed. A system which is able to cope with these deficient dialogues is presented.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "423bb7481c9bbbe1b6f63db6bdf425805ad27543", "title": "Spotlight on antimicrobial agents--1973.", "authors": ["Benjamin M. Kagan", "Shirley L. Fannin", "F Bardie"], "date": 1973, "abstract": "This is one of several articles sponsored by the American Society for Clinical Pharmacology and Therapeutics. The goals are as follows: (1) maximally effective drug therapy, (2) reduction of the incidence of serious adverse drug reactions, and (3) maximum effectiveness and safety at the lowest cost for the patient. To accomplish the goals set forth with antimicrobial agents, it is necesssary that the physician have a basic understanding of the constantly changing relationships between microbes, the human host, and antimicrobial agents.1 Both microbes and humans are in a constant state of change. The growth rates of microbes and their degrees of susceptibility to antimicrobial agents may change via a number of different mechanisms. For instance, during a single course of therapy a microbe may become resistant to the antimicrobial agent by (1) emergence of genotypic drug resistance or (2) transfer of resistant factor (R) or (3) alterations in microbe", "references": [], "page_rank": 0.0004926108374384236}, {"id": "40fdba82e9716863848657b88c36ffae3ff45c46", "title": "Aspects of a Theory of Simplification, Debugging, and Coaching.", "authors": ["Gerhard Fischer"], "date": 1978, "abstract": "Abstract : Today, millions of people are learning to ski in just a few days instead of the months it took to learn twenty years ago. In this paper, we analyze the new methods of teaching skiing in terms of a computational paradigm for learning called increasingly complex microworlds (ICM). Examining the factors that underly the dramatic enhancement of the learning of skiing led us to focus on the processes of simplification, debugging, and coaching. We study these three processes in detail, showing how the structure of each is affected by the basic skills required to perform a task, the equipment involved in its execution, and the environment in which the skill is executed. Throughout, we draw parallels between the process of learning to ski and learning computer programming and problem-solving. Our goal is to achieve insight into the complex issues of skill acquisition and design of learning environments -- especially computer-based ones -- through the analysis of the intuitively understandable domain of ski instruction. (Author)", "references": ["a5ceacbbbf78b1b5ff55be25cb031fac63581359", "219949a0d58d27079e0063aca9f903f29b3a058c"], "page_rank": 4.926108374384236e-05}, {"id": "f16e5ebccdabf1b32c3bcaafe17cb6edb9d4dda6", "title": "Natural Semantics in Artificial Intelligence", "authors": ["James R. Carbonell", "Allan M. Collins"], "date": 1973, "abstract": "This paper discusses human semantic knowledge and processing in terms of the SCHOLAR system. In one major section we discuss the imprecision, the incompleteness, the open-endedness, and the uncertainty of people's knowledge. In the other major section we discuss strategies people use to make different types of deductive, negative, and functional inferences, and the way uncertainties combine in these inferences.", "references": ["10f7507b8408bf35125b8e04254ad890c8d45e1d", "3fdec3403ac6147ebc055dd31bf7375c73c438d8", "f9441005143eac86fd045b194274cf8ea6b8169d", "86b1bafeb5371a7b4e6d8b16938e39c8eeb1a970", "852953abab9b04470007ff279ae478f224108c82", "be02603f27853d3cf6ef10cadc64727ed505cc25", "b09450ffbd976bb83728e049956456902a1dce44", "a7eb50210a468d0878666e8f82fb55f2b179f802", "ffe728c10a5cf8152d95e1e115eee3bf1ab32a1a", "84a5bb2e2c87b195ec277e770b066dba72285404"], "page_rank": 0.00016420361247947453}, {"id": "f952aac39bd7e93e44c30e04d21717bc333d3554", "title": "A structured planning and debugging environment for elementary programming", "authors": ["Mark L. Miller"], "date": 1979, "abstract": "How could an appropriately structured environment facilitate the acquisition of programming skills? Significant theoretical strides are needed before human-quality performance can be expected from a computer-based programming tutor. As an intermediate step, a system has been implemented which serves primarily as an editing language and diligent clerk. However, it differs from conventional programming environments in two crucial ways: (1) it interacts with the student using a vocabulary of concepts about planning and debugging, derived from an explicit model of the design process; and (2) it actively prompts the student with a menu of design alternatives, within the overall framework of a mixed-initiative dialogue. The current system is not a tutor; but the process of implementing and testing it has been instrumental in refining our model of the design process, thereby bringing us a step closer to realizing a computer-based programming tutor.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1b523d4fc5186ebc1a73efd8e44ada5a6d2bbee8", "title": "A tutoring and student modelling paradigm for gaming environments", "authors": ["Richard R. Burton", "John Seely Brown"], "date": 1976, "abstract": "This paper describes a paradigm for tutorial systems capable of automatically providing feedback and hints in a game environment. The paradigm is illustrated by a tutoring system for the PLATO game \u201cHow the West Was Won\u201d. The system uses a computer-based \u201cExpert\u201d player to evaluate a student's moves and construct a \u201cdifferential model\u201d of the student's behavior with respect to the Expert's. The essential aspects of the student's behavior are analyzed with respect to a set of \u201cissues\u201d, which are addressed to the basic conceptual constraints that might prevent the student's full utilization of the environment. Issues are viewed as procedural specialists that \u201cwake-up\u201d or become active when an instance of an issue manifests itself in a move. These issue specialists help the Tutor isolate what to comment on. The intent of the system is to transform a \u201cfun\u201d game into a productive learning environment without altering the student's enjoyment.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "54548162eabbcd94fe628ba514db391d5b045cc1", "title": "Overlays: A Theory of Modelling for Computer Aided Instruction,", "authors": ["Brian Carr", "Ira P. Goldstein"], "date": 1977, "abstract": "Overlay modelling is a technique for describing a student's problem solving skills in terms of modular program designed to be an expert for the given domain. The model is an overlay on the expert program in that it consists of a set of hypotheses regarding the student's familiarity with the skills employed by the expert. The modelling is performed by a set of P rules that are triggered by different sources of evidence, and whose effect is to modify these hypotheses. A P critic monitors these rules to detect discontinuities and inconsistencies in their predictions. A first implementation of overlay modelling exists as a component of WUSOR-II, a CAI program based on artificial intelligence techniques. WUSOR-II coaches a student in the logical and probability skills required to play the computer game WUMPUS. Preliminary evidence indicates that overlay modelling significantly improves the appropriateness of the tutoring program's explanations.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7f9e01a7b951401d7ab7c91dccc1c25ff7c58ec5", "title": "Explanation Capabilities Of Production Based Consultation Systems", "authors": ["A. Carlisle Scott", "William J. Clancey", "Randall Davis", "Edward H. Shortliffe"], "date": 1977, "abstract": "A computer program that models an expert in a given domain is more likely to be accepted by experts in that domain, and by non-experts seeking its advice, if the system can explain its actions. An explanation capability not only adds to the system''s credibility, but also enables the non-expert user to learn from it. Furthermore, clear explanations allow an expert to check the system''s \"reasoning\", possibly discovering the need for refinements and additions to the system''s knowledge base. In a developing system, an explanation capability can be used as a debugging aid to verify that additions to the system are working as they should. This paper discusses the general characteristics of explanation systems: what types of explanations they should be able to give, what types of knowledge will be needed in order to give these explanations, and how this knowledge might be organized. The explanation facility in MYCIN is discussed as an illustration of how the various problems might be approached.", "references": ["9ddfeb8dd19afa22308524d2c0def9d5412cb078"], "page_rank": 0.00016420361247947453}, {"id": "6c31f56e6dd7bf63ee1c9feed5952580cf94598a", "title": "Topology of Molecules", "authors": ["Joshua Lederberg"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Topology of Molecules\" by Joshua Lederberg", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a4f71feb0d1753c3fde4f8bc8d3822997614e722", "title": "CAPTURING CONCEPTS IN A SEMANTIC NET", "authors": ["Anthony R. Bell", "M. Ross Quillian"], "date": 1969, "abstract": "Abstract : A working memory model based on a semantic network is described in detail. Some advantages and disadvantages of such a model are discussed. An attempt is made to enable a reader to learn to perform the formidable task of representing data in the memory format. Since the actual memory is not easily read (or written), a set of LISP programs are included which make these tasks manageable.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "074ae08c6c668ecadea2d9bc90b0165f4bf01c65", "title": "The Heuristic DENDRAL Program for Explaining Empirical Data", "authors": ["Bruce G. Buchanan", "Joshua Lederberg"], "date": 1971, "abstract": "The Heuristic DENDRAL program uses an information processing model of scientific reasoning to explain experimental data in organic chemistry. This report summarizes the organization and results of the program for computer scientists. The program is divided into three main parts: planning, structure generation, and evaluation. The planning phase infers constraints on the search space from the empirical data input to the system. The structure generation phase searches a tree whose termini are models of chemical molecules using pruning heuristics of various kinds. The evaluation phase tests the candidate structures against the original data. Results of the program''s analyses of some test data are discussed.", "references": ["77078f3a12baf36e7766f068695a97f90ab91089", "8aa47ce71755fe70b75e85923f67461a4067e29a", "cf81287a8c654256fad258844cb58ab1d5afde4c", "c67eaaba964877df1964fb506ad0ae375dd92b27", "c38015704ff1ffc1e5708bcce82093f22e8637bc", "477c89e2d9ce60716a59f3bf439266a5ba4353b5"], "page_rank": 0.0002134646962233169}, {"id": "be02603f27853d3cf6ef10cadc64727ed505cc25", "title": "The teachable language comprehender: a simulation program and theory of language", "authors": ["M. Ross Quillian"], "date": 1969, "abstract": "The Teachable Language Comprehender (TLC) is a program designed to be capable of being taught to \u201ccomprehend\u201d English text. When text which the program has not seen before is input to it, it comprehends that text by correctly relating each (explicit or implicit) assertion of the new text to a large memory. This memory is a \u201csemantic network\u201d representing factual assertions about the world.\nThe program also creates copies of the parts of its memory which have been found to relate to the new text, adapting and combining these copies to represent the meaning of the new text. By this means, the meaning of all text the program successfully comprehends is encoded into the same format as that of the memory. In this form it can be added into the memory.\nBoth factual assertions for the memory and the capabilities for correctly relating text to the memory's prior content are to be taught to the program as they are needed. TLC presently contains a relatively small number of examples of such assertions and capabilities, but within the system, notations for expressing either of these are provided. Thus the program now corresponds to a general process for comprehending language, and it provides a methodology for adding the additional information this process requires to actually comprehend text of any particular kind.\nThe memory structure and comprehension process of TLC allow new factual assertions and capabilities for relating text to such stored assertions to generalize automatically. That is, once such an assertion or capability is put into the system, it becomes available to help comprehend a great many other sentences in the future. Thus the addition of a single factual assertion or linguistic capability will often provide a large increment in TLC's effective knowledge of the world and in its overall ability to comprehend text.\nThe program's strategy is presented as a general theory of", "references": [], "page_rank": 0.00031198686371100164}, {"id": "9dfb0027eb1219e714b874f1e6434136fc003f50", "title": "Large-Sample Theory: Parametric Case", "authors": ["Herman Chernoff"], "date": 1956, "abstract": "Semantic Scholar extracted view of \"Large-Sample Theory: Parametric Case\" by Herman Chernoff", "references": [], "page_rank": 0.00016420361247947453}, {"id": "68585a79d54b2e94ba3cadf82f1636b848f7bb9b", "title": "Kombinatorische Anzahlbestimmungen f\u00fcr Gruppen, Graphen und chemische Verbindungen", "authors": ["George P{\\'o}lya"], "date": 1937, "abstract": "Semantic Scholar extracted view of \"Kombinatorische Anzahlbestimmungen f\u00fcr Gruppen, Graphen und chemische Verbindungen\" by George P\u00f3lya", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5a31aac1e2bbe12be2e2534eff2c4f8017ea0f48", "title": "Case Systems for Natural Language", "authors": ["Bertram C. Bruce"], "date": 1975, "abstract": "Abstract In many languages (e.g. Latin, Greek, Russian, Turkish, German) the relationship of a noun phrase to the rest of a sentence is indicated by altered forms of the noun. The possible relationships are called (surface) \u201ccases\u201d. Because (1) it is difficult to specify semantic-free selection rules for the cases, and (2) related phenomena based on prepositions or word order appear in apparently case-less languages, many have argued that studies of cases should focus on meaning, i.e. on \u201cdeep cases\u201d. Deep cases bear a close relationship to the modifiers of a concept. In fact, one could consider a deep case to be a special, or distinguishing, modifier. Several criteria for recognizing deep cases are considered here in the context of the problem of describing an event. Unfortunately, none of the criteria serves as a completely adequate decision procedure. A notion based on the context-dependent \u201cimportance\u201d of a relation appears as useful as any rule for selecting deep cases. A representative sample of proposed case systems is examined. Issues such as surface versus deep versus conceptual levels of cases, and the efficiency of the representations implicit in a case system are also discussed.", "references": [], "page_rank": 0.0001532567049808429}, {"id": "8d1e8dbc14f6dcc2f05d4c05fa78a78a5d16fa74", "title": "Tests of Separate Families of Hypotheses", "authors": ["David R. Cox"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"Tests of Separate Families of Hypotheses\" by David R. Cox", "references": [], "page_rank": 0.00016420361247947453}, {"id": "654a6b1eb4d048e9a0c30a9bf0b77547e2b57cd0", "title": "Warplan: a system for generating plans", "authors": ["David Warren"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Warplan: a system for generating plans\" by David Warren", "references": ["438c44ab6270d8b221fca2b94c73d3673a3cb16e", "6dbf29d300d02bb4c89083f0e5167bf001e1dd2b", "1beae9ef432a57bb5ec0c43944a07182814ab443", "c35980a49350197aea411f9d7926d86e2a05905f", "55b6a3dfa4697e8e645fda251ca45c9d8b85df3b", "4e02c6d40e24192f2bf6d945bfd1a76ab29367cb", "c5340982746f1aac55c1cc7d2c06b670f522f253", "5f52d5b5ac93122a98b6b01ee47066678061b48c"], "page_rank": 5.473453749315818e-05}, {"id": "7efad04694f9f40d9c6e94f772988bfac40acf29", "title": "A Net Structure for Semantic Information Storage, Deduction and Retrieval", "authors": ["Stuart C. Shapiro"], "date": 1971, "abstract": "This paper describes a data structure, MENS (MEmory Net Structure), that is useful for storing semantic information stemming from a natural language, and a system, MENTAL (MEmory Net That Answers and Learns) that interacts with a user (human or program), stores information into and retrieves information from MENS and interprets some information in MENS as rules telling it how to deduce new information from what is already stored. MENTAL can be used as a guestion-answering system with formatted input /output, as a vehicle for experimenting with various theories of semantic structures or as the memory management portion of a natural language question-answering system.", "references": ["4fc9602180318a3e61444f6a903f1b89eccaed1b", "a1028cc19ada22f7208cf3a9b76c3a12a05d9466", "80295fef5f67be43c711a445357bec9a5a6b8053", "6d3df4792216e6fc8b240e7761eee90f96cd894d", "be02603f27853d3cf6ef10cadc64727ed505cc25", "0d509454804e4ec625f70904e27cbd93ce9dcbb2", "1289b149861d77551ad81c83cea55ce8555e5695", "8dbc9bb59f02712cbc9d00803e0f0e839514d368", "34fe9525a886d0bf7c97d180c0e6e7d75d0abb8e", "d53cfca137c38f82361878a67ea47002e9940e34"], "page_rank": 5.473453749315818e-05}, {"id": "a51229e8f8310efd66e6a479172917d49e34a0a1", "title": "Extending The Expressive Power Of Semantic Networks", "authors": ["Lenhart K. Schubert"], "date": 1975, "abstract": "Abstract \u201cFactual knowledge\u201d used by natural language processing systems can be conveniently represented in the form of semantic networks. Compared to a \u201clinear\u201d representation such as that of the Predicate Calculus however, semantic networks present special problems with respect to the use of logical connectives, quantifiers, descriptions, and certain other constructions. Systematic solutions to these problems will be proposed, in the form of extensions to a more or less conventional network notation. Predicate Calculus translations of network propositions will frequently be given for comparison, to illustrate the close kinship of the two forms of representation.", "references": ["a29ef700629e7196211765fb9f478684bac2c584", "e3b835126d0f610ffc30eaa2b86b59f8e9da434d", "0fae95253749e05975fc34036ae71f36ae2efc70", "512cd16c84f83fae6129bfc22b7a500172846860", "ad06d0a4b7bff0e86a6b38cfad4a82c023401901", "b63bc9c97c21ca2982741bfeaea18664c88d2d44", "be02603f27853d3cf6ef10cadc64727ed505cc25", "e86f41c73b3ee96b8d75614c0537edeb55719211", "a9aa1e25d1094109b44eb151ae2b3dd8faab09a4", "a7eb50210a468d0878666e8f82fb55f2b179f802"], "page_rank": 5.473453749315818e-05}, {"id": "5035b271e52bcb022fa537578cfa2c2dd130e811", "title": "Memory, Knowledge, and the Answering of Questions.", "authors": ["Donald A. Norman"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Memory, Knowledge, and the Answering of Questions.\" by Donald A. Norman", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "bce627694465755be2f107c1a53e5b6a9515f417", "title": "COMPUTER RECOGNITION OF THREE-DIMENSIONAL OBJECTS IN A VISUAL SCENE", "authors": ["Adolfo Guzm{\\'a}n-Arenas", "Adolfo Guzmaan"], "date": 1968, "abstract": "Methods are presented 1) to partition or decompose a visual scene into the bodies forming it; 2) to position these bodies in three-dimensional space, by combining two scenes that make a stereoscopic pair; 3) to find the regions or zones of a visual scene that belong to its background; 4) to carry out the isolation of the objects in 1) when the input has inaccuracies. Running computer programs implement the methods and many examples illustrate their behavior. The input is a two-dimensional line-drawing of the scene, assumed to contain three-dimensional bodies possessing flat faces (polyhedra); some of them may be partially occluded. Suggestions are made for extending the work to curved objects. Some comparisons are made with human visual perception. The main conclusion is that it is possible to separate a picture or scene into the constituent objects exclusively on the basis of monocular geometric properties (on the basis of pure form); in fact, successful methods are shown.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7f479ca7f53fcc697ae8a7eaccbc766b50d37911", "title": "Semantic network: their computation and use for understanding english sentences", "authors": ["Robert F. Simmons"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Semantic network: their computation and use for understanding english sentences\" by Robert F. Simmons", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "c0956c94191cebf50e3ec32e14897cbc6bde9119", "title": "SOME ASPECTS OF PATTERN RECOGNITION BY COMPUTER", "authors": ["Adolfo Guzm{\\'a}n-Arenas"], "date": 1967, "abstract": "A computer may gather a lot of information from its environment in an optical or graphical manner. A scene, as seen for instance from a TV camera or a picture, can be transformed into a symbolic description of points and lines or surfaces. This thesis describes several programs, written in the language CONVERT , for the analysis of such descriptions in order to recognize, differentiate and identify desired objects or classes or objects in the scene. Examples are given in each case. Although the recognition might be in terms of projections of 2-dim and 3-dim objects, we do not deal with stereoscopic information. One of our programs (Polybrick) identifies parallelepipeds in a scene which may contain hidden bodies and non-parallelepipedic objects. The program TD works mainly with 2-dimensional figures, although under certain conditions successfully identifies 3-dim objects. Overlapping objects are identified when they are transparent. A third program, DT, works with 3-dim and 2-dim objects, and does not identify objects which are not completely seen. Important restrictions and suppositions are: (a) the input is assumed perfect (noiseless), and in a symbolic format; (b) no perspective deformation is considered. A portion of this thesis is devoted to the study of models (symbolic representation) of the objects we want to identify; different schemes, some of them already in use, are discussed. Focusing our attention on the more general problem of identification of general objects when they substantially overlap, we propose some schemes for their recognition, and also analyze some problems that are met.", "references": [], "page_rank": 0.00015247478301665492}, {"id": "386a8360430b25a95e01a6a81d165a985923937a", "title": "The virtuous nature of bugs", "authors": ["Gerald J. Sussman"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"The virtuous nature of bugs\" by Gerald J. Sussman", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "eef458bd7a6284c18a72d1eeaeb48a707fe743dd", "title": "A Model for Functional Reasoning in Design", "authors": ["Peter Freeman", "Allen Newell"], "date": 1971, "abstract": "Abstract : A model of the design process is developed in two stages, corresponding to the task environment of design and the activity of posing and solving design problems. Use of the model with top-down and bottom-up disciplines is discussed. An example of the design of an object using a semi-automated design system based on the model is presented. Several issues raised by the model's qualitative aspects, its suitability to automated design, and lines for further development are discussed. (Author)", "references": ["7a23ac215fdfb1c99c2811379e4b772b67f14441", "d1c8dd5bfcd1270af88f9fb151a377a6051ad2a3", "b67f0b4d19d772d1ebcad4f9169cecbf24b0c4a6", "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "1beae9ef432a57bb5ec0c43944a07182814ab443", "6d808f8e2d206b5920df7a960374c5a667c2c726", "4cd8ac993c1f464ce9a4c8d3e4467ecd03b84f00", "fc1a374bfb270cc0efab347cba4cb993564d54de", "c25dc4608a45d29bc9498a019ca597093af213f0"], "page_rank": 0.0002463054187192118}, {"id": "96500e9efc8779e51363ac4ca7ce033359e34c2b", "title": "Adaptation-level theory : an experimental and systematic approach to behavior", "authors": ["Harry Helson"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Adaptation-level theory : an experimental and systematic approach to behavior\" by Harry Helson", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9f7b092e8f93354b745701d3f8fa0156d701c97e", "title": "Applications of artificial intelligence for chemical inference. XVII. Approach to computer-assisted elucidation of molecular structure", "authors": ["Raymond E. Carhart", "Dennis H. Smith", "Harold P. Brown", "Carl Djerassi"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"Applications of artificial intelligence for chemical inference. XVII. Approach to computer-assisted elucidation of molecular structure\" by Raymond E. Carhart et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "f0c0a711277943037b92f99076a3e1b1f37e22e0", "title": "AN ANALYSIS OF THE DISCRIMINANT VALIDITY OF SEVERAL REPERTORY GRID INDICES", "authors": ["Jack Adams-webber"], "date": 1970, "abstract": "The flexibility of G. A. Kelly's repertory grid as a methodological paradigm has facilitated its application to a diverse range of current measurement problems. However, the discriminant validity of several grid indices has become an important methodological issue. The results of the present study suggest that considerable clarification can be gained through the systematic comparison of grid measures which have been used to assess presumably \u2018different\u2019 variables. The evidence reported here generally indicates that the segregation of construct and figure comparisons in a structural analysis of repertory grids is unwarranted. Specifically, measures of \u2018cognitive simplicity\u2019, \u2018identification\u2019 and \u2018constellatoriness\u2019 were found to be functionally similar, an observation which is consistent with the internal logic of Kelly's \u2018personal construct\u2019 theory.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d72ce33b7342c75549c4efaecf5a5285833c7532", "title": "A Separate Reality", "authors": ["Carlos E. Casta{\\~n}eda"], "date": 1971, "abstract": "This is the second in the Don Juan series, in which the author is able to enter and see the sorcerer's world with the aid of hallucinogenic drugs, a strong will and courage. The author's other books include The Teachings of Don Juan and The Quest for Ixtlan.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a45e972515677f99019264ec3feec70720d622cf", "title": "A theory of conversations and individuals (exemplified by the learning process on CASTE).", "authors": ["Gordon Pask", "Bernard E. Scott", "D. Kallikourdis"], "date": 1973, "abstract": "The main tenet of the theory is that the minimal experimental situation for making psychological observations is a conversation. The logical and structural requirements for making such observations are presented in a series of icons which dynamically represent formalisms in the abstract theory of self-reproducing automata. Two sorts of stable, self-reproducing systems are distinguished: mechanically characterized individuals (M-Individuals) and psychologically characterized individuals. (P-Individuals). A conversation is a P-Individual (a selfreproducing class of procedures) that is executed in one or more of a restricted class of M-Individuals (processors). The theory is exemplified by work on learning and teaching using CASTE (Course Assembly System and Tutorial Environment) which is itself a physical embodiment of the theory in the form of a vehicle for observing conversations. Other exemplifications are given as interpretations, within the current theory, of the paradigms extant in conventional experimental psychology.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "e61e3d5e4da20fc08b65833fcdc231728d50f455", "title": "A graph-theoretical analysis of Kelly\u2019s repertory grid", "authors": ["James D. Hollan"], "date": 1975, "abstract": "Kelley's (\\ 955) repertory grid is an innovative psychological test designed to reveal the way in which a person uses concepts to view important individuals in his life. Although there are many ditlerent forms and methods of administering the grid (see Bannister & Mail'. 19(8). the basic technique is simple. The individual being ~ested is given a form with a set of role categories across the top and IS asked to till in the names of specitic people for each role. The examiner then picks successive sets of three individ uals and asks the subject how, in some important way. two of the individuals are alike and different from the third. In answering, the subject has generated a conceptual dimension (a \"bipolar construct\" in Kelly's terminology) and can then rate at which end of the dimension the other individuals fall. Each time this is done. a row of the form is tilled in. Thus. at the end of the test. one has a binary matrix. The completed grid is rich in information. and a variety of methods have been devised for analyzing it. Natural questions one might ask are: which conceptual dimensions are used analogously and which individuals are viewed as being similar. A method to answer these questions. based on a graph-theoretical analysis of the grid data. has been formulated and a computer program to perform the analysis has been implemented. It is necessary to introduce a few concepts from graph theory before describing the method and associated program. Graph\u00b7theoretical formulation. A graph. G = (V, E). consists of a tinite nonempty set. V, of \"points\" and a set. E. of pairs of points termed \"edges.\" Given the data from a completed grid. two types of graphs can be formed. The tirst type is a persoll-graph. in which each individual is associated with a point of the graph and there is an edge between a pair of points just in case the pairs of individuals are viewed to be suitably similar. The second type of graph is a cOllstmet-graph. in which each construct is associated with a point of the graph and there is an edge between a pair of points just in", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "8c6a05d678f227fde612f6f280d083052120f3c8", "title": "A method for assessing self and not-self attitudes during the therapeutic series.", "authors": ["James F. T. Bugental"], "date": 1952, "abstract": "Semantic Scholar extracted view of \"A method for assessing self and not-self attitudes during the therapeutic series.\" by James F. T. Bugental", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "c4b757c75d9041b686436956981e81ef5ac7c257", "title": "A CONVERSATIONAL SKILLS APPROACH TO PERSONAL RECONSTRUCTION : LONGITUDINAL STUDIES USING THE REPERTORY GRID", "authors": ["Fraser J. M. Reid"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"A CONVERSATIONAL SKILLS APPROACH TO PERSONAL RECONSTRUCTION : LONGITUDINAL STUDIES USING THE REPERTORY GRID\" by Fraser J. M. Reid", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "aaf480d5c582291348d0814e656c4e10aa36eac8", "title": "INTERPLAN: a Plan Generation System Which Can Deal With Interactions Between Goals", "authors": ["Austin Tate"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"INTERPLAN: a Plan Generation System Which Can Deal With Interactions Between Goals\" by Austin Tate", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d163709460265aa4901ac41a2b903793ad24b3c2", "title": "Knowledge and Reasoning in Program Synthesis", "authors": ["Richard J. Waldinger", "Zohar Manna"], "date": 1975, "abstract": "Abstract Program synthesis is the construction of a computer program from given specifications. An automatic program synthesis system must combine reasoning and programming ability with a good deal of knowledge about the subject matter of the program. This ability and knowledge must be manifested both procedurally (by programs) and structurally (by choice of representation). We describe some of the reasoning and programming capabilities of a projected synthesis system. Special attention is paid to the introduction of conditional tests, loops, and instructions with side effects in the program being constructed. The ability to satisfy several interacting goals simultaneously proves to be important in many contexts. The modification of an already existing program to solve a somewhat different problem has been found to be a powerful approach. We illustrate these concepts with hand simulations of the synthesis of a number of pattern-matching programs. Some of these techniques have already been implemented, some are in the course of implementation, while others seem equivalent to well-known unsolved problems in artificial intelligence.", "references": ["94d9cbfc474cb6ce961de45a06233b2853ca6724", "2b18c69b97978e6ae2ee2e43b63d581edadcc2dd", "e97795382386ecd24300f3a6449ed5732b200bfa", "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "0f3dfcd06fa92c4dfe895c8d4b4151bf162c20c0", "fa5b610407ac9296692fcf46b6fe19209ffbee95", "f606c5c942bf2cbe5ec18e7e36fc48fce1dd24a9", "1beae9ef432a57bb5ec0c43944a07182814ab443", "2edc8083073837564306943aab77d6dcc19d0cdc", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8"], "page_rank": 5.473453749315818e-05}, {"id": "141ea38bea3ecac14fff05cfd80c4b9b30b73f6a", "title": "Progress On A Computer Based Consultant", "authors": ["Peter E. Hart"], "date": 1975, "abstract": "Computer based consultants are systems that incorporate specialized bodies of knowledge and make this knowledge conveniently available to users who are not computer experts. This paper summarizes initial progress on a computer based consultant project aimed at helping a novice mechanic work with electromechanical equipment. We describe some properties and abilities of consultants, and present results to date on the problem solving, vision, and natural language components of our evolving system.", "references": ["10f7507b8408bf35125b8e04254ad890c8d45e1d", "e5ef4e039539a80d27d9d12d26d73aae36f1dea8", "efbb7a9c0b8ee75338c61d9e4517a2f18ff00b79", "056fa6bd2461b8fd978b66fc452662dfdcfbaadf", "fd891f74ead8dce57e4c6228f58dc4c1f9577fd5", "e1c64018bfdae71fa93368900403ebb66025ff76", "e2f4c842943725fc005c09a8a51b517d6ec86b27", "074ae08c6c668ecadea2d9bc90b0165f4bf01c65", "6531efdf8429cebe936b7deddf7bfe529871231b", "bc587390dcf5198671fb0fa9304f52dea5afdacb"], "page_rank": 0.00012510751427007583}, {"id": "84a5bb2e2c87b195ec277e770b066dba72285404", "title": "An Information-processing Model of Intermediate-level Cognition", "authors": ["Joseph D. Becker"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"An Information-processing Model of Intermediate-level Cognition\" by Joseph D. Becker", "references": [], "page_rank": 0.00013683634373289543}, {"id": "ffe728c10a5cf8152d95e1e115eee3bf1ab32a1a", "title": "Mixed-initiative man-computer instructional dialogues.", "authors": ["Jaime R. Carbonell"], "date": 1970, "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b09450ffbd976bb83728e049956456902a1dce44", "title": "Analysis and Synthesis of Tutorial Dialogues", "authors": ["Allan Collins", "Eleanor H. Warnock", "Joseph J. Passafiume"], "date": 1975, "abstract": "Publisher Summary This chapter analyzes the strategies through which tutors adapt their teaching to individual students, so that these strategies could be synthesized in a computer system called SCHOLAR. To find out the strategies used by tutors, dialogues are tape-recorded between various tutors and students on the topic of South American geography. As SCHOLAR is a well-defined program, it was possible to analyze such ill-defined naturalistic data in terms of the structure and processing of information in SCHOLAR. The dialogues are analyzed concentrating on one aspect at a time and based on the analysis, several hypotheses on how the tutor relates his teaching to the individual student are proposed. A student-computer dialogue is presented, which illustrates how some of these strategies are implemented in a modified form in SCHOLAR. The analytical method presented here could be extended to a wide range of conversational situations and this method (Dialogue Analysis) would permit psychologists to study questions related to interactive aspects of human language processing that cannot even be considered with traditional laboratory methods.", "references": ["f9441005143eac86fd045b194274cf8ea6b8169d", "ea5dab63bfee579a6f84acd001132bfb6be98707", "6302b4a1bd6c7b99c0cd74836b097e305962f99c", "5c3db31cc95ef7376d41919422b372cad7853d0d"], "page_rank": 5.473453749315818e-05}, {"id": "9ddfeb8dd19afa22308524d2c0def9d5412cb078", "title": "Computer-based consultation: mycin", "authors": ["E. H. Shortcliffe"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Computer-based consultation: mycin\" by E. H. Shortcliffe", "references": [], "page_rank": 0.0004926108374384236}, {"id": "852953abab9b04470007ff279ae478f224108c82", "title": "The lunar sciences natural language information system: final report", "authors": ["William A. Woods", "Ronald M. Kaplan", "B. L. Nash-webber"], "date": 1972, "abstract": "A portable instrument of the pocket type designed primarily for monitoring atmospheric contamination in uranium mines by selectively detecting the alpha -particles emitted simultaneously by the daughter products of radon, namely radium A and radium C'.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a7eb50210a468d0878666e8f82fb55f2b179f802", "title": "Learning Structural Descriptions From Examples", "authors": ["Patrick Henry Winston"], "date": 1970, "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.", "references": [], "page_rank": 0.0001710454296661193}, {"id": "f9441005143eac86fd045b194274cf8ea6b8169d", "title": "AI in CAI : An artificial intelligence approach to computer-assisted instruction", "authors": ["Jaime R. Carbonell"], "date": 1970, "abstract": "The main purpose of the research reported here is to show that a new and more powerful type of computer-assisted instruction (CAI), based on extensive application of artificial-intelligence (AI) techniques, is feasible, and to demonstrate some of its major capabilities. A set of computer programs was written and given the name SCHOLAR. Due to its complexity, only the conception and educational aspects of this system (including an actual on-line protocol) are presented in this paper. In what may be called conventional ad hoc-frame-oriented (AFO) CAI, the data base consists of many \"frames\" of specific pieces of text, questions, and anticipated answers entered in advance by the teacher. By contrast, an information-structure-oriented (ISO) CAI system is based on the utilization of an information network of facts, concepts, and procedures; it can generate text, questions, and corresponding answers. Because an ISO CAI system can also utilize its information network to answer questions formulated by the student, a mixed-initiative dialogue between student and computer is possible with questions and answers from both sides.", "references": [], "page_rank": 0.0001778872468527641}, {"id": "8aa47ce71755fe70b75e85923f67461a4067e29a", "title": "TOWARD AN UNDERSTANDING OF INFORMATION PROCESSES OF SCIENTIFIC INFERENCE IN THE CONTEXT OF ORGANIC CHEMISTRY", "authors": ["Bruce G. Buchanan", "Georgia L. Sutherland", "Edward A. Feigenbaum"], "date": 1969, "abstract": "Abstract : The program called Heuristic DENDRAL solves scientific induction problems of the following type: given the mass spectrum of an organic molecule, what is the most plausible hypothesis of organic structure that will serve to explain the given empirical data. Its problem solving power derives in large measure from the vast amount of chemical knowledge employed in controlling search and making evaluations. A brief description of the task environment and the program is given in Part 1. Recent improvements in the design of the program and the quality of its performance in the chemical task environment are noted. The acquisition of task-specific knowledge from chemist-'experts', the representation of this knowledge in a form best suited to facilitate the problem solving, and the most effective deployment of this body of knowledge in restricting search and making selections were major foci of the research. Part II discusses the techniques used and problems encountered in eliciting mass spectral theory from a cooperative chemist. A sample 'scenario' of a session of the representation of the chemical knowledge and the design of processes that utilize it effectively. The initial, rather straightforward, implementations were found to have serious defects. These are discussed. Part IV is concerned with our presently-conceived solutions to some of these problems, particularly the rigidity of processes and knowledge-structures. (Author)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "c67eaaba964877df1964fb506ad0ae375dd92b27", "title": "Mechanization of Inductive Inference in Organic Chemistry", "authors": ["Joshua Lederberg", "Edward A. Feigenbaum"], "date": 1967, "abstract": "Abstract : Topics include: HEURISTIC DENDRAL--A summary from the standpoint of artificial intelligence research; Organic chemistry--the problem context of DENDRAL; Isomers; Reference to data; Performance; Building DENDRAL; Pattern recognition; Programming as induction.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "3fdec3403ac6147ebc055dd31bf7375c73c438d8", "title": "SIR: A COMPUTER PROGRAM FOR SEMANTIC INFORMATION RETRIEVAL", "authors": ["Bertram Raphael"], "date": 1964, "abstract": "SIR is a computer system, programmed in the LISP language, which accepts information and answers questions expressed in a restricted form of English. This system demonstrates what can reasonably be called an ability to \"understand\" semantic information. SIR''s semantic and deductive ability is based on the construction of an internal model, which uses word associations and property lists, for the relational information normally conveyed in conversational statements. A format-matching procedure extracts semantic content from English sentences. If an input sentence is declarative, the system adds appropriate information to the model. If an input sentence is a question, the system searches the model until it either finds the answer or determines why it cannot find the answer. In all cases SIR reports its conclusions. The system has some capacity to recognize exceptions to general rules, resolve certain semantic ambiguities, and modify its model structure in order to save computer memory space. Judging from its conversational ability, SIR is more \"intelligent\" than any existing question-answering system. The author describes how this ability was developed and how the basic features of SIR compare with those of other systems. The working system, SIR , is a first step toward intelligent machine communication. The author proposes a next step by describing how to construct a more general system which is less complex and yet more powerful than SIR . This proposed system contains a generalized version of the SIR model, a formal logical system called SIR1 , and a computer program for testing the truth of SIR1 statements with respect to the generalized model by using partial proof procedures in the predicate calculus. The thesis also describes the formal properties of SIR1 and how they relate to the logical structure of SIR .", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "219949a0d58d27079e0063aca9f903f29b3a058c", "title": "Some Poetic and Social Criteria for Education Design", "authors": ["Seymour Papert"], "date": 1976, "abstract": "A single volume control for a plurality of audio channels is formed by a pair of gain controllable differential amplifiers operated as small-signal amplifiers in an open-ended configuration. The current source to each differential amplifier forms the signal path between the input and output of the volume control circuit. The transistor forming the current path, and signal path, is a class A amplifier. By altering the DC bias on one of the current control devices in each of the differential amplifiers, the amount of signal coupled to the output thereof is correspondingly varied. A single potentiometer provides a DC voltage control for the variable impedance to the selected transistor of each of the differential amplifiers which, in turn, functions as a volume control for all channels.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "10f7507b8408bf35125b8e04254ad890c8d45e1d", "title": "Procedures As A Representation For Data In A Computer Program For Understanding Natural Language", "authors": ["Terry Winograd"], "date": 1971, "abstract": "Abstract : The paper describes a system for the computer understanding of English. The system answers questions, executes commands, and accepts information in normal English dialog. It uses semantic information and context to understand discourse and to disambiguate sentences. It combines a complete syntactic analysis of each sentence with a 'heuristic understander' which uses different kinds of information about a sentence, other parts of the discourse, and general information about the world in deciding what the sentence means.", "references": [], "page_rank": 0.0002975213073735241}, {"id": "34fe9525a886d0bf7c97d180c0e6e7d75d0abb8e", "title": "Semantics For a Question-Answering System", "authors": ["William A. Woods"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Semantics For a Question-Answering System\" by William A. Woods", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "title": "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving", "authors": ["Richard Fikes", "Nils J. Nilsson"], "date": 1971, "abstract": "We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model into a model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbi trary collection of first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formulas. It employs a resolution theorem prover to answer (juestions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model.", "references": ["1beae9ef432a57bb5ec0c43944a07182814ab443", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "4efdcacca99408c434f34c44ecd29198757a0ded", "b49bb7ecd2afd6461c78ff29536839b5ee45cd15", "c25dc4608a45d29bc9498a019ca597093af213f0", "6758ec88d7d0b2056a45b07d752a861badedb93a", "eb3a66f92106e467d1aabeb24c42b0e421b3c02f"], "page_rank": 0.0003721948549534756}, {"id": "8dbc9bb59f02712cbc9d00803e0f0e839514d368", "title": "Nouns and noun phrases", "authors": ["Emmon W. Bach"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"Nouns and noun phrases\" by Emmon W. Bach", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "d53cfca137c38f82361878a67ea47002e9940e34", "title": "THE CASE FOR CASE.", "authors": ["Charles J. Fillmore"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"THE CASE FOR CASE.\" by Charles J. Fillmore", "references": [], "page_rank": 0.0002545155993431855}, {"id": "a9aa1e25d1094109b44eb151ae2b3dd8faab09a4", "title": "A Study in Descriptive Representation of Pictorial Data", "authors": ["Oscar Firschein", "Martin A. Fischler"], "date": 1971, "abstract": "Although much effort has been expended on automatic scene description, especially in the various robot and \"hand-eye\" projects, these efforts have usually been directed toward description for immediate use, i.e., description of a scene for the purpose of stacking blocks using a manipulator arm, or for allowing a robot to move through an environment. This paper concerns a somewhat different type of description in which a scene is described in general terms to be stored for an unspecified future use. This type of description has application in (1) advanced robot systems, where the robot, similar to the human, will build up an \"encyclopedia\" of descriptions for possible use, and (2) in question-answering systems for image data bases in which the descriptions are to represent an \"encyclopedic\" knowledge of the image. Experiments in non-goal-directed description using human subjects are described, experiments which seek to determine how general descriptions are generated and the nature of such descriptions.", "references": ["4ef9110450d374ef5a17be87e3789f1f58e48486", "47f2261792cef069622607ba74bf7e3178651667", "10f7507b8408bf35125b8e04254ad890c8d45e1d", "045ee13af3477e4a50f7880ae5d0c169458b7a40", "bbb9104d6d438c8560999e966a216d829d460807", "a7eb50210a468d0878666e8f82fb55f2b179f802", "59adfa3bbeb5fc1dec86f82ac1aa4f37e14e0f05", "6fd4ea089e7584fb117c832ecb3d087911ce8247"], "page_rank": 5.473453749315818e-05}, {"id": "b63bc9c97c21ca2982741bfeaea18664c88d2d44", "title": "On the problems of time retrieval of temporal relations causality, and coexistence", "authors": ["Nicholas V. Findler", "David Chen"], "date": 2004, "abstract": "Intelligent question-answering programs do more than retrieve \u201craw\u201d data; they make deductive inferences in order to return all valid responses. They report logical inconsistencies, possibly at the data input phase. Similarly, more information is requested from the user if a question asked proves to be ambiguous. A question-answering system of the above type has been designed and implemented. Besides retrieving explicit and implicit temporal relations, the system discovers potentially causal relationships which also satisfy different time restrictions. Questions concerning a generalized concept of coexistence can also be answered. It is hoped that programs of a similar nature will become of much pragmatic use to researchers in physics, chemistry, biology, and so on, in evaluating complex, interrelated experimental data. Several additional applications for this type of program are mentioned, ranging from problems in criminology to air traffic control. The Associative Memory, Parallel Processing Language, AMPPL-II, was found rather satisfactory for the project. It is finally suggested that the system being described could serve as a component in a complex cognitive mechanism.", "references": ["bce7e8da71c17f9d653997756dbf233945c8a0e2", "2d504759b4e1571075c85813d03304f0faf8e2fe", "4cd8ac993c1f464ce9a4c8d3e4467ecd03b84f00", "ff84a05f27d33474b8e57e8ed572c3e114ea6960", "a9d0e8420a88f14bcb8b73d879e6da02eb1fa17e", "652e501264bf7c9f478dd8192af8ee468f2605c6"], "page_rank": 5.473453749315818e-05}, {"id": "e86f41c73b3ee96b8d75614c0537edeb55719211", "title": "MARGIE: Memory Analysis Response Generation, and Inference on English", "authors": ["Roger C. Schank", "Neil M. Goldman", "Charles J. Rieger", "Christopher Riesbeck"], "date": 1973, "abstract": "A program is described that accepts natural language input and makes inferences from it and paraphrases of it. The Conceptual Dependency framework is the basis of this system.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "b26295924e3ee077d050bd775f8ea165f92336b7", "title": "Planning in a hierarchy of abstraction spaces", "authors": ["Earl D. Sacerdott"], "date": 1973, "abstract": "A problem domain can be represented as a hierarchy of abstraction spaces in which successively finer levels of detail are introduced. The problem sotver ABSTRIPS, a modification of STRIPS, can define an abstraction space hierarchy from the STRIPS representatien of a problem domain, and it can utilize the hierarchy in solving problems. Examples of the system's performance are presented that demonstrate the significant increases in problem-solving power that this approach provides. Then some further implications of the hierarchical planning approach are explored.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "ad06d0a4b7bff0e86a6b38cfad4a82c023401901", "title": "A Model for Temporal References and Its Application in a Question Answering Program", "authors": ["Bertram C. Bruce"], "date": 1972, "abstract": "A formal model is presented which represents the structure underlying temporal references in natural language. Serving as a framework for analysis of tenses, time relation, and other references to time in language, the model consists of a partially ordered set, called \"time\" and successively defined concepts such as \"time-segment\", \"duration\", \"time-segment relation\", \"tense\", \"reference time\", and \"tense marker\". This model is justified by making informal arguments, by giving English language examples, and by showing that it is a generalization of several other systems. A computer question answering program, \"Chronos\", which uses the concepts of tense and time-segment relations, illustrates and supports the validity of the model. Chronos is shown to be a simple program which nevertheless can understand most of the temporal meaning in a sentence.", "references": ["b2cf888ef32cdeccee3e47d384ca71b45f465168", "bbbfd6feef1282e56e03d8b43a4e7791d1cadc81"], "page_rank": 5.473453749315818e-05}, {"id": "fc1a374bfb270cc0efab347cba4cb993564d54de", "title": "Theory of problem solving - an approach to artificial intelligence", "authors": ["Ranan B. Banerji"], "date": 1969, "abstract": "No wonder you activities are, reading will be always needed. It is not only to fulfil the duties that you need to finish in deadline time. Reading will encourage your mind and thoughts. Of course, reading will greatly develop your experiences about everything. Reading theory of problem solving an approach to artificial intelligence is also a way as one of the collective books that gives many advantages. The advantages are not only for you, but for the other peoples with those meaningful benefits.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7a23ac215fdfb1c99c2811379e4b772b67f14441", "title": "Explorations of the cognitive processes in design", "authors": ["Charles M. Eastman"], "date": 1968, "abstract": "Intuitive design, as carried out by architects, product designers, and some engineers, is analyzed as a problem solving task within the framework of an information processing theory of cognition. A study of intuitive design processes was carried out utilizing four protocols as experimental data. Two of the protocols are presented in this report in their entirety. From the protocols were identified the information used, the transformations carried out on the information, and some of the administrative processes directing particular sequences of activities. Analysis of the protocols led to an operational model of design and hand simulations largely replicating one protocol. Fundamental issues of design methodology are outlined. Of particular interest is the insight offered into semantic memory retrieval processes utilized by designers and the representational languages used in complex problem solving.", "references": ["76c86f58da0fc1270eb9655cc0b08468e4120cdd", "fc06e32be22ec9fa295986ed41e63b9b5e710fba", "29f0de0502f890281f3a25b0f4392565eabe3dd6", "6d808f8e2d206b5920df7a960374c5a667c2c726", "34ae68a5fc0ff46f49e575eb4c1432dac87b914e", "16c762445f11fa2020994918dc4f93e76264df17", "5ff92958deca0e7990a3358852cdae3638c25f22", "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69"], "page_rank": 4.926108374384236e-05}, {"id": "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "title": "Experiments with a Heuristic Compiler", "authors": ["Herbert A. Simon"], "date": 1963, "abstract": "This report describes some experiments in constructing a compiler that makes use of heuristic problem~solving techniques such as those incorporated in the General Problem Solver (GPS) [1]. The experiments were aimed at the dual objectives of throwing light on some of the problems of constructing more powerful programming languages and compilers, and of testing whether the task of writing a computer program can be regarded as a \"problem\" in the sense in which that term is used in GPS. The present paper is concerned primarily with the second objective--with analyzing some of the problem-solving processes that are involved in writing computer programs. At the present stage of their development, no claims will be made for the heuristic programming procedures described here as practical approaches to the construction of compilers. Their interest lies in what they teach us about the nature of the programming task.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "c25dc4608a45d29bc9498a019ca597093af213f0", "title": "GPS: A Case Study in Generality and Problem Solving.", "authors": ["George W. Ernst", "Allen Newell"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"GPS: A Case Study in Generality and Problem Solving.\" by George W. Ernst et al.", "references": [], "page_rank": 0.00023399014778325121}, {"id": "b67f0b4d19d772d1ebcad4f9169cecbf24b0c4a6", "title": "Methods for the computer-implemented solution of a class of 'floor plan' design problems", "authors": ["John Grason"], "date": 1970, "abstract": "Abstract : The work presented is intended as a case study in computer-implemented design. Its purpose is to illustrate the relationship between the representation chosen for a design problem and the methods developed for solving that problem. A formal class of 'floor plan'-type design problems is defined. In these problems a set of rectangular rooms is specified, and an allowable list of dimensions is given for each room. In addition, a set of required adjacencies between rooms, or between a room and an outside wall of the building, is given. The problem is to produce a rectangular floor plan of a building that contains all of the specified rooms, and that satisfied all of the adjacency and dimension requirements. A linear graph representation for floor plans is developed. This graph is the dual graph of the floor plan, itself treated as a linear graph. Thus, the nodes of the dual graph correspond to rooms, and the edges correspond to adjacencies between rooms. The design methods are implemented in a computer program, GRAMPA, written in IPL-V. Several illustrative problems solved by GRAMPA are discussed. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6d808f8e2d206b5920df7a960374c5a667c2c726", "title": "Notes on the Synthesis of Form", "authors": ["Christopher Alexander"], "date": 1964, "abstract": "Every design problem begins with an effort to achieve fitness between two entities: the form in question and its context. The form is the solution to the problem; the context defines the problem. We want to put the context and the form into effortless contact or frictionless coexistence, i.e., we want to find a good fit. For a good fit to occur in practice, one vital condition must be satisfied. It must have time to happen. In slow-changing, traditional, unselfconscious cultures, a form is adjusted soon after each slight misfit occurs. If there was good fit at some stage in the past, no matter how removed, it will have persisted, because there is an active stability at work. Tradition and taboo dampen and control the rate of change in an unselfconscious culture's designs. It is important to understand that the individual person in an unselfconscious culture needs no creative strength. He does not need to be able to improve the form, only to make some sort of change when he notices a failure. The changes may not always be for the better; but it is not necessary that they should be, since the operation of the process allows only the improvements to persist. Unselfconscious design is a process of slow adaptation and error reduction. In the unselfconscious process there is no possibility of misconstruing the situation. Nobody makes a picture of the context, so the picture cannot be wrong. But the modern, selfconscious designer works entirely from a picture in his mind - a conceptualization of the forces at work and their interrelationships - and this picture is almost always wrong. To achieve in a few hours at the drawing board what once took centuries of adaptation and development, to invent a form suddenly which clearly fits its context - the extent of invention necessary is beyond the individual designer. A designer who sets out to achieve an adaptive good fit in a single leap is not unlike the child who shakes his glass-topped puzzle fretfully, expecting at one shake to arrange the bits inside correctly. The designer's attempt is hardly as random as the child's is; but the difficulties are the same. His chances of success are small because the number of factors which must fall simultaneously into place is so enormous. The process of design, even when it has become selfconscious, remains a process of error-reduction. No complex system will succeed in adapting in a reasonable amount of time or effort unless the adaptation can proceed component by component, each component relatively independent of the others. The search for the right components, and the right way to build the form up from these components, is the greatest challenge faced by the modern, selfconscious designer. The culmination of the modern designer's task is to make every unit of design both a component and a system. As a component it will fit into the hierarchy of larger components that are above it; as a system it will specify the hierarchy of smaller components of which it itself is made.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "512cd16c84f83fae6129bfc22b7a500172846860", "title": "D-Script: A Computational Theory of Descriptions", "authors": ["Robert C. Moore"], "date": 1973, "abstract": "This paper describes D-Script, a language for representing knowledge in artificial intelligence (AI) programs. D-Script contains a powerful formalism for descriptions, which permits the representation of statements that are problematical for other systems. Particular attention is paid to problems of opaque contexts, time contexts, and knowledge about knowledge. The design of a deductive system for this language is also considered.", "references": ["10f7507b8408bf35125b8e04254ad890c8d45e1d", "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "e97795382386ecd24300f3a6449ed5732b200bfa", "19306a3d8bc9c7732fb6d25f605ace765198bdd2", "4a2986f8a3b4a385ef410bfac509ace84401e961", "62d0535077cdc622f6e68b9f49f59b97dd482a11", "8e64b0378e22f1df9d73630e9ad62c57ea25c5c3"], "page_rank": 5.473453749315818e-05}, {"id": "e3b835126d0f610ffc30eaa2b86b59f8e9da434d", "title": "Language Processing Via Canonical Verbs and Semantic Models", "authors": ["Gary G. Hendrix", "C. W. Thompson", "Jonathan Slocum"], "date": 1973, "abstract": "A natural language question answering system is presented. The system's parser maps semantic paraphrases into a single deep structure characterized by a canonical verb. A modeling scheme using semantic nets and STRIPS-like operators assimilates the sequence of input information. Natural language responses to questions are generated from a data base of semantic nets by \"parsing\" syntactic rules retrieved from the lexicon.", "references": ["09550accec47459a61fe1710a0a32c2ec22449bd", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "c82330fd123cc50c6dd0c5add0786d253727f383", "95f38868b1f9ec38d52db6a60be7c0efd5c8d836", "291f9735ab9c33b165d871c6f197fbb4e8b85a2d", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "ecf1ad12596f7319397f866d58878e3e87849687"], "page_rank": 5.473453749315818e-05}, {"id": "0fae95253749e05975fc34036ae71f36ae2efc70", "title": "The fourteen primitive actions and their inferences.", "authors": ["Roger C. Schank"], "date": 1973, "abstract": "In order to represent the conceptual information underlying a natural language sentence, a conceptual structure has been established that uses the basic actor-action-object framework. It was the intent that these structures have only one representation for one meaning, regardless of the semantic form of the sentence being represented. Actions were reduced to their basic parts so as to effect this. It was found that only fourteen basic actions were needed as building blocks by which all verbs can be represented. Each of these actions has a set of actions or states which can be inferred when they are present.", "references": [], "page_rank": 0.00013683634373289543}, {"id": "fa5b610407ac9296692fcf46b6fe19209ffbee95", "title": "Automatic generation of programs containing conditional statements", "authors": ["David C. Luckham", "Jack Buchanan"], "date": 1974, "abstract": "An experimental system for automatically generating certain simple kinds of programs is described. The programs constructed are expressed in a subset of ALGOL containing assignments, function calls, conditional statements, while loops, and non-recursive procedure calls. The system has been used to generate programs for symbolic manipulation, robot control, every day planning, and computing arithmetical functions. This system has previously been described in [Buchanan and Luckham 1974]. The present report focuses on the generation of conditional statements and describes applications to mechanical assembly and symbolic manipulation problems.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "94d9cbfc474cb6ce961de45a06233b2853ca6724", "title": "Toward automatic program synthesis", "authors": ["Zohar Manna", "Richard J. Waldinger"], "date": 1971, "abstract": "An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees.\nIn order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency.\nIt is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f606c5c942bf2cbe5ec18e7e36fc48fce1dd24a9", "title": "Some transformations for developing recursive programs", "authors": ["Rod M. Burstall", "John Darlington"], "date": 1975, "abstract": "The paper describes a system of rules for transforming programs, the programs being in the form of recursion equations. The idea is to start with a very simple, lucid and hopefully correct program, then to transform it into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. We hope to throw some light on the alternative structures for programs, also to indicate a possible initial phase for an automatic or semi-automatic program manipulation system.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d1c8dd5bfcd1270af88f9fb151a377a6051ad2a3", "title": "Cognitive Processes and III-Defined Problems: A Case Study from Design", "authors": ["Charles M. Eastman"], "date": 1969, "abstract": "In this paper the information processing theory of problem solving is extended to include ill-defined problems. A protocol of problem solving in architectural design and its analysis is presented. The significant difference between well-and ill-defined problem solving is shown to be a specification process similar to information retrieval processes now studied in artificial intelligence. A variety of issues in this retrieval process are examined. The search process involved in the space planning aspect of design is shown to correspond well with existing formulations of search. The interactive effects of retrieval and search processes are examined.", "references": ["80295fef5f67be43c711a445357bec9a5a6b8053", "0366fb9db0e086800f0c8189499275a39a8f0b02"], "page_rank": 0.00010399562123700055}, {"id": "5c3db31cc95ef7376d41919422b372cad7853d0d", "title": "Computing machinery and intelligence", "authors": ["Edward A. Feigenbaum", "Jacob Feldman"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Computing machinery and intelligence\" by Edward A. Feigenbaum et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "0f3dfcd06fa92c4dfe895c8d4b4151bf162c20c0", "title": "On automating the construction of programs.", "authors": ["Jack Buchanan", "David C. Luckham"], "date": 1974, "abstract": "An experimental system for automatically generating certain simple kinds of programs is described. The programs constructed are expressed in a subset of ALGOL containing assignments, function calls, conditional statements, while loops, and non-recursive procedure calls. The input is an environment of primitive programs and programming methods specified in a lnaugage currently used to define the semantics of the output programming language. The system has been used to generate programs for symbolic manipulation, robot control, every day planning, and computing arithmetical functions.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "6531efdf8429cebe936b7deddf7bfe529871231b", "title": "The SRI speech understanding system", "authors": ["Donald E. Walker"], "date": 1975, "abstract": "This paper describes the structure of the second version of the Stanford Research Institute (SRI) speech understanding system and presents the data gathered on its performance. The system is distinctive in the way that knowledge from various sources is coordinated by a \"best-first\" parser to predict the sequence of words in an utterance, and in the use of word functions-programs that represent the acoustic characteristics of a word-to test the predictions.", "references": ["02162c1df948a0618a78378e753d283a710aab11"], "page_rank": 4.926108374384236e-05}, {"id": "6302b4a1bd6c7b99c0cd74836b097e305962f99c", "title": "Perfect recall and total forgetting: A problem for models of short-term memory", "authors": ["Stephen Madigan", "Linda Mccabe"], "date": 1971, "abstract": "In two paired-associate probe experiments, the items in the last input position in five-pair lists were recalled perfectly in immediate recall, but practically never in a subsequent delayed recall test. Other items in the list were recalled at intermediate levels in both tests. This \u201cnegative recency\u201d effect in delayed recall occurred regardless of whether a pair had or had not been tested in immediate recall. These results, at variance with predictions derived from some current models of memory, suggest that pairs in terminal input positions are deliberately not processed and stored by S s in a manner allowing their later recall from long-term memory.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "ea5dab63bfee579a6f84acd001132bfb6be98707", "title": "The fate of primary memory items in free recall", "authors": ["Fergus I. M. Craik"], "date": 1970, "abstract": "The question of whether items retrieved from primary memory (PM) are as well registered in memory as those retrieved from secondary memory (SM) was examined in a free-recall study. It was found that words in terminal serial positions were retrieved best in immediate recall but least well in a second recall session. The conclusion was drawn that PM items are less well learned than SM items, and the implications for models of memory were examined. Subsidiary findings were that auditory presentation was superior to visual presentation and written recall was superior to spoken recall in PM. Also, words retrieved late in immediate recall had the highest probability of retrieval on the second recall session.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "e1c64018bfdae71fa93368900403ebb66025ff76", "title": "On locating objects by their distinguishing features in multisensory images", "authors": ["Jay M. Tenenbaum"], "date": 1973, "abstract": "This paper reports preliminary work on a knowledge-based perceptual system for a robot that must function in an actual office environment. This system is distinguished by the following pragmatic considerations: (1) It is designed to find specific objects needed by the robot in the performance of a task rather than attempting the usually unnecessary and very much harder job of completely describing an environment of potentially overwhelming complexity. (2) It is based on the premise that in real scenes there is a sufficient redundancy of perceptual clues, as well as contextual constraints among objects, so that an intelligent system can devise a relatively simple strategy for distinguishing the specific objects of interest from others likely to be present. (3) It relies heavily on multisensory (i.e., color and range) data to increase the likelihood of finding distinguishing surface attributes for a particular object. Similarly, detailed descriptive representations for complex attributes (e.g., shape and texture) are avoided in favor of the simplest representations sufficient to distinguish the object of interest.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e2f4c842943725fc005c09a8a51b517d6ec86b27", "title": "Symbolic integration the stormy decade", "authors": ["Joel Moses"], "date": 1971, "abstract": "Three approaches to symbolic integration in the 1960's are described. The first, from Artificial Intelligence, led to Slagle's SAINT and to a large degree to Moses' SIN. The second, from algebraic manipulation, led to Manove's implementation and to Horowitz' and Tobey's re-examination of the Hermite algorithm for integrating rational functions. The third, from mathematics, led to Richardson's proof of the unsolvability of the problem for a class of functions and for Risch's decision procedure for the elementary functions. Generalizations of Risch's algorithm to a class of special functions and programs for solving differential equations and for finding the definite integral are also described.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "fd891f74ead8dce57e4c6228f58dc4c1f9577fd5", "title": "The Greenblatt chess program", "authors": ["Richard D. Greenblatt", "Donald E. Eastlake", "Stephen D. Crocker"], "date": 1967, "abstract": "Since mid-November 1966 a chess program has been under development at the Artificial Intelligence Laboratory of Project MAC at M.I.T. This paper describes the state of the program as of August 1967 and gives some of the details of the heuristics and algorithms employed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "efbb7a9c0b8ee75338c61d9e4517a2f18ff00b79", "title": "A best-first parser", "authors": ["William H. Paxton"], "date": 1975, "abstract": "A parser for a speech understanding system is described. The parser uses a best-first strategy in which alternative paths are assigned priorities and paths are suspended as long as there is a higher priority alternative to explore. Discussions are included on the types of steps in a parse, the assignment of priorities, cooperation among competing parses, and experimental results.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "title": "QA4: A Procedural Calculus for Intuitive Reasoning.", "authors": ["Johns F. Rulifson", "Jan A. Derksen", "Richard J. Waldinger"], "date": 1972, "abstract": "Abstract : This report presents a language, called QA4, designed to facilitate the construction of problem-solving systems used for robot planning, theorem proving, and automatic program synthesis and verification. QA4 integrates an omega-order logic language with canonical composition, associative retrieval, and pattern matching of expressions; process structure programming; goal-directed searching; and demons. Thus it provides many useful programming aids. More importantly, however, it provides a semantic framework for common sense reasoning about these problem domains. The interpreter for the language is extraordinarily general, and is therefore an adaptable tool for developing the specialized techniques of intuitive, symbolic reasoning used by the intelligent systems.", "references": [], "page_rank": 0.0002797325826882477}, {"id": "2b18c69b97978e6ae2ee2e43b63d581edadcc2dd", "title": "Achieving several goals simultaneously", "authors": ["Richard J. Waldinger"], "date": 1977, "abstract": "In the synthesis of a plan or computer program, the problem of achieving several goals simultaneously presents special difficulties, since a plan to achieve one goal may interfere with attaining the others. This paper develops the following strategy: to achieve two goals simultaneously, develop a plan to achieve one of them and then modify that plan to achieve the second as well. A systematic program modification technique is presented to support this strategy. The technique requires the introduction of a special \u201cskeleton model\u201d to represent a changing world that can accommodate modifications in the plan. This skeleton model also provides a novel approach to the \u201cframe problem.\u201d The strategy is illustrated by its application to three examples. Two examples involve synthesizing the following programs: interchanging the values of two variables and sorting three variables. The third entails formulating tricky blocks-world plans. The strategy has been implemented in a simple QLISP program. It is argued that skeleton modelling is valuable as a planning technique apart from its use in plan modification, particularly because it facilitates the representation of \u201cinfluential actions\u201d whose effects may be far reaching. The second part of the paper is a critical survey of contemporary planning literature, which compares our approach with other techniques for facing the same problems. The following is the outline of contents.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "5f52d5b5ac93122a98b6b01ee47066678061b48c", "title": "Proving the Impossible is Impossible is Possible: Disproofs Based on Hereditary Partitions", "authors": ["Laurent Sikl{\\'o}ssy", "John W. Roach"], "date": 1973, "abstract": "A novel technique, called hereditary partitions, is Introduced. It permits the rigorous proof that, in a given axiomatization, certain states can never be reached. The technique is implemented in a computer program, DISPROVER, and is applied to robot worlds. DISPROVER cooperates with a path-finding program when the latter encounters difficulties.", "references": ["fbc1c06b9aeb1679891486df9e85f5065a8dd293"], "page_rank": 6.157635467980295e-05}, {"id": "e5ef4e039539a80d27d9d12d26d73aae36f1dea8", "title": "SOPHIE: A Sophisticated Instructional Environment.", "authors": ["John Seely Brown", "Richard R. Burton", "Alan G. Bell", "Robert J. Bobrow"], "date": 1974, "abstract": "Abstract : The SOPHIE program, which implements mixed initiative CAI within a simulated electronics troubleshooting training laboratory interaction, has been extended in several manners. The language processor now accepts ellipses and other nonspecific requests and resolves these from dialogue context. A help requesting facility has been provided which will suggest possible faults (based on the student's knowledge about the circuit at the time of request) which could explain the symptoms he has observed. The net effect of modifications is that a dialogue is much more like a conversation with a very skilled tutor who can infer what a student means, based on a complete interaction session, and respond appropriately. The resulting program can be accessed through the ARPA network of computers.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1289b149861d77551ad81c83cea55ce8555e5695", "title": "Notes on Existence and Necessity", "authors": ["Willard van Orman Quine"], "date": 1943, "abstract": "Semantic Scholar extracted view of \"Notes on Existence and Necessity\" by Willard van Orman Quine", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "652e501264bf7c9f478dd8192af8ee468f2605c6", "title": "The Logic of Change, Action, and Norms", "authors": ["H{\\'e}ctor-Neri Casta{\\~n}eda"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"The Logic of Change, Action, and Norms\" by H\u00e9ctor-Neri Casta\u00f1eda", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "eb3a66f92106e467d1aabeb24c42b0e421b3c02f", "title": "Towards automatic program synthesis", "authors": ["Zohar Manna", "Richard J. Waldinger"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Towards automatic program synthesis\" by Zohar Manna et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "c5340982746f1aac55c1cc7d2c06b670f522f253", "title": "Predicate Logic as Programming Language", "authors": ["Robert A. Kowalski"], "date": 1974, "abstract": "A stroller having side frame sections which are adapted to fold longitudinally and also laterally toward and away from one another, with a flexible seat and a flexible seat back extending therebetween, and with preferably transparent side panels extending vertically from the seat back to the side sections of the frame, to retain a child between the side panels both in a downwardly retracted generally horizontal position of the seat back and in upper variously inclined positions thereof.", "references": [], "page_rank": 0.00014367816091954023}, {"id": "ff84a05f27d33474b8e57e8ed572c3e114ea6960", "title": "The Logic of Action: A Sketch", "authors": ["G. H. Vonwright"], "date": 1963, "abstract": "A pneumatic alarm system in which a single source of gas under pressure is connected to one side of a balanced regulator valve while a plurality of rupturable sensors are connected by a manifold to the other side of the balanced regulator valve which is maintained in an inactive state when the manifold pressure and the source pressure bear a predetermined relationship to each other. When the relationship terminates by the rupture of one or more of the sensors, the valve activates and connects the pressurized single gas source to an audible warning device to cause an alarm to be sounded.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a9d0e8420a88f14bcb8b73d879e6da02eb1fa17e", "title": "Project Management with CPM and PERT", "authors": ["Joseph J. Moder", "Cecil Phillips"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Project Management with CPM and PERT\" by Joseph J. Moder et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "bce7e8da71c17f9d653997756dbf233945c8a0e2", "title": "On a new tool in artificial intelligence research: an associate memory, parallel processing language, AMPPL-II", "authors": ["Nicholas V. Findler", "Wiley R. McKinzie"], "date": 1969, "abstract": "One of the remarkable things about human information processing is the way we store and retrieve information. The human associative memory is a device that has been only rather crudely approximated with computers inflexibility, self-modifiability, and complexity of associations. \n \nIn this paper, we descria computer language, AMPPL-II, which stimulates a machine with an extensive, variables size associative memory and parallel processing capability. It was to be made fairly machine-independent and is, therfore, embedded in a high-level algerabic language. The version outlined here is built as an extension of SLIP (Symmetric List Processing Language), itself being embedded in FORTRAN IV. \n \nThe expected range of applicability of AMPPL-II covers a rather wide area. Candides in the non-numerical field are game playing, picture processing and image identification, encoding-decoding, simulation of cognitive behavior, multikey information retrieval, language analysis, sorting, merging, collating, query systems, and so on. Numerical applications would be matrix calculations, solution of partial differential equations, computation of auto- and cross-correlation functions, signal correction, etc. \n \nStimulated by this language, new computational techniques and algorithms may, hopefully, be developed. These, on one hand, and technological advances, on the other, may render the hardware implementaion of an AMPPL-like machine economical.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b2cf888ef32cdeccee3e47d384ca71b45f465168", "title": "Specification and English tenses", "authors": ["David Crystal"], "date": 1966, "abstract": "The adverbials,1 as far as I am aware, have never been systematically considered in relation to English verbal description and definition, most traditional text-books resolving the relationship in terms of a vaguely-defined notional 'modification'. Scholars have of course noted the existence of regular formal co-occurrences between temporal adverbials and tense-forms, but this has been only for the most obvious cases, and there has been no general, empirical study of all the mutual restrictions and formally definable correlations in English. For example, Jespersen (1933) introduces adverbials into some sections of his description of English tenses (e.g. 23.43) but omits them from others (e.g. 23.41 or 23.54) where one could suggest a comparable relevance, and in many sections (e.g. 23.63-23.67) gives only a partial picture of the total number of possible relationships. Again, Ward (1954:44) notes the frequency of adverbials with 'past ordinary' tenses, but gives only a few examples, and then lists 'exceptions' with no apparent order. Adverbials with other tense-forms are given little mention, and are in any case given a different orientation, which makes it difficult to compare the sets of information. This approach seems typical of that found in most teaching handbooks on the subject: one could instance Zandvoort (1957: 58-63), who gives adverbials only incidental mention in his description of present, past and perfect tenses, despite the fact that the majority of his examples involve their obligatory use. However, Ota (1963) has studied in more detail correlations between verb", "references": ["f249468651e0caa9853ca38852bb3d4f3b29f1c9", "7e74718cbe94fddcd17103c4b07b1f3babcdeeac", "c799a24ae9864be829a9f79498daf5c8b7220f62", "8bb236399f4d54d0916390c4b90fc8836b92d9cc", "96135ea3bb8eb7616f88f5f5617e03f2ddc9f4c0", "0f6fe9550a16209372667adb788e92862337d1d8", "bbbfd6feef1282e56e03d8b43a4e7791d1cadc81"], "page_rank": 0.0002463054187192118}, {"id": "6fd4ea089e7584fb117c832ecb3d087911ce8247", "title": "LINGUISTIC METHODS IN PICTURE PROCESSING - A SURVEY *", "authors": ["W. F. hliller", "A. C. Slmw"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"LINGUISTIC METHODS IN PICTURE PROCESSING - A SURVEY *\" by W. F. hliller et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "bbbfd6feef1282e56e03d8b43a4e7791d1cadc81", "title": "The Chronological System of the English Verb", "authors": ["William Diver"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"The Chronological System of the English Verb\" by William Diver", "references": [], "page_rank": 0.0003166783954961295}, {"id": "34ae68a5fc0ff46f49e575eb4c1432dac87b914e", "title": "The morphological approach to engineering design", "authors": ["Kr Norris"], "date": 1963, "abstract": "Apparatus for fetching instructions to an instruction register of a central processing unit, including instruction buffers for storing instructions prior to their execution in the CPU (look-ahead) and apparatus for storing instructions which have been executed in the CPU (look-behind) in anticipation of their further use in, for example, programming loops. The look-behind apparatus comprises a multi-word buffer with its associated data register. The buffer data register, in addition to its function as part of the look-behind apparatus, also provides an additional level of look-ahead.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69", "title": "Chess-Playing Programs and the Problem of Complexity", "authors": ["Allen Newell", "J. C. Shaw", "Herbert A. Simon"], "date": 1958, "abstract": "This paper traces the development of digital computer programs that play chess. The work of Shannon, Turing, the Los Alamos group, Bernstein, and the authors is treated in turn. The efforts to program chess provide an indication of current progress in understanding and constructing complex and intelligent mechanisms.", "references": [], "page_rank": 0.00029693486590038313}, {"id": "29f0de0502f890281f3a25b0f4392565eabe3dd6", "title": "The Vancouver Experiment", "authors": ["Henry Elder"], "date": 1966, "abstract": "Experiments in architectural education, involving a course or a design year, have been described in these pages before. The Vancouver experiment, so to call it, is on a different scale; it involves a whole curriculum and a whole school, the School of Architecture of the University of British Columbia. Most teachers of architecture in North America have been aware for some years that something essentially different is going on at UBC, and many of them have had their curiosity and interest further stimulated when the director of the school, Henry Elder, has had the floor at ACSA meetings. Here he describes and discusses, for the first time in print, the aims, progress, problems and philosophy of the school.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ecf1ad12596f7319397f866d58878e3e87849687", "title": "Grope: a graph processing language and its formal definition.", "authors": ["Daniel Paul Freidman"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"Grope: a graph processing language and its formal definition.\" by Daniel Paul Freidman", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "8e64b0378e22f1df9d73630e9ad62c57ea25c5c3", "title": "Assimilation of New Information by a Natural Language Understanding System", "authors": ["D. McDermott"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Assimilation of New Information by a Natural Language Understanding System\" by D. McDermott", "references": [], "page_rank": 0.0001319493314567206}, {"id": "291f9735ab9c33b165d871c6f197fbb4e8b85a2d", "title": "Meaning and the Structure of Language", "authors": ["Ronald W. Langacker", "Wallace L. Chafe"], "date": 1970, "abstract": "THE NON-LINGUIST who has conscientiously tried to keep abreast of developments in linguistic theory may well be ready to give up. Linguistics, especially transformational grammar, has matured recently at an alarming rate, so that transformational grammarians may seem to have developed increasingly narrow interests and, moreover, to have become so embroiled in the muddy business of securing their own positions, digging themselves in on a narrow front, that whether they are involved in civil war or are continuing to extend the frontiers of linguistic knowledge is often very unclear-even to themselves. I fancy that scarcely a single transformationalist will bother to raise his head as Professor Chafe wings his way overhead firing enthusiastically but erratically in all directions. The outsider is much more likely to notice the high-flier, and he needs some help in assessing the significance of Chafe's sally-perhaps it would not be out of place to give him at the same time some reports from the transformational trenches, and to assure him that all is still well there. I shall assume that he is reasonably familiar with Chomsky's Syntactic Structures1 and the main developments in transformational grammar up to about 1965, when Chomsky published his Aspects of the Theory of Syntax.2 Not that I believe the college English teacher has any (narrow professional) reason to bother much about contemporary linguistics. On the contrary, recent developments in transformational grammar should make it perfectly clear that there is no hope whatever of making direct use of that approach to linguistics in English teaching-at any rate not along the lines of existing attempts. And Chafe's work seems even less relevant.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "95f38868b1f9ec38d52db6a60be7c0efd5c8d836", "title": "Primitive concepts underlying verbs of thought.", "authors": ["Roger C. Schank", "Neil M. Goldman", "Charles J. Rieger", "Christopher K. Riesbeck"], "date": 1972, "abstract": "In order to create conceptual structures that will uniquely and unambiguously represent the meaning of an utterance, it is necessary to establish ''primitive'' underlying actions and states into which verbs can be mapped. This paper presents analyses of the most common mental verbs in terms of such primitive actions and states. In order to represent the way people speak about their mental processes, it was necessary to add to the usual ideas of memory structure the notion of Immediate Memory. It is then argued that there are only three primitive mental ACTs.", "references": ["cb69a5bcbdc2f707617b1f5d4dfcdbb8e12f2594", "9c4c473666d1d33efb33f21ccb77241fc7197fa0", "7adb3c40ef03a458d35a3851fa66046936211cc3", "62d0535077cdc622f6e68b9f49f59b97dd482a11", "84a5bb2e2c87b195ec277e770b066dba72285404", "2932a16f87dd9bad2cc59145a8263239c6a9cfcc"], "page_rank": 9.852216748768472e-05}, {"id": "c82330fd123cc50c6dd0c5add0786d253727f383", "title": "Generating English discourse from semantic networks", "authors": ["Robert F. Simmons", "Jonathan Slocum"], "date": 1972, "abstract": "A system is described for generating English senterraces from a form of semantic ~e{s ia rrhid~ #~e nodes are ~'ord-sense mea~dngs a~d the paths are primarily deep case relations. The grammar ~sed by {he system is ia the form of a nef~ork that imposes an ordering on a set of syntactic transfnrmations {ha{ are expressed as t J S P flmctio~ts. The generation algorithm ~ses the information in the semantic net,~ork to select appropri~, ate genera{bin paths through the grammar. The system is designed for ase as a computational tool that allahs a linguist *o develop and stmly methods for ge~erating s~rfaee strings from an undeHying semantic sm~ctt~re. Initial findings ~ith regard fo form determiners such as voice, form, tense, and mood, some n~es fk~r embedding sentences, and some attention to pronominal substitution are reported. The system is programmed in I3SP 1~5 and is avMiab|e from the authors.", "references": [], "page_rank": 0.00018062397372742197}, {"id": "62d0535077cdc622f6e68b9f49f59b97dd482a11", "title": "A conceptural dependency representation for a computer-oriented semantics", "authors": ["Roger C. Schank"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"A conceptural dependency representation for a computer-oriented semantics\" by Roger C. Schank", "references": [], "page_rank": 0.00025099695050433965}, {"id": "5ff92958deca0e7990a3358852cdae3638c25f22", "title": "The nature of human intelligence.", "authors": ["Joy Paul Guilford"], "date": 1968, "abstract": "There is a lot of books, user manual, or guidebook that related to The Nature Of Human Intelligence PDF, such as : classical mechanics upadhyaya bond more third papers in non verbal reasoning 9 10 years fiesta mk4 manual a visit of charity origami insects dover origami papercraft robert j lang author powerone bdsm big magic creative living beyond fear the weaver of tomorrow and dawn strider two stories calculus finney demana waits kennedy 3rd edition mathematical models in population biology and epidemiology texts in applied mathematics", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "19306a3d8bc9c7732fb6d25f605ace765198bdd2", "title": "Formal Methods in the Design of Question-Answering Systems", "authors": ["Erik Sandewall"], "date": 1971, "abstract": "This paper contributes to the discussion whether and how predicate calculus should be used as a deep structure in question-answering programs. The first part of the paper stresses that there are several possible ways of using predicate calculus, and argues that predicate calculus has significant advantages above competing deep structures if the way of using it is carefully selected. The second half gives hints on how various natural-language constructions can be encoded in a consistent way, and how axion sets that define these encodings can be written and debugged.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "fc06e32be22ec9fa295986ed41e63b9b5e710fba", "title": "STUDIES IN PROBLEM SOLVING: SUBJECT 3 ON THE CRYPT-ARITHMETIC TASK DONALD + GERALD = ROBERT,", "authors": ["Allen Newell"], "date": 1967, "abstract": "Abstract : The behavior of a single subject on a symbolic task (crypt-arithmetic) is examined in some detail. The study is part of a continuing effort to understand the information processes involved in problem solving. Following a brief review of the advantages and difficulties in using protocols to aid in protocol analysis. These are essentially descriptive in nature, consisting in a specification of the problems space in which the subject is working, and in a display of his behavior, of the total behavior in a series of decision points, and permits an analysis of the adequacy of the productions (essentially, a system of conditional statements). A rather extensive analysis of the adequacy of the production system is included. Finally, there is some discussion of the implications of the production system, not only as a descriptive tool, but as a theoretical scheme.", "references": ["d075466245c0a58a6c2c98198ae2c6d937b0af11"], "page_rank": 0.00017104542966611932}, {"id": "02162c1df948a0618a78378e753d283a710aab11", "title": "The structure of task oriented dialogs", "authors": ["Barbara J. Grosz"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"The structure of task oriented dialogs\" by Barbara J. Grosz", "references": [], "page_rank": 0.0004926108374384236}, {"id": "76c86f58da0fc1270eb9655cc0b08468e4120cdd", "title": "On the analysis of human problem solving protocols", "authors": ["Allen Newell"], "date": 1966, "abstract": "Abstract : The last decade has seen the emergence of information processing theories of human problem solving, expressed usually as computer programs that simulate behavior. These theories have led to a resurgence of interest in protocols as a source of data, meaning the continuous verbal behavior of the subject operating under instructions to 'think aloud'. The purpose of the paper is to discuss the analysis of protocol data and to suggest one new line of attack for strengthening it. The intent is somewhat methodological, but some new material is introduced. (Author)", "references": ["fc06e32be22ec9fa295986ed41e63b9b5e710fba", "e1b5cc576f71f5c99355b316277a3a311c7ef3b8", "8fcf060606495f711e98ac4acb1eb1a7def35c9b", "fcb85373fec90723b4561bf326769548ab84662a", "54accdec243cfbf396692ca52a330f89a06c83af", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "2edc8083073837564306943aab77d6dcc19d0cdc", "14d82bfbbdfe6c002dfa728771a1dfa87db4b421", "a7314410cf36a8690c31e8aa432649b01d8a3602", "077c16c7e4312817bf037c12f40adf6330691bc6"], "page_rank": 0.00011631089217296114}, {"id": "59adfa3bbeb5fc1dec86f82ac1aa4f37e14e0f05", "title": "A grammar-controlled pattern analyzer", "authors": ["Thomas G. Evans"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"A grammar-controlled pattern analyzer\" by Thomas G. Evans", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "bbb9104d6d438c8560999e966a216d829d460807", "title": "Language as a symbolic process in communication. A psychological perspective.", "authors": ["Robert M. Krauss"], "date": 1968, "abstract": "To combat an old human prejudice in favor of eyewitness testimony ... the expert must intimate that he has access to some occult source or science not available to either reporter or reader. He is the Priest of Eleusis, the man with the big picture. . . . All is manifest to him, since his conclusions are not limited by his powers of observation. Logis tics . . . favor him, since it is possible not to see many things at the same time [lj.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "045ee13af3477e4a50f7880ae5d0c169458b7a40", "title": "Encyclopedic storage of scientific and technical knowledge", "authors": ["Harry Schwarzlander"], "date": 1970, "abstract": "Encyclopedic storage is a method of storing knowledge that avoids unnecessary duplication and provides convenient access to the stored materials. This paper discusses a particular approach to encyclopedic storage of scientific and technical information which can now be implemented. Motivation for this approach is developed here by considering first some of the basic properties of the process of knowledge transfer via the written record, leading to the development of a unit  of knowledge. In terms of this unit, the `size' of scientific and technical knowledge, and of electrical engineering knowledge is estimated. This unit also becomes the basic building block for the proposed encyclopedic system. A very small experimental version of such a system is now in existence.", "references": ["cb287e6a499ace7ee78662c765a8fa1fecf8eb91"], "page_rank": 6.157635467980295e-05}, {"id": "6758ec88d7d0b2056a45b07d752a861badedb93a", "title": "The frame problem in problem-solving systems", "authors": ["Bertram Raphael"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"The frame problem in problem-solving systems\" by Bertram Raphael", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "title": "Monitored Execution of Robot Plans Producted by STRIPS", "authors": ["Richard Fikes"], "date": 1971, "abstract": "Abstract : We describe PLANEXl, a plan executor for the Stanford Research Institute robot system. The problem-solving program STRIPS creates a plan consisting of a sequence of actions, and the PLANEXI program carries out the plan by executing the actions. PLANEXI is designed so that it executes only that portion of the plan necessary for completing the task, reexecutes any portion of the plan that has failed to achieve the desired results, and initiates replanning in situations where the plan can no longer be effective in completing the task. The scenario for an example plan execution is given.", "references": [], "page_rank": 0.0003489326765188834}, {"id": "4ef9110450d374ef5a17be87e3789f1f58e48486", "title": "Machine Perception and Description of Pictorial Data", "authors": ["Martin A. Fischler"], "date": 1969, "abstract": "This investigation of machine processing of pictorial data is based on the premise that people can recognize visual objects and describe them well enough so that other individuals can recognize the object from the description. Given a system of linguistic communication between a person and a digital computer, and given that the computer possesses adequate perceptual machinery, many currently refractory problems in pictorial data processing would be open to solution. \n \nThis paper describes a computer system which can perceive a limited class of graphical objects, create linguistic descriptions for the objects, and classify objects by comparison with a reference set of descriptions as might be produced in normal human communication.", "references": ["f0843619ac5937c2267cdacd2992aad219b32bcc", "a8bfb8986e0b7cf938b8e9235bec712256bc084e", "fda841df3b0620c9322514d4434ca1dcccddd1f8", "22fe47458970409fd796eb91b55a2c84e4272df1", "a68886c4fa0fa4500832b3a9fcb4c0e44b2301f8", "5a17fa86ef53b7d89f0aae659cd183a071076390", "f395c0b69dc0da249fd9c6a2a964ddc3bb79babd", "ef2e340331efc529e59b40c95f51618b60f06a74", "fada1648e34399c1fab2596aebaa1a9a14d868c6", "c8c9782797d277f8e0c1827e364d489d1c1de336"], "page_rank": 6.157635467980295e-05}, {"id": "4efdcacca99408c434f34c44ecd29198757a0ded", "title": "Robot Planning, Execution, and Monitoring in an Uncertain Environment", "authors": ["John H. Munson"], "date": 1971, "abstract": "An intelligent robot, operating in an external environment that cannot be fully modeled in the robot's software, must be able to monitor the success of its execution of a previously generated plan. This paper outlines a unifled i ormalism for describing and relating the various functions oi a robot operating in such an environment. After exploring the distinetion between the external world and the robot's internal model of it, and the distinction between actions that interact with the world and the robot's descriptions of those actions, we formalize the concepts of a plan and of its execution. Current developments at Stanford Research Institute, and the benchmark idea oi an ultimate rational robot, are both analyzed in this framework.", "references": ["62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "a45c86255ace7de61e92d42925eca9ca201c6368", "c25dc4608a45d29bc9498a019ca597093af213f0", "46bb9b198b3d2284a0d06b29d41c04dea0d0b243"], "page_rank": 6.157635467980295e-05}, {"id": "fbc1c06b9aeb1679891486df9e85f5065a8dd293", "title": "A Hierarchy-driven Robot Planner Which Generates Its Own Procedures", "authors": ["Laurent Sikl{\\'o}ssy", "J. Druessi"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"A Hierarchy-driven Robot Planner Which Generates Its Own Procedures\" by Laurent Sikl\u00f3ssy et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "b49bb7ecd2afd6461c78ff29536839b5ee45cd15", "title": "Problem-solving methods in artificial intelligence", "authors": ["Nils J. Nilsson"], "date": 1971, "abstract": "Feel lonely? What about reading books? Book is one of the greatest friends to accompany while in your lonely time. When you have no friends and activities somewhere and sometimes, reading book can be a great choice. This is not only for spending the time, it will increase the knowledge. Of course the b=benefits to take will relate to what kind of book that you are reading. And now, we will concern you to try reading problem solving methods in artificial intelligence as one of the reading material to finish quickly.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0d509454804e4ec625f70904e27cbd93ce9dcbb2", "title": "The list set generator: a construct for evaluating set expressions", "authors": ["Stuart C. Shapiro"], "date": 1970, "abstract": "The list set generator is defined and algorithms for its use are given. The list set generator is a construct which may be added to a list processing system or any system that handles sets. It efficiently generates the set which results from any expression involving sets and set operators. The efficiency derives from evaluating the expression as a whole and in parallel, rather than evaluating subexpressions and then using those sets to arrive at the final result.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "title": "PROW: A Step Toward Automatic Program Writing", "authors": ["Richard J. Waldinger", "Richard C. T. Lee"], "date": 1969, "abstract": "This paper Describes a program, called \"PROW\", which writes programs PROW accepts the specification of the program in the language of predicate calculus, decides the algorithm for the program and then produces a LISP program which is an implementation of the algorithm. Since the construction of the algorithm is obtained by formal theorem-proving techniques, the programs that PROW writes are free from logical errors and do not have to be debugged The user of PROW can make PROW write programs in languages other than LISP by modifying the part of PROW that translates an algorithm to a LISP program. Thus PROW can be modified to write programs in any language In the end of this paper, it is shown that PROW can also be used as a question-answering program", "references": [], "page_rank": 0.00022167487684729062}, {"id": "6d3df4792216e6fc8b240e7761eee90f96cd894d", "title": "THE MIND SYSTEM: THE MORPHOLOGICAL-ANALYSIS PROGRAM,", "authors": ["Martin Kay", "Gary R. Martins"], "date": 1970, "abstract": "Abstract : A detailed description of the ANALYZE module of the MIND information system is presented. This part of the system receives English sentences as input, segments then into words, and reduces the words to their ultimate lexical components through morphological analysis. Dictionary entries providing morphological, syntactic, and semantic information about the components are retrieved and amalgamated into word-level grammatical representations, and are then concatenated to form a bottom-level grammatical representation of the input sentence. This representation is passed to the PARSE module for further processing. Significant developments incorporated in the module include: (1) a sophisticated morphological analysis procedure, (2) a very efficient dictionary referencing organization, (3) powerful techniques for representing lexical generalizations, and (4) simple but effective means for morphological recombination of lexical components. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "80295fef5f67be43c711a445357bec9a5a6b8053", "title": "RESEARCH ON INTELLIGENT QUESTION-ANSWERING SYSTEMS", "authors": ["C. Cordell Green", "Bertram Raphael"], "date": 1967, "abstract": "Abstract : This report describes progress toward an 'intelligent question- answering system'--a system that can accept facts, retrieve items from memory, and perform logical deductions necessary to answer questions. Two versions of such a system have been implemented, and the authors expect these to be the first in an evolving series of question-answerers. The first system, QA1, is based upon relational information organized in a list-structured memory. The data consist of general facts about relations as well as specific facts about objects. QA1 has limited deductive ability. QA2 is based upon formal theorem- proving techniques. Facts are represented by statements in the predicate calculus. Although the memory organization is simpler than that of QA1, the sophisticated logical abilities of QA2 result in greater question-answering power. The report gives examples of the performance of QA1 and QA2 on typical problems that have been done by previous question-answerers, and describes plans for extending the capabilities of QA2.", "references": [], "page_rank": 0.000541871921182266}, {"id": "4e02c6d40e24192f2bf6d945bfd1a76ab29367cb", "title": "Planning in a Hierarchy of Abstraction Spaces", "authors": ["Earl D. Sacerdoti"], "date": 1973, "abstract": "Abstract A problem domain can be represented as a hierarchy of abstraction spaces in which successively finer levels of detail are introduced. The problem solver ABSTRIPS, a modification of STRIPS, can define an abstraction space hierarchy from the STRIPS representation of a problem domain, and it can utilize the hierarchy in solving problems. Examples of the system's performance are presented that demonstrate the significant increases in problem-solving power that this approach provides. Then some further implications of the hierarchical planning approach are explored.", "references": ["e97795382386ecd24300f3a6449ed5732b200bfa", "77078f3a12baf36e7766f068695a97f90ab91089", "7bc81e50d8434f76a5a0405d06c1e8c94ac249ab", "97876c2195ad9c7a4be010d5cb4ba6af3547421c"], "page_rank": 6.157635467980295e-05}, {"id": "0f6fe9550a16209372667adb788e92862337d1d8", "title": "Tense and Aspect of Present-Day American English", "authors": ["Martin Joos", "Akira Ota"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Tense and Aspect of Present-Day American English\" by Martin Joos et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "a1028cc19ada22f7208cf3a9b76c3a12a05d9466", "title": "A deductive question-answerer for natural language inference", "authors": ["Robert M. Schwarcz", "John F. Burger", "Robert F. Simmons"], "date": 1970, "abstract": "The question-answering aspects of the Protosynthex III prototype language processing system are described and exemplified in detail. The system is written in LISP 1.5 and operates on the Q-32 time-sharing system. The system's data structures and their semantic organization, the deductive question-answering formalism of relational properties and complex-relation-forming operators, and the question-answering procedures which employ these features in their operation are all described and illustrated. Examples of the system's performance and of the limitations of its question-answering capability are presented and discussed. It is shown that the use of semantic information in deductive question answering greatly facilitates the process, and that a top-down procedure which works from question to answer enables effective use to be made of this information. It is concluded that the development of Protosynthex III into a practically useful system to work with large data bases is possible but will require changes in both the data structures and the algorithms used for question answering.", "references": ["f10ec15182945fb17baeb5fe1334a9ba501a8a49", "11094935eac233f71e592906bddc7bee17367aa7", "3872994b05c54c8b589a1cabcc48e19fc7f8a107"], "page_rank": 4.926108374384236e-05}, {"id": "55b6a3dfa4697e8e645fda251ca45c9d8b85df3b", "title": "Linear Resolution with Selection Function", "authors": ["Robert A. Kowalski", "Donald Kuehner"], "date": 1971, "abstract": "Linear resolution with selection function (SL.resolution) is a restricted form of linear resolution. The main restriction is e~ected by a selection function which chooses fro:~ each clause a sit, gle literal to be resolved upon in that clause. This and other restrictions are adapted to linear resolution from Loveland's model elimination. We show that SL-resolution achieves a substantial reduction in the generation of redundant and irrelevant derivations and does so without significantly increasing the complexity o f simplest proofs. We base our argument for the increased efficiency of SL-resolution upon precise calculation of these quantities. A more far reaching advantage of SL-resolution is its suitability fo .~. ristic search. In particular, classification trees, subgoals, lemmas, and and/~./ search tret n all be used to increase the efficiency of flndino refutations. These considerations alone sug. r. :t the superiority of SL-resolution to theorem-proving procedures constructed solely for their I1euristie attraction. From comparison with other theorem-proving methods, we conjectur~ that best proof procedures for first order logic will be obtained by further elaboration of ~ ~.-resolution.", "references": ["a1bab6a5b88790ccb28c8d749dbe345dad5837f4", "76611284eee4fa4e8c21670c371a0c9c2cabbbd0", "b35aa4d90f7368fefaf05ca94f76edf03134eee7", "3b1a9f3c09d2c69f7169b8ccccff38ebd1c33ee9", "281078fedf0c31f81221845ab99ec33e26e8be15", "ffe701d533cffe8b81d1d2921b07a72e93648fa3", "221aa3be55a4ead8fc2aa83b12aac370bfba72f5", "516b414bdc4a3c3259684c83cf71f224ff58b2af", "ae02ad0063f1d312070334806c78952aa1f115ad"], "page_rank": 6.157635467980295e-05}, {"id": "4fc9602180318a3e61444f6a903f1b89eccaed1b", "title": "A Net Structure Based Relational Question Answerer: Description and Examples", "authors": ["Stuart C. Shapiro", "George H. Woodmansee"], "date": 1969, "abstract": "A question answering system is described which uses a net structure for storage of infor\u00ad mation. The net structure consists of nodes and labelled edges, which represent relations be\u00ad tween the nodes. The labels are also nodes, and therefore definitions of relations may be stored in the net. It is demonstrated that the generality and complexity of this memory struc\u00ad ture allows a surprisingly powerful question an\u00ad swering system to be constructed using comparit ively simple executive routines. Output from the question answerer, which is currently run\u00ad ning on an interactive, time sharing system, is included, showing its range of applicabi l i ty i n \u00ad cluding question answering, inductive and de\u00ad ductive inference, simple theorem proving and problem solving.", "references": ["c0585a46cebf7224d51ddbfc6746b4bb49e4cb68"], "page_rank": 4.926108374384236e-05}, {"id": "8bb236399f4d54d0916390c4b90fc8836b92d9cc", "title": "Adverbial positions in English", "authors": ["Sven Jacobson"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Adverbial positions in English\" by Sven Jacobson", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "96135ea3bb8eb7616f88f5f5617e03f2ddc9f4c0", "title": "Guide to Patterns and Usage in English", "authors": ["Albert Sydney Hornby"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Guide to Patterns and Usage in English\" by Albert Sydney Hornby", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "c35980a49350197aea411f9d7926d86e2a05905f", "title": "Description and theoretical analysis of planner", "authors": ["Carl Hewitt"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Description and theoretical analysis of planner\" by Carl Hewitt", "references": [], "page_rank": 0.0002961529439361951}, {"id": "c799a24ae9864be829a9f79498daf5c8b7220f62", "title": "A linguistic study of the English verb", "authors": ["Frank Robert Palmer"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"A linguistic study of the English verb\" by Frank Robert Palmer", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "7e74718cbe94fddcd17103c4b07b1f3babcdeeac", "title": "Studies in the correspondence of prosodic to grammatical features in English", "authors": ["Randolph Quirk", "Arthur Duckworth", "Jan Svartvik", "J. P. L. Rusiecli", "Andrew J. T. Colin"], "date": 1962, "abstract": "A mineral mining installation composed of a longwall conveyor having a ramped guide member at its mineral free side. The guide member locates on the face and acts as a housing for a traction member having interconnected elements which may be provided with cutters for cutting a groove at the floor region or at the floor and roof regions of the face. The elements, which are flexibly interconnected, have teeth which mesh with pinions driven by a number of hydraulic drive motors distributed along the length of the working. The motors can be housed in the guide member or disposed at the stowage side of the conveyor.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "9c4c473666d1d33efb33f21ccb77241fc7197fa0", "title": "Verbs of judging: An exercise in semantic description", "authors": ["Charles J. Fillmore"], "date": 1969, "abstract": "A tool for holding a flexible water hose which is shaped as a pair of hinged tongs, with each tong formed with a handle section at one end and a set of two clamp jaws at the other end of the tong.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f249468651e0caa9853ca38852bb3d4f3b29f1c9", "title": "Classification of word-groups", "authors": ["A. L. De Groot"], "date": 1956, "abstract": "Abstract Word-groups can be classified in several ways, e.g., according to order of members, word-class of members, omissibility of members, syntactic distribution, and meaning. The importance of any classification depends on its purpose. Of primary importance in linguistics is a structural classification, i.e. according to meaning as expressed by other features, i.e. according to similarities and dissimilarities of meanings, or \u2018oppositions\u2019. This kind of classification is applied, in a preliminary way, to Chinese, Latin, and English. Discussion of some aspects to word-groups in Indo-European languages: 1. The primitive System, 2. Compound Verbs, 3. Attitudinal Word-Groups.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "cb69a5bcbdc2f707617b1f5d4dfcdbb8e12f2594", "title": "A Conceptual Parser for Natural Language", "authors": ["Roger C. Schank", "Lawrence G. Tesler"], "date": 1969, "abstract": "This paper describes an operable automatic parser for natural language. It is a conceptual parser, concerned with determining the underlying meaning of the input utilizing a network of concepts explicating the beliefs inherent in a piece of discourse.", "references": ["8d487fd63f715640f118cb66f2eab23ec0379ae9", "be02603f27853d3cf6ef10cadc64727ed505cc25", "62d0535077cdc622f6e68b9f49f59b97dd482a11", "86669149d1ee67367096a73d58919b4df39c4ff0", "cf6a4f6de10bc0e55806f11982d484d783756fd9"], "page_rank": 8.210180623973726e-05}, {"id": "1beae9ef432a57bb5ec0c43944a07182814ab443", "title": "Application of Theorem Proving to Problem Solving", "authors": ["C. Cordell Green"], "date": 1969, "abstract": "This paper shows how an extension of the resolution proof procedure can be used to construct problem solutions. The extended proof procedure can solve problems involving state transformations. The paper explores several alternate problem representations and provides a discussion of solutions to sample problems including the \"Monkey and Bananas\" puzzle and the 'Tower of Hanoi\" puzzle. The paper exhibits solutions to these problems obtained by QA3, a computer program bused on these theorem-proving methods. In addition, the paper shows how QA3 can write simple computer programs and can solve practical problems for a simple robot.", "references": ["05b44597834f6df07c1c1290fb33a979bdf99067", "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "672fe922b933b188b167c6f50e433add2daffc3a", "bb84be1454b80de35032a1260b60384ccdf65489", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "dd15a1958d5a7bfa45548eb101bef4186acf2299", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "c8ed24d86755095c263a2f031752f817e668da3e", "375c0850cd418002ee2be0fb2b6ec177573f950a", "83f054294ba2726d02aa03e471da773c3383b146"], "page_rank": 0.00023399014778325121}, {"id": "7adb3c40ef03a458d35a3851fa66046936211cc3", "title": "The structure of a semantic theory", "authors": ["Jerrold J. Katz", "Jerry A. Fodor"], "date": 1963, "abstract": "Process for eliminating reactive chlorinated organic compounds from aqueous solutions, especially waste waters, which comprises treating the waters at 70 DEG -300 DEG C, preferably 100 DEG -160 DEG C, with ammonia or compounds splitting off the same, and/or primary amines or compounds splitting off the amines, thereby obtaining insoluble reaction products which are mechanically separated from the waters.", "references": [], "page_rank": 0.00021893814997263268}, {"id": "6dbf29d300d02bb4c89083f0e5167bf001e1dd2b", "title": "A Proof Procedure Using Connection Graphs", "authors": ["Robert A. Kowalski"], "date": 1975, "abstract": "Various deficiencies of resolution systems are investigated and a new theorem-proving system designed to remedy those deficiencms is presented The system is notable for eliminating re- dundancies present in SL-resolutlon, for incorporating preprocessing procedures, for liberahzing the order in which subgoals can be activated, for incorporating multidirectmnal searches, and for giving immediate access to pairs of clauses which resolve Examples of how the new system copes with the defic2encies of other theorem-proving systems are chosen from the areas of predicate logic program- ming and language parsing. The paper emphasizes the historical development of the new system, beginning as a supplement to SL-resolution in the form of classificatmn trees and incorporating an analogue of the Waltz algorithm for picture Interpretation The paper ends with a discussion of the opportunities for using look-ahead to guide the search for proofs", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "534177394b6b3ed4b0a8ba42c0e63d66f43029d9", "title": "WHAT'S IN A LINK: Foundations for Semantic Networks", "authors": ["William A. Woods"], "date": 1975, "abstract": "Publisher Summary This chapter focuses on the theoretical underpinnings for semantic network representations. It also focuses on several issues, such as the meaning of semantics, the need for explicit understanding of the intended meanings for various types of arcs and links, the need for careful thought in choosing conventions for representing facts as assemblages of arcs and nodes, and several specific difficult problems in knowledge representation\u2014especially problems of relative clauses and quantification. When the semantics of the notations are made clear, many of the techniques used in existing semantic networks are inadequate for representing knowledge in general. The chapter presents the logical inadequacies of almost all current network notations for representing quantified information and also discusses some of the disadvantages of a few logically adequate techniques.", "references": [], "page_rank": 0.00022362968175776055}, {"id": "077c16c7e4312817bf037c12f40adf6330691bc6", "title": "An ALGOL compiler", "authors": ["C. BoussardJ."], "date": 1966, "abstract": "An ALGOL translator has been prepared and integrated into the IBSYS Operating System. Assembly and go features of IBSYS permit immediate execution with optional listings, decks and debugging inform...", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a7314410cf36a8690c31e8aa432649b01d8a3602", "title": "Programming a computer model of neurosis", "authors": ["Kenneth Mark Colby", "John P. Gilbert"], "date": 1964, "abstract": "Abstract Computer simulation of pathological thought and emotion is considered. A model of neurosis and its realization in the form of a program are outlined. The organization of data structures and their processing to yield a program output similar to verbal and nonverbal behavior typical of neurotic patients are described in detail.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d8e690690cf505481cde5e47c2eb7922db988c4b", "title": "Dose-response curves of toxic and infective actions of adenovirus in HeLa cell cultures.", "authors": ["H. G. Pereira", "Barbara Kelly"], "date": 1957, "abstract": "SUMMARY: Study of the dose-response curves in titrations of adenovirus type 5 in cultures of HeLa cells suggests two different mechanisms of cytopathic effect. A late effect, caused by relatively small virus doses, is considered to be a manifestation of virus infectivity and is in good agreement with the hypothesis of independent action of virus units in the initiation of infection. The early effect, on the other hand, requires large virus doses and is considered to be of a toxic nature. Infective and toxic properties of adenovirus type 5 are distinguished by the greater sensitivity of the latter to inactivation by ultraviolet light.", "references": ["d11487ef4cb4c4aa34f0b92d27eed5bf1b20586f", "490763046ed9b28245440ccdcfa2a0fc46375d7a", "43585a4dca70342817c9b0f8ffe3d4e95cd2fe17", "7acfcfd2617a2b1738c72e87023b120001a5adfb", "79879563fcd71eef274a655a1cfc4af4548a5105", "c3b9205ff849306523a71de987204e467c981322", "12fa265642e47d4f2f4854864e3c5c62500cf030", "2c9de0094260b221e8303688a827a906ac66f871", "7dba5c469a48eda8d404679c6bd0689ec5c8d05c", "e961861ccdba3aad056fb72c0277c3a262a2f587"], "page_rank": 0.00016420361247947453}, {"id": "cb287e6a499ace7ee78662c765a8fa1fecf8eb91", "title": "Technology's Great Need: Information Retrieval", "authors": ["Walter K. Macadam"], "date": 1968, "abstract": "Early man's first limitation was language; with the invention of writing, he became literacy-limited; with printing, he eventually became literature-limited as the amount of knowledge grew beyond his capacity to keep up with it. Now he can work his way out of the masses of paper that beset him only if he does not become information-retrieval-limited. Our conventional indexing tools are already overloaded, however, and we must provide new means if we are to avoid this final limitation, which eventually would make it more profitable to take a chance on duplicating research effort than to search existing literature. Centralized repositories are one answer, and better communications facilities are another, but only the surface of the problem has thus far been scratched.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "547a664cf042af7ce4f171a65577441833ba673e", "title": "Human Problem Solving", "authors": ["Allen Newell"], "date": 1972, "abstract": "Abstract : The aim of the book is to advance the understanding of how humans think. It seeks to do so by putting forth a theory of human problem solving, along with a body of empirical evidence that permits assessment of the theory. (Author)", "references": [], "page_rank": 0.0002134646962233169}, {"id": "54accdec243cfbf396692ca52a330f89a06c83af", "title": "EMPIRICAL EXPLORATIONS OF A HYPOTHESIS-TESTING MODEL OF BINARY CHOICE BEHAVIOR", "authors": ["Julian Feldman", "Fred M. Tonge", "Herschel Kanter"], "date": 1961, "abstract": "Abstract : To explore behavior in binary choice experiments, a hypothesis-testing model is employed with subjects represented in a computer program. The basic assumption of this type of model is that the subject is entertaining hypotheses about patterns that he perceives in the random series of events. Various stimuli have been used for events of different experiments e.g., flashes of light, symbols on a deck of cards, spoken words. Results and discussion of simulated experiments are included.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "fcb85373fec90723b4561bf326769548ab84662a", "title": "An information-processing theory of some effects of similarity, familiarization, and meaningfulness in verbal learning", "authors": ["Herbert A. Simon", "Edward A. Feigenbaum"], "date": 1964, "abstract": "Summary Results obtained by simulating various verbal learning experiments with the Elementary Perceiving and Memorizing Program (EPAM), an information-processing theory of verbal learning, are presented and discussed. Predictions were generated for experiments that manipulated intralist similarity ( Underwood, 1953 ); interlist similarity ( Bruce, 1933 ); and familiarity and meaningfulness. The stimulus materials were nonsense syllables learned as paired-associates. A description of the EPAM-III model is given. The predictions made by the model are generally in good agreement with the experimental data. It is shown that the quantitative fit to the Underwood data can be improved considerably by hypothesizing a process of \u201caural recoding.\u201d The fit of the EPAM predictions to data of Chenzoff (1962) lends support to the hypothesis that the mechanism by means of of which a high degree of meaningfulness of items facilitates learning is the high familiarity of these items. The effects of varying degrees of stimulus and response familiarization on ease of learning were studied, and are shown to be surprisingly complex.", "references": ["7b1886c53c92500acfe9f51a6f92313f3554fa89", "60d16b23b8795dfbac7906c13be666cffd4e1f66", "4e1e2fdea4231a14bd32c96c6a750ad4984e6adb", "eb47b84bcf3839b8656a2d06023d4e4f8b791873", "d3f1aa9cf094559982867cd2154411e0e06f3d46"], "page_rank": 5.473453749315818e-05}, {"id": "14d82bfbbdfe6c002dfa728771a1dfa87db4b421", "title": "Pattern recognition over distortions, by human subjects and by a computer simulation of a model for human form perception.", "authors": ["Leonard Uhr", "Charles Vossler", "James S. Uleman"], "date": 1962, "abstract": "The recognition of single randomly generated, \"meaningless\" patterns has been studied during the past few years by a number of investigators (Attneave, 1957; Crook, Gray, Hanson, & Weisz, 1959; Vanderplas & Garvin, 1959b). Several experiments have further examined the abilities of human 5s in recognizing such patterns over certain simple variations, such as random noise (Crook et al., 1959; Hillix, 1960),contour noise (Fitts & Leonard, 1957), and systematically continued transformations (LaBerge & Lawrence, 1957). But no experiments have been reported in which 5s were asked to learn sets of variants of a pattern through experience with individual examples of these sets. Yet this is the typical procedure for computer \"pattern recognition\" simulation programs (Selfridge & Neisser, 1960; Uhr, 1962). It also presents a good experimental paradigm for the learning of patterns by human beings. Given N patterns and n variants of each pattern, 5 is presented with each of the nN particular pattern instances, asked to give it the correct name of the N names, and then told the correct name. This, in more systematic form, is what the child experiences during the natural experiments of his dayby-day learning. That is, the child is given particular instances of such things as \"chair,\" \"dog,\" the letter \"A,\" or his mother's face; and he learns to build up general concepts, or pattern classes. This sort of experiment, then, should serve as a convenient method for studying concept formation and perceptual learning of patterns in human 5s. It should also give a convenient framework within which to examine the predictive fit of pattern recognition simulations by computer programs, when they purport to be embodiments of theoretical models of human form perception. The use of meaningless patterns for stimulus sets reduces the wealth of information that the human can bring to bear. It also allows for control and systematic variation of the parameters of the stimulus sets. The main experiment examined the learning of pattern sets as a function of (a) interrelations between patterns, (&) practice, and (c) individual differences between 5s (including the simulated model as an 5). Subsidiary experiments examined the effects of different (a) durations of stimulus presentation, (&) size of pattern set, and (c) complexity of pattern. In addition, three form perception", "references": ["f10659021f097f79bc0e3ea55897bab36a62d8c5", "8def1503d588f2dc413e0b3e769be5e033d9e737", "4ac5952a1926da9d4f38480ed594b17ec04d21c0", "693e791966b333ad5bec096999cea2cce122cd38", "58e6e681587c64ef70dd6965bfbacdff2d4b68ad", "0c22e57b5acf2e4b6df20f2682827d7bb5f2f4be", "92ab102c45986d5073526be39eb3517651239b72", "991ea273a57e1c34a685ed0c6929161719ee0e7b", "4f27370762bafe14689fbffadbaaffc2fbfd3bf1"], "page_rank": 5.473453749315818e-05}, {"id": "e1b5cc576f71f5c99355b316277a3a311c7ef3b8", "title": "An example of human chess play in the light of chess playing programs.", "authors": ["A Newell", "Herbert A. Simon"], "date": 1965, "abstract": "Abstract : This paper is concerned with the use of chess programs to study human thinking. The work on chess programs has produced a collection of mechanisms sufficient to play chess of modest caliber. Independently of their detailed characteristics, they help understand what must be done in order to play chess. The approach used was to examine in some detail the behavior of a man deciding what move to make in a specific middle game position. Having available a protocol, a transcript of the verbal behavior of the man while he is analysing the board and making his decision. Previous work with protocols in other tasks (proving theorems, guessing sequences, learning concepts) has aimed at constructing computer programs that match the behavior in detail. In this paper the authors undertake only the first stages of such an analysis, laying bare the reasoning the subject employed, by examining his protocol in detail. The analysis draws upon ones general knowledge about reasoning mechanisms and how to organize information processing.", "references": ["f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69", "e50129abcbd13b2fe4196681590026f7ce7a6bb6", "e847968822837f5bb1dce2eefb18fc137d92fd08", "caba6432fdd5689d824623378271c61e311a2b1e", "97876c2195ad9c7a4be010d5cb4ba6af3547421c"], "page_rank": 5.473453749315818e-05}, {"id": "c8c9782797d277f8e0c1827e364d489d1c1de336", "title": "CYCLOPS-1: a second-generation recognition system", "authors": ["T. Marrill", "Alice K. Hartley", "Thomas G. Evans", "Burton H. Bloom", "David Michael Ritchie Park", "Timothy P. Hart", "D. Lucille Darley"], "date": 1963, "abstract": "CYCLOPS-1 is a recognition system programmed for a general-purpose digital computer, the PDP-1. It uses no special-purpose hardware. The three principal modes of operation of the system are (a) pattern input, (b) input identification, and (c) scene analysis.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "fada1648e34399c1fab2596aebaa1a9a14d868c6", "title": "Experiments in the recognition of hand-printed text, part I: character recognition", "authors": ["John H. Munson"], "date": 1968, "abstract": "Among the many subject areas in the field of pattern recognition, the recognition of machine-printed and hand-printed alphanumeric characters has perhaps been the classic example to which people have referred in exemplifying the field. Interest in character recognition has long run high; an extensive literature in handprinted character recognition alone dates back to at least 1955.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a45c86255ace7de61e92d42925eca9ca201c6368", "title": "Some Properties of Fuzzy Logic", "authors": ["Richard C. T. Lee", "Chin-Liang Chang"], "date": 1971, "abstract": "In this paper, the fuzzy set ZZadeh (1965)] is viewed as a multivalued logic with a continuum of truth values in the interval Z0, 1]. The concepts of inconsistency, validity, prime implicant and prime implicate are extended to fuzzy logic and various properties of these notions in the context of fuzzy logic are established. It is proved that a formula is valid (inconsistent) in fuzzy logic iff it is valid (inconsistent) in two-valued logic. An algorithm that generates fuzzy prime implicants (implicates) is introduced. A proof of the completeness of this algorithm is also given.", "references": ["f93d7109d1f1e4b4a5c70a5aa10d74bdbab50ec8", "ee787c6090c333eb83786ea48e3e1b1ae6499662", "107dd7e51dee6d83a0f4782333d708c788cbfc77", "1d029e10d24cd18755e14508ec3fd2703e282e0a", "92c1316871b83d4adfa9461c589be81d7a88b595"], "page_rank": 0.0001231527093596059}, {"id": "46bb9b198b3d2284a0d06b29d41c04dea0d0b243", "title": "Problem-Solving Methods in AI", "authors": ["Nils J. Nilsson"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Problem-Solving Methods in AI\" by Nils J. Nilsson", "references": [], "page_rank": 0.00032019704433497534}, {"id": "8fcf060606495f711e98ac4acb1eb1a7def35c9b", "title": "Problem Solving", "authors": ["Martin Scheerer"], "date": 1963, "abstract": "Once you have defined the problem you want to make sure you generate the best solution. Sometimes problems may seem unsolvable or they may appear to have only one solution. Nothing is more dangerous than an idea, when it is the only one you have. Perseverance is perhaps the most notable characteristic of successful problem solvers, so you shouldn\u2019t become discouraged when solutions aren\u2019t immediately evident. Many times mental blocks hinder your progress toward a solution. The first step is to recognize them, and then use blockbusting techniques to more forward toward the best solution.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "f395c0b69dc0da249fd9c6a2a964ddc3bb79babd", "title": "A heuristic program to solve geometric-analogy problems", "authors": ["Thomas G. Evans"], "date": 1964, "abstract": "The purpose of this paper is to describe a program now in existence which is capable of solving a wide class of the so-called 'geometric-analogy' problems frequently encountered on intelligence tests. Each member of this class of problems consists of a set of labeled line drawings. The task to be performed can be concisely described by the question: 'figure A is to figure B as figure C is to which of the given answer figures?' For example, given the problem illustrated as Fig. 1, the geometric-analogy program (which we shall subsequently call ANALOGY, for brevity) selected the problem figure labeled 4 as its answer. It seems safe to say that most people would agree with ANALOGY's answer to this problem (which, incidentally, is taken from the 1942 edition of the Psychological Test for College Freshmen of the American Council on Education). Furthermore, if one were required to make explicit the reasoning by which he arrived at his answer, prospects are good that the results would correspond closely to the description of its 'reasoning' produced by ANALOGY.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "ef2e340331efc529e59b40c95f51618b60f06a74", "title": "On the Encoding of Arbitrary Geometric Configurations", "authors": ["Herbert Freeman"], "date": 1961, "abstract": "A method is described which permits the encoding of arbitrary geometric configurations so as to facilitate their analysis and manipulation by means of a digital computer. It is shown that one can determine through the use of relatively simple numerical techniques whether a given arbitrary plane curve is open or closed, whether it is singly or multiply connected, and what area it encloses. Further, one can cause a given figure to be expanded, contracted, elongated, or rotated by an arbitrary amount. It is shown that there are a number of ways of encoding arbitrary geometric curves to facilitate such manipulations, each having its own particular advantages and disadvantages. One method, the so-called rectangular-array type of encoding, is discussed in detail. In this method the slope function is quantized into a set of eight standard slopes. This particular representation is one of the simplest and one that is most readily utilized with present-day computing and display equipment.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "7bc81e50d8434f76a5a0405d06c1e8c94ac249ab", "title": "Edge detection in pictures by computer using planning", "authors": ["Michael D. Kelly"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Edge detection in pictures by computer using planning\" by Michael D. Kelly", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5a17fa86ef53b7d89f0aae659cd183a071076390", "title": "An approach to general pattern recognition", "authors": ["Martin A. Fischler", "R. L. Mattson", "Oscar Firschein", "L. D. Healy"], "date": 1962, "abstract": "In this paper, the pattern recognition problem is considered to be composed of two subproblems. The first subproblem is one of abstracting significant features or characteristics from the patterns being dealt with, while the second subproblem is concerned with identifying the pattern which gave rise to a particular set of features (i. e., a decision-making problem). The decision-making system to be discussed is a special purpose digital computer, which simulates a network of threshold elements designed to correctly identify a preselected set of \"typical\" patterns. In addition to describing the design procedures, methods of feature extraction, and the hardware employed, the resuits of experiments involving several thousand handwritten characters will also be presented.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a8bfb8986e0b7cf938b8e9235bec712256bc084e", "title": "Computers in the study of learning: namer\u2014A pattern-recognition system for generating sentences about relations between line drawings", "authors": ["Manfred Kochen", "Dave L. Londe", "Robert F. Simmons"], "date": 1964, "abstract": "THIS PAPER reports on a series of experimental programs which are used to recognize line drawings and the spatial relationships between them. At the pattern-recognition level the programs learn to associate a name with a generalized bit pattern representing a line drawing. At the relation-learning level, the programs abstract characteristics which relate to such spatial concepts as \u201cabove,\u201d \u201cleft,\u201d \u201cthicker than,\u201d etc. The names that have been learned in association with a drawing, and relation names which the program selects as true for the spatial relations between two drawings, are substituted into a simple generation grammar; variations of sentences that are true for the picture are then generated.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "22fe47458970409fd796eb91b55a2c84e4272df1", "title": "The Recognition of Handwritten Numerals by Contour Analysis", "authors": ["Evon C. Greanias", "Philip F. Meagher", "Reini J. Norman", "Pierre Essinger"], "date": 1963, "abstract": "A character recognition system has been developed for the recognition of handwritten numerals. This system uses a logically controlled cathode ray tube scanner to generate basic measurements that characterize significant features of the numeral shapes. A contour-follower procedure is used to control the scanner. In addition, special scanner subroutines initiated by feedback from the recognition logic are utilized. Character shape data are generated in a sequential form, which can be analyzed for recognition with an easily realizable logic. \n \nAn experimental model has been built that recognized 99.3% of numerals written by 45 subjects after 30 minutes of training. The error rate ftohre se characters was 0.11%. The rejected character ratew as 0.59%.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "c0585a46cebf7224d51ddbfc6746b4bb49e4cb68", "title": "Deacon, Direct English Access and Control", "authors": ["James A. Craig", "Susan C. Berezner", "Homer C. Carney", "Christopher R. Longyear}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Deacon, Direct English Access and Control\" by James A. Craig et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "97876c2195ad9c7a4be010d5cb4ba6af3547421c", "title": "Report on a general problem-solving program", "authors": ["Allen Newell", "J. C. Shaw", "Herbert A. Simon"], "date": 1959, "abstract": "Semantic Scholar extracted view of \"Report on a general problem-solving program\" by Allen Newell et al.", "references": [], "page_rank": 0.0008374384236453201}, {"id": "ae02ad0063f1d312070334806c78952aa1f115ad", "title": "A review of automatic theorem-proving", "authors": ["Jane J. Robinson"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"A review of automatic theorem-proving\" by Jane J. Robinson", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "516b414bdc4a3c3259684c83cf71f224ff58b2af", "title": "Search strategies for theorem proving", "authors": ["Robert A. Kowalski"], "date": 1969, "abstract": "A printing ribbon has two differently constructed portions interconnected by an uninked connector strip welded to the ends of said portions to form a three component ribbon.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "fda841df3b0620c9322514d4434ca1dcccddd1f8", "title": "Syntax-directed interpretation of classes of pictures", "authors": ["R. Narasimhan"], "date": 1966, "abstract": "A descriptive scheme for classes of pictures based on labeling techniques using parallel processing algorithms was proposed by the author some years ago. Since then much work has been done in applying this to bubble chamber pictures. The parallel processing simulator, originally written for an IBM 7094 system, has now been rewritten for a CDC 3600 system. This paper describes briefly the structure of syntactic descriptive models by considering their specific application to bubble chamber pictures. How the description generated in this phase can be embedded in a larger \u201cconversation\u201d program is explained by means of a certain specific example that has been worked out. A partial generative grammar for \u201chandwritten\u201d English letters is given, as are also a few computer-generated outputs using this grammar and the parallel processing simulator mentioned earlier.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "ffe701d533cffe8b81d1d2921b07a72e93648fa3", "title": "Refinement theorems in resolution theory", "authors": ["David Luckham"], "date": 1970, "abstract": "The paper discusses some basic refinements of the Resolution Principle which are intended to improve the speed and efficiency of theorem-proving programs based on this rule of inference. It is proved that two of the refinements preserve the logical complete\u00adness of the proof procedure when used separately, but not when used in conjunction. The results of some preliminary experiments with the refinements are given.", "references": [], "page_rank": 0.0001532567049808429}, {"id": "3b1a9f3c09d2c69f7169b8ccccff38ebd1c33ee9", "title": "Some linear herbrand proof procedures : an analysis", "authors": ["Donald W. Loveland"], "date": 1970, "abstract": "Several Herbrand proof procedures proposed during the 1960 decade are shown to be related in varying degrees. Most of the paper deals with a relationship between s-linear resolution and model elimination. Refinements of each are proposed and the spaces of ground deductions are shown to be isomorphic in a suitable sense. The two refined procedures are then studied at the general level where they are no longer isomorphic and do not always relate to a natural ground level counterpart. Other topics considered are the introduction of an added merge condition to model elimination and also an expanded possible use of lemmas. Finally, the model elimination procedure is interpreted in the linked conjunct procedure of Davis and the matrix reduction procedure of Prawitz. Some Linear Herbrand Proof Procedures: An Analysis", "references": ["281078fedf0c31f81221845ab99ec33e26e8be15", "7bf360421e1ed036e025606bc5b9c6089f8adfdb", "f516d9dd73b117eca287bd3a07c2f3d26523b765"], "page_rank": 5.473453749315818e-05}, {"id": "375c0850cd418002ee2be0fb2b6ec177573f950a", "title": "The Correctness of Programs", "authors": ["Zohar Manna"], "date": 1969, "abstract": "This paper is concerned with the relationship between the correctness of programs and the satisfiability (or unsatisfiability) of certain formulas of the first-order predicate calculus. Results on the equivalence of programs are also included.", "references": ["bc9e164cae0db0cdb7b830120a83d750a37a3608", "47eed66353713f7a723c71ae1f6aa73383653f2c", "c43ebe201e2015d84cf44d8c17438ddbeddf3af9"], "page_rank": 6.157635467980295e-05}, {"id": "281078fedf0c31f81221845ab99ec33e26e8be15", "title": "A Linear Format for Resolution With Merging and a New Technique for Establishing Completeness", "authors": ["Robert Anderson", "W. W. Bledsoe"], "date": 1970, "abstract": "A new technique is given for establishing the completeness of resolution-based deductive systems for first-order logic (with or without equality) and several new completeness results are proved using this technique. The technique leads to very simple and clear completeness proofs and can be used to establish the completeness of most resolution-based deductive systems reported in the literature. The main new result obtained by means of this technique is that a linear format for resolution with merging and set of support and with several further restrictions is a complete deductive system for the first-order predicate calculus.", "references": [], "page_rank": 0.0002189381499726327}, {"id": "cf6a4f6de10bc0e55806f11982d484d783756fd9", "title": "The syntactic analysis of english by machine", "authors": ["James Peter Thorne", "Paul Bratley", "Hamish Dewar"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"The syntactic analysis of english by machine\" by James Peter Thorne et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "c8ed24d86755095c263a2f031752f817e668da3e", "title": "Experiments with a deductive question-answering program", "authors": ["James R. Slagle"], "date": 1965, "abstract": "As an investigation in artificial intelligence, computer experiments on deductive question-answering were run with a LISP program called DEDUCOM, an acronym for DEDUctive COMmunicator. When given 68 facts, DEDUCOM answered 10 questions answerable from the facts. A fact tells DEDUCOM either some specific information or a method of answering a general kind of question. Some conclusions drawn in the article are: (1) DEDUCOM can answer a wide variety of questions. (2) A human can increase the deductive power of DEDUCOM by telling it more facts. (3) DEDUCOM can write very simple programs (it is hoped that this ability is the forerunner of an ability to self-program, which is a way to learn). (4) DEDUCOM is very slow in answering questions. (5) DEDUCOM's search procedure at present has two bad defects: some questions answerable from the given facts cannot be answered and some other answerable questions can be answered only if the relevant facts are given in the \"right\" order. (6) At present, DEDUCOM's method of making logical deductions in predicate calculus has two bad defects: some facts have to be changed to logically equivalent ones before being given to DEDUCOM, and some redundant facts have to be given to DEDUCOM.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "221aa3be55a4ead8fc2aa83b12aac370bfba72f5", "title": "A Formal Basis for the Heuristic Determination of Minimum Cost Paths", "authors": ["Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael"], "date": 1968, "abstract": "Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "672fe922b933b188b167c6f50e433add2daffc3a", "title": "How a computer system can learn", "authors": ["Aiko M. Hormann"], "date": 1964, "abstract": "The process by which a computer can learn is demonstrated by asking it to solve increasingly more difficult versions of the Tower of Hanoi puzzle. Ultimately, the system may learn how to generalize in a particular problem domain Present uses of computers, valuable as they may be, are far from the ultimate in what might be accomplished. One of the reasons is that the solution of even well-defined problems, for which goals and rules are precisely known, can be extremely difficult to program (e.g., chess-playing programs). But intellectual capacities of machines might be extended by means of an adaptive system to handle increasingly complex and varied tasks.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "86669149d1ee67367096a73d58919b4df39c4ff0", "title": "RECENT DEVELOPMENTS IN THE MITRE SYNTACTIC ANALYSIS PROCEDURE", "authors": ["Donald E. Walker"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"RECENT DEVELOPMENTS IN THE MITRE SYNTACTIC ANALYSIS PROCEDURE\" by Donald E. Walker", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "bb84be1454b80de35032a1260b60384ccdf65489", "title": "Sufficient Conditions for the Success of GPS", "authors": ["George W. Ernst"], "date": 1969, "abstract": "A formal model of a problem is developed and its relationship to the General Problem Solver (GPS) is discussed. Before GPS can work on a problem it must be given differences, a difference-ordering, and a table of connections, in addition to the specifications of a problem. Formal definitions of this additional information are given, and sufficient conditions for the success of GPS are derived. These conditions point out the utility of differences and a difference-ordering that yield a \u201ctriangular\u201d table of connections. Several different formulations of the Tower of Hanoi are given to illustrate the formal concepts. The use of subproblems in narrowing search is discussed.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d11487ef4cb4c4aa34f0b92d27eed5bf1b20586f", "title": "A NON-TRANSMISSIBLE CYTOPATHOGENIC EFFECT OF INFLUENZA VIRUS IN TISSUE CULTURE ACCOMPANIED BY FORMATION OF NON-INFECTIOUS HEMAGGLUTININS", "authors": ["Gertrude Henle", "Anthony J. Girardi", "Werner Henle"], "date": 1955, "abstract": "Various strains of influenza virus produce a cytopathogenic effect in cultures of HeLa cells. The virus could not be passed in series. Virus partially or even completely inactivated with respect to infectivity by exposure to 37\u00b0C. or ultraviolet light retained some of its cytopathogenic effect. No evidence has been obtained of an increase in infectious virus in HeLa cultures, but an increase in hemagglutinins and in both viral and soluble complement-fixing antigens became detectable during incubation. These virus materials apparently were not released from these cells prior to their destruction. These results suggested that HeLa cells are capable of supporting an incomplete reproductive cycle of influenza virus. The fact that radioactive phosphorus was readily incorporated into the hemagglutinin supplies strong evidence for this interpretation.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7dba5c469a48eda8d404679c6bd0689ec5c8d05c", "title": "A Dose-Response Equation for the Invasion of Micro-Organisms", "authors": ["S Peto"], "date": 1953, "abstract": "A series of kc doses (n', n , , nk micro-organisms) is administered to ml , m2 , ... , mk test animals respectively, and the corresponding survivors are observed to number r, r2, , .rk (The terms \"survivors\" and \"killed\" whenever they occur subsequently, are meant to cover also the case when \"not infected\" are compared with \"infected\"). This paper derives a dose-response relation from a hypothesis based on the mode of action of the micro-organisms against their host first suggested by H. A. Druett (2). Assuming that", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "991ea273a57e1c34a685ed0c6929161719ee0e7b", "title": "Complexity, association value, and practice as factors in shape recognition following paired associates training.", "authors": ["James M. Vanderplas", "Everett A. Garvin"], "date": 1959, "abstract": "Semantic Scholar extracted view of \"Complexity, association value, and practice as factors in shape recognition following paired associates training.\" by James M. Vanderplas et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4f27370762bafe14689fbffadbaaffc2fbfd3bf1", "title": "Stimulus correlates of visual pattern recognition: a probability approach.", "authors": ["Paul M. Fitts", "M Weinstein", "M Rappaport", "Norman O. Anderson", "Julia A. Leonard"], "date": 1956, "abstract": "Semantic Scholar extracted view of \"Stimulus correlates of visual pattern recognition: a probability approach.\" by Paul M. Fitts et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e961861ccdba3aad056fb72c0277c3a262a2f587", "title": "Particle counts and infectivity titrations for animal viruses.", "authors": ["Anthony D. Isaacs"], "date": 1957, "abstract": "Publisher Summary The principles of measuring the concentration of virus particles are based on the techniques evolved for counting bacteria. In this chapter, the principles are discussed along with their application to certain animal viruses. Three aspects of this subject are discussed: (1) measurement of the infectivity titer of a preparation, (2) measurement of the total number of virus particles in a preparation, and (3) calculation of the ratio of the infectivity titer to the virus particle count and the significance of different ratios. There are three types of method used for measuring virus infectivity: direct method, all-or-none response method, and indirect methods. Measurement of the total number of virus particles in a preparation includes those methods in which the particles can be seen and counted directly in the electron microscope such as calculations from the mass, volume, and density. Direct methods (including electron microscopic enumeration) and indirect methods (dosage\u2013response curve) are used. It is suggested that virus particle counting techniques may be useful in searching for incomplete forms of other viruses, and in trying to elucidate which properties of the virus particles and which cell constituents are important in determining what will be the end result of the virus-cell interaction.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "dd15a1958d5a7bfa45548eb101bef4186acf2299", "title": "The use of theorem-proving techniques in question-answering systems", "authors": ["C. Green", "Bertram Raphael"], "date": 1968, "abstract": "For the purpose of this paper, a question-answering system is a computer program that has at least the following three characteristics:\n (1) The ability to accept statements of fact and store them in its memory\n (2) The ability to search stored information efficiently and to recognize items that are relevant to a particular query\n (3) The ability to respond appropriately to a question by identifying and presenting the answer if it is present in memory, and by deducing a reasonable logical response from relevant knowledge if the complete answer is not explictly available.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "caba6432fdd5689d824623378271c61e311a2b1e", "title": "Computers in psychology", "authors": ["Alan F. Newell", "Herbert A. Simon"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Computers in psychology\" by Alan F. Newell et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "2c9de0094260b221e8303688a827a906ac66f871", "title": "Effect of Cortisone on Mouse Resistance to Intravenous Toxicity of Influenza Virus.\u2217 \u2020", "authors": ["Newton Khoobyarian", "Duard L. Walker"], "date": 1957, "abstract": "Summary 1. Treatment of mice with cortisone significantly reduced their resistance to the intravenous toxicity of PR8 influenza A. Lee influenza B, and Newcastle disease viruses. 2. Deaths of cortisone treated mice were prevented by mixing specific immune serum with the viral inoculum or by passive immunization of mice prior to challenge. Immune serum injected intravenously 15 minutes after challenge gave some protection but was without effect when injected 3 hours after challenge. 3. For full effect, closes of 2.5 mg of cortisone had to be used when treatment was begun 24 hours before challenge. Smaller doses were effective if begun earlier. Doses as large as 5.0 mg had no significant effect if given only 3 hours before challenge.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e847968822837f5bb1dce2eefb18fc137d92fd08", "title": "A Chess Playing Program for the IBM 704", "authors": ["Arnold Bernstein", "Malcolm D. Roberts", "T Y Arbuckle", "Martin A. Belsky}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"A Chess Playing Program for the IBM 704\" by Arnold Bernstein et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "92ab102c45986d5073526be39eb3517651239b72", "title": "A Note on Optimum Pattern Recognition Systems", "authors": ["Wilbur H. Highleyman"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"A Note on Optimum Pattern Recognition Systems\" by Wilbur H. Highleyman", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e50129abcbd13b2fe4196681590026f7ce7a6bb6", "title": "Simulation of behavior in the binary choice experiment", "authors": ["Julian Feldman"], "date": 1961, "abstract": "A modern, high-speed digital computer has been used to simulate the behavior of individual human subjects in a classical psychological experiment where the subject is asked to predict a series of binary events. The representation of models of human behavior in the form of computer programs has permitted the construction and study of more realistic hypothesis-testing models of behavior in this experiment rather than the oversimplified conditioning models previously proposed. A model for one subject is described in detail, and the problem of comparing the behavior of the model to the behavior of the subject is also discussed.", "references": [], "page_rank": 0.00018062397372742197}, {"id": "1d029e10d24cd18755e14508ec3fd2703e282e0a", "title": "Interpolation Theorems for Resolution in Lower Predicate Calculus", "authors": ["James R. Slagle"], "date": 1970, "abstract": "The resolution principle is an inference rule for quantifier-free first-order predicate calculus. In the past, the completeness theorems for resolution and its refinements have been stated and proved for finite sets of clauses. It is easy (by G\u00f6del's Compactness Theorem) and of practical interest to extend them to countable sets, thus allowing schemata representing denumerably many axioms. In addition, some theorems similar to Craig's Interpolation Theorem are proved for deduction by resolution. In propositional calculus, the theorem proved is stronger, whereas in predicate calculus the theorems proved are in some ways stronger and in some ways weaker than Craig's theorem. These interpolation theorems suggest procedures which could be embodied in computer programs for automatic proof finding and consequence finding.", "references": ["7a36d6f67916a37bd9d1076be19319fb42df175c", "9beb5df44f0d98fe676710b3e8b5bca78c593ca6"], "page_rank": 9.852216748768472e-05}, {"id": "92c1316871b83d4adfa9461c589be81d7a88b595", "title": "A New Algorithm for Generating Prime Implicants", "authors": ["James R. Slagle", "Chin-Liang Chang", "Richard C. T. Lee"], "date": 1970, "abstract": "This paper describes an algorithm which will generate all the prime implicants of a Boolean function. The algorithm is different from those previously given in the literature, and in many cases it is more efficient. It is proved that the algorithm will find all the prime implicants. The algorithm may possibly generate some nonprime implicants. However, using frequency orderings on literals, the experiments with the algorithm show that it usually generates very few ( possibly none) nonprime implicants. Furthermore, the algorithm may be used to find the minimal sums of a Boolean function. The algorithm is implemented by a computer program in the LISP language.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "f93d7109d1f1e4b4a5c70a5aa10d74bdbab50ec8", "title": "Fuzzy Logic and its Application to Switching Systems", "authors": ["Peter N. Marinos"], "date": 1969, "abstract": "Fuzzy logic deals with propositions which may be ascribed values between falsehood and truth (0 and 1) subjectively in either a continuous or a discrete fashion. This is in contrast to ordinary logic (two-valued or k-valued logic) in which a given proposition is ascribed values objectively using either deterministic or probabilistic approaches.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "58e6e681587c64ef70dd6965bfbacdff2d4b68ad", "title": "A Computer Simulation of Pattern Perception and Concept Formation", "authors": ["Charles Vossler", "Leonard Uhr"], "date": 1962, "abstract": "Living organisms manage to survive as entities, in a complex environment which continually threatens their disintegration, by their knowledge of this environment, and by their ability to use this knowledge to maintain their own existence. Unlike a stone, whose passive existence is simply the result of its own hardness, and of the fact that few natural forces tend to change it, living organisms, including man, must actively seek certain stable conditions under which their continued existence is possible.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "107dd7e51dee6d83a0f4782333d708c788cbfc77", "title": "A Theorem About Infinite-Valued Sentential Logic", "authors": ["Robert McNaughton"], "date": 1951, "abstract": "In this paper we shall use a logic with truth values ranging over all the real numbers x such that 0 \u2266 x \u2266 1.1 will be \u201ccomplete truth\u201d and 0 will be \u201ccomplete falsity.\u201d The primitive sentential connectives are \u2018\u2283\u2019 and \u2018\u223c\u2019; other connectives are \u2018 \u2228 \u2019 and \u2018\u00b7\u2019. Assume that \u2018 p \u2019 and \u2018 q \u2019 are sentential variables, whose truth values are respectively x and y . Then 1.1. \u2018 p \u2283 q \u2019 has the value min(1 \u2212 x + y , 1), 1.2. \u2018\u223c p \u2019 has the value 1 \u2212 x , 1.3. \u2018 p \u2228 q \u2019 has the value max( x, y ), and 1.4. \u2018 p\u00b7q \u2019 has the value min ( x, y ). \u2018 \u2228 \u2019 and \u2018\u00b7\u2019 can be defined as follows: It is the purpose of this paper to prove a theorem which will be stated in the next section. The following symbolism and convention will be used throughout the paper: S is a logical formula. \u03bd ( S ) is the value of S . \u2018 p \u2019, \u2018 pi 1 , \u2019 p 2 , \u2026, \u2018 q \u2019, are sentential variables. \u03bd ( p ) = x and \u03bd ( x 1 ) = x 1 , etc. \u03bd ( S ) = \u03c3 and \u03bd ( S 1 ) = \u03c3 1 , etc. If S contains the sentential variables \u2018 p 1 \u2019, \u2018 p 2 \u2019, \u2026, then we write for S , S ( p 1 , P 2 , \u2026). Also \u03bd { S ( p 1 , p 2 , \u2026)) = \u03c3 ( x 1 , x 2 , \u2026). A logical formula is defined in the usual manner. 1. A sentential variable is a logical formula; 2. if S is a logical formula then \u00b7 S is a logical formula; and 3. if S and S \u2032 are logical formulae then ( S \u2283 S \u2032) is a logical formula.", "references": [], "page_rank": 0.0002627257799671592}, {"id": "ee787c6090c333eb83786ea48e3e1b1ae6499662", "title": "ALGEBRAIC ANALYSIS OF MANY VALUED LOGICS(", "authors": ["Charles C. Chang"], "date": 1958, "abstract": "This paper is an attempt at developing a theory of algebraic systems that would correspond in a natural fashion to the No-valued propositional calculus(2). For want of a better name, we shall call these algebraic systems MV-algebras where MV is supposed to suggest many-valued logics. It is known that the classical two-valued logic gives rise to the study of Boolean algebras and, as can be expected, every Boolean algebra will be an MValgebra whereas the converse does not hold. However, many results for Boolean algebras can be appropriately carried over to MV-algebras, although in some cases the proofs become more subtle and delicate. The motivation behind the present study is to find a proof of the completeness of the Novalued logic by using some algebraic results concerning MV-algebras; more specifically, it is known that the completeness of the two-valued logic is a consequence of the Boolean prime ideal theorem and we wish to exploit just some such corresponding result for MV-algebras(3). It will be seen that our effort in duplicating this result is only partially successful. In the first four sections of this paper we present various theorems concerning both the arithmetic in MV-algebras and the structure of these algebras. In the last section we give some applications of our results to the study of completeness of NO-valued logic and some related topics. We point out here that the treatment of MV-algebras as given here is not meant to be complete and exhaustive. 1. Axioms of MV-algebras and some elementary consequences. An MV", "references": ["4e2374294ac9a5961cdca1f1164cc6ee218ad258", "8eb404d1b5d73e78fe0c272e42c42a52811e02cc", "c4d52202de4d7279ff0fdf8b1e9ed09143401aa2"], "page_rank": 9.852216748768472e-05}, {"id": "0c22e57b5acf2e4b6df20f2682827d7bb5f2f4be", "title": "\"Pattern recognition\" computers as models for form perception.", "authors": ["Leonard Uhr"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"\"Pattern recognition\" computers as models for form perception.\" by Leonard Uhr", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4ac5952a1926da9d4f38480ed594b17ec04d21c0", "title": "Transfer of experience with a class-schema to identification-learning of patterns and shapes.", "authors": ["Fred Attneave"], "date": 1957, "abstract": "The idea that experience provides the organism with some \"apperceptive mass\" which facilitates later perception and learning is an old one, with many variants and elaborations. The concept of a \"schema\" as an entity mediating the effects of past experience has been prominent in recent British psychology, chiefly because of the thinking of Bartlett (5) and his associates, as reflected in the summary and review of Oldfield and Zangwill (12). Woodworth (13) and Hebb (7) have used the term \"schema\" in a sense which is somewhat more restricted and more definite than Bartlett's. After considering a number of experiments on memory for form, Woodworth concluded that a new configuration is usually remembered in terms of a \"schema, with correction.\" For example, a figure which may be described as \"a square with a nick on one side\" is easier to learn than", "references": ["0094175f4e0e341af2e045734664c9bd5404471d", "6d0198460198fdb49b89d1646049712b3a0683df", "33fcb6d718d34927fd1941da5d442ddd0b77b107", "f51adec455670fa0e926bc82e36026ae8320b9f4", "99d99c1f79b0b61238fafe5e1fd2a9d624be4fe1", "313630eaeb2c362f623d24538f64cb3a49e94e31"], "page_rank": 6.157635467980295e-05}, {"id": "f10659021f097f79bc0e3ea55897bab36a62d8c5", "title": "The association value of random shapes.", "authors": ["James M. Vanderplas", "Everett A. Garvin"], "date": 1959, "abstract": "It is well known that verbal materials vary in meaningfulness or association value, and that these variations are related to learning and retention. Patterns and shapes may vary similarly; however, little systematic control over such variation has been exercised in studies of perceptual learning and retention. Indeed, little effort to standardize such materials has been expended (cf., e.g., the discussions by Hilgard [1951, p. 547] and Graham [1951, pp. 911-915]). A number of experiments have appeared in which random shapes have been used as stimuli in tasks involving perceptual learning and retention. Random shapes have also been employed in studies of mediated transfer and \"predifferentiation.\" In most of these studies, control has not been exercised over possible effects of association value of the shapes upon performance of 5s. The present experiment was undertaken to provide a pool of random shapes with known association value for use in studies of the effects of certain stimulus variables and pretraining upon recognitive performance. It was considered desirable to provide for variation of association value and stimulus", "references": ["0094175f4e0e341af2e045734664c9bd5404471d", "1d671b87a0ec0553b73a84215d98c22c909afba3", "313630eaeb2c362f623d24538f64cb3a49e94e31"], "page_rank": 6.157635467980295e-05}, {"id": "4e1e2fdea4231a14bd32c96c6a750ad4984e6adb", "title": "Forgetting in an association memory", "authors": ["Edward A. Feigenbaum", "Herbert A. Simon"], "date": 1961, "abstract": "Though the phenomenon of forgetting is an everyday and commonplace experience of human intelligent systems it has been inadequately treated by theorists of intelligent systems (human and computer).\n On the one hand there is the stark behavioral fact that humans who were, at one point in time, able to respond in some particular way to some particular stimuli are, at a later time, no longer able to respond to those stimuli. On the other hand, there are the ever-intriguing experiences, shared by us all, that humans are able to recall stimuli whose age-in-memory is of the order of many years and that a person's technique of association can be pressed into the service of retrieving \u201clong lost\u201d information from his memory (sometimes quite consciously!) Furthermore, some modern experiments (4), involving the implantation and stimulation of microelectrodes in the brain, have suggested that experiences far removed in time can be evoked in great informational detail by appropriate stimulation of barin tissue.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "d3f1aa9cf094559982867cd2154411e0e06f3d46", "title": "The Simulation of Verbal Learning Behavior", "authors": ["Edward A. Feigenbaum}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"The Simulation of Verbal Learning Behavior\" by Edward A. Feigenbaum", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "eb47b84bcf3839b8656a2d06023d4e4f8b791873", "title": "A theory of the serial position effect.", "authors": ["Edward A. Feigenbaum", "Herbert A. Simon"], "date": 1962, "abstract": "The paper proposes a theory of the well-known serial position effect that makes quantitative predictions, acceptable by non-parametric tests, of the observed amount of bowing of the serial position curve. The theory, which stems from viewing the central nervous system as an information-processing system, is compared with the Lepley-Hull hypothesis and Atkinson's theory of the serial phenomena, and is shown to be more satisfactory than the older explanations.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "7b1886c53c92500acfe9f51a6f92313f3554fa89", "title": "Generalization of an Elementary Perceiving and Memorizing Machine", "authors": ["Edward A. Feigenbaum", "Herbert A. Simon"], "date": 1962, "abstract": "The Elementary Perceiver and Memorizer (EPAM) is a context, and contains sufficient information to retrieve the model of human associative memory and the processes of image of the stimulus from the association memory when human verbal learning, written in the list-processing language necessary. IPL-V. A generalization of the EPAM model (called EPAM The complex image building processes also provide an assoIII), based on earlier experiments on EPAM 11, has been dative mechanism by means of which earlier learning can formulated and programmed. usefully be brought to bear on later learning. New stimuli arc In EPAM 111. stimulus configurations and their internal learned by associating together and modifying images of images are allowed indefinite complexity of sub-parts. Complex stimuli already learned. The model is an attempt to resolve internal images are built by associating together, in list structhe problem of how meaningful learning differs from nontures, tokens for retrieving information about the sub-parts meaningful learning, from the association memory. Empirical explorations of the model are concerned generally The concatenation of tokens by the image building process with variations in behaviour as a result of varying stimulus allows for the association of stimuli in many different contexts. and response meaningfulness. The token signifies the occurrence of a stimulus in a particular 3.ieM6HTapHan nporpaMMa jhh \"H3yqaioinHx MauaeT coSbiTHe CTHMyjia b oco66hhom KOHTeKCTe, a npiiuihh\" (3IIJIHM (EPAM)) npeacTa\u00df^weTCH Moaejibio cyTCTByeT b aaHHOM naineKe aocTaToiHO HHdpopMauHH, accouHaTH\u00dfiioil nfiMfiTH h npouecco\u00df H3yiemm mob KeHHn-iiMeiomHeMHoroypo\u00dfHeflHH(})opMauHH hux panbine BOo6pajKeHHft. MoaeJib npeacTa\u00dfJineT o no.tCTpyuTypectpohtch cnocoSoM accouHHpo\u00dfaHHH coCoft nonbrricy peuiHTb 3a\u00abaqy o tom, KaKHM oopa3OM 3iia\u00abiKon miMRKOB, npw noMomH kotopux cBe^eHHH pa3JinqaTb 3HaiaromHe CBeaenHH ot He3HaiaiomHx. o no.macTnx mo>kho BUBO\u00abHTb H3 accouHaTH\u00dfHofl 3MnHpmiecKHe onwTbi npo\u00dfOflHmnecH Ha Mo^e^b, naMtiTii. Boo6me ro\u00dfopn, hmciot c paaHUMH BH\u00abaMH no\u00dfe-3auen.ieHne iiaMeKOB npoueccoM co3\u00abaHHH boosaeHHn, npoHCxofIHWHMH H3 pa3HOBHfIHOCTH 3HaienHfi pn>Kenmi no3Bo.ineT accou.HHpo\u00dfaTb cTHMy^u, npoHCTHMy^a h oTBeTa. cxoanmiio H3 Miioro pa3Hbix kohtckctob. HaMeK 03HaLa machine elementaire pour percevoir et mettre en mblir Information concernant les sous-parties, a partir de la moire (EPAM) est un modele de la memoire humaine d'assomemoire dissociation. ciation. et dcs process de la facon humaine d'apprendre L'enchainement dcs signes au moyen de l'image eUabor^e verbalement, 6crit en langage IPL-V. En partant dcs r\u00absulpermet d'associer dcs excitations dans un grand nombre de tnts de tests anteVieurs, on a dlabore' et programme\" une contextes diffe>ents. Le signe montre qu'il existe une excitagcneralisation dv modele EPAM, appele EPAM 111. tion dans un contexte particulier et une information suffisante Sur EPAM 111, les configurations d'excitation et les images peut, grace ace signe, regene>er l'image de I'excitation a inteneures de ces configurations ont une complexitc indefinie partir de la memoire dissociation, quand cela est necessaire. de sous-parlies. Dcs images internes complexes sont dlaborees Les procedis complexes gui 61aborent l'image fournissent en groupant. en liste de structures, les signes destine* a retaaussi un m6canisme dissociation, permettant d'elaborer faci405 IX, 1] automata theory lement la toute premiere information a partir de la toute informations ayant un sens, de celles gui n'en ont pas. derniere. Les tests empiriques dv modele se rapportent gene>alement On decouvre de nouvelles excitations en groupant (et en aux variations de fonctionnement, celles-ci \u00a3tant le resultat modifiant) les images d'excitations deja connues. Cc modele d'excitations et de reponses variables. est un essai de r6soudre le probleme de la differentiation dcs", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "60d16b23b8795dfbac7906c13be666cffd4e1f66", "title": "Studies of distributed practice. VIII. Learning and retention of paired nonsense syllables as a function of intralist similarity.", "authors": ["Benton J. Underwood"], "date": 1953, "abstract": "The rate of learning patred<*dje6tive lists is unrelated to iatertriai intervals up to 2 mm. when lists are ttted at a 2:2\u00abtec. rare, i.e., 2 sec. the stimulus aloae and 2 see for the stimulus and response appearing -together (t). With materials of tew meaaiagfultiess the data ake somewhat contradictory. la one study Hovlaad (4) found that a 2-mie. rest alter each trial resulted iu no faster learning than did a 6-aec. rest for paired nonsense syllables, Is a subsequent study (S) however, the same conditions did produes more rapid teaming with spaced practice than with massed. Furthermore, the differences were magaiied if pairs were presented at a lil-sec. rate. Ho resolution of these conflicttag data is available. Early studies by Hovland (e.g., 3) have consistently shown that in sens! learning distributed pi acrice facilitates acquisition. Previously in his writ*", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "f516d9dd73b117eca287bd3a07c2f3d26523b765", "title": "Resolution With Merging", "authors": ["Peter B. Andrews"], "date": 1968, "abstract": "A refinement of the resolution method for mechanical theorem proving is presented. A resolvent C of clauses A and B is called a merge if literals from A and B merge together to form some literal of C. It is shown that the resolution method remains complete if it is required that two noninitial clauses which are not merges never be resolved with one another. It is also shown that this strategy can be combined with the set-of-support strategy.", "references": [], "page_rank": 0.000509031198686371}, {"id": "47eed66353713f7a723c71ae1f6aa73383653f2c", "title": "Termination of algorithms", "authors": ["Zohar Manna"], "date": 1968, "abstract": "Abstract : The thesis contains two parts which are self-contained units. In Part 1 we present several results on the relation between the problem of termination and equivalence of programs and abstract programs, and the first order predicate calculus. Part 2 is concerned with the relation between the termination of interpreted graphs, and properties of well-ordered sets and graph theory. (Author)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "title": "Assigning meaning to programs", "authors": ["Robert W. Floyd"], "date": 1967, "abstract": "Floyd 3 On page 25, the author states \u201cthis fact offers the possibility of automatic verification of programs, the programmer merely tagging entrances and one edge in each innermost loop; the verifying program would extend the interpretation and verify it, if possible, by mechanical theorem-proving techniques.\u201d Give a brief overview of the state of formal verification (yes, you can just Google this). You may also find it interesting to look at this Digital Library article.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "12fa265642e47d4f2f4854864e3c5c62500cf030", "title": "STUDIES ON THE TOXICITY OF INFLUENZA VIRUSES", "authors": ["Gertrude Henle", "Werner Henle"], "date": 1946, "abstract": "Intracerebral injection of preparations of influenza viruses into mice led to tonic and clonic convulsions and death in tetanus, usually within 24 to 72 hours. Histological examination revealed the destruction of the ependymal lining of the ventricles as the dominant finding. These reactions were obtained in four different strains of mice as well as in rats, guinea pigs, and hamsters. They were observed in mice after injection of mouse and egg-adapted virus, and all strains of virus tested gave these responses as long as sufficient quantities of the agents were injected. Control materials, such as normal allantoic fluids, particulate components of normal chorio-allantoic membranes, and suspensions of normal murine lungs, gave uniformly negative results. Activation of a latent virus as well as the inadvertent admixture of a neurotropic agent to the influenza cultures was excluded. Propagation of the influenza viruses in the central nervous system could not be demonstrated and, in fact, the agents were no longer demonstrable in 4 days. It was concluded that the observed neurological reactions were the result of toxic activities rather than of virus propagation. Comparison between the infectivity, hemagglutinating capacity, and the toxic activity showed that preparations with the higher titers of virus and hemagglutinin were also the more toxic ones. The toxic agents could not be separated from the infectious particles by the use of such technics as differential high speed centrifugation, adsorption onto and elution from chicken red cells, and precipitation by protamine. The toxic effect of influenza A virus preparations was specifically neutralized by anti-influenza A and not by anti-influenza B serum, and conversely. In addition, antigenic differences were noted between two strains of influenza A virus by this method of testing.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "bc9e164cae0db0cdb7b830120a83d750a37a3608", "title": "Properties of Programs and the First-Order Predicate Calculus", "authors": ["Zohar Manna"], "date": 1969, "abstract": "This paper is concerned with the relationship of the termination problem for programs and abstract programs to the validity of certain formulas in the first-order predicate calculus. By exploiting this relationship, subclasses of abstract programs for which the termination problem is decidable can be isolated. Moreover, known proof procedures for the first-order predicate calculus (e.g. resolution) can be applied to prove the termination of both programs and abstract programs. The correctness and equivalence problems of abstract programs are shown to be reducible to the termination problem.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "7bf360421e1ed036e025606bc5b9c6089f8adfdb", "title": "The Unit Proof and the Input Proof in Theorem Proving", "authors": ["Chin-Liang Chang"], "date": 1970, "abstract": "A resolution in which one of the two parent clauses is a unit clause is called a unit resolution, whereas a resolution in which one of the two parent clauses is an original input clause is called an input resolution. A unit (input) proof is a deduction of the empty clause \u25a1 such that every resolution in the deduction is a unit (input) resolution. It is proved in the paper that a set S of clauses containing its unit factors has a unit proof if and only if S has an input proof. A LISP program implementing unit resolution is described and results of experiments are given.", "references": ["b35aa4d90f7368fefaf05ca94f76edf03134eee7"], "page_rank": 0.00016420361247947453}, {"id": "c3b9205ff849306523a71de987204e467c981322", "title": "THE EFFECT OF ULTRAVIOLET IRRADIATION ON VARIOUS PROPERTIES OF INFLUENZA VIRUSES", "authors": ["Werner Henle", "Gertrude Henle"], "date": 1947, "abstract": "The effect of ultraviolet irradiation on various properties of the influenza viruses Types A and B has been analyzed. The studies involved propagation and interference in the allantoic sac of the chick embryo, inhibition of embryonic development, toxicity for white mice, hemagglutination including the adsorption-elution mechanism, immunizing capacity for mice and, finally, complement fixation activities in the presence of antibodies to the 600S antigen (human convalescent and postvaccination sera) and the 30S antigen (convalescent sera only). It has been shown that the various activities of the influenza viruses were affected by irradiation at different rates, indicating that they are based, at least in part, on different constituents of the virus particle. On account of these differences in the susceptibility of the various properties to ultraviolet light it was possible (a) to differentiate between the interference phenomenon as observed in the allantoic sac, and the development of non-agglutinability in red cells by either homologous or heterologous fresh virus, and (b) to separate individual steps involved in the mechanism of infection of susceptible host cells. The implications of these findings are discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7acfcfd2617a2b1738c72e87023b120001a5adfb", "title": "STUDIES ON THE TOXICITY OF INFLUENZA VIRUSES : I. THE EFFECT OF INTRACEREBRAL INJECTION OF INFLUENZA VIRUSES.", "authors": ["Gertrude Henle", "Werner Henle"], "date": 1946, "abstract": "Intracerebral injection of preparations of influenza viruses into mice led to tonic and clonic convulsions and death in tetanus, usually within 24 to 72 hours. Histological examination revealed the destruction of the ependymal lining of the ventricles as the dominant finding. These reactions were obtained in four different strains of mice as well as in rats, guinea pigs, and hamsters. They were observed in mice after injection of mouse and egg-adapted virus, and all strains of virus tested gave these responses as long as sufficient quantities of the agents were injected. Control materials, such as normal allantoic fluids, particulate components of normal chorio-allantoic membranes, and suspensions of normal murine lungs, gave uniformly negative results. Activation of a latent virus as well as the inadvertent admixture of a neurotropic agent to the influenza cultures was excluded. Propagation of the influenza viruses in the central nervous system could not be demonstrated and, in fact, the agents were no longer demonstrable in 4 days. It was concluded that the observed neurological reactions were the result of toxic activities rather than of virus propagation. Comparison between the infectivity, hemagglutinating capacity, and the toxic activity showed that preparations with the higher titers of virus and hemagglutinin were also the more toxic ones. The toxic agents could not be separated from the infectious particles by the use of such technics as differential high speed centrifugation, adsorption onto and elution from chicken red cells, and precipitation by protamine. The toxic effect of influenza A virus preparations was specifically neutralized by anti-influenza A and not by anti-influenza B serum, and conversely. In addition, antigenic differences were noted between two strains of influenza A virus by this method of testing.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "43585a4dca70342817c9b0f8ffe3d4e95cd2fe17", "title": "Studies on adenoviruses (APC-RI-ARD) in tissue culture. Correlation between the amount of virus inoculated and the time needed for production of cellular degeneration", "authors": ["Lars Kjell{\\'e}n"], "date": 2005, "abstract": "A linear relationship between incubation period (time after inoculation of virus before cytopathic effect) and log concentration of seed virus has been demonstrated for some adenoviruses in tissue culture. Different strains and even different lines of the same strain of virus showed somewhat varying regression coefficients. The addition of tryptose to the medium increased the degeneration rate of one of the virus lines.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "79879563fcd71eef274a655a1cfc4af4548a5105", "title": "STUDIES ON THE TOXICITY OF INFLUENZA VIRUSES", "authors": ["Werner Henle", "Gertrude Henle"], "date": 1946, "abstract": "Upon intra-abdominal or intravenous injection of allantoic fluids infected with influenza viruses, mice frequently died within 8 to 96 hours. Similar results were observed upon injection of rabbits, rats, and guinea pigs. Autopsy of the mice revealed widespread necrosis of liver and spleen, hemorrhages into the intestines, pleural exudation, and other occasional findings. Survivors frequently developed pulmonary consolidation or jaundice. The dominant type of lesion depended on the strain of virus used. All attempts to demonstrate propagation of the influenza viruses outside of the respiratory tract failed. It was concluded that the early lesions were the result of toxic activities of the virus and not of virus multiplication in the affected tissues. Injection into chick embryos of highly diluted inocula produced higher titers of virus, hemagglutmin, and toxicity in the allantoic fluids than the use of more concentrated seed culture. Serial passage of various strains in high dilution frequently increased the toxic activity. The infectivity often reached its peak in 24 hours when tests for toxicity were still negative. Maximal toxicity was usually not attained before 48 hours. The toxic activity could not be separated from the infective property by such means as differential centrifugation and adsorption onto and elution from chicken red cells. However, upon heating, formalinization, and irradiation with ultraviolet light, the ability of the agents to propagate was lost at a faster rate than the toxic property. The toxic property remained stable for 2 to 3 months at 4\u00b0C. This stability was comparable to that of the infectivity for chick embryos. Specific immune sera neutralized in high dilution the toxic activity of the homologous virus. Non-specific neutralization occurred in low dilutions of normal and heterologous immune sera. Strain differences were indicated by this method of testing. Vaccination of mice by the subcutaneous or intra-abdominal routes protected mice specifically against the toxic effects of intra-abdominally or intravenously injected preparations of virus.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "8d487fd63f715640f118cb66f2eab23ec0379ae9", "title": "Outline of a conceptual semantics for computer generation of coherent discourse", "authors": ["Roger C. Schank"], "date": 1969, "abstract": "Abstract This article develops a method for generating coherent sentences. A conceptual semantics is presented that, when coupled with a conceptual dependency abstraction of meaning, allows concepts to be linked in a manner consonant with the system's knowledge of the world. The article is part of a series concerned with the problem of language synthesis for artificially intelligent systems.", "references": [], "page_rank": 0.00014778325123152708}, {"id": "76611284eee4fa4e8c21670c371a0c9c2cabbbd0", "title": "Two Results on Ordering for Resolution with Merging and Linear Format", "authors": ["Raymond Reiter"], "date": 1971, "abstract": "Resolution with merging, linear format, and set of support is shown to be com- patible with an A-ordering restriction; in any resolution operation under a linear format, the literal resolved upon in the near parent must be a maximal literal in that clause. The far parent must either be a merge, in which case the literal resolved upon must be a merge literal, or a member of the original set of clauses. A new kind of ordering, called C-ordering, is introduced and shown to be compatible with merge, linear format, and set of support resolution. Its advantage over A-ordering is that the literal to be resolved upon in the near parent is uniquely specified.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b35aa4d90f7368fefaf05ca94f76edf03134eee7", "title": "A linear format for resolution", "authors": ["Donald W. Loveland"], "date": 1970, "abstract": "The Resolution procedure of J. A. Robinson is shown to remain a complete proof procedure when the refutations permitted are restricted so that clauses C and D and resolvent R of clauses C and D meet the following conditions: (1) C is the resolvent immediately preceding R in the refutation if any resolvent precedes R, (2) either D is a member of the given set S of clauses or D precedes C in the refutation and R subsumes an instance of C or R is the empty clause, and (3) R is not a tautology.", "references": ["05b44597834f6df07c1c1290fb33a979bdf99067", "f516d9dd73b117eca287bd3a07c2f3d26523b765", "9e15384995e5e8b0bd8ceb197804b37285a77c70"], "page_rank": 0.0006020799124247399}, {"id": "490763046ed9b28245440ccdcfa2a0fc46375d7a", "title": "FURTHER STUDIES OF THE INFECTIOUS UNIT OF VACCINIA", "authors": ["Robert F. Parker", "Lewis H. Bronson", "Robert H. Green"], "date": 1941, "abstract": "A study has been made of the comparative virulence of several strains of vaccine virus for a number of hosts, and wide variation in animal susceptibility has been demonstrated. The results obtained in experiments with a chick-embryo-adapted strain are interpreted as indicating that the particles of virus are of essentially uniform virulence. Results of statistical analyses are presented which indicate that as the virulence of a strain of virus increases the number of elementary bodies per infectious unit approaches 1, and at that limit the chance of infection is governed primarily by the presence or absence of virus in the inoculum. With lower virulence the chance of a lesion following inoculation of virus is still described by the binomial theorem, but the actual distribution is primarily of susceptible cells not of viral particles. It is postulated that with regard to the proportion of cells available for parasitism, differences exist between different animals of a species, and that this distribution is of a normal character.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a1bab6a5b88790ccb28c8d749dbe345dad5837f4", "title": "Compatibility and Complexity of Refinements of the Resolution Principle", "authors": ["Richard B. Kieburtz", "David C. Luckham"], "date": 1972, "abstract": "This paper studies a number of logically complete search strategies (refinements) for improving the performance of automatic theorem-proving programs based on the resolution principle. These strategies restrict the number of deductions generated by the program at the expense of sometimes missing the shortest proof.By considering elementary proof-preserving transformations on resolution proof trees, (i) it is shown that the conjunction of set-of-support, resolution-with-merging, and linear form deduction is again a complete refinement; (ii) bounds are obtained on the possible increase in complexity of the proof trees when the linear form and resolution-with-merging refinements are imposed.Finally, examples are given which demonstrate the savings in time and storage when refinements are used to prove some theorems of moderate difficulty in group theory and ternary boolean algebra.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "11094935eac233f71e592906bddc7bee17367aa7", "title": "A model for a fact retrieval system", "authors": ["Roger W. Elliott"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"A model for a fact retrieval system\" by Roger W. Elliott", "references": [], "page_rank": 0.00016420361247947453}, {"id": "f0843619ac5937c2267cdacd2992aad219b32bcc", "title": "SCENE ANALYSIS USING THE CONCEPT OF MODEL", "authors": ["Adolfo Guzm{\\'a}n"], "date": 1967, "abstract": "Abstract : A symbolic notation (FDL-1) for the description of pictures composed of rectilinear segments is developed. Visual objects, aggregates of objects (scenes) and generalized classes of objects (models) may be expressed in this notation. A program is described which, given a scene S and a model of an object O, finds all instances of O in S. (O and S are expressed in FDL-1). The program, written in the language CONVERT, can identify overlapping objects when they are transparent. Examples are given.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9beb5df44f0d98fe676710b3e8b5bca78c593ca6", "title": "A completeness theorem and a computer program for finding theorems derivable from given axioms", "authors": ["Char-Tung Lee"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"A completeness theorem and a computer program for finding theorems derivable from given axioms\" by Char-Tung Lee", "references": [], "page_rank": 0.0002463054187192118}, {"id": "f10ec15182945fb17baeb5fe1334a9ba501a8a49", "title": "A grammar base question-answering procedure", "authors": ["Peter S. Rosenbaum"], "date": 1967, "abstract": "The letters we have generated were drawn o~ a specific machine, the Stromberg.-Carlson 4020 microfilm printer. A. brief description of the SC 4020 will assist in using the rector letters with other machines. The SCC 4020 has two modes of operation. First, it can draw vectors, which can start at aw raster point on its 1024 X 1024 grid and extend up to 64 grid spaces in either or both x and y directions. Secondly, it can produce a total of 64 different characters by shaping the electron beam with an appropriate nmsk in the cathode-ray tube. One character is a dot. This mode allows one to construct shapes using closely spaced dots, or any other available character, as building blocks. In the type fonts described here, only the vector *~iode ~>i operatior~, was used. Measurement of the widtt~ o:l i.i~, vector indicates that it is equal to 2.3 grid spaces. 'fi~ ... means that a character which is 23 grid spaces high t~a:~ :~ resoluti which will be numerically described and computer dra.~-;,~. The generality of the representation is clear from the ease with which the vectors can be adapted to other compu}-e>> and other cathode-ray tubes. We believe the fonts wi!! have great utility. The subject of this paper is a procedure for the automatic retrieval of certain segments of stored information, either explicitly or implicitly represented, through questions posed in natural language sentences. This procedure makes use of a sentence recognition device for the class of grammars which will correctly decide between the grammatical and ungram-matical sentences of a natural language. It is possible to make use of a recognition device of this sort for the following reason: Much data is fully expressible as a set of sentences in a natural language, a set which can be exhaustively and exclusively generated by a grammar. Based upon the rules of this grammar, a sentence recognizer will evaluate sentences , questions in the normal situation. Since the recognition function succeeds just in case the posed question is drawn from the set of sentences expressing the data, or, more correctly, is grammatical in terms of the grammar for this set of sentences, sentence recognition itself is a procedure for retrieving information. When the recognition \u2026", "references": [], "page_rank": 0.00016420361247947453}, {"id": "d075466245c0a58a6c2c98198ae2c6d937b0af11", "title": "The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference", "authors": ["John W. Backus"], "date": 1959, "abstract": "This paper gives a summary of the syntax and interpretation rules of the proposed international algebraic language put forward by the Zurich ACM-GAMM Conference, followed by a formal , complete presentation of the same information. Notations are presented for numbers, numerical variables, Boolean variables , relations, n-dimensional arrays, functions, operators and algebraic expressions. Means are provided in the language for the assignment of values to variables, conditional execution of statements , iterative procedures, formation of compound statements from sequences of statements, definition of new statements for arbitrary procedures, and the re-use and alteration of program segments. The proposed language is intended to provide convenient and concise means for expressing virtually all procedures of numerical computation while employing relatively few syntactical rules and types of statement. La syl1taxe et la semantique de langage algebraic international propose par la Conference de Zurich (ACM et GAMM). L'autcur caracterise brievement la syntaxe et les regles d'inter-pretation du langage algebrique international propose a la Conference de Zurich (ACM-GAMM) puis en donne un expose formel et complet. II indique les notations utilisees pour designer les nombres, les variables numeriques ou booIeennes, les relations, les agencements pluri-dimensionnels, les fonctions, les operateurs et les expressions algebriques. Ce langage permet d'exprimer difierentes operations: affectation de valeurs aux variables, execution conditionnelle des expressions, procedes iteratifs, formation d'expressions complexes a partir d'une suite d'expressions elementaires, definition de nouvelles expressions pour des operations arbitraires, reemploi et modification de certaines parties du programme. Le lang age envisage est conyu pour permettre d'exprimer la quasi totalite des procedes de calcul numerique de maniere com-mode et concise a l'aide d'un nombre relativement restreint de regles de syntaxe et d'expressions-types.", "references": ["52219ea02e97f867892435f023215d31cdcc5fa2"], "page_rank": 0.000541871921182266}, {"id": "3872994b05c54c8b589a1cabcc48e19fc7f8a107", "title": "Recent issues in semantic theory", "authors": ["Jerrold J. Katz"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Recent issues in semantic theory\" by Jerrold J. Katz", "references": [], "page_rank": 0.00016420361247947453}, {"id": "7a36d6f67916a37bd9d1076be19319fb42df175c", "title": "Linear Reasoning. A New Form of the Herbrand-Gentzen Theorem", "authors": ["William Craig"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"Linear Reasoning. A New Form of the Herbrand-Gentzen Theorem\" by William Craig", "references": [], "page_rank": 0.0002463054187192118}, {"id": "056fa6bd2461b8fd978b66fc452662dfdcfbaadf", "title": "An artificial intelligence program to advise physicians regarding antimicrobial therapy.", "authors": ["Edward H. Shortliffe", "Stanton G. Axline", "Bruce G. Buchanan", "ThomasC. Merigan", "Sanford N. Cohen"], "date": 1973, "abstract": "Abstract An antimicrobial therapy consultation system has been developed which utilizes a flexible representation of knowledge. The novel design facilitates interactive advice-giving sessions with physicians. An ability to display reasons for making decisions at the request of the user permits the program to serve a tutorial as well as consultative role. The feasibility of the judgmental rule approach which the program uses has been demonstrated with a limited knowledge base of approximately 100 rules. Its ultimate success as a clinically useful tool depends upon acquisition of additional rules and thus upon co-operation of infectious disease experts willing to improve the program's knowledge base. The techniques for acquisition, representation, and utilization of knowledge, plus considerations of natural language processing, draw upon and contribute to current Artificial Intelligence research.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "a99216a454ab727d3ea08076b70a638822d17585", "title": "Computer-assisted digoxin therapy.", "authors": ["Carl C Peck", "Lewis B. Sheiner", "Carroll M. Martin", "D. T. Combs", "Kenneth L. Melmon"], "date": 1973, "abstract": "Abstract In 42 patients requiring digitalis, and randomly divided into two groups, the performance of a computer program using patient size and renal function to compute digoxin dosage was compared to that of unaided physician judgment. Serum digoxin concentrations were measured repeatedly. Efficacy was measured by changes in the manifestations of heart failure, and toxicity by electrocardiographic criteria. For each patient, physicians specified a desired serum digoxin concentration and predicted this concentration at each visit. For one group, the computer program suggested the dosage needed to achieve the desired digoxin concentration. Efficacy was the same in both groups, and there was no toxicity. Although the computer slightly outperformed the physicians, prediction and achievement errors were unacceptably large. Hence, much between-patient variability in serum digoxin concentrations remains unexplained after adjustments for dose, body size and renal function. This argues for measurement of digoxin ...", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "aa649fd05903ba3aa73923172b7a10c922122e31", "title": "A computer-based medical-history system.", "authors": ["Warner V. Slack", "G. Phillip. Hicks", "Charles E. Reed", "L J Van Cura"], "date": 1966, "abstract": "IN spite of the homage devoted to the importance of the medical history, there has been remarkably little research on the subject. Neither the method of history taking and recording nor the reliability and usefulness of the data collected has been studied as rigorously as the other tools of clinical medicine, in large part because neither the method nor the data lend themselves well to research. The traditional method of taking and recording medical histories involves serious problems for both the practicing physician and the clinical research worker. History taking consumes a large amount of the physician's time, and inadequate .\u00a0.\u00a0.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "37701059b94920c0c93d1e46c0b133eef5eebf1e", "title": "Decision analysis as the basis for computer-aided management of acute renal failure.", "authors": ["G. Anthony Gorry", "Jerome P. Kassirer", "Alvin Essig", "William B. Schwartz"], "date": 1973, "abstract": "Abstract In recent years many attempts have been made to use the computer as an aid to diagnosis, but little has been done to exploit the potential of computer technology as a more general aid to decision making. We describe the use of the discipline of decision analysis as the basis for an experimental interactive computer program designed to assist the physician in the clinical management of acute oliguric renal failure. The program deals with alternative courses of action, either tests or treatments, for which the potential risks or benefits may be large, and it balances the anticipated risk of a given strategy against the anticipated benefit that it offers the patient. The appraisals of the different courses of action open to the physician are expressed in quantitative terms as expected value. The program has been evaluated by comparing its recommendations to those of experienced nephrologists in 18 simulated cases of acute oliguric renal failure. Agreement between the nephrologists and the program was found in more than 90 per cent of cases, but the experiments identified a series of problems that must be resolved if the program is eventually to be widely useful as a \"consultant.\" For example, it will be necessary to develop strategies for dealing with multiple diseases occurring simultaneously, with signs and symptoms that frequently are not independent of one another, with changes in the pattern of a disease over time, and with the weighing of priorities for carrying out tests and treatments. We conclude that computer-aided management with the aid of decision analysis is a promising area for further investigation.", "references": ["16b48bbc8f7d8dd9fa9ecf75e7ed50f856d1b5f1", "009dd583a81b8186273573e38c38b6d04ab2af72", "0c54da0d933bb8594cc373d1d38628bc0e7656c4", "a976694da59b8de7461413b749f37966f027d220"], "page_rank": 4.926108374384236e-05}, {"id": "33fcb6d718d34927fd1941da5d442ddd0b77b107", "title": "Discrimination of complex stimuli: the relationship of training and test stimuli in transfer of discrimination.", "authors": ["Kenneth H. Kurtz"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"Discrimination of complex stimuli: the relationship of training and test stimuli in transfer of discrimination.\" by Kenneth H. Kurtz", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "99d99c1f79b0b61238fafe5e1fd2a9d624be4fe1", "title": "Memory mechanisms and the theory of schemata.", "authors": ["Richard Charles Oldfield"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Memory mechanisms and the theory of schemata.\" by Richard Charles Oldfield", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "f51adec455670fa0e926bc82e36026ae8320b9f4", "title": "HEAD'S CONCEPT OF THE SCHEMA AND ITS APPLICATION IN CONTEMPORARY BRITISH PSYCHOLOGY", "authors": ["R. C. Oldfield", "O. L. Zangwill"], "date": 1942, "abstract": "Semantic Scholar extracted view of \"HEAD'S CONCEPT OF THE SCHEMA AND ITS APPLICATION IN CONTEMPORARY BRITISH PSYCHOLOGY\" by R. C. Oldfield et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "313630eaeb2c362f623d24538f64cb3a49e94e31", "title": "Stimulus predifferentiation: some generalizations and hypotheses.", "authors": ["Malcolm D. Arnoult"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"Stimulus predifferentiation: some generalizations and hypotheses.\" by Malcolm D. Arnoult", "references": [], "page_rank": 0.0002627257799671592}, {"id": "c4d52202de4d7279ff0fdf8b1e9ed09143401aa2", "title": "Lattice Theory Revised Edition", "authors": ["Garrett Birkhoff"], "date": 1948, "abstract": "Semantic Scholar extracted view of \"Lattice Theory Revised Edition\" by Garrett Birkhoff", "references": [], "page_rank": 0.00016420361247947453}, {"id": "8eb404d1b5d73e78fe0c272e42c42a52811e02cc", "title": "Contributions to the theory of models. III", "authors": ["Alfred Tarski"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Contributions to the theory of models. III\" by Alfred Tarski", "references": [], "page_rank": 0.00016420361247947453}, {"id": "1d671b87a0ec0553b73a84215d98c22c909afba3", "title": "Physical determinants of the judged complexity of shapes.", "authors": ["Fred Attneave"], "date": 1957, "abstract": "Many psychological tasks vary in difficulty with the complexity of the stimulus objects involved. Complex visual objects are not only harder to reproduce from memory than simple ones (4, 6), but also harder to learn by name (4, 7) and to match (11). Complexity is an ill-defined variable, however. No two of the experiments referenced above employ exactly the same operations of physical measurement , and an essential communality between them is not easy to specify in objective terms. In the study reported here, ratings of the complexity of nonrepresenta-tional shapes were obtained from a large number of Ss and related to measurable physical characteristics of the shapes. The results not only have interest in their own right, but also serve to indicate the physical variables most likely to be relevant to other tasks, like those initially mentioned , on which data are typically harder to get and less precise. The relationship of judged complexity to informational content, \"degrees of freedom,\" compactness, and certain other variables will be considered. METHOD Materials.-\u2014The stimuli were 72 \"random\" shapes, each constructed by the following general method. In a k X k matrix, n random points were plotted, i.e., all coordinates of the points were random numbers between 1 and k. The points were then connected randomly into a polygon of n sides. This connecting process involved two steps. First, peripheral points were connected into a convex polygon which enclosed all the points not included in its contour (as if a pin were stuck into each point and a rubber band snapped around the whole cluster). Second, the unconnected points were given a random order and each in turn was \"taken into\" a randomly chosen segment of the surrounding polygon (as if by hooking that segment of the rubber band over the interior pin). Since lines connecting the points were not permitted to cross, the number of alternative segments into which a given point might be taken could either increase or decrease as the process continued; hence the assignment of a random sequence to the unconnected points. Connections which placed certain points outside the polygon were permitted, though in consequence the ways in which such points could themselves be connected were restricted. Some of the polygons thus constructed were further developed into curved figures by replacing angles with inscribed arcs of random 221", "references": ["0094175f4e0e341af2e045734664c9bd5404471d", "6d0198460198fdb49b89d1646049712b3a0683df", "8fe5a31ea4ce332a9cd77259b98ba8e1bfdf564a", "4aca5c719d51b774aaca565bc07bd013dd28655c", "d41a0482e206e247057b49a857b141603e74c6bd", "18c522cce1542e6673baceedab5fc6c6983a6126", "4b0eef44868093912fb54c2e9f2d3653691be145", "1dea2c1ba796b15e75abdf69c1f96915323b2ebc"], "page_rank": 0.00023457658925639217}, {"id": "0094175f4e0e341af2e045734664c9bd5404471d", "title": "The quantitative study of shape and pattern perception.", "authors": ["Malcolm D. Arnoult", "Fred Attneave"], "date": 1956, "abstract": "The pre-eminent importance of formal or relational factors in perception has been abundantly demonstrated during some forty years of gestalt psychology. It seems extraordinary, therefore, that so little progress has been made (and, indeed, that so little effort has been expended) toward the systematizing and quantifying of such factors. Our most precise knowledge of perception is in those areas which have yielded to psychophysical analysis (e.g., the perception of size, color, and pitch), but there is virtually no psychophysics of shape or pattern. Several difficulties may be pointed out at once: (a) Shape is a multidimensional variable, though it is often carelessly referred to as a \"dimension,\" along with brightness, hue, area, and the like, (b) The number of dimensions necessary to describe a shape is not fixed or constant, but increases with the complexity of the shape, (c) Even if we know how many dimensions are necessary in a given case, the choice of particular descriptive terms (i.e., of referenceaxes in the multidimensional space with which we are dealing) remains a problem; presumably some such terms have more psychological meaningfulness than others.", "references": ["1d671b87a0ec0553b73a84215d98c22c909afba3", "6d0198460198fdb49b89d1646049712b3a0683df", "0e9b804adcedfe82d5139b572c5c165fa161efb7", "6e33fd816697dca154e394067c9912ccbddbc6e0", "2e8136581d934e18eb17d124f0224a32b01272d5", "ed31282548e34b04a9c27ad6dafb81e43aa17447", "b3aab5b387de1acc3f25f20a3e887e5f321b59d7", "e7269ba40686ddc3c38ff5240fe26fd2e829831b", "0fcc7bd70d460bfc050d0936df79eef221994ec3"], "page_rank": 0.0003330987567440769}, {"id": "4e2374294ac9a5961cdca1f1164cc6ee218ad258", "title": "FRAGMENTS OF MANY-VALUED STATEMENT CALCULI", "authors": ["Alan Rose", "J. Barkley Rosser"], "date": 1958, "abstract": "Semantic Scholar extracted view of \"FRAGMENTS OF MANY-VALUED STATEMENT CALCULI\" by Alan Rose et al.", "references": ["107dd7e51dee6d83a0f4782333d708c788cbfc77", "bfcd62b8887306333fdfe9f4c409381cfd475329", "eb40339b13500645ee627337d2f4ea0663c8f0b0"], "page_rank": 0.00016420361247947453}, {"id": "230da6a0aee8f87d036c8339438b96aa3c203603", "title": "Computer-assisted interview of patients with functional headache.", "authors": ["William W. Stead", "Albert Heyman", "Howard K. Thompson", "William Edward Hammond"], "date": 1972, "abstract": "A computer-based, self-administered, interactive questionnaire has been designed for interview and diagnosis of patients with functional headaches. Patients respond on a simple keyboard to a sequence of questions on a cathode-ray oscilloscope. The system selects pertinent questions depending on the patient's individual responses. The questionnaire included clinical symptoms, neurological manifestations, prior treatment, emotional factors, and personality problems. Upon completion, a computer-generated printed summary is presented for the clinical record. The system utilizes the acquired data to differentiate between common, classical, and cluster migraine, muscle contraction, and other types of headache. Computer diagnoses agreed with those of the physician in 36 of 50 patients. The automated interview saves physician's time, offers a data base for research, and provides a diagnostic aid for patients with functional headaches.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "3490c242c3d6b5a735792a75058de0d108d04731", "title": "DIMENSIONS OF REPRESENTATION", "authors": ["Daniel G. Bobrow"], "date": 1975, "abstract": "Publisher Summary This chapter focuses on the structure of alternative solutions to the design issues and illustrates the design options through three specific representations. It is often convenient and sometimes necessary to use several different representations within a single system. In this way, it is sometimes possible to combine the advantages of different representational forms within one system. The use of multiple representations leads to two primary problems\u2014choice and consistency. Representations determine the ease of answering certain questions, and of performing updating operations. At times, it is best to enter information directly into one representational form and then, from there, compute the way to enter it into the other form. Thus, an object's position might first be entered by its coordinates, and then its position relative to all others is computed and inserted into the appropriate representation. In a system with multiple representations, the same information can be stored in more than one form. When one form is changed, the other forms must be checked for consistency.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "64ad6855a9a16bca3278d5dd6a7cd742e480a8c3", "title": "Computer-based consultation. Electrolyte and acid-base disorders.", "authors": ["Howard L. Bleich"], "date": 1972, "abstract": "Abstract A computer program has been developed to assist the physician in managing patients with electrolyte and acid-base disorders. The program directs a dialogue during which the user communicates with the computer and supplies whatever clinical and laboratory information may be available. Based upon the abnormalities presented the program asks further questions as needed to characterize the electrolyte and acid-base disturbances, and it then produces an evaluation note that varies in length from four words to three pages, depending upon the complexity of the abnormalities detected and the completeness of the information available. When appropriate the evaluation note includes an explanation of the pathophysiology, a list of diagnostic possibilities, general and specific therapeutic recommendations, precautionary measures required by the illness or by its treatment, suggestions for additional laboratory studies and references to the medical literature. The time required to enter the data and to obtain the evaluation note is approximately seven minutes; the cost is thought to be reasonable when compared with the usefulness of the information provided.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "56e27eea812ff76e90cfabf511d2501361f22ff3", "title": "Toward A Model Of Children''s Story Comprehension", "authors": ["Eugene Charniak"], "date": 1972, "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1972. Ph.D.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "91a267cf5f202607bc0df597ae1c5098f4361603", "title": "Computer evaluation of acid-base disorders.", "authors": ["Howard L. Bleich"], "date": 1969, "abstract": "With the advent of electronic computers that operate in the time-sharing mode, it has become possible to develop an automated system that can assist a physician in solving clinical problems. In the present study a teletype terminal has been linked to a time-sharing computer which has been programmed to evaluate clinical and laboratory information concerning patients with acid-base disorders. The program checks the data for evidence of internal consistency and requests additional information as needed to solve the acid-base aspects of the clinical problem. If sufficient information is provided, the program generates an evaluation note designed to review with the physician the pathophysiology of the disorder and to assist him in its management. If the input data are incomplete, the program draws the most useful conclusions possible based on the data provided, specifies the limitations which pertain to these conclusions, suggests further studies designed to circumvent these limitations, and while awaiting the results, suggests appropriate interim therapeutic measures. The time required to enter a patient's data and to print the evaluation note is approximately 4 min; the cost is comparable to that of many laboratory tests.", "references": [], "page_rank": 0.0002134646962233169}, {"id": "f16418fbaf212b8c6d6b911812fec8bfe0d255b3", "title": "CONCEPTS FOR REPRESENTING MUNDANE REALITY IN PLANS", "authors": ["Robert P. Abelson"], "date": 1975, "abstract": "Publisher Summary This chapter discusses the scripts, because theory requires a good foundation at the script level. The simplest relevant conceptual script content includes the puuposeful transactional activities of social actors, such as communication with others, enlisting the help of others, and a set of specifications of the realities constraining the possibilities of action\u2013knowledge of transportation and communication systems, the necessary properties of objects, etc. The major type of script is a plan, specified by the actor, the goal state, and the sequence of intended steps to reach the goal. With the aid of conceptual dependency formalism, a small set of primitive act concepts is sufficient to represent almost any describable event. Acts as steps-in-plans have two special features distinguishing them from acts as events-in-reality. Both features stem from the intentionality of plans. First, the reason why low-level primitives such as GRASP are not the most useful components in the cognition of plans is that for human actors they are usually not problematic\u2014they are taken for granted by both planner and observer. The second consideration is closely related to the first. Acts as steps-in-plans are characterized by their intended effects rather than by their physical nature.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e5bcd47142a2eafb2f9ba80d0f885a218756c4d9", "title": "In and Out the Garbage Pail", "authors": ["Frederick S. Perls"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"In and Out the Garbage Pail\" by Frederick S. Perls", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "80d8c8e4ae969e5564c7b574e6772c012940a535", "title": "Interactive Computer Programs for Euciting Personal Models of the World", "authors": ["Mildred L. G. Shaw}"], "date": 1940, "abstract": "The Centre for the Study of Human Learn is interested in encouraging self-organisation in learning by helping people to investigate, expand and rebuild models for construing which will enable them to be more successful learners and users of experience. This paper describes how conversational methods are used which are content free, and which lend themselves superbly well to the real-time data processing of a computer. The application of these model building facilities has been in areas such as learning skills; psychotherapy and 'becoming'; management selection and development; industrial inspection and quality control; art and architecture; the maintenance of electronic equipment; career guidance and the training of counsellors; and in the education of both children and teachers. Conversations may take pJ ace between two , in a group of people, or within one person such as s' (1969) 'top dog' and f \" or Pask's (1973) Ip Indi-viduals'. Conversational tics have been embodied in content free computer programs which have the capacity to encourage and control conversation as rigorously and systematically as traditional experimental methods are monitored and controlled. In this context the Centre can be seen as a psycholo tool-making unit. The Programs The repertory grid is used as a conversational tool to help people to become more aware of the patterns of tllOUght and feeling implicit in their responses. The FOCUS program takes a completed repertory grid and reorders it for talk-back purposes. The elements and constructs are sorted in such a way as to highlight the of responses in terms of the similarities and FOCUS is described later in more detail. PEGASUS is an interactive program which elicits a grid Using a conversational heuristic. Feedback commentary is giVen immediately the responses are entered. Again, it will be described in detail later.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "9e15384995e5e8b0bd8ceb197804b37285a77c70", "title": "Automatic Deduction with Hyper-Resolution", "authors": ["John Alan Robinson"], "date": 1983, "abstract": "Semantic Scholar extracted view of \"Automatic Deduction with Hyper-Resolution\" by John Alan Robinson", "references": [], "page_rank": 0.0002463054187192118}, {"id": "7ccb03436e4302b8f9cc5d4b1916034a3afe3644", "title": "Conceptual structure in thought-disordered schizophrenics.", "authors": ["Donald W. Bannister"], "date": 1960, "abstract": "Semantic Scholar extracted view of \"Conceptual structure in thought-disordered schizophrenics.\" by Donald W. Bannister", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f276b4124b18f0c0f7dbfd3253e6eb9fc042054d", "title": "A grid test of schizophrenic thought disorder.", "authors": ["Donald W. Bannister", "Fay Fransella"], "date": 1966, "abstract": "In two previous studies (Bannister, 1960 and 1962) thought disordered schizophrenics were discriminated from normals and other psychiatric groups (including non-thought disordered schizophrenics) by forms of the repertory grid test. The primary characteristics of thought disordered schizophrenics, in terms of such tests, appear to be their failure to manifest substantial intercorrelations between constructs and their inability to maintain in a second grid the specific pattern of intercorrelations found in the first. In Construct Theory terms (Kelly, 1955; Bannister, 1962a) schizophrenics are limited to an overly loose and inconsistent subsystem for construing people, in conventional terms their ideas about people are both poorly related and unstable. \n \n \n \nThe forms of repertory grid test used in the two previous studies are not suitable for clinical purposes in that they are cumbersome and lacking in normative data. The purpose of the present study was to produce a clinically economic and adequately standardised grid test for detecting the presence of schizophrenic thought disorder.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "ead6b050ecd88030bef12e6555c8c8c15a0b674f", "title": "Conversational heuristics for eliciting shared understanding", "authors": ["Mildred L. G. Shaw"], "date": 1979, "abstract": "A conversational method is necessary for experimenter and subject to collaborate in the exploration of the world of human beings. Individuals cannot be treated as objects, or be instructed how to take part in an experiment, without the recognition of the autonomy of each person and the invitation to participate jointly in co-operative exploration of the nature of man. An individual can be seen as a personal scientist who forms theories about the world and tests these theories against his personal experience of reality, adapting his theories for a more effective anticipation of events and hence a more competent interaction with his environment. A suite of computer programs (PEGASUS, FOCUS, MINUS, CORE, ARGUS and SOCIOGRIDS) has been developed, each one acting as a cybernetic tool to enhance man's capabilities to understand both himself and his relationships with other perspectives of the world. PEGASUS is described, including PEGASUS-BANK which can be used to explore the relationship of an individual with another individual (or group). The CORE program can be used to chart change in a person over time, and to find the level of understanding and agreement between two people. Shared understanding within small groups can be investigated using the SOCIOGRIDS program which produces a mapping of the intra-group relationships, and the subject content which shows the extent of agreement in the group. A study involving the exchange of subjective standards in human judgement is briefly described, and an analogy drawn to the understanding of different perspectives in the treatment of a medical or clinical patient.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "a130daf60ae16581744d0051d8197c9fc7ce653c", "title": "Getting the Envisionment Right", "authors": ["Benjamin Kuipers"], "date": 1982, "abstract": "The central component of commonsense reasoning about causality is the envisionment: a description of the behavior of a physical system that is derived from its structural description by qualitative simulation. Two problems with creating the envisionment are the qualitative representation of quantity and the detection of previously-unsuspected points of qualitative change. The representation presented here has the expressive power of differential equations, and the qualitative envisionment strategy needed for commonsense knowledge. A detailed example shows how it is able to detect a previously unsuspected point at which the system is in stable equilibrium.", "references": ["6813e5400681a1704c4c4aef2cb7a805fa99c30b", "e5b9efabfef885ec48528c4d1c9e1e506a193d57"], "page_rank": 5.473453749315818e-05}, {"id": "3087b4b62d15d89a69b5764c0591a1c438091a94", "title": "Rediscovering Physics with BACON.3", "authors": ["Pat Langley"], "date": 1979, "abstract": "BACON.3 is a production system that discovers empirical laws. The program uses a few simple heuristics to solve a broad range of tasks. These rules detect constancies and trends in data, and lead to the formulation of hypotheses and the definition of theoretical terms. BACON.3 represents data at varying levels of description, where the lowest have been directly observed and the highest correspond to hypotheses that explain everything so far observed. The system can also run and relate multiple experiments, collapse hypotheses with identical conditions, ignore differences between similar concepts, and discover and ignore irrelevant variables. BACON.3 has shown its generality by rediscovering versions of the Ideal gas law, Kepler's third law, Coulomb's law, Ohm's law, and Galileo's laws for the pendulum and constant acceleration.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "2fcf66998c4b67b389627fa2aaa31a103cbde102", "title": "Measurement Interpretation in Qualitative Process Theory", "authors": ["Kenneth D. Forbus"], "date": 1983, "abstract": "Interpreting measurements of physical systems consists in part of constructing an account of \"what's happening\" in terms of our commonsense physical theories. Since most systems involve change, qualitative dynamics plays a central role in such deductions. This paper presents a theory of measurement interpretation at an instant, based on Qualitative Process Theory. Appropriate notions of measurement and interpretation are defined and the computational issues involved in constructing interpretations are examined. After describing an algorithm and illustrating its use by example, possible extensions to interpreting measurements over time will be discussed.", "references": ["46f41bcaf5d69e3586571c6b8c91f525096726f5"], "page_rank": 5.473453749315818e-05}, {"id": "6eaa91c4a12b5ae2ade897179a1b96bbb4c9a698", "title": "Markovian representation of stochastic processes and its application to the analysis of autoregressive moving average processes", "authors": ["Hirotugu Akaike"], "date": 1974, "abstract": "SummaryThe problem of identifiability of a multivariate autoregressive moving average process is considered and a complete solution is obtained by using the Markovian representation of the process. The maximum likelihood procedure for the fitting of the Markovian representation is discussed. A practical procedure for finding an initial guess of the representation is introduced and its feasibility is demonstrated with numerical examples.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "e78fea4c45a983c3d2083799e7d54119be58a217", "title": "Causality and modern science", "authors": ["Mario Bunge"], "date": 1979, "abstract": "The causal problem has become topical once again. While we are no longer causalists or believers in the universal truth of the causal principle we continue to think of causes and effects, as well as of causal and noncausal relations among them. Instead of becoming indeterminists we have enlarged determinism to include noncausal categories. And we are still in the process of characterizing our basic concepts and principles concerning causes and effects with the help of exact tools. This is because we want to explain, not just describe, the ways of things. The causal principle is not the only means of understanding the world but it is one of them.The demand for a fourth edition of this distinguished book on the subject of causality is clear evidence that this principle continues to be an important and popular area of philosophic enquiry. Non-technical and clearly written, this book focuses on the ontological problem of causality, with specific emphasis on the place of the causal principle in modern science. Mario Bunge first defines the terminology employed and describes various formulations of the causal principle. He then examines the two primary critiques of causality, the empiricist and the romantic, as a prelude to the detailed explanation of the actual assertions of causal determinism.Bunge analyzes the function of the causal principle in science, touching on such subjects as scientific law, scientific explanation, and scientific prediction. In so doing, he offers an education to layman and specialist alike on the history of a concept and its opponents. Professor William A. Wallace, author of \"Causality and Scientific Explanation\" said of an earlier edition of this work: \"I regard it as a truly seminal work in this field.\"", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "18ade4f5a83e3b1915a3d8d15f092af6ee63c9f9", "title": "The Declarative Representation and Procedural Simulation of Causality in Physical Mechanisms", "authors": ["Chuck Rieger", "Maurice Grinberg"], "date": 1977, "abstract": "A theory of cause-effect representation is used to describe man-made mechanisms and natural laws. The representation, consisting of 10 link types that interconnect events into large declarative patterns, is illustrated on a relatively sophisticated device, the home gas forced air furnace, Next, a procedure and framework for translating the declarative description of a mechanism into a population of associatively triggerable computation units is described. The associative, of procedural, form can then be used to perform a discrete cause-effect simulation of the device. The declarative to procedural translation, including a simulation trace, is shown for the furnace. Topics of mechanism abstraction and mechanism invention are discussed, and the entire \"Mechanisms Laboratory\" is placed in the larger perspective of our research into human problem solving.", "references": ["e494e85bda74b856f20eee75bd274ea74e4861aa", "0dc8b5d3452ddfd26b998cd923f4c46cdd80faca", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "7cfee4257cd03c70151af5ba71915b1d0f608484"], "page_rank": 5.473453749315818e-05}, {"id": "c2dc03a92f03dbbf145a6f8b6568740abb325e19", "title": "Qualitative and Quantitative Knowledge in Classical Mechanics", "authors": ["Johan de Kleer"], "date": 1975, "abstract": "Abstract : This thesis investigates what knowledge is necessary to solve mechanics problems. A program NEWTON is described which understands and solves problems in a mechanics mini-world of objects moving on surfaces. Facts and equations such as those given in a mechanics text need to be represented. However, this is far from sufficient to solve problems. Human problem solvers rely on 'common sense' and 'qualitative' knowledge which the physics text tacitly assumes to be present. A mechanics problem solver must embody such knowledge. Quantitative knowledge given by equations and more qualitative common sense knowledge are the major research points exposited in this thesis. The major issue in solving problems is planning. Planning involves tentatively outlining a possible path to the solution without actually solving the problem. Such a plan needs to be constructed and debugged in the process of solving the problem. Envisionment, or qualitative simulation of the event, plays a central role in this planning process. (Author)", "references": [], "page_rank": 0.0001532567049808429}, {"id": "52219ea02e97f867892435f023215d31cdcc5fa2", "title": "Preliminary report: international algebraic language", "authors": ["Alan J. Perlis", "Klaus Samelson"], "date": 1958, "abstract": "Editor's Notes: Although this method is not novel, it has been printed here to summarize for the benefit of a new generation of computer personnel. I t should be noted that : 1) This method seems advantageous if only a few significant figures are required. Otherwise the normal method, Log-Multiply-Antilog, is more desirable and faster in particular for higher order roots. These subroutines are normally required for other purposes anyway and space is not lost. 2) One immediately notices many tricky ways of coding this method for a computer, via looping and the use of tables or converting instructions. Note that, as one proceeds, the contribution of the left-hand term becomes proportionately large enough such tha t it alone might be used within accuracy limits after a certain number of digits are developed. 3) Although the author states that this method used more memory space than other routines, it seems tha t the converse could well be true if advantage were taken of higher order differences in building up the subtrahend. This appears to be a natural method for a 256 memory machine, if it had good indexing and looping features. Remember that An(X ~) = a constant n[", "references": [], "page_rank": 0.0004926108374384236}, {"id": "157e750a1259d0a5f839bb5cb8779ccb9d7702d6", "title": "A Study of Qualitative and Geometric Knowledge in Reasoning about Motion. Revision.", "authors": ["Kenneth D. Forbus"], "date": 1981, "abstract": "Abstract : Reasoning about motion is an important part of our commonsense knowledge, involving fluent spatial reasoning. This work studies the qualitative and geometric knowledge required to reason in a world that consists of balls moving through space constrained by collisions with surfaces, including dissipative forces and multiple moving objects. An analog geometry representation serves the program as a diagram, allowing many spatial questions to be answered by numeric calculation. It also provides the foundation for the construction and use of a place vocabulary, the symbolic descriptions of space required to do qualitative reasoning about motion in the domain. The actual motion of a ball is described as a network consisting of descriptions of qualitatively distinct types of motion. Implementing the elements of these networks in a constraint language allows the same elements to be used for both analysis and simulation motion. A qualitative description of the actual motion is also used to check the consistency of assumptions about motion. A process of qualitative simulation is used to describe the kinds of motion possible from some state. The ambiguity inherent in such a description can be reduced by assumptions about physical properties of the ball or assumptions about its motion. Each assumption directly rules out some kinds of motion, but other knowledge is required to determine the indirect consequences of making these assumptions.", "references": [], "page_rank": 0.0001532567049808429}, {"id": "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "title": "FRAME REPRESENTATIONS AND THE DECLARATIVE/PROCEDURAL CONTROVERSY", "authors": ["Terry Winograd"], "date": 1975, "abstract": "Publisher Summary This chapter presents some criteria for evaluating ideas for representation. It also presents a rough sketch of a particular version of a frame representation, and discusses the ways in which it can deal with the issues raised. The proceduralists assert that human knowledge is primarily a knowing how. The human information processor is a stored program device, with its knowledge of the world embedded in the programs. The declarativists do not believe that knowledge of a subject is intimately bound with the procedures for its use. They see intelligence as resting on two bases: a quite general set of procedures for manipulating facts of all sorts, and a set of specific facts describing particular knowledge domains. In thinking, the general procedures are applied to the domain-specific data to make deductions. Often this process has been based on the model of axiomatic mathematics. The facts are axioms and the thought process involves proof procedures for drawing conclusions from them.", "references": [], "page_rank": 0.00023594495269372114}, {"id": "eff548e34b8bc36195c69b2ae72de3d62114f060", "title": "Generalized procedure calling and content-directed invocation", "authors": ["Randall Davis"], "date": 1977, "abstract": "We suggest that the concept of a strategy can profitably be viewed as knowledge about how to select from among a set of plausibly useful knowledge sources , and explore the framework for knowledge organization which this implies. We describe meta rules , a means of encoding strategies that has been implemented in a program called TEIRESIAS, and explore their utility and contribution to problem solving performance. Meta rules are also considered in the broader context of a tool for programming. We show that they can be considered a medium for expressing the criteria for retrieval of knowledge sources in a program, and hence can be used to define control regimes. The utility of this as a programming mechanism is considered. Finally, we describe the technique of content-directed invocation used by meta rules, and consider its use as a way of implementing strategies. It is also considered in historical perspective as a knowledge source invocation technique, and its advantage over some existing mechanisms like goal-directed invocation is considered. This work was supported in part by the Bureau of Health Sciences Research and Evaluation of HEW under Grant HS-01544 and by the Advanced Research Projects Agency under ARPA Order 2494. It was carried out on the SUMEX Computer System, supported by the NIH under Grant RR-00785. The views expressed are solely those of the author.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a976694da59b8de7461413b749f37966f027d220", "title": "Symbols, Signals, and Noise: The Nature and Process of Communication.", "authors": ["John R. Pierce"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"Symbols, Signals, and Noise: The Nature and Process of Communication.\" by John R. Pierce", "references": [], "page_rank": 0.0001231527093596059}, {"id": "0c54da0d933bb8594cc373d1d38628bc0e7656c4", "title": "Strategies for computer-aided diagnosis", "authors": ["G. Anthony Gorry"], "date": 1968, "abstract": "Abstract A system consisting of a diagnostic program and a variety of support routines is described. The diagnostic program differs from ones previously reported in several important aspects. The program is based on a model diagnostic problem that subsumes the principal features of a number of real diagnostic problems. The user specializes the program to a specific area by providing information derived from past experience in the area. The program operates in an interactive mode. It performs sequential diagnosis, obtaining additional information from tests that it selects for the user to perform. In evaluating tests, the program considers both the cost of tests and the seriousness of possible misdiagnoses. The inference function employs Bayesian analysis of observed attributes, and can accomodate a variety of interattribute dependencies. The system facilitates the study of a variety of heuristics for various program functions. Some results from the application of the program to a problem in medical diagnosis are presented.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "009dd583a81b8186273573e38c38b6d04ab2af72", "title": "Sequential diagnosis by computer.", "authors": ["G. Anthony Gorry", "G. Octo Barnett"], "date": 1968, "abstract": "A computer program for sequential diagnosis has been developed for use in a variety of problem areas. The program employs sequential decision-making to balance the risk of making a diagnosis against the cost of further testing and the value of the evidence which can be obtained. Basically the program consists of an information structure which describes the problem area, an inference function which interprets signs and symptoms in terms of the medical knowledge in the information structure, and a test selection function which continually reevaluates the potential value of diagnostic tests in the light of increased information about the patient and the costs of tests and possible misdiagnoses.", "references": [], "page_rank": 0.00028735632183908046}, {"id": "754d8ce1fb8511a780a72b120bfad7b7d73a583e", "title": "Transfer Of Rule-Based Expertise Through A Tutorial Dialogue", "authors": ["William J. Clancey"], "date": 1979, "abstract": "Abstract : This dissertation describes an intelligent, computer-aided instructional (ICAI) program, named GUIDON, with capabilities to carry on a structured case method dialogue, generate teaching material from production rules, construct and verify a model of what the student knows, and explain expert reasoning. The principle objective of this research has been to convert MYCIN, a knowledge-based consultation program, into an effective instructional tool. GUIDON combines the subject matter knowledge of the consultation system with tutorial discourse knowledge, while keeping the two distinct. MYCIN-like knowledge-based consultation programs are designed to provide expert-level advice about difficult scientific and medical problems. High performance is attained by interpreting a large, specialized set of facts and domain relations that take the form of rules about what to do in a given circumstance. Such a rule base is generally built by interviewing human experts to formulate the knowledge that they use to solve similar problems in their area of expertise. While it is generally believed that these programs have significant educational potential, little work has been done to evaluate the problems of realizing this potential.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "16b48bbc8f7d8dd9fa9ecf75e7ed50f856d1b5f1", "title": "Decision analysis and clinical judgment.", "authors": ["William B. Schwartz", "G. Anthony Gorry", "Jerome P. Kassirer", "Alvin Essig"], "date": 1973, "abstract": "Abstract Sound clinical judgments derive both from the command of a sufficient body of facts and from the skill to combine such facts appropriately. Most undergraduate and graduate medical education concentrates on the first of these elements, the acquisition of knowledge; little formal effort is directed to the logic of dealing with clinical problems. In this discussion we suggest that the theory and technics of decision analysis provide new and useful strategies appropriate for dealing with complex clinical situations. In their qualitative aspects these formal strategies closely resemble those that the expert clinician employs informally, but which he is often unable to communicate explicitly. When applied quantitatively, the formalism affords greater precision than is otherwise readily attainable. To illustrate the application and utility of decision analysis we have considered the problems posed by severely hypertensive patients with possible functional renal artery stenosis, and have examined, both qualitatively and quantitatively, the alternative courses of action available to the clinician.", "references": ["df3eb497138fefbeea98615d1f1eafdf69dad877", "91a267cf5f202607bc0df597ae1c5098f4361603", "009dd583a81b8186273573e38c38b6d04ab2af72"], "page_rank": 0.0001231527093596059}, {"id": "403feb58ace9e76994f9955147d3c9999cace6e9", "title": "Meta-Rules: Reasoning about Control", "authors": ["Randall Davis"], "date": 1980, "abstract": "Abstract How can we insure that knowledge embedded in a program is applied effectively? Traditionally the answer to this question has been sought in different problem solving paradigms and in different approaches to encoding and indexing knowledge. Each of these is useful with a certain variety of problem, but they all share a common problem: they become ineffective in the face of a sufficiently large knowledge base. How then can we make it possible for a system to continue to function in the face of a very large number of plausibly useful chunks of knowledge? In response to this question we propose a framework for viewing issues of knowledge indexing and retrieval, a framework that includes what appears to be a useful perspective on the concept of a strategy. We view strategies as a means of controlling invocation in situations where traditional selection mechanisms become ineffective. We examine ways to effect such control, and describe meta-rules, a means of specifying strategies which offers a number of advantages. We consider at some length how and when it is useful to reason about control, and explore the advantages meta-rules offer for doing this.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0fcc7bd70d460bfc050d0936df79eef221994ec3", "title": "The discrimination of dot patterns as a function of number and average separation of dots.", "authors": ["Rolland Sydney French"], "date": 1953, "abstract": "An apparatus for hydrodesulfurizing residual hydrocarbon oils in a substantially liquid phase continuous process wherein catalyst is periodically introduced at the top of the reactor and spent catalyst is periodically withdrawn from the bottom thereof, said apparatus comprising at least one high-pressure reaction vessel containing a catalyst bed and having an inlet for introducing residual oil and hydrogen, an outlet for withdrawing reaction products and being connected by conduits and valves to a high-pressure vessel from which fresh catalyst can be introduced to the top of said catalyst bed and to a high-pressure vessel into which spent catalyst can be discharged from the bottom of said catalyst bed.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "0ec332427fe34ade797b701ec817ec8daf9080ce", "title": "A Simulation of Visual Imagery", "authors": ["Stephen M. Kosslyn", "Steven P. Shwartz"], "date": 1977, "abstract": "This paper describes an operational computer simulation of visual mental imagery in humans. The structure of the simulation was motivated by results of experiments on how people represent information in, and access information from, visual images. The simulation includes a \u201csurface representation,\u201d which is spatial and quasi-pictorial, and an underlying \u201cdeep representation,\u201d which contains \u201cperceptual\u201d information encoding appearance plus \u201cpropositional\u201d information describing facts about an object. The simulation embodies a theory of how surface images are generated from deep representations, and how surface images are processed when one accesses information embedded in them. The simulation also offers an account of various sorts of imagery transformations.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "1dea2c1ba796b15e75abdf69c1f96915323b2ebc", "title": "Symmetry, information, and memory for patterns.", "authors": ["Fred Attneave"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"Symmetry, information, and memory for patterns.\" by Fred Attneave", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "a5ceacbbbf78b1b5ff55be25cb031fac63581359", "title": "Diagnostic Models for Procedural Bugs in Basic Mathematical Skills", "authors": ["John Seely Brown", "Richard R. Burton"], "date": 1978, "abstract": "A new diagnostic modeling system for automatically synthesizing a deep-structure model of a student's misconceptions or bugs in his basic mathematical skills provides a mechanism for explaining why a student is making a mistake as opposed to simply identifying the mistake. This report is divided into four sections: The first provides examples of the problems that must be handled by a diagnostic model. It then introduces procedural networks as a general framework for representing the knowledge underlying a skill. The challenge in designing this representation is to find one that facilitates the discovery of misconceptions or bugs existing in a particular student's encoding of this knowledge. The second section discusses some of the pedagogical issues that have emerged from the use of diagnostic models within an instructional system. This discussion is framed in the context of a computer-based tutoring/gaming system developed to teach students and student teachers how to diagnose bugs strategically as well as how to provide a better understanding of the underlying structure of arithmetic skills. The third section describes our uses of an executable network as a tool for automatically diagnosing student behavior, for automatically generating \u201cdiagnostic\u201d tests, and for judging the diagnostic quality of a given exam. Included in this section is a discussion of the success of this system in diagnosing 1300 school students from a data base of 20,000 test items. The last section discusses future research directions. Development of the general framework of Diagnostic Models which underlies this research was supported, in part, by the Advanced Research Projects Agency, Air Force Human Resources Laboratory, Army Research Institute for Behavioral and Social Sciences, and Navy Personnel Research and Development Center under Contract No. MDA903-76-C-0108.", "references": ["e64db9777a991f55029bd13c9d03741fa2b17046"], "page_rank": 0.0003448275862068965}, {"id": "eb40339b13500645ee627337d2f4ea0663c8f0b0", "title": "The Degree of Completeness of the N0-Valued Lukasiewicz Propositional Calculus", "authors": ["Alan Rose"], "date": 1953, "abstract": "Semantic Scholar extracted view of \"The Degree of Completeness of the N0-Valued Lukasiewicz Propositional Calculus\" by Alan Rose", "references": [], "page_rank": 0.00016420361247947453}, {"id": "6e33fd816697dca154e394067c9912ccbddbc6e0", "title": "Concerning Rashevsky's theory of the \"Gestalt.\"", "authors": ["Alston S. Householder"], "date": 1939, "abstract": "In experiments of Kluver on monkeys and of Hertz on bees, observed reactions indicate the presence of a neural mechanism which serves to order geometric figures in a linear sequence, so that any given figure occupies a definite place in a one-dimensional array. In a theoretical mechanism described by Rashevsky, presentation of a visual stimulus-figure results in an excitation at a given center whose intensity is a function upon the curve is examined in some special cases, and the mode of variation is further described qualitatively.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "bfcd62b8887306333fdfe9f4c409381cfd475329", "title": "Introduction to a General Theory of Elementary Propositions", "authors": ["Emil L. Post}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Introduction to a General Theory of Elementary Propositions\" by Emil L. Post", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2e8136581d934e18eb17d124f0224a32b01272d5", "title": "The discrimination of visual number.", "authors": ["Eugenia Kaufman", "My Lord"], "date": 1949, "abstract": "Suppose that there are two collections or groups of objects-coins, trees, beans, or aircraft-and we do not know how many objects there are. Suppose further that for some reason we cannot count the number of objects in either group. Still, some property of each group makes it possible for a person to say that one of these groups is greater-than, lessthan, or equal-to the other group. It is this property of a collection of objects that we define as numerousness.l We might say that numerousness is that property of a group of objects which we can discriminate, without counting, under instruction to judge how many objects the group contains. We shall wish to modify this definition later as a result of the experiments reported in this paper, but it is adequate for the present discussion of the problem. The judgment of 'numerousness' may be made in several different ways: (a) it may be comparative-more numerous or less numerous, larger or smaller, etc.; (b) or it may be 'absolute.' There is one special form that the absolute judgment of numerousness can take. It is called the direct reporting of number. In this method of reporting, a numeral is assigned to represent how many things there are in any given collection of objects. After a brief look-so brief that counting is impossible-we say 10, 23, or 250 to indicate that we estimate that the group contained 10, 23, or 250 members.", "references": [], "page_rank": 0.00012510751427007583}, {"id": "b3aab5b387de1acc3f25f20a3e887e5f321b59d7", "title": "The conceptual framework of psychology", "authors": ["Egon Brunswik"], "date": 1954, "abstract": "Reviews the book by Egon Brunswik (see record 1952-05895-000). Physically this is a slim volume, hardly a book, rather, a monograph of barely a hundred pages. Intellectually this is the equivalent of three books or, to put it conservatively, of one well-sized book and two monographs of about one hun", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4b0eef44868093912fb54c2e9f2d3653691be145", "title": "Stimulus complexity and the recognition of visual patterns", "authors": ["Meyer Weinstein"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"Stimulus complexity and the recognition of visual patterns\" by Meyer Weinstein", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "ed31282548e34b04a9c27ad6dafb81e43aa17447", "title": "On Growth and Form", "authors": ["J. Thomson"], "date": 1940, "abstract": "THIS book, at once substantial and stately, is to the credit of British science and an achievement for its distinguished author to be proud of. It is like one of Darwin's books, well-considered, patiently wrought-out, learned, and cautious-a disclosure of the scientific spirit. It is an application of some of the concepts of physical science and sundry mathematical methods to the study of organic form. \u201cMy sole purpose is to correlate with mathematical statement and physical law certain of the simpler outward phenomena of organic growth and structure or form: while all the while regarding, ex hypothesi, for the purposes of this correlation, the fabric of the organism as a material and mechanical configuration.\u201d \u201cOf how it is that the soul informs the body, physical science teaches me nothing. \u2026 But of the construction and growth and working of the body, as of all that is of the earth earthy, physical science is, in my humble opinion, our only teacher and guide.\u201d We think that it will be difficult to justify the word \u201conly,\u201d for in the working of the body the soul (to use the author's dualistic terminology) takes part, as when a strong emotion influences our suprarenals, and, willy-nilly, we are back in psycho-biology:On Growth and Form.By D'Arcy Wentworth Thompson. Pp. xv + 793. (Cambridge: At the University Press, 1917.) Price 21s. net.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4aca5c719d51b774aaca565bc07bd013dd28655c", "title": "A method of graded dichotomies for the scaling of judgments.", "authors": ["Fred Attneave"], "date": 1949, "abstract": "Semantic Scholar extracted view of \"A method of graded dichotomies for the scaling of judgments.\" by Fred Attneave", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "18c522cce1542e6673baceedab5fc6c6983a6126", "title": "Identification of dot patterns from memory as a function of complexity.", "authors": ["Rolland Sydney French"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Identification of dot patterns from memory as a function of complexity.\" by Rolland Sydney French", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "8fe5a31ea4ce332a9cd77259b98ba8e1bfdf564a", "title": "The nomenclature of information theory", "authors": ["Donald M. MacKay"], "date": 1953, "abstract": "This chapter contains sections titled: What Information Theory is About, Explanatory Glossary, Postscript on Structural Information-Content and Optical Resolution", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "d41a0482e206e247057b49a857b141603e74c6bd", "title": "An Investigation of the Learning of Visually Perceived Forms", "authors": ["Elizabeth V. Fehrer}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"An Investigation of the Learning of Visually Perceived Forms\" by Elizabeth V. Fehrer", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e5b9efabfef885ec48528c4d1c9e1e506a193d57", "title": "The Origin and Resolution of Ambiguities in Causal Arguments", "authors": ["Johan de Kleer"], "date": 1979, "abstract": "The causal arguments that people typically use to explain the behavior of physical systems contain ambiguities and hidden assumptions which result from imposing a particular point of view on the behavior of the system. The causality of such an argument is an artifact of imposing this point of view. Usually there exist other equally \"valid\" but conflicting arguments based on the same evidence. The inherent local nature of causal arguments makes it impossible for them to capture the more global effects that arc needed to resolve these ambiguities. However, their local nature makes causal arguments computationally simple to construct. This paper discusses these ideas in the context of electronics after first presenting a general theory of causal arguments. The causal rules that electrical engineers appear to use to reason about circuits are presented, and their use in constructing causal arguments for circuit behavior is discussed.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "3573a937d63f8c2a0873c4c32c423457c9be778f", "title": "Automated Theory Formation in Mathematics", "authors": ["Douglas B. Lenat"], "date": 1977, "abstract": "A program called \"AM\" is described which cairies on simple mathematics research: defining, and studying new concepts under the guidance of a large body of heuiistic rules. The 250 heurKtus communicate via an agenda mechanism, a global priority queue of small bisk', for the program to perform and teasons why each task is plausible (e.g., \"Find PENCRAHZTION. of 'prnes', because turued out to be so useful a Concept\"). Each concept is an active, structured knowledge module. One bundled very incomplete modules are initially supplied, each one corresponding to an elementary set theoretic concept (e.g., union). This provides a definite but immense space which AM begins to explore. In one boor, AM rediscovers hundreds of common concepts (including singleton sets, natural numbers, arithmetic) and theorems (e.g., unique factorization).", "references": ["34fce7ae570071c7a01468cea9fca519fd54f10d", "3eca05cf111caff6c586b788f3f836cdbea46f5d", "35f91d56bc5ce74578a712f0ebdc54203a0e5992", "7cfee4257cd03c70151af5ba71915b1d0f608484", "46bb9b198b3d2284a0d06b29d41c04dea0d0b243"], "page_rank": 5.473453749315818e-05}, {"id": "07b82b58e1fd76540cf2217ed4537136855685d5", "title": "Prolegomena to a theory of formal reasoning", "authors": ["Richard W. Weyhrauch"], "date": 1978, "abstract": "This paper is an introduction to the mechanization of a theory of reasoning. Currently formal systems are out of favor with the AI community. The aim of this paper is to explain how formal systems can be used in AI by explaining how traditional ideas of logic can be mechanized in a practical way. The paper presents several new ideas. Each of these is illustrated by giving simple examples of how this idea is mechanized in the reasoning system FOL. That is, this is not just theory but there is an existing running implementation of these ideas. In this paper: 1) we show how to mechanize the notion of model using the idea of a simulation structure and explain why this is particularly important to AI, 2) we show how to mechanize the notion of satisfaction, 3) we present a very general evaluator for first order expressions, which subsumes PROLOG and we propose as a natural way of thinking about logic programming, 4) we show how to formalize metatheory, 5) we describe reflection principles, which connect theories to their metatheories in a way new to AI, 6) we show how these ideas can be used to dynamically extend the strength of FOL by \"implementing\" subsidiary deduction rules, and how this in turn can be extended to provide a method of describing and proving theorems about heuristics for using these rules, 7) we discuss one notion of what it could mean for a computer to learn and give an example, 8) we describe a new kind of formal system that has the property that it can reason about its own properties, 9) we give examples of all of the above.", "references": ["236c58d8ba3e9e9d5ed8562084690090752bf5df", "3750098165eec68d9ea680b52ef1338f1eaa0b7c", "d8ce4b5489ef14e8878c869101e30432d057599c", "3c2eecf7fdcac147c06da1d53efe36a926530eee", "f54a584f23c0940a6314c3782dd59b9893b8ae0a", "c5340982746f1aac55c1cc7d2c06b670f522f253", "22fd3066b7e8051907309f6358a64638446c324f"], "page_rank": 0.00010946907498631636}, {"id": "0fc425a8004830fdd7f207efd4fa7a2331d56d3f", "title": "The Structure of Scientific Revolutions", "authors": ["Thomas Samuel Kuhn", "David F. Hawkins"], "date": 1963, "abstract": "This book has thirteen chapters and a postscript developed after seven years of this edition. The organization of the book is disclosed here with. First, the book tried to see the contribution of history to the very existence of science in the different epochs. Secondly, it also considered the route, nature, and puzzle solving role of normal science. Third, the reason why paradigms are considered as the prioritized models in science is briefly treated. Fourth, anomalies as new problems that could not be solved with the known algorithm and the attendant reactions to this situations-discovery are well taken-in. Fifth, the possible responses of scientists about crisis and the attendant outcomes of sciencenew scientific theories which are realized through discovery are also part of the book. In the end, the book tried to explicate points such as progress, resolution, invisibility, nature and necessity, and how world view is changed by scientific revolution. Generally, I found the book a high level literary work. I learnt some basic scientific research concepts which I did not come across through any other means so far in my professional as well as student hood years. My level of understanding of the basic thesis of this book triggered me to say that Kuhn is an intellectual \u201cangel\u201d who tried to lift up science from an extreme positivist tradition to the consideration of bothmainly the subjective world. Specifically speaking, the book has the following strengths: It tells about the role of history in science-which other research texts give scant consideration, It examines the route, nature and role of normal science, which we need to know as would be researchers, It enables users of the essay to have a profound understanding about paradigm, which implies the benefit behind framework, and paradigm shift and its causes, such as anomalies and crisis, and It provides a unified view of scientific revolutions Though the strengths over weigh its weaknesses, the critic of this book identified the following limitations: Difficulties and misunderstandings created due to the old diction usage of the essay, Considering science as mere belief, subjective and non rational enterprise, Using one term to convey more than two meaningsexample \u2018paradigm\u2019, The assertion that research can be done without referring a paradigm, Equating paradigm shift to revolution, The assertion that science has a certain pick time by which it becomes dormant to novelties, The assertion that research results in normal science are anticipated before hand, The assertion that paradigms guide research in the absence of scientific rules, and Piercing the scientist for failure without considering other relevant factors Examples provided to elaborate concepts; issues, etc do not serve their purpose. They rather make the Essay complex and ambiguous to users. Even I, as a natural science background as a high school student and mathematics minor in my First Degree, failed to understand most of the examples. As we all know from pedagogical principles, examples are believed to concretize concepts, issues, etc raised by the author. But they failed to realize this purpose that needs revisiting in this essay, and The author said nothing about the possible contributing factors to the occurrence of anomalies in the history of science.", "references": ["6eb6d61c83a7eb4f712c55952492ddd3fdbae77c", "c7fc9b9cc03651970efc237813fb87d3a1ba374b", "f3519478514f98fdb7071f7db6e6bec58056b911", "3491b1df9e378ebcfb44eac73514036bd364bdda"], "page_rank": 5.473453749315818e-05}, {"id": "f31608741569fb17ecae885c2c259a7a4f08039a", "title": "The use of schemata in the acquisition and transfer of knowledge", "authors": ["Perry W. Thorndyke", "Barbara Hayes-Roth"], "date": 1979, "abstract": "Abstract A learning model based on \u201cmemory schemata\u201d is presented. The model assumes that knowledge substructures in memory are shared by multiple representations of information from diverse contexts. These substructures, or schemata, are collections of concepts and associations that occur together repeatedly and act as unitary, higher-order concepts. When knowledge is represented in terms of a schema, associations from the schema to additional concepts specify the detailed information for that context. This organization of knowledge entails both costs and benefits for the acquisition and retention of new information that utilizes a schema. The use of a familiar encoding structure facilitates memory access at storage and retrieval time. Multiple uses of the shared structure produce interference among concepts from the various contexts. The predictions of the model were tested in two transfer experiments in prose learning. Experiment l demonstrated the simultaneous effects of both facilitation and interference in the learning of diverse information conforming to a single schema. Recall of information from the schema was a non-monotonic function of schema strength. In Experiment 2 facilitation was preserved while interference was eliminated by increasing the discriminability among competing contexts. Results from both experiments confirmed the predictions of the model.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "46f41bcaf5d69e3586571c6b8c91f525096726f5", "title": "The Naive Physics Manifesto", "authors": ["Patrick J. Hayes"], "date": 1990, "abstract": "A steam iron soleplate, generator, and distributor subassembly of a thin soleplate with a coverplate spaced from and supported on the soleplate by spaced peripheral rib means to define a steam distributing passage therebetween. The coverplate is integrally attached to the soleplate by a continuous weld between the ribs and soleplate and steam generating means are provided in the upper surface of the cover-plate separate and spaced from the soleplate and ducted below to the steam passage means. A heat generating element forms an integral part of the coverplate for heat transfer to the soleplate through the ribs primarily by conduction. Both the method of assembly and the subassembly itself are disclosed.", "references": [], "page_rank": 0.0005747126436781609}, {"id": "d09e73d42f2aa42a0abc4aa27d84e72faf712cc7", "title": "What the Mind\u2019s Eye Tells the Mind\u2019s Brain: A Critique of Mental Imagery", "authors": ["Zenon W. Pylyshyn"], "date": 1973, "abstract": "This paper presents a critique of contemporary research which uses the notion of a mental image as a theoretical construct to describe one form of memory representation. It is argued that an adequate characterization of \u2018what we know\u2019 requires that we posit abstract mental structures to which we do not have conscious access and which are essentially conceptual and propositional, rather than sensory or pictorial, in nature. Such representations are more accurately referred to as symbolic descriptions than as images in the usual sense. Implications of using an imagery vocabulary are examined, and it is argued that the picture metaphor underlying recent theoretical discussions is seriously misleading \u2014 especially as it suggests that the image is an entity to be perceived. The relative merits of several alternative modes of representation (propositions, data structures, and procedures) are discussed. The final section is a more speculative discussion of the nature of the representation which may be involved when people \u2018use\u2019 visual images.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a998dc182dec98f4ac29739d35f223415b3c7d81", "title": "Segmentation and labeling of speech: a comparative performance evaluation.", "authors": ["Henry Gilbert Goldberg"], "date": 1976, "abstract": "Abstract : This thesis studies speech recognition at the parametric level. It attempts to evaluate and understand the relative merits of a number of alternative design choices at that level. In particular, it involves an investigation of segmentation and labeling techniques, and the use of parametric representations for the acoustic signal. Every speech recognition system employs some parametric representation and some initial signal to symbol transformation. The author shows the performance currently available for these initial processes, and asserts that such performance is comparable to human performance. After presenting the relative merits of some typical parametric representations, we develop a methodology for such comparative evaluation. Simple, parameter- independent schemes for segmenting, labeling, and training are also developed. The role of pattern classification techniques is clarified, as it relates to the initial signal to symbol transformation. Four parametric representations were chosen for study: a set of amplitudes and zero-crossing measurements from 5 octave filters; a set of energy measurements from a 1/3 octave filter bank; a smoothed, short-time spectrum computed from the LPC filter and the LPC coefficients themselves. Note that the first two involve the use of analog devices. Each method yields a set of measurements at uniform, short intervals--a pattern. Distance functions, chosen from pattern classification theory, are then applied to the parameter patterns as measures of acoustic similarity.", "references": ["7bfdf301ace72b5316915789f0230f7714522163", "418f7d39be7c3476da96acb91b400364f53c2cdf", "f027ce53a12f36f93897a2b5733549ca323c18d0", "e1a6ba42e66713d81cecedc4451ed06fc2d6823d", "6fc7b1e15b664d483e613a05afe6ba7807ce52ea", "c2af4a77b5082310c7aaf98268d2d3ee8e0ccad1", "3034a2fc90ec0baad8cb71c3a0037a9d6b1fd38a", "263201404ebd09ece6406fb0f27efc0e9409922d", "80f165b68355ce663b391a78ba84eee9cbda3237", "a2c9aea19862e26810fd8113254ab82914014d0c"], "page_rank": 4.926108374384236e-05}, {"id": "7cfee4257cd03c70151af5ba71915b1d0f608484", "title": "Applications of Meta Level Knowledge to the Construction", "authors": ["R. Jeffrey Davis"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Applications of Meta Level Knowledge to the Construction\" by R. Jeffrey Davis", "references": [], "page_rank": 0.00022167487684729062}, {"id": "0dc8b5d3452ddfd26b998cd923f4c46cdd80faca", "title": "An Organization of Knowledge for Problem Solving and Language Comprehension", "authors": ["Chuck Rieger"], "date": 1976, "abstract": "Abstract Plan synthesis and language comprehension, or more generally, the act of discovering how one perception relates to others, are two sides of the same coin, because they both rely on a knowledge of cause and effect\u2014algorithmic knowledge about how to do things and how things work. I will describe a new theory of representation for commonsense algorithmic world knowledge, then show how this knowledge can be organized into larger memory structures, as it has been in a LISP implementation of the theory. The large-scale organization of the memory is based on structures called bypassable causal selection networks. A system of such networks serves to embed thousands of small commonsense algorithm patterns into a larger fabric which is directly usable by both a plan synthesizer and a language comprehender. Because these bypassable networks can adapt to context, so will the plan synthesizer and a language comprehender. I will propose that the model is an approximation to the way humans organize and use algorithmic knowledge, and as such, that it suggests approaches not only to problem solving and language comprehension, but also to learning. I'll describe the commonsense algorithm representation, show how the system synthesizes plans using this knowledge, and trace through the process of language comprehension, illustrating how it threads its way through these algorithmic structures.", "references": ["a0adea7988254f3d0740b587334c8ca6357cdd8b", "6d801505d744dff6bb787b284ded9c2ef901ebbc", "67a69a1d0124e014d3cf8e7c214d395f1befdf95", "2cfe1adca4b3fd0b20eb37260d7013303f112111", "1efcd7be3b52e46de800e06e17268ce7d535d9a7", "e64db9777a991f55029bd13c9d03741fa2b17046", "438c44ab6270d8b221fca2b94c73d3673a3cb16e", "8e64b0378e22f1df9d73630e9ad62c57ea25c5c3"], "page_rank": 0.0001231527093596059}, {"id": "e494e85bda74b856f20eee75bd274ea74e4861aa", "title": "The Causal Representation and Simulation of Physical Mechanisms.", "authors": ["Chuck Rieger", "M A Grinberg"], "date": 1976, "abstract": "Abstract : This paper describes a theoretical framework and a LISP implementation for describing and simulating the cause-effect behavior of mechanisms. For the purposes of this research, a mechanism is defined to be any physical device, complex or simple, which exhibits cause and effect relationships useful to humans. Ordinarily, this will mean any purposively constructed object, such as a vacuum cleaner, a pencil, a button, a lightbulb or a computer. However, the authors also include in the definition any naturally-occurring physical devices and principles whose cause and effect relationships are of use to humans. Also considered are information-manipulating 'mechanisms' such as computer programs, as though they were physical in nature.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "e9112720dc2b97462c76006ad6d48982e1f6d022", "title": "On automatic speech-understanding systems", "authors": ["Georgette Silva"], "date": 1975, "abstract": "systems that afford a faster communication and lessen the dependence upon intermediaries, and that often provide help to those who are not familiar with how the system operates and don't know how to express their requirements formally. But in spite of advances in man-machine interaction, the mechanisms provided for communication remain remote from the ideal. When a potential user first approaches a computer he finds that the first thing he must do, before he can even think of getting any work done, is to learn how to communicate with the machine. At the very least, this involves learning a new \"language\" the language of the \"interface\" to the machine. This is not necessarily a programming language, but certainly one that allows him to tell the machine what to do. In the past, such interface languages have been conceived and implemented by system designers and programmers with a view to optimizing system capabilities rather than accommodating human needs. The use of such languages often requires thought patterns and processes which are foreign to the way people usually think and talk to each other and involves a certain amount of effort to learn.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9c99fd71e874844c4d8c982b18610a4df28fba33", "title": "Production systems as a programming language for artificial intelligence applications.", "authors": ["Michael D. Rychener"], "date": 1976, "abstract": "Abstract : EPAM is a simple model of verbal learning that was developed to simulate certain features of human learning, but it has also turned out to be useful for certain kinds of discriminations in Al programs. This chapter describes a production system for EPAM, featuring the automatic addition of productions by the basic system to represent incremental learning of three-letter nonsense syllables. The design of the network represented by the added productions is discussed and its growth described. Details of the EPAM production system raise several issues with respect to general EPAM variations and with respect to production system issues such as the right set of production-building primitives. A comparison of the present program to a similar one by Waterman, using a radically different production system architecture, is carried out, highlighting the advantages of the present one. (Author)", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "27e21381178340e476d5ca1ba98b282684ea583f", "title": "A Result on Consistency and its Application to theTheory of Definition", "authors": ["Abraham Robinson"], "date": 1956, "abstract": "Semantic Scholar extracted view of \"A Result on Consistency and its Application to theTheory of Definition\" by Abraham Robinson", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "14714614ffff82184bd5a2532982ef81b1eed4e3", "title": "Controlled and Automatic Human Information Processing: 1. Detection, Search, and Attention.", "authors": ["Walter Schneider", "Richard M. Shiffrin"], "date": 1977, "abstract": "A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activation of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically\u2014without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a sequence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.", "references": ["f721df0e032761342efb5a55c73175fddb13213d", "f0d04b47cadd67bb577b44b09bbb4f48b8356e65", "577b2493cba7a3b1faccea5061e3eaa6978e4cae", "7d384dd3773e514f2de9d5af949739af12c91939", "488b5370b0e5a8d6972c8e59c7e5c945c770eb58", "05ab4e1b25d905c9dcbc4b9235e0edf573027785", "c677814a378a00a16289eeded1b0b2ecd5ca5d2d", "9d6d9e9286d90b131ced007baea6e1aa340afaa0", "be3e86b216033e2f152f9e3208082d37bbc3a140", "6b747dace16f85b69d6d889193a9eae57c8d21ce"], "page_rank": 4.926108374384236e-05}, {"id": "11eb86e0e9f44c8c53fca959aba2f272fa445049", "title": "On the categoricity in power of elementary deductive systems and some related problems", "authors": ["Jerzy \u0141o\u015b"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"On the categoricity in power of elementary deductive systems and some related problems\" by Jerzy \u0141o\u015b", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "5805e4838505299c059fb937bc6bf99501e72355", "title": "Applications of the theory of Boolean rings to general topology", "authors": ["Marshall Harvey Stone"], "date": 1937, "abstract": "In an earlier paperf we have developed an abstract theory of Boolean algebras and their representations by algebras of classes. We now relate this theory to the study of general topology. The first part of our discussion is devoted to showing that the theory of Boolean rings is mathematically equivalent to the theory of locally-bicompact totally-disconnected topological spaces. In R we have already prepared the way for a topological treatment of the perfect representation of an arbitrary Boolean ring. Continuing in this way, we find that the perfect representation is converted by the introduction of a suitable topology into a space of the indicated type. We have no difficulty in inverting this result, proving that every locally-bicompact totally-disconnected topological space arises by the same procedure from a suitable Boolean ring.' It is thus convenient to call the spaces corresponding in this manner to Boolean rings, Boolean spaces. The algebraic properties of Boolean rings can, of course, be correlated in detail with the topological properties of the corresponding Boolean spaces. A simple instance of the correlation is the theorem that the Boolean rings with unit are characterized as those for which the corresponding Boolean spaces are bicompact. A familiar example of a bicompact Boolean space is the Cantor discontinuum or ternary set, which we discuss at the close of Chapter I. Having established this direct connection between Boolean rings and topology, we proceed in the second part of the discussion to considerations of a yet more general nature. We propose the problem of representing an arbitrary TVspace by means of maps in bicompact Boolean spaces. Our solution of this problem embodies an explicit construction of such maps, which we shall now describe briefly. In a given TVspace dt, the open sets and the nowhere dense sets generate a Boolean ring, with 9\u00ce as unit, which characterizes the topological structure of 9\u00ce. Those subrings which contain 9\u00ce and which are so large that the interiors of their member sets constitute bases for 9\u00ee, also char-", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "3172cb4b50f1f226e89747089eb98b83f8b17ab4", "title": "The Completeness of the First-Order Functional Calculus", "authors": ["Leon Henkin"], "date": 1949, "abstract": "Although several proofs have been published showing the completeness of the propositional calculus (cf. Quine (1)), for the first-order functional calculus only the original completeness proof of Godel (2) and a variant due to Hilbert and Bernays have appeared. Aside from novelty and the fact that it requires less formal development of the system from the axioms, the new method of proof which is the subject of this paper possesses two advantages. In the first place an important property of formal systems which is associated with completeness can now be generalized to systems containing a non-denumerable infinity of primitive symbols. While this is not of especial interest when formal systems are considered as logics \u2014i.e., as means for analyzing the structure of languages\u2014it leads to interesting applications in the field of abstract algebra. In the second place the proof suggests a new approach to the problem of completeness for functional calculi of higher order. Both of these matters will be taken up in future papers. The system with which we shall deal here will contain as primitive symbols and certain sets of symbols as follows: (i) propositional symbols (some of which may be classed as variables , others as constants ), and among which the symbol \u201c f \u201d above is to be included as a constant; (ii) for each number n = 1, 2, \u2026 a set of functional symbols of degree n (which again may be separated into variables and constants ); and (iii) individual symbols among which variables must be distinguished from constants . The set of variables must be infinite.", "references": ["7e2b5d9e76b9e5d50b5602511e3341981be958e9", "ddd06f4501b9389883aae4fe22978be9e9fa28a5"], "page_rank": 0.00014074595355383532}, {"id": "64026ed78f9259d524f001a20c409c23ceb7b709", "title": "The Paradoxical Position of Kurt Goldstein in the History of Aphasia", "authors": ["Norman Geschwind"], "date": 1964, "abstract": "A widely-accepted version of the history of the study of aphasia states that before the First World War this discipline was dominated by a group of workers who, with occasional exceptions, were unsophisticated psychologically and who attempted on the basis of inadequate clinical and pathological data to fit aphasia into a narrow anatomical scheme. According to this view a group of reformers appeared, Marie, von Monakow, Head, and Goldstein, who destroyed this inadequate classical scheme.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "686812651628808e11f6fe661e681bc2a572c7da", "title": "A decision method for elementary algebra and geometry", "authors": ["Alfred Tarski"], "date": 1951, "abstract": "By a decision method for a class K of sentence (or other expressions) is meant a method by means of which, given any sentence \u03b8, one can always decide in a finite number of steps whether \u03b8 is in K; by a decision problem for a class K we mean the problem of finding a decision method for K. A decision method must be like a recipe, which tells one what to do at each steps so that no intelligence is required to follow it; and the method can be applied by anyone so long as he is able to read and follow directions.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "df3eb497138fefbeea98615d1f1eafdf69dad877", "title": "Experience with a model of sequential diagnosis.", "authors": ["G. Anthony Gorry", "G. Octo Barnett"], "date": 1968, "abstract": "Abstract A physician must have available two different classes of information when he attempts to deal with a diagnostic problem: the presenting signs and symptoms of the patient, and the pertinent medical knowledge about the disease state. The physician must then use this information in arriving at certain decisions: what additional information about the condition of the patient should be collected; what disease states are most probable; what are the potential consequences of misdiagnosis; what are the therapeutic and prognostic implications of the particular clinical situation. The possibility of significant errors in a diagnostic problem coupled with the decision-making activities involved have motivated the development of a wide range of tools to improve the performance of the diagnostician. This paper reports on an effort to exploit computer technology as such a tool for assistance in the diagnostic activity.", "references": ["00e4987bd8ed6de94c1c9462f299e55e228f599b"], "page_rank": 0.00016420361247947453}, {"id": "a2d7ec2ff9afdcded67abcdff27dd7fb44b73007", "title": "Representations of Knowledge in a Program for Solving Physics Problems", "authors": ["Gordon S. Novak"], "date": 1977, "abstract": "A computer p rogram w h i c h s o l v e s p h y s i c s p r o b lems s t a t e d i n E n g l i s h i s d e s c r i b e d i n terms o f t h e knowledge w h i c h i s used t o t r a n s f o r m one t y p e o f r e p r e s e n t a t i o n i n t o a n o t h e r . The E n g l i s h s e n t e n c e s o f t h e p r o b l e m s t a t e m e n t a re p r o g r e s s i v e l y t r a n s f o r m e d i n t o a s e m a n t i c n e t w o r k f o r m , a l a n g u a g e f r e e i n t e r n a l model o f t h e o b j e c t s i n t h e p r o b l e m and t h e i r a t t r i b u t e s and r e l a t i o n s h i p s , a s e t o f c a n o n i c a l o b j e c t f rames w h i c h i n t e r p r e t a c t u a l o b j e c t s as c a n o n i c a l o b j e c t s (such as a p o i n t mass) , a g e o m e t r i c m o d e l , a s e t of equat ions , and a p i c t u r e m o d e l . The g e n e r a l n o t i o n o f a c a n o n i c a l o b j e c t f r a m e , w h i c h a b s t r a c t s a s u b s e t o f t h e p r o p e r t i e s o f a n o b j e c t t o f o rm a r e p r e s e n t a t i o n o f a c a n o n i c a l o b j e c t whose i n t e r a c t i o n s w i t h r e l a t e d c a n o n i c a l o b j e c t s can b e f o r m a l l y m o d e l l e d , i s d i s c u s s e d as a method o f o r g a n i z i n g p r o b l e m s o l v i n g p r o g r a m s .", "references": [], "page_rank": 0.000541871921182266}, {"id": "10511d47f9f44e27467443d6431a3cd494bc975d", "title": "Analysis of writing errors in Japanese aphasic patients: kanji versus kana words.", "authors": ["Sumiko Sasanuma", "Osamu Fujimura"], "date": 1972, "abstract": "Summary Fifty aphasic and 30 non-aphasic hemiplegic patients, screened for visuo-motor or visuo-spatial problems, performed a task of writing 20 high-frequency nouns designating familiar objects. For 10 of the nouns (non-imported words), the subject was required to write both in kanji (ideogram) and in hiragana (one type of phonogram) and for the other 10 (imported words) in katakana (another type of phonogram). Combined visual-auditory stimuli (the picture of the words and the names of the pictures spoken by the examiner) were employed for eliciting responses from the subject. Any response which fell short of the standard orthography of a given word was scored as an error and subjected to a detailed analysis. (The scates of the six aphasic subjects, which showed either zero error or 100% errors on the whole test, were excluded from the analysis.) The results may be summarized as follows: (1) In the kanji transcriptions of non-imported words, both the aphasic and non-aphasic groups made errors, the former group making a larger mean percentage of errors (38%) than the latter group (22%). (2) In the kana transcriptions of both imported and non-imported words, only the aphasic group made errors, the mean percentage errors being 53% for the former and 57% for the latter. (3) The types of kanji errors exhibited by both aphasic and non-aphasic groups were essentially similar to each other, \"graphical confusions\" being the most frequent type. (4) The types of kana errors, exhibited exclusively by the aphasic subjects, were quite different from those of kanji errors; \u201cphonological confusion\u201d accounted for the most of the errors while there was only a negligible amount of \u201cgraphical confusion.\u201d (5) No correlation was found between the respective levels of performance of the aphasic patients in the kanji and kana tasks, indicating that the processing of kanji and kana can be impaired independently in different patients. (6) The aphasics with apraxia of speech showed, as a group, a significantly greater number of errors in the kana transcription of words than did the aphasics without it. These findings provide support for the hypothesis proposed therein, in part duplicating the previous result ( Sasanuma and Fujimura, 1971 ), that kana and kanji transcriptions can be processed in different modes, the former involving a \u201cphonological processor\u201d\" and the latter without one.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "19822b8fa02307cd02719bb6d80d581878db8d0b", "title": "Directed attack elicited from hypothalamus.", "authors": ["Marcy Ilene Wasman", "James P. Flynn"], "date": 1962, "abstract": "The role of the hypothalamus in aggressive behavior remains unclear. The heightened rage elicited in decorticate cats 1-3 by the application of sensory stimuli is often characterized as sham rage or as a pseudoaffective response. The failure of the animals to show a directed attack, which was thought attributable to decortication, left open the question of the character of the response. Masserman 10 found in intact cats that electrical stimulation of the hypothalamus produced patterns expressive of rage, which despite the presence of the cortex failed to culminate in directed attack. He concluded that the hypothalamus plays a minimal role in affective experience and behavior. The question is in respect to the role that the hypothalamus plays in the perceptual and emotional part of the reaction. There is general agreement that the hypothalamus integrates the muscular and glandular responses which accompany aggression. Hess and Brugger, 6 Hunsperger, 7 and Nakao,", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "255db9a0fa6051d9d24c2cfeec71dfcbafe5b733", "title": "Medical diagnosis: an information-processing approach.", "authors": ["Paul M. Wortman"], "date": 1972, "abstract": "Abstract A theory viewing human problem solving as a memory search of information organized into a hierarchy of categories was evaluated through the application of information-processing techniques to medical diagnosis. Information-processing methodology essentially involves the determination of the thought processes or symbolic manipulations necessary to the performance of a task and their precise specification as a computer program or model. To this end, a boarded clinical neurologist was asked to \u201cthink aloud\u201d while solving a number of diagnostic problems. The information processes, decision rules and other evaluation criteria characteristic of diagnosis were extracted from these subjective verbal reports and incorporated in a computer program of the diagnostic process in medicine. The results supported the theory of diagnostic problem solving and the computer model based on this theory proved sufficient to perform diagnosis.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5236b4b8febea74174977614009cc9ec305f9856", "title": "The Formation of Composite Hypotheses in Diagnostic Problem Solving: An Exercise in Synthetic Reasoning", "authors": ["Harry E. Pople"], "date": 1977, "abstract": "The INTERNIST system, which is a computer-based diagnostic problem solver having Internal Medicine as i ts domain of discourse, employs a novel attention focusing heuristic in order to deal sequentially with the component parts of a complex clinical problem. The strategy of sequential problem formation and solution has proved to be effective in sorting out the complexities and rendering a correct diagnosis in the great majority of clinical cases tested. Experience with the system suggests, however, that many aspects of the search process could be significantly enhanced if it were possible to attend to the various component problems and their interrelationships simultaneously. Recent work on a successor system has been directed towards the development of strategies for the synthesis and analysis of composite hypotheses, which may be expected to yield more rapid convergence to the correct conclusion in many cases, and in at least some cases to prevent missed diagnoses.", "references": ["5fc541a7f6725ca6dc98e797df38ed63c4af8397", "5756bf50591599e2b2b03b00cdcca597daca2050", "91a1b89eb1b286e6edd3aeba3562a9e195d6fb11", "c6c3bc4ecb6e4f3d07233b0ef2613f9f436278c6", "46bb9b198b3d2284a0d06b29d41c04dea0d0b243"], "page_rank": 4.926108374384236e-05}, {"id": "e0a662717ae6ee239d8ab357ed47701ae34450fe", "title": "Cognitive Skill: Implications for Spatial Skill in Large-Scale Environments.", "authors": ["William G. Chase", "Michelene T. H. Chi"], "date": 1979, "abstract": "Abstract : The recent cognitive skills literature is reviewed and several principles of skilled performance are derived. One of the central issues in cognitive skills concerns the organization of knowledge and it was concluded that in a variety of domains, spatial knowledge is hierachically organized. This issue was explored in a map-drawing study and it was found that the most serious errors were due to a normalizing error, an error that is symptomatic of hierarchical organization. It was concluded that normalizing errors in large-scale environments are caused by incorrect representations at higher levels in the hierarchy (global errors). Finally, from an information processing analysis of spatial knowledge, the paper addresses the question of what are the cognitive processes underlying cognitive maps and cognitive mapping. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "40b5db8813ed24b52830c5a9383543964ad07b16", "title": "Research on Expert Problem Solving in Physics", "authors": ["Gordon S. Novak", "Agustin A. Araya"], "date": 1980, "abstract": "Physics problems cannot in general be solved by methods of deductive search in which the laws of physics are stated as axioms. In solving a real physics problem, it is necessary to treat the problem as a \"nearly decomposable system\" and to design a method of analysis which accounts for the salient factors in the problem while ignoring insignificant factors. The analysis method which is chosen will depend not only on the objects in the problem and their interactions, but also on the context, the accuracy needed, the factors which are known, the factors which are desired, and the magnitudes of certain quantities. Expert problem solvers are able to recognize many frequently occurring problem types and use analysis methods which solve such problems efficiently. Methods by which a program might learn such expertise through practice are discussed.", "references": ["a2d7ec2ff9afdcded67abcdff27dd7fb44b73007"], "page_rank": 4.926108374384236e-05}, {"id": "c1a1249f3413f70823db0b26f93cb71516eb4da5", "title": "Multiple Representations of Knowledge in a Mechanics Problem-Solver", "authors": ["Johan de Kleer"], "date": 1977, "abstract": "Expert problem-solving programs have focused on working problems which humans consider difficult Oddly, many such problem-solvers could not solve less difficult versions of the problems addressed by their expertise This shortcoming also contributed to these program's inability to solve harder problems. To overcome this 'paradox' requires multiple representations of knowledge, inferencing schemes for each, and communication schemes between them. \n \nThis paper presents a program, NEWTON, applying this idea to the domain of simple classical mechanics. NEWTON employs the method of envisionment, whereby simple questions may be answered directly, and plans produced for solving more complex problems. Envisioning enables NEWTON to use qualitative arguments when possible, with resorts to mathematical equations only if the qualitative reasoning fails to produce a solution.", "references": ["67fa3b4575f2fe24582b001fb5839f7f41c44671", "ab07107fd787419708a0a67d127660f86258358d", "272772319ec475b1bb49c918863adf0cee8f715d", "3eff5c712b67f76bfc6cecb1823eeadf2a38d46b", "97e0e7161f9d0e6762c396d9714f9043b8079b48", "4bb87a5e74082fe18848fed416cec0954bf78593", "9fc77941297522cc420ce9292193dd04ed2ed1af"], "page_rank": 0.00014778325123152708}, {"id": "4c7086da527b069915041a97a452e730c6955a49", "title": "Perception in chess", "authors": ["William G. Chase", "Herbert A. Simon"], "date": 1973, "abstract": "Abstract This paper develops a technique for isolating and studying the perceptual structures that chess players perceive. Three chess players of varying strength \u2014 from master to novice \u2014 were confronted with two tasks: (1) A perception task, where the player reproduces a chess position in plain view, and (2) de Groot's (1965) short-term recall task, where the player reproduces a chess position after viewing it for 5 sec. The successive glances at the position in the perceptual task and long pauses in the memory task were used to segment the structures in the reconstruction protocol. The size and nature of these structures were then analyzed as a function of chess skill.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "35f91d56bc5ce74578a712f0ebdc54203a0e5992", "title": "ON REPRESENTATIONS AND MODELING IN PROBLEM SOLVING AND ON FUTURE DIRECTIONS FOR INTELLIGENT SYSTEMS", "authors": ["S. Amarel"], "date": 1968, "abstract": "Abstract : The problem of representations in problem solving is introduced, and its significance for progress in machine intelligence is discussed. The related problem of forming and using models in machine problem solving is also discussed. Some current research in these areas is reviewed, and open problems are outlined. In addition, more general comments on the state of the art in artificial intelligence are made from the viewpoint of a designer of complex, computer-based, information processing systems. The question of establishing a conceptual framework for intelligent problem solving that would enable designers to generalize and transfer results from specific research projects in the area into their 'real life' situations receives special attention. (Author)", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "17555e276e8db1db4b55d49e539228d25a4b9db8", "title": "Expert and Novice Performance in Solving Physics Problems", "authors": ["Jh Larkin", "John Mcdermott", "Dorothea P. Simon", "Herbert A. Simon"], "date": 1980, "abstract": "Although a sizable body of knowledge is prerequisite to expert skill, that knowledge must be indexed by large numbers of patterns that, on recognition, guide the expert in a fraction of a second to relevant parts of the knowledge store. The knowledge forms complex schemata that can guide a problem's interpretation and solution and that constitute a large part of what we call physical intuition.", "references": ["8ef05b5772f86c5018d8347e199b3cc48d6d5fbb", "88abd9d69680c2b40330c2502422612aebdf8ec2", "42580c5507b3fb1b378de57650b949fc37c8f53c", "ced5e378c44d8acdd8f10cbe2a239d2390d01e43"], "page_rank": 4.926108374384236e-05}, {"id": "9010d8c1850605bbfed06f60be208b6639c95aff", "title": "Situations, Actions, and Causal Laws", "authors": ["John W. Mccarthy"], "date": 1963, "abstract": "Abstract : A formal theory is given concerning situations, causality and the possibility and effects of actions is given. The theory is intended to be used by the Advice Taker, a computer program that is to decide what to do by reasoning. Some simple examples are given of descriptions of situations and deductions that certain goals can be achieved.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "46bed4c578e96e05fa3e5704620c4ffa0746d78f", "title": "A Learning Machine: Part I", "authors": ["Richard M. Friedberg"], "date": 1958, "abstract": "Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "22fd3066b7e8051907309f6358a64638446c324f", "title": "Five Notes on the Application of Proof Theory to Computer Science.", "authors": ["Georg Kreisel"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Five Notes on the Application of Proof Theory to Computer Science.\" by Georg Kreisel", "references": ["102765f4d74cc8e2de52c9fa26854887afb635f8"], "page_rank": 8.210180623973726e-05}, {"id": "e762c5acd9607907b6dbab223d746ac8f5e884b5", "title": "Towards a Mathematical Science of Computation", "authors": ["John McCarthy"], "date": 1962, "abstract": "In this paper I shall discuss the prospects for a mathematical science of computation. In a mathematical science, it is possible to deduce from the basic assumptions, the important properties of the entities treated by the science. Thus, from Newton\u2019s law of gravitation and his laws of motion, one can deduce that the planetary orbits obey Kepler\u2019s laws.", "references": ["d8ce4b5489ef14e8878c869101e30432d057599c", "e128c8750eee8781cdc76c9aa00df037180febf0"], "page_rank": 4.926108374384236e-05}, {"id": "3491b1df9e378ebcfb44eac73514036bd364bdda", "title": "Galileo and Plato", "authors": ["Alexandre Koyr{\\'e}"], "date": 1943, "abstract": "Semantic Scholar extracted view of \"Galileo and Plato\" by Alexandre Koyr\u00e9", "references": [], "page_rank": 0.0001231527093596059}, {"id": "f54a584f23c0940a6314c3782dd59b9893b8ae0a", "title": "A Transformation System for Developing Recursive Programs", "authors": ["Rod M. Burstall", "John Darlington"], "date": 1977, "abstract": "A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "80f165b68355ce663b391a78ba84eee9cbda3237", "title": "A 16-bit A-D-A conversion system for high-fidelity audio research", "authors": ["Jiri Kriz"], "date": 1975, "abstract": "An A-D and D-A converter system with exceptionally wide dynamic range and low distortion is discussed. The system utilizes 12-bit floating-point approximation at conversion instead of true 16-bit conversion. A deglitching track and hold is illustrated which avoids heterodyning between signal components and the sample clock. Rate-limiting during the transition from hold to track is discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f3519478514f98fdb7071f7db6e6bec58056b911", "title": "Art and Illusion: A Study in the Psychology of Pictorial Representation", "authors": ["Johannes A. Gaertner", "E. H. Gombrich"], "date": 1960, "abstract": "Semantic Scholar extracted view of \"Art and Illusion: A Study in the Psychology of Pictorial Representation\" by Johannes A. Gaertner et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "c7fc9b9cc03651970efc237813fb87d3a1ba374b", "title": "\"Scientific Autobiography\" and Other Papers", "authors": ["J. Rud Nielsen", "Max Planck", "Frank Gaynor"], "date": 1950, "abstract": "Semantic Scholar extracted view of \"\"Scientific Autobiography\" and Other Papers\" by J. Rud Nielsen et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "6eb6d61c83a7eb4f712c55952492ddd3fdbae77c", "title": "A history of the theories of aether and electricity", "authors": ["Edmund Taylor Whittaker"], "date": 1954, "abstract": "Market: Physicists, interested lay readers, and historians of science. This survey of the history of electrodynamics provides insight into the revolutionary advances made in physics during 19th and the first quarter of the 20th centuries. The first volume covers the theories of classical physics from the time of Plato to the end of the 19th century. The second volume examines the origins of the discoveries that paved the way for modern physics with the emphasis on special relativity, quantum theories, general relativity, matrix mechanics, and wave mechanics.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "a2c9aea19862e26810fd8113254ab82914014d0c", "title": "Pattern Classification and Scene", "authors": ["Richard O. Duda", "Peter E. Hart"], "date": 1973, "abstract": "In a skateboard truck with a central depending axle housing having extending to each side an axle on which a wheel is disposed and secured by a nut, a protective plastic shield covering the lowest portion of the housing and extending substantially from wheel to wheel. The shield is secured in place by a washer on each side disposed on the axle inside of the wheel and having a first flange fitting the end of the housing and having a second flange receiving a generally semicircular, reduced-diameter adjacent end of the shield.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "67a69a1d0124e014d3cf8e7c214d395f1befdf95", "title": "Conceptual Overlays: A Mechanism for the Interpretation of Sentence Meaning in Context", "authors": ["Charles J. Rieger"], "date": 1975, "abstract": "This paper describes a context mechanism for a natural language understanding system. Since no sentence is ever perceived outside some context, it is reasonable to inquire into the nature of context as it affects the interpretation of sentence meaning at a deep conceptual level. A theory, called conceptual oyerlays, is described. This theory (1) defines C(T1,..., Ti), the context established by the meaningful sequence of thouqhts T1...Tj: (2) defines I(Tj+1, C(T1,... Tj)), the high-level interpretation of Ti+i in the context established by Ti,.,.,Ti; and (3) specifies an effective algorithm and data structure for computing I(t,K) for arbitrary thought T in context K. In particular, a prototype LISP system, EX-SPECTRE-1, which solves simple cases of I(T2., C(T1)) is described. The system is based on an expectancy/fulfillment paradigm. Expectancies are spontaneously activated by a pattern-directed invocation technique. Each expectancy implicitly references large chunks of common-sense algorithms. A collection of such implicitly activated algorithms constitutes context, and the interpretive process is one of identifying future input as steps in these algorithms. Context switching and uses of I(T,K) in a language comprehension system are discussed.", "references": ["a0adea7988254f3d0740b587334c8ca6357cdd8b"], "page_rank": 6.157635467980295e-05}, {"id": "a0adea7988254f3d0740b587334c8ca6357cdd8b", "title": "The commonsense algorithm as a basis for computer models of human memory, inference, belief and contextual language comprehension", "authors": ["Chuck Rieger"], "date": 1975, "abstract": "The notion of a commonsense algorithm is presented as a basic data structure for modeling human cognition. This data structure unifies many current ideas about human memory and information processing. The structure is defined by specifying a set of proposed cognitive primitive links which, when used to build up large structures of actions, states, statechanges and tendencies, provide an adequate formalism for expressing human plans and activities, as well as general mechanisms and computer algorithms. The commonsense algorithm is a type of framework (as Minsky has defined the term) for representing algorithmic processes, hopefully the way humans do.", "references": [], "page_rank": 0.0007266009852216747}, {"id": "263201404ebd09ece6406fb0f27efc0e9409922d", "title": "Natural Communication with Computers. Volume 1. Speech Understanding Research at BBN", "authors": ["William A. Woods", "Madeleine Bates", "Bertram C. Bruce", "John J. Colarusso", "Craig Cook"], "date": 1974, "abstract": "Abstract : The report covers the development of the BBN speech project over the last four years from its early beginnings as part of the natural language understanding research at BBN prior to the inception of the ARPA Speech Understanding Project. At this point, the project is in the middle of the 5- year program projected by the ARPA Speech Understanding Research Steering Committee. This report is a final report on the first phase of this project and marks the transition of the Speech Project from a part of a larger contract on Natural Communications with Computers to a separate contract of its own.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6b747dace16f85b69d6d889193a9eae57c8d21ce", "title": "The description and luminous calibration of cathode ray oscilloscope visual displays", "authors": ["George Sperling"], "date": 1971, "abstract": "A description of a CRO display should include descriptions of: (1) typical display contents (e.g., a photograph), (2) CRO output parameters (e.g., refresh rate), and (3) luminous measurements. Luminous calibrations are unorthodox because CRO displays are discontinuous in space and in time, and because they are sources, not reflectors, of light. The appropriate luminous quantities are luminousintensity and the integral of luminous intensity (luminous directional energy, LDE); the appropriate measurements are ofLDE per point and ofLDE per unit line length. A simple calibration procedure is described, and the formulas relating these quantities to luminances are given.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "2cfe1adca4b3fd0b20eb37260d7013303f112111", "title": "A Planning System for Robot Construction Tasks", "authors": ["Scott E. Fahlman"], "date": 1974, "abstract": "This paper describes BUILD, a computer program which generates plans for building specified structures out of simple objects such as toy blocks. A powerful heuristic control structure enables BUILD to use a number of sophisticated construction techniques in its plans. Among these are the incorporation of pre-existing structure into the final design, pre-assembly of movable sub-structures on the table, and the use of extra blocks as temporary supports and counterweights in the course of the construction. \n \nBUILD does its planning in a modeled 3-space in which blocks of various shapes and sizes can be represented in any orientation and location. The modeling system can maintain several world models at once, and contains modules for displaying states, testing them for inter-object contact and collision, and for checking the stability of complex structures involving frictional forces. \n \nSuggestions are included for the extension of BUILD-like systems to other domains. Also discussed are the merits of BUILD's implementation language, conniver, for this type of problem solving.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "ddd06f4501b9389883aae4fe22978be9e9fa28a5", "title": "Completeness of the Propositional Calculus", "authors": ["W. V. Quine"], "date": 1938, "abstract": "Semantic Scholar extracted view of \"Completeness of the Propositional Calculus\" by W. V. Quine", "references": [], "page_rank": 0.0002463054187192118}, {"id": "05ab4e1b25d905c9dcbc4b9235e0edf573027785", "title": "Some models of observer behavior in two-channel auditory signal detection", "authors": ["Robert D. Sorkin", "Lawrence D. Pohlmann"], "date": 1973, "abstract": "Five models of 0 behavior in a two-channel exclusive-or (XOR) detection task were evaluated. The models included two types of single-channel and three types of two-channel O. Only the most efficient two-channel model adequately described human performance in a set of monaural and dichotic XOR conditions. Detectability measures derived from the XOR task matched those obtained from separate single-channel control conditions. We concluded that. in this two-channel task. the O\u2019s performance was not limited by any inability to monitor signals arriving simultaneousl.v in two different earphone or two different frequency channels. The implications of this result for two-channel information processing and for multicomponent and sequential signal detectmn are discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9d6d9e9286d90b131ced007baea6e1aa340afaa0", "title": "Is selective attention selective perception or selective response? A further test.", "authors": ["Anne Treisman", "James G. Riley"], "date": 1969, "abstract": "Ss repeated back 1 of 2 lists of synchronized digits presented dichotically. Ss were also asked to listen for occasional letters in either ear and to stop shadowing at once and tap with a ruler if they detected one of these letters. They detected significantly more letters in the ear whose digits th", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c677814a378a00a16289eeded1b0b2ecd5ca5d2d", "title": "A note on the identifiability of parallel and serial processes", "authors": ["James T. Townsend"], "date": 1971, "abstract": "Due to the significant research effort devoted to discovering whether certain psychological processes are serial or parallel, it seems important to establish the degree to which such processes are identifiable and to investigate possible ways in which such knowledge can improve our experiments. General definitions of parallel and serial systems are given, followed by a qualitative summary of identifiability results obtained with special classes of exponential systems. Some of these results are applied to a current experimental paradigm, and possible", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "be3e86b216033e2f152f9e3208082d37bbc3a140", "title": "Contextual Cues in Selective Listening", "authors": ["Anne Treisman"], "date": 1960, "abstract": "Two messages were presented dichotically and subjects were asked to \u201cshadow\u201d whatever they heard on one ear. Somewhere in the middle the two passages were switched to the opposite ears. Subjects occasionally repeated one or two words, at the break, from the wrong ear, but never transferred to it for longer than this. The higher the transition probabilities in the passage the more likely they were to do this. One explanation might be that the \u201cselective filter\u201d (Broadbent, 1958) acts by selectively raising thresholds for signals from the rejected sources rather than acting as an all-or-none barrier.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "00e4987bd8ed6de94c1c9462f299e55e228f599b", "title": "A SYSTEM FOR COMPUTER-AIDED DIAGNOSIS", "authors": ["G. Anthony Gorry"], "date": 1967, "abstract": "This thesis describes a model diagnostic problem and a computer program designed to deal with this problem. The model diagnostic problem is an abstract problem. A major contention of this thesis, however, is that this problem subsumes the principal features of a number of ostensibly different real diagnostic problems including certain problems of medical diagnosis and the diagnosis of machine failures. A second major contention of this thesis is that strategies for the solution of the model diagnostic problem can be formulated in terms sufficiently explicit to permit their incorporation in a computer program. The diagnostic program was implemented on the time-sharing system at Project Mac. It was applied to two medical problems, the diagnosis of congenital heart disease, and the diagnosis of primary bone tumors. The results obtained here suggest 1) that a computer program can be of considerable value as a diagnostic tool, and 2) that it is quite advantageous for such a program to perform sequential diagnosis as it interacts with the user.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "488b5370b0e5a8d6972c8e59c7e5c945c770eb58", "title": "Extremely Rapid Visual Search: The Maximum Rate of Scanning Letters for the Presence of a Numeral", "authors": ["George Sperling", "J Budiansky", "J G Spivak", "Mark C. Johnson"], "date": 1971, "abstract": "Subjects searched a rapid sequence of computer-produced letter arrays for the presence of a numeral in one of the arrays. The subjects' scanning rates were computed from their precentage of correct detections of the location of the numeral. Scanning rates were very high and approximately the same for a wide variety of conditions; the highest scanning rates (125 and 75 letters per second for two subjects) occurred when there were 9 or 16 letters in each of the arrays and when new arrays were presented every 40 to 50 milliseconds. Giving the subject advance knowledge of the numeral to be presented made little difference in the scores.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c6c3bc4ecb6e4f3d07233b0ef2613f9f436278c6", "title": "Probability, Logic and Medical Diagnosis", "authors": ["Robert S. Ledley", "Lee B. Lusted"], "date": 1959, "abstract": "Semantic Scholar extracted view of \"Probability, Logic and Medical Diagnosis\" by Robert S. Ledley et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "7e2b5d9e76b9e5d50b5602511e3341981be958e9", "title": "Die Vollst\u00e4ndigkeit der Axiome des logischen Funktionenkalk\u00fcls", "authors": ["Kurt G{\\\"o}del"], "date": 1940, "abstract": "Jakina da Whiteheadek eta Russellek logika eta matematika eraiki dutela ageriko zenbait proposizio axiomatzat hartuz, eta horietatik, zehatz azaldutako inferentzia printzipioetan oinarrituz, logikako eta matematikako teoremak guztiz modu formalean ondorioztatu dituztela (hau da, sinboloen esanahiaren erabilpen gehigarririk egin gabe). Horrelako jardunbideak galdera bat dakarkigu segituan burura, hasieran postulatutako axiomen eta inferentzia printzipioen sistema osoa ote den, hau da, proposizio logikomatematiko guztiak eratorri ahal izateko nahikoa den edo, akaso, egiazkoak diren eta sistema honetan ondorioztatu ezinak diren proposizioak otu dakizkigukeen. Kalkulu proposizionaleko formulen kasuan galderak baiezko erantzuna jaso du, hau da, erakutsi2 da, kalkulu proposizionaleko formula zuzen guztiak Principia Mathematican emandako axiometatik ondorioztatzen direla. Gauza bera egingo dugu hemen \u00abkalkulu funtzional mugatua\u00bb3 deitutako eremu zabalagoko formulentzat; hau da, ondokoa frogatuko dugu:", "references": [], "page_rank": 0.0002463054187192118}, {"id": "f0d04b47cadd67bb577b44b09bbb4f48b8356e65", "title": "Searching for many targets: An analysis of speed and accuracy", "authors": ["Albert Yonas", "John B. Pittenger"], "date": 1973, "abstract": "Three Ss scanned matrices of letters for 40 sessions in a test of Neisser\u2019s claim that feature tests in high-speed searches operate independently and in parallel. In the multiple-target condition (MTC), the matrix contained any one of four target letters, while in the four single-target conditions (STC), the S knew which particular target was embedded in the list. In contrast to previous studies, the error rates for individual target letters in the MTC were analyzed separately rather than being pooled. Two Ss made more errors on the hardest target when searched for in the MTC than in the STC. This difference would be masked by pooling error rates. The third S\u2019s scanning rate in the MTC was not as rapid as in the STC. Neither a sequential nor a strictly parallel feature processing model can account for these data.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "577b2493cba7a3b1faccea5061e3eaa6978e4cae", "title": "VERBAL CUES, LANGUAGE, AND MEANING IN SELECTIVE ATTENTION.", "authors": ["Anne Treisman"], "date": 1964, "abstract": "In 1953, Cherry did a series of experiments requiring a selective response to one of two verbal messages presented either binaurally or dichotically.1 While the irrelevant message caused negligible interference with the repeating back or 'shadowing' in the dichotic condition, it made selection extremely difficult when both messages were presented binaurally, at equal intensities and in the same voice. The Ss needed many attempts at the same message and pieced it together only gradually, making errors and transpositions which were influenced by the probability-structure of English. They did eventually, however, succeed in separating out one of the two messages when these were normal prose, but failed when presented with strings of cliches with transitional probabilities which were high within phrases but fell to zero between them. This performance seems qualitatively different from that in the dichotic condition, and may not involve selective attention at all. The Ss might have heard both messages and retrospectively put together what made sense (in fact they did much better if allowed to write), whereas in the dichotic condition they succeeded on the first trial and could report none of the verbal content of the irrelevant message, nor even that in one case it changed to a foreign language. Broadbent put forward a general theory of selective attention which accounts for the finding with dichotic presentation by postulating a filter which selects signals arriving on one of two or more input channels and rejects others before they are fully analyzed.2 The aim of the present experiment was to investigate selective attention when it depends solely on the identification of verbal or linguistic features, using a task where the messages are presented only once and require an immediate and continuous response. In more detail, it was hoped:", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7d384dd3773e514f2de9d5af949739af12c91939", "title": "SOME RESULTS CONCERNING THE IDENTIFIABILITY OF PARALLEL AND SERIAL PROCESSES", "authors": ["James T. Townsend"], "date": 1972, "abstract": "A mathematical characterization of serial, parallel and hybrid processes is given, and this characterization is related to several current experimental paradigms. Non-identifiability (mimicking) between two systems (i.e. models of systems) is defined as equivalence of probability distributions on element completion times for the two systems, where n elements are available for processing by each. Results are then presented for a class of systems with exponential processing times, and it is seen that several interesting cases of parallel and serial systems are equivalent to systems of the opposite type. Evidence that will allow accurate discrimination between parallel and serial processing for this and other classes of systems either requires more complete and precise information about the actual probability distributions of the systems or more specialized sets of converging operations than is usually obtained in psychological experimentation. For example, it is noted that at the level of first moments (means), even a parallel independent system can predict results usually associated with a serial system (an overall increasing linear mean reaction time curve as a function of the number of elements to be processed). Next, a functional equation is developed that must hold in order for mimicking to occur between parallel and serial systems within the same general family of probability distributions, and three special cases are examined. A parallel system with gamma-distributed processing times for element completion is then investigated, and it is shown that a strictly serial system cannot mimic it, but an interesting hybrid system can. This is followed by discussion of two kinds of partial identifiability, mimicking at the level of means and possible predicted differences at higher levels, and mimicking by approximation. Some qualitative considerations that may enter into conclusions as to parallelity or seriality of processing are then introduced. Last, it is suggested that in a broad sense questions related to parallel and serial systems concern fundamental aspects of information-processing structure and distribution of processing energy and hence merit further mathematical investigation.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5756bf50591599e2b2b03b00cdcca597daca2050", "title": "Pattern Recognition Approach to Medical Diagnosis", "authors": ["Casimir A. Kulikowski"], "date": 1970, "abstract": "A sequential method of pattern recognition was used to recognize hyperthyroidism in a sample of 2208 patients being treated at the Straub Clinic in Honolulu, Hawaii. For this, the method of class featuring information compression (CLAFIC) [1] was used, introducing some significant improvements in computer medical diagnosis, which by its very nature is a pattern recognition problem. A unique subspace characterizes each class at every decision stage, and the most prominent class features are selected. Thus the symptoms which best distinguish hyperthyroidism are extracted at every step and the number of tests required to reach a diagnosis is reduced.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4bb87a5e74082fe18848fed416cec0954bf78593", "title": "Computer Understanding Of Physics Problems Stated In Natural Language", "authors": ["Gordon S. Novak"], "date": 1979, "abstract": "Semantic Scholar extracted view of \"Computer Understanding Of Physics Problems Stated In Natural Language\" by Gordon S. Novak Jr.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "91a1b89eb1b286e6edd3aeba3562a9e195d6fb11", "title": "A model and stack implementation of multiple environments", "authors": ["Daniel G. Bobrow", "Ben Wegbreit"], "date": 1973, "abstract": "Many control and access environment structures require that storage for a procedure activation exist at times when control is not nested within the procedure activated. This is straightforward to implement by dynamic storage allocation with linked blocks for each activation, but rather expensive in both time and space. This paper presents an implementation technique using a single stack to hold procedure activation storage which allows retention of that storage for durations not necessarily tied to control flow. The technique has the property that, in the simple case, it runs identically to the usual automatic stack allocation and deallocation procedure. Applications of this technique to multitasking, coroutines, backtracking, label-valued variables, and functional arguments are discussed. In the initial model, a single real processor is assumed, and the implementation assumes multiple-processes coordinate by passing control explicitly to one another. A multiprocessor implementation requires only a few changes to the basic technique, as described.", "references": ["a8ff6783af496c7e1830f21a7a67f5993d829cb0", "9d5d4a641fb5612803ace0777f25ae303a1f1c03", "2e10e5896dc48f248a0db480a21d9726d2a7a807"], "page_rank": 0.0002463054187192118}, {"id": "97e0e7161f9d0e6762c396d9714f9043b8079b48", "title": "The Conniver Reference Manual", "authors": ["D. McDermott", "Gerald J. Sussman"], "date": 1972, "abstract": "Abstract : The manual is an introduction and reference to the latest version of the Conniver programming language, an artificial intelligence language with general control and data-base structures. (Author)", "references": [], "page_rank": 0.00014074595355383532}, {"id": "3eff5c712b67f76bfc6cecb1823eeadf2a38d46b", "title": "An Introduction to Mechanics", "authors": ["Daniel Kleppner", "Robert J. Kolenkow"], "date": 1973, "abstract": "1. Vectors and kinematics - a few mathematical preliminaries 2. Newton's laws - the foundations of Newtonian mechanics 3. Momentum 4. Work and energy 5. Some mathematical aspects of force and energy 6. Angular momentum and fixed axis rotation 7. Rigid body motion and the conservation of angular momentum 8. Noninertial systems and fictitious forces 9. Central force motion 10. The harmonic oscillator 11. The special theory of relativity 12. Relativistic kinematics 13. Relativistic momentum and energy 14. Four-vectors and relativistic invariance.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "9fc77941297522cc420ce9292193dd04ed2ed1af", "title": "Natural Language Input for a Computer Problem Solving System", "authors": ["Daniel G. Bobrow"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Natural Language Input for a Computer Problem Solving System\" by Daniel G. Bobrow", "references": [], "page_rank": 0.0001688951442646024}, {"id": "272772319ec475b1bb49c918863adf0cee8f715d", "title": "Will it Reach the Top? Prediction in the Mechanics World", "authors": ["Alan Bundy"], "date": 1978, "abstract": "We describe an extension of a mechanics problem solving program to the set of ''roller coaster'' problems, i.e. problems about the motion of a particle on a complex path. The reasoning strategy adopted by the program is described and compared to earlier work in this domain. Conclusions are drawn about the representation of motion and prediction. Questions are raised about Frames and Multiple Representations.", "references": ["c2dc03a92f03dbbf145a6f8b6568740abb325e19", "c1a1249f3413f70823db0b26f93cb71516eb4da5", "b8d0db8a1728e34ab45c9fe896f57c39a8d4dbaa", "ed165fa39e4ecda494049f459bbe57448101ecc4", "711c569996524f6fedbc67216d12723a4417d03e", "4ccfcee35ac6002b17fa045f0e276b9566a507f9"], "page_rank": 7.037297677691766e-05}, {"id": "e128c8750eee8781cdc76c9aa00df037180febf0", "title": "CHECKING MATHEMATICAL PROOFS BY COMPUTER", "authors": ["John McCarthy"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"CHECKING MATHEMATICAL PROOFS BY COMPUTER\" by John McCarthy", "references": [], "page_rank": 0.0002463054187192118}, {"id": "ced5e378c44d8acdd8f10cbe2a239d2390d01e43", "title": "Levels of batrachotoxin and lack of sensitivity to its action in poison-dart frogs (Phyllobates).", "authors": ["John W. Daly", "Charles W. Myers", "Jordan E. Warnick", "Edson X. Albuquerque"], "date": 1980, "abstract": "Batrachotoxin is present in remarkably high amounts in the skin of Phyllobates terribilis. Levels of batrachotoxin tend to be reduced when P. terribilis is maintained in captivity, but even after being confined for up to 6 years, these frogs were still at least five times more toxic than other Phyllobates species used by natives for poisoning blowgun darts. Batrachotoxin was not detectable in F1 progeny reared to maturity in captivity. Nerve and muscle preparations from wild-caught frogs and from the nontoxic F1 frogs were both insensitive to batrachotoxin. The regulatory site controlling sodium-channel activation and permeability appears to have been minimally altered to prevent interaction with batrachotoxin, but is still sensitive to other sodium conductance activators (veratridine, grayanotoxin) to which the frogs arenot exposed naturally.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "ab07107fd787419708a0a67d127660f86258358d", "title": "CARPS, A PROGRAM WHICH SOLVES CALCULUS WORD PROBLEMS", "authors": ["Eugene Charniak"], "date": 1968, "abstract": "A program was written to solve calculus word problems. The program CARPS (Calculus Rate Problem Solver), is restricted to rate problems. The overall plan of the program is similar to Bobrow''s STUDENT , the primary difference being the introduction of \"structures\" as the internal model in CARPS. Structures are stored internally as trees. Each structure is designed to hold the information gathered about one object. A description of CARPS is given by working through two problems, one in great detail. Also included is a critical analysis of STUDENT .", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "67fa3b4575f2fe24582b001fb5839f7f41c44671", "title": "Analyzing mathematical proofs (or reading between the lines)", "authors": ["Alan Bundy"], "date": 1975, "abstract": "He study equation solving and analyse the solutions of experienced mathematicians. We find that traditional theorem proving methods are inadequate to explain the directness of these solutions, and that the well known algorithms for polynomials etc. are inadequate to explain the wide variety of equations solved. Our analysis reveals a system of high-level descriptions, strategies and goals, which can be used to guide the search through an explosively large search space. A few of these strategies will be investigated in detail. It is hoped that this analysis will eventually be incorporated into a computer program that solves equations.", "references": [], "page_rank": 0.00015247478301665492}, {"id": "42580c5507b3fb1b378de57650b949fc37c8f53c", "title": "Arctic Steppe-Tundra: A Yukon Perspective", "authors": ["Les C. Cwynar", "Jerry C. Ritchie"], "date": 1980, "abstract": "The first reliable, securely dated full- and late-glacial pollen stratigraphy from Eastern Beringia forces the rejection of the widely held hypothesis of a steppetundra or grassland associated with extinct vertebrates and early humans. The arctic-alpine fossil flora and low pollen influx suggest a sparse tundra similar to modern herb fell-field vegetation.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "8ef05b5772f86c5018d8347e199b3cc48d6d5fbb", "title": "Anionic Constitution of 1-Atmosphere Silicate Melts: Implications for the Structure of Igneous Melts", "authors": ["David Virgo", "B. O. Mysen", "Ikuo Kushiro"], "date": 1980, "abstract": "A structural model is proposed for the polymeric units in silicate melts quenched at 1 atmosphere. The anionic units that have been identified by the use of Raman spectroscopy are SiO44\u2013 monomers, Si2O76\u2013 dimers, SiO32\u2013 chains or rings, Si2O52\u2013 sheets, and SiO2 three-dimensional units. The coexisting anionic species are related to specific ranges of the ratio of nonbridging oxygens to tetrahedrally coordinated cations (NBO/Si). In melts with 2.0 < NBO/Si < \u223c 4.0, the equilibrium is of the type [See equation in the PDF file]. In melts with NBO/Si \u223c 1.0 to 2.0, the equilibrium anionic species are given by [See equation in the PDF file]. In alkali-silicate melts with NBO/Si <~ 1.3 and in aluminosilicate melts with NBO/T < 1.0, where T is (Si + Al), the anionic species in equilibrium are given by [See equation in the PDF file]. In multicomponent melts with compositions corresponding to those of the major igneous rocks, the anionic species are TO2, T2O5, T2O6, and TO4, and the coexisting polymeric units are determined by the second and third of these disproportionation reactions.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "5fc541a7f6725ca6dc98e797df38ed63c4af8397", "title": "Models for medical diagnosis.", "authors": ["John E. Overall", "Cal. M. Williams"], "date": 1961, "abstract": "The great variety of ills that man is heir to and the tremendous number of symptoms associated with such diseases make the task of medical diagnosis more speculative than scientific. There is an expanding interest in the possibilities of using computers to help make medical diagnoses more quickly and accurately, but here again, the vastness of the task complicates the establishment of a general quantitative diagnostic procedure. Suggested here are models which simplify this procedure into a sequence of decision processes of manageable size for a statistical approach. Perhaps automatic data processing can make an initial inroad into the area of medical diagnosis through the accumulation and continuous revision of base rates appropriate to the various levels in the diagnostic decision process.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "102765f4d74cc8e2de52c9fa26854887afb635f8", "title": "Natural Deduction: A Proof-Theoretical Study", "authors": ["Dag Prawitz"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Natural Deduction: A Proof-Theoretical Study\" by Dag Prawitz", "references": [], "page_rank": 0.0004926108374384236}, {"id": "88abd9d69680c2b40330c2502422612aebdf8ec2", "title": "Spaceborne Imaging Radar: Monitoring of Ocean Waves", "authors": ["Robert C. Beal"], "date": 1980, "abstract": "A well-organized, very low energy ocean swell system off the East Coast of the United States was tracked with the Seasat synthetic aperture radar from deep water, across the continental shelf, and into shallow, water. The results indicate that spaceborne imaging radar may be used to accurately measure ocean wavelength and direction, even in coastal areas and in the presence of a mixed ocean.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "f721df0e032761342efb5a55c73175fddb13213d", "title": "The discovery of processing stages: Extensions of Donders' method", "authors": ["Saul Sternberg"], "date": 1969, "abstract": "Abstract A new method is proposed for using reaction-time (RT) measurements to study stages of information processing. It overcomes limitations of Donders' and more recent methods, and permits the discovery of stages, assessment of their properties, and separate testing of the additivity and stochastic independence of stage durations. The main feature of the additive-factor method is the search for non-interacting effects of experimental factors on mean RT. The method is applied to several binary-classification experiments, where it leads to a four-stage model, and to an identification experiment, where it distinguishes two stages. The sets of stages inferred from both these and other data are shown to carry substantive implications. It is demonstrated that stage-durations may be additive without being stochastically independent, a result that is relevant to the formulation of mathematical models of RT.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c2af4a77b5082310c7aaf98268d2d3ee8e0ccad1", "title": "Automatic Recognition of Fricatives and Plosives in Continuous Speech Using a Linear Prediction Method", "authors": ["Lee Molho"], "date": 1974, "abstract": "A linear prediction method, the low\u2010coefficient LPC (LCLPC), provides meaningful spectra of fricative and plosive areas of speech. Spectra produced by the LCLPC correspond well both with acoustic phonetic theory and with the experimental results of others. Time resolution is sufficiently narrow to allow independent spectral analysis of the release, frication, and aspiration portions of an unvoiced plosive or to demonstrate spectral change within a consonant cluster, so that clusters such as /ks/ and /ts/ may often be distinguished. For analysis of unvoiced speech, the LCLPC uses the autocorrelation method with eight coefficients and a 6\u2010msec Hamming window, at a sampling rate of 20000 samples/sec. A computer program has been written that uses sequences of LCLPC spectra to perform automatic recognition of fricatives and plosives within an operating continuous speech understanding system. Current results and examples of LCLPC spectra are presented.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f027ce53a12f36f93897a2b5733549ca323c18d0", "title": "Organization of the Hearsay II speech understanding system", "authors": ["Victor R. Lesser", "Richard D. Fennell", "Lee D. Erman", "Dabbala Rajagopal Reddy"], "date": 1975, "abstract": "Hearsay II (HSII) is a system currently under development at Carnegie-Mellon University to study the connected speech understanding problem. It is similar to Hearsay I (HSI) in that it is based on the hypothesize-and-test paradigm, using cooperating independent knowledge sources communicating with each other through a global data structure (blackboard). It differs in the sense that many of the limitations and shortcomings of HSI are resolved in HSII. The main new features of the Hearsay II system structure are: 1) the representation of knowledge as self-activating, asynchronous, parallel processes, 2) the representation of the partial analysis in a generalized three-dimensional network (the dimensions being level of representation (e.g., acoustic, phonetic, phonemic, lexical, syntactic), time, and alternatives) with contextual and structural support connections explicitly specified, 3) a convenient modular structure for incorporating new knowledge into the system at any level, and 4) a system structure suitable for execution on a parallel processing system. The main task domain under study is the retrieval of daily wire-service news stories upon voice request by the user. The main parametric representations used for this study are 1/3-octave filter-bank and linear-predictive coding (LPC)-derived vocal tract parameters [10], [11]. The acoustic segmentation and labeling procedures are parameter-independent [7]. The acoustic, phonetic, and phonological components [23] are feature-based rewriting rules which transform the segmental units into higher level phonetic units. The vocabulary size for the task is approximately 1200 words. This vocabulary information is used to generate word-level hypotheses from phonetic and surface-phonemic levels based on prosodic (stress) information. The syntax for the task permits simple English-like sentences and is used to generate hypotheses based on the probability of occurrence of that grammatical construct [19]. The semantic model is based on the news items of the day, analysis of the conversation, and the presence of certain content words in the partial analysis. This knowledge is to be represented as a production system. The system is expected to be operational on a 16-processor minicomputer system [3] being built at Carnegie-Mellon University. This paper deals primarily with the issues of the system organization of the HSII system.", "references": [], "page_rank": 0.0003549925717413402}, {"id": "3034a2fc90ec0baad8cb71c3a0037a9d6b1fd38a", "title": "Motivation and overview of SPEECHLIS: An experimental prototype for speech understanding research", "authors": ["William A. Woods"], "date": 1975, "abstract": "SPEECHLIS is a research prototype of an intelligent speech understanding system which makes use of advanced techniques of artificial intelligence, natural language processing, and acoustical and phonological analysis in an integrated way to determine the interpretation of continuous speech utterances. This paper describes a number of the characteristics of the speech understanding task which influence the ways in which syntactic, semantic, pragmatic and lexical knowledge interact with acoustical and phonological information in the process of understanding speech utterances. The focus is on what the different knowledge sources have to contribute at different points in the analysis and the organization of a computer system to combine these different sources of information into an integrated system.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6fc7b1e15b664d483e613a05afe6ba7807ce52ea", "title": "Acoustic Cues in Natural Speech: Their Nature and Potential Uses in Speech Recognition.", "authors": ["Franklin S. Cooper"], "date": 1976, "abstract": "Abstract : This final report summarizes the work carried out to study those acoustic cues in natural speech that are of potential use for speech-recognition purposes. The principal objectives of the program were: (a) to carry out basic research on automatically identifying the acoustic cues of natural speech. This research was aimed to bear directly on the scientific problems encountered in building a Speech Understanding System. (b) To work closely with the principal contractors in the program whose tasks were to build complete speech-understanding systems. It was expected that this close cooperation would allow the research results to be quickly incorporated in these systems and their usefulness to be readily evaluated.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "418f7d39be7c3476da96acb91b400364f53c2cdf", "title": "A phonetic-context controlled strategy for segmentation and phonetic labeling of speech", "authors": ["Paul Mermelstein"], "date": 1975, "abstract": "This paper considers a sequential strategy for acoustic-phonetic speech analysis. Each analysis process is applied to an appropriately labeled speech segment and results in a possible sub-segmentation of the original segment. The segments resulting from the analysis are labeled according to the analysis results. The advantages of the strategy are that no more segments are considered than those actually differentiated by the analysis steps. The extraction of acoustic cues pertinent to a phonetic feature can be tuned to classes of sounds separated on the basis of other cues, and this serves to increase the reliability of segment labeling. The analysis sequence yields a structure for the syllabic units of the speech signal that may be used to retrieve similar syllabic units for detailed comparison.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "3c2eecf7fdcac147c06da1d53efe36a926530eee", "title": "The Application of Theorem Proving to Question-Answering Systems", "authors": ["C. Cordell Green"], "date": 1969, "abstract": "This paper shows how a question-answering system can use first-order logic as its language and an automatic theorem prover, based upon the . Annual Review in Automatic Programming: International Tracts in. Google Books Result theorem-proving has concentrated on developing new inference systems. C. 1969b The application of theorem-proving to question\u2014answering systems. Symbolic Logic and Mechanical Theorem Proving Google Books Result deduction system, with an axiomatic application-domain theory, as the central. attachment mechanism, which allows the theorem prover to behave as if Artificial Intelligence Google Books Result ?QUESTION ANSWERING A question-answering system accepts information about some subject. The Application of Theorem Proving to Question Answering. niques, the application of theorem-proving techniques to new problem domains. the use of limited natural language input to a question-answering system. +. A. The application of theorem proving to question-answering systems. question-answering systems that use list-structured data bases and formal theorem-proving techniques to store facts, extract relevant data, and deduce logical . Deductive Question Answering Information Sciences Institute Finding Hypothetical Answers with a Resolution Theorem Prover. Intelligent Question-Answering Systems. formal theorem-proving techniques, the application of theorem-proving techniques to new problem domains, and the 10 Search Strategies for Theorem-Proving Department of Computing and question answering is modelled as a theorem proving task. To aid 2007 uses a machine learning method to develop the question and answer classifier Question Answering: From Partitions to Prolog* The application of theorem proving to question-answering systems. Claude Cordell. Green on Amazon.com. *FREE* shipping on qualifying offers. RESEARCH ON INTELLIGENT QUESTION-ANSWERING SYSTEMS to the Application of Theorem Proving to Problem Solving1. system that depended exclusively on formal theorem-proving methods to search for the only within a given world model to answer questions about it concerning which operators The use of theorem-proving techniques in question-answering. tics of questions in a simple question answering algorithm. The algorithm is sound, complete, and based on tableau theorem proving. The algorithm relies on a Automation of Reasoning: 2: Classical Papers on Computational. Google Books Result Using logical relevance for question answering Abstract. The Multiple ANSwer EXtraction system is a framework for interpreting a The use of theorem proving to implement question answering has received Theorem-proving by resolution as a basis for question-answering We expect to demonstrate the feasibili ty of question-answering systems that use both list-structure semantic models and formal theorem-proving techniques to . 11 Theorem-Proving by Resolution as a Basis for Question. 18 Jan 2006. In actual systems, theorem proving has long been used as a model for Applying the \u201clogical\u201d approach to question answering outlined above", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "3750098165eec68d9ea680b52ef1338f1eaa0b7c", "title": "The Interaction Of Observation And Inference", "authors": ["Robert E. Filman"], "date": 1979, "abstract": "Abstract : An intelligent computer program must have both a representation of its knowledge, and a mechanism for manipulating that knowledge in a reasoning process. This thesis examines the problem of formalizing the expression and solution of reasoning problems in a machine manipulable form. It is particularly concerned with analyzing the interaction of the standard form of deductive steps with an observational analogy obtained by performing computation in a semantic model. This dissertation is centered on the world of retrograde analysis chess, a particularly rich domain for both observational tasks and long deductive sequences. A formalization is embodied in its axioms, and a major portion is devoted to both axiomatizing the rules of chess, and discussing and comparing the representational decisions involved in that axiomatization. Consideration was given not only to the necessity for these particular choices (and possible alternatives) but also the implications of these results for designers of representational systems for other domains. Using a reasoning system for first order logic, 'FOL', a detailed proof of the solution of a difficult retrograde chess puzzle was constructed. The close correspondence between this 'formal' solution to the problem, and an 'informal, descriptive' analysis a human might present was shown. The proof and axioms were then examined for their relevance to general epistemological formalisms.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "e1a6ba42e66713d81cecedc4451ed06fc2d6823d", "title": "A pattern recognition approach to voiced-unvoiced-silence classification with applications to speech recognition", "authors": ["Bishnu S. Atal", "Lawrence R. Rabiner"], "date": 1976, "abstract": "In speech analysis, the voiced-unvoiced decision is usually performed in conjunction with pitch analysis. The linking of voiced-unvoiced (V-UV) decision to pitch analysis not only results in unnecessary complexity, but makes it difficult to classify short speech segments which are less than a few pitch periods in duration. In this paper, we describe a pattern recognition approach for deciding whether a given segment of a speech signal should be classified as voiced speech, unvoiced speech, or silence, based on measurements made on the signal. In this method, five different measurements are made on the speech segment to be classified. The measured parameters are the zero-crossing rate, the speech energy, the correlation between adjacent speech samples, the first predictor coefficient from a 12-pole linear predictive coding (LPC) analysis, and the energy in the prediction error. The speech segment is assigned to a particular class based on a minimum-distance rule obtained under the assumption that the measured parameters are distributed according to the multidimensional Gaussian probability density function. The means and covariances for the Gaussian distribution are determined from manually classified speech data included in a training set. The method has been found to provide reliable classification with speech segments as short as 10 ms and has been used for both speech analysis-synthesis and recognition applications. A simple nonlinear smoothing algorithm is described to provide a smooth 3-level contour of an utterance for use in speech recognition applications. Quantitative results and several examples illustrating the performance of the method are included in the paper.", "references": ["20e71e4135d44a77d67c2849264332bdd2534a69", "72b74927e14404ee070f46459e796d4e99de5822"], "page_rank": 4.926108374384236e-05}, {"id": "7bfdf301ace72b5316915789f0230f7714522163", "title": "A New Time-Domain Analysis of Human Speech and Other Complex Waveforms.", "authors": ["Janet M. Baker"], "date": 1975, "abstract": "Abstract : The purpose of this research is to explore the usefulness of a new time-domain analysis of complex waveforms, especially with respect to human speech. Essentially three separate investigations are presented, with the last two predicated on the results of the first: (1) Cycle-based time-domain parameters were extracted from the speech waveforms of many hundreds of utterances, and were then subjected to extensive scrutiny, both by hand and by machine. (2) Based solely on time-domain phenomena found in the previous study, the authors wrote an automatic segmentation program for continuous speech. (3) They examined the time-domain acoustic characteristics of 228 allophones of fricatives and stop consonants, for each of three speakers (2 males, 1 female). Finally, they present a personal view of the synergism inherent in the utilization of these time-domain techniques with the traditional frequency- domain techniques. In addition, suggestions are presented for applying these generalizable time-domain techniques to other complex waveforms, especially amenable to such analysis. Specific examples are drawn from music (e.g. violin) and animal (e.g. bou-bou shrike) vocalizations.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "3eca05cf111caff6c586b788f3f836cdbea46f5d", "title": "Semi-Automated Mathematics", "authors": ["J. R. Guard", "F. C. Oglesby", "J. H. Bennett", "L. G. Settle"], "date": 1969, "abstract": "The fifth in a series of experiments in semi-automated mathematics is described. These experiments culminated in large complex computer programs which allow a mathematician to prove mathematical theorems on an man/machine basis. SAM V, the fifth program, is oriented primarily toward the development of efficient automatic techniques for handling some of the more basic processes of mathematical deduction, and toward the realization of efficient real-time interaction between man and machine through the use of cathode-ray tube displays. SAM V's most notable success is the solution of an open problem in lattice theory.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "75ffaa9fd92e72c10c046f2cab543d46d3b25a35", "title": "FORMALIZATION OF PROPERTIES OF PROGRAMS", "authors": ["Zohar Manna"], "date": 1968, "abstract": "Abstract : Given a program, an algorithm will be described for constructing an expression, such that the program is valid (i.e., terminates and yields the right answer) if and only if the expression is inconsistent. Similar result for the equivalence problem of programs is given. These results suggest a new approach for proving the validity and the equivalence of programs. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "34fce7ae570071c7a01468cea9fca519fd54f10d", "title": "Toward Mechanical Mathematics", "authors": ["Hao Wang"], "date": 1960, "abstract": "Results are reported here of a rather successful attempt at proving all theorems, totalling near 400, of Principia Mathematica which are strictly in the realm of logic, viz., the restricted predicate calculus with equality. A number of other problems of the same type are discussed. It is suggested that the time is ripe for a new branch of applied logic which may be called \"inferential\" analysis, which treats proofs as numerical analysis does calculations. This discipline seems capable, in the not too remote future, of leading to machine proofs of difficult new theorems. An easier preparatory task is to use machines to formalize proofs of known theorems. This line of work may also lead to mechanical checks of new mathematical results comparable to the debugging of a program.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "dc668c1c80a7bc533ad7b20ead1f737e953f79f9", "title": "THE PROGRAMMING LANGUAGE LISP: ITS OPERATION AND APPLICATIONS,", "authors": ["Paul W. Abrahams", "Edmund C. Berkeley", "Fischer Black", "Daniel G. Bobrow", "Thomas G. Evans"], "date": 1967, "abstract": "Abstract : An introduction to LISP is given on an elementary level. Topics covered include the programming system, 240 exercises with solutions, debugging of LISP programs, and styles of programming. More advanced discussions are contained in the following articles: Techniques using LISP for automatically discovering interesting relations in data; Automation, using LISP, of inductive inference on sequences; Application of LISP to machine checking of mathematical proofs; METEOR: A LISP interpreter for string transformations; Notes on implementing LISP for the M-460 computer; LISP as the language for an incremental computer; The LISP system for the Q-2 computer; An auxiliary language for more natural expression -- the A-language. Some applications of the utilization of the LISP programming language are given in the appendices.", "references": [], "page_rank": 0.0001313628899835796}, {"id": "349e996dcd41195d862d57b8ca230affdd1e344c", "title": "The Modes of Modality", "authors": ["Jaakko Hintikka"], "date": 1969, "abstract": "By the modes of modality I do not mean the changing fashions that prevail or have prevailed in the study of modal logics, although I would be tempted to comment on them, too.1 I am referring to those modes or modifications which have given modal logic its name. In other words, I have in mind the variety of systems of modal logic and the variety of philosophically interesting interpretations which can often be given of them. The point of my paper is to recommend a specific method for the study of this variety, which to my mind constitutes a veritable embarrassment of riches. This embarrassment also affects my paper, I am afraid; the major part of it is a series of sketches for applications of my methods rather than a continuous argument.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "b1b61c2bf83a33531a6b8503c9963a48fd3df0cf", "title": "The substitution interpretation of the quantifiers", "authors": ["J. Michael Dunn", "Nuel Belnap"], "date": 1968, "abstract": "I The basic formal ideas of this interpretation were in a certain sense anticipated by E. W. Beth with his \"reduced logic\" in [3]. We are grateful to Professor Bas C. van Fraassen for bringing this to our attention. The idea has also been employed by P. T. Geach in his \"Quantification theory and the problem of identifying objects of reference\" in Acta Philosophica Fennica, 1963, and by Henry S. Leonard in his \"Essences, attributes, and predicates\" in the Proceedings of the American Philisophical Association, 1963-64. Further, H. Leblanc has studied the interpretation in a pair of forthcoming papers, \"A simplified strong completeness proof for QC=,\" and \"A simplified account of validity and implication for quantification logic.\"", "references": ["670bd553cd1db22576b54c728d520c1b16f826c6"], "page_rank": 4.926108374384236e-05}, {"id": "2d504759b4e1571075c85813d03304f0faf8e2fe", "title": "An Algebraic Study of Tense Logics with Linear Time", "authors": ["R. A. Bull"], "date": 1968, "abstract": "In [2] Prior puts forward a tense logic, GHi, which is intended to axiomatise tense logic with time linear and rational; he also contemplates the tense logic with time linear and real. The purpose of this paper is to give completeness proofs for three axiom systems, GH1, GHlr, GHli, with respect to tense logic with time linear and rational, real, and integral, respectively.' In a fourth section I show that GHI and GHlr have the finite model property, but that GHli lacks it. GHI has the operators of the classical propositional calculus, together with operators P, H, F, G for 'It has been the case that', 'It has always been the case that', 'It will be the case that', 'It will always be the case that', respectively. G and H are primitive, the others being given by", "references": [], "page_rank": 0.0001313628899835796}, {"id": "1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1", "title": "Programming a computer for playing chess", "authors": ["Claude E. Shannon"], "date": 1950, "abstract": "This paper is concerned with the problem of constructing a computing routine or \u201cprogram\u201d for a modern general purpose computer which will enable it to play chess. Although perhaps of no practical importance, the question is of theoretical interest, and it is hoped that a satisfactory solution of this problem will act as a wedge in attacking other problems of a similar nature and of greater significance. Some possibilities in this direction are:- \n \n(1) \n \nMachines for designing filters, equalizers, etc. \n \n \n \n \n(2) \n \nMachines for designing relay and switching circuits. \n \n \n \n \n(3) \n \nMachines which will handle routing of telephone calls based on the individual circumstances rather than by fixed patterns. \n \n \n \n \n(4) \n \nMachines for performing symbolic (non-numerical) mathematical operations. \n \n \n \n \n(5) \n \nMachines capable of translating from one language to another. \n \n \n \n \n(6) \n \nMachines for making strategic decisions in simplified military operations. \n \n \n \n \n(7) \n \nMachines capable of orchestrating a melody. \n \n \n \n \n(8) \n \nMachines capable of logical deduction.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "9d5d4a641fb5612803ace0777f25ae303a1f1c03", "title": "Multi-path control structures for programming languages", "authors": ["Charles J. Prenner"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Multi-path control structures for programming languages\" by Charles J. Prenner", "references": [], "page_rank": 0.00016420361247947453}, {"id": "d807145539dc1ebffc0a67182748ef5ce8c6aaab", "title": "From a Logical Point of View", "authors": ["Richard Milton Martin", "Willard van Orman Quine"], "date": 1953, "abstract": "algebra 81; entities; 311, 45, 73: terms 21. 30. 76. 78. Set: also Attribute, Class, hame Abstraction: of attributes 76, 156; of classes 30, 76ff, 87, 94ff,. 104, 159; of functions 104; of relatrons 88; of universals 117ff; principle of 90,, 96ff; vacuous 95, 159 Accident 22, 158 ACKEXMANN, Wilhelm 9011, 12011 Actuality 3f Aggregate 114f. See alSo Class Algebra: abstract 81; of classes 87,. 92, 128; of numbers 18, 45; of relations 128 Alternation 84 Alternative denial Slf, 84,94 Ambiguity 32, 58, 67 Anla$tmal: geometry 81; philosophy Analyticity 20, 22f* and existence 161, 163; and modality 143, 159-153, 156f.; and postulation 35; and reductiomsm 41; and synonymy m\u2019, 3lf, 151; contrasted with truth 130, in artificial languages 32-37 138; Ancestor 115 Antinomy, see Contradiction, Paradox ABIST~TLE 22,81, 155f Arithmetic 18f, 45, 81, 92f, 127f Artificial language 32-37 Atomic sentence 23, 30, 166 Attribute 8-11, 18, 75, 108, 156ff; versa3 class 107f, 119, 122f, 15:3 haso?&runa 96 BI.AIZK, Max 15n Bmorr, Bernard 5On, 52n BI.ANMFIELD, Leonard 5On, 52n BOOLE, George 87, 92 Bound, least upper 127 Bound variable 86f, 102ff; in ontological commitment 6, 12ff, 103, 108ff, 113, 128; in stratification 91n; erstwhile schematic 113f, llSf, 121-124. restricted to elements 97 :lOO~ Greek 111. See also Quantificadon, qariable BROUWER, L. E. J. 14, 125n BUHLER, Karl 51 CANTOR, Georg 14,92n, 121f, 126f, 129 CARNAP, Rudolf 14, 23ff, 45f, 158n; Aufbau 39ff; on modality 144n, 152156; on semantical rules 33, 36; OD synonym 29n, 32 CASSIRER, E mst 61 hADWICK, J.A.166 t%ZC Zom of 89 CHUR& Alonao 14 104, 11.6; on modalitv\u2019l52ff, 156;\u2018on semantics 108, 132x( 135n, 142, 145n; his theorem 5,96n Classes: abstraction of 30, 76ff, 87, 94ff, 104, 159; abstractness of 114; algebra of 87, 92, 128; commitment to 45f, 113, 115, 122; determination of 89; existence of, in general 14, 18 114ff 125f; existence of, in speciai theollies 39.81.90.93-109. 113. 128: Axiom 35,8&T, 96f, IOOf; of infinity 89, 93; of reducibility 125n, 127 names of 30, 108, 113ff; &s&g of 71-77, 117-127; versus attributes BARCAN, Ruth 156 Collection 107, 114 122f, 157 Behaviorism 48 Combinator 104f Begi.nsEnde nonbeing lff, 7. Sets also Commitment, ontological lff, S-11,44f, Belief 142, 144, 148 to abstract entities lOf, 45, 73, Bentham, Jeremy 39, 42 78; 13Of; m logic and semantics 96, 112f, BERNAYS, Paul 15n, 9On, 97n 116; in mathematics 13, !9 103,122, BERRY? G. G. 134 127ff. See also Criterion, .dypostasis, Bicondrtional32, 84 Ontology Common sense 45 Bind, 84% Bound Completeness 19, 89, 96, 1X6, 131, 137", "references": ["a964c10db88ffadcc907aaa4277297cd57182085", "99383f03f6698eafe37cf0b646b0483fe651723b", "3172cb4b50f1f226e89747089eb98b83f8b17ab4", "ba584e40e69f55bdeaad144e7ef3e6f92102b933", "3e865b8767daec19dceb3acdcda5b4e1e8cb52b1", "d169a71a085681efe4edf3521081fea95b20e780", "96b00a66f2f001ec0d5697f5d52311dffee7cffc"], "page_rank": 4.926108374384236e-05}, {"id": "fca44a0cd0b99e848b32d338f559c5ea879accad", "title": "One Thing Leads to Another: A New Approach to Elicitation in the Repertory Grid Technique", "authors": ["Terence R. Keen", "Richard C. Bell"], "date": 1980, "abstract": "This paper describes an interactive computer program for the elicitation of a repertory grid. The elicitation approach adopted is unique in that it can only be practically undertaken by computer. This represents a move from \u201cclassical\u201d techniques (interactive or otherwise), and enables the respondent to be an active rather than passive participant. The approach is claimed by the authors to be nearer to Kelly's concept of conversation than other interactive techniques.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4ccfcee35ac6002b17fa045f0e276b9566a507f9", "title": "Can Domain Specific Knowledge Be Generalized?", "authors": ["Alan Bundy"], "date": 1977, "abstract": "The MECHO p r o j e c t (see Cl]) cons is ts of w r i t \u00ad ing a computer program which can colve a wide va r \u00ad i e t y o f simple mechanics problems s ta ted in Eng l i sh . This program is being used as a veh ic le f o r s tudy\u00ad ing methods f o r gu id ing search in a semant ica l ly r i c h domain. Our methodology is to f i n d genera l , j u s t i f i a b l e , in ference ru les which can be combined to ca r ry out the reasoning necessary to solve the mechanics problems. As is we l l known, when ru les l i k e these are run on a general in ference machine the r e s u l t is o f ten a combinator ia l exp los ion . Rules are combined in unexpected ways and the search f o r a s o l u t i o n is developed along unreason\u00ad able paths . These f a i l u r e s are used to debug the ru les by adding to them l o c a l , domain s p e c i f i c c o n t r o l i n fo rma t i on . F i n a l l y these techniques are genera l ized and incorporated in the in ference mechanism. We hope t h a t t h i s methodology w i l l lead us to the design of a computat ional l og i c f o r na tu ra l reasoning. In t h i s paper one such t r a n s i t i o n from domain s p e c i f i c to general in ference technique w i l l be descr ibed. We w i l l use t h i s example to emphasize the importance of t h i s genera l i za t i on stage. W i th \u00ad out i t one might be led to s u p e r f i c i a l and fa l se conclusions about the nature of na tu ra l reasoning. Suppose we have ava i l ab le the fo l l ow ing r e \u00ad l a t i o n s : Ve l (Ob jec t , v, t ime) (v is the ve loc \u00ad i t y o f ob jec t dur ing t i m e ) ; A t (Ob jec t , p l ace , moment) (object is at place at moment); F i n a l ( p e r i o d , moment) (moment is the f i n a l moment of t ime i n t e r v a l p e r i o d ) . We may have discovered the f o l l ow ing domain s p e c i f i c i n \u00ad format ion enabl ing us to guide the search fo r problem so lu t i ons along successful l i n e s , ( i ) I f the program i s desperate to s a t i s f y Ve l (Ob jec t , v, t i m e ) , ob jec t and t ime being known, but a l l in ferences have ground to a h a l t , then a new in termedia te unknown v can be created and as\u00ad ser ted to be the v e l o c i t y o f ob jec t a t t ime , ( i i ) I f the program i s asked to conf i rm t h a t A t ( o b j e c t , p l a c e l , moment), but i t a l ready knows t ha t ob jec t is at some d i f f e r e n t place (place2) at moment, then the attempt to prove A t ( o b j e c t , p l a c e l , moment) can be abandoned, be\u00ad cause ob jec ts can on ly be in one place at a t ime , ( i i i ) I f the f i n a l moment o f pe r iod is found to be momentl, but l a t e r processing f a i l s , i t i s no use backing up to r eca l cu la te F i n a l ( p e r i o d , ?x ) , since the same answer is bound to be g iven . A t f i r s t glance i t might seem as i f these ex\u00ad amples argued f o r the i n t e r v e n t i o n of r i c h domain s p e c i f i c in fo rmat ion a t a l l c o n t r o l po in t s and t h a t a programming language which f a c i l i t a t e d such i n t e r v e n t i o n was requ i red . But these conclusions are not j u s t i f i e d from the examples ( i ) ( i i i ) above. In f a c t ( i ) ( i i i ) represent d i f f e r e n t facets o f a general phenomenon. To see t h i s no t i ce t h a t V e l , At and F i na l are a l l r e a l l y f unc t i ons : Vel is a f unc t i on from ob jec ts and t imes to v e l o c i t i e s ; At is a f unc t i on from ob jec ts and moments to places and F i n a l is a f unc t i on from per iods to moments. What separates func t ions from other r e l a t i o n s is t ha t they are s ing le va lued, tha t i s t h e i r value is guaranteed to e x i s t and to be unique\u2666 ( i ) is an example of t h i s existence proper ty being used and ( i i ) and ( i i i ) d i f f e r e n t uses of the unique\u00ad ness p roper t y . The genera l i za t ions o f ( i ) , ( i i ) and ( i i i ) are ( i ) ' I f the program is desperate to f i n d a func\u00ad t i o n value given i t s arguments, and a l l in ferences have f a i l e d then a new e n t i t y can be created and asserted to be t ha t va lue . (Note tha t we do not want the program to create a new e n t i t y whenever it is l ega l to do so as t h i s con t r ibu tes to the com\u00ad b i n a t o r i a l exp los ion . The no t ion of being des\u00ad perate f o r the answer can be genera l ized (see [21). ( i i ) * I f the program is t r y i n g to conf i rm a func\u00ad t i o n va lue , but i t a lready has a con t rad i c to ry value s tored then the conf i rmat ion attempt is to be abandoned. ( i i i ) ' I f the program has ca lcu la ted a f unc t i on value then i t should not reca lcu la te t h i s on back up. Note t h a t ( i ) ' ( i i i ) ' improve on the p red ica te ca lcu lus representa t ion of the existence and unique\u00ad ness of func t ion va lues , by g i v i ng procedural i n \u00ad format ion about when t h i s in fo rmat ion is to be used. They also improve on the l o c a l domain s p e c i f i c em\u00ad bodiment of ( i ) ( i i i ) by represent ing a large amount of such in fo rmat ion in concise form, e .g . the existence proper ty can now be used on At and F ina l and the uniqueness proper ty can be used on V e l . A l l t h a t i s necessary to share the bene f i t s o f the con t ro l i n fo rmat ion embodied in ( i ) ' ( i i i ) ' i s to spec i fy which r e l a t i o n s have func t i on va lues . For func t ions of one argument the implement\u00ad a t i on of ( i ) ' ( i i i ) ' can be ass is ted by ma in ta in \u00ad ing a s ing le s l o t on the proper ty l i s t of the argu\u00ad ment. However, something more complicated is needed f o r func t ions of more than one argument. Some r e l a t i o n s may be func t ions in more than one way, e . g . i f T imesys(per iod, ini tmom, finmom) means t h a t initmom is the i n i t i a l moment and finmom is the f i n a l moment of per iod then Timesys is a func t i on in 3 ways: (a) from per iod to Initmom (b) from per iod to finmom (c) from initmom and finmom to pe r iod", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "2e10e5896dc48f248a0db480a21d9726d2a7a807", "title": "Why Conniving is Better than Planning", "authors": ["Gerald J. Sussman", "D. McDermott"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Why Conniving is Better than Planning\" by Gerald J. Sussman et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "a8ff6783af496c7e1830f21a7a67f5993d829cb0", "title": "A LISP garbage-collector for virtual-memory computer systems", "authors": ["Robert Fenichel", "Jerome C. Yochelson"], "date": 1969, "abstract": "In this paper a garbage-collection algorithm for list-processing systems which operate within very large virtual memo, ies is described. The object of the algorithm is more the compaction of active storage than the discovery of free storage. Because free storage is never really exhausted, the decision to garbage collect is not easily made; therefore, various criteria of this decision are discussed.", "references": ["decda49b97ff3846fae8accf238750eb2fed20c3"], "page_rank": 0.00016420361247947453}, {"id": "c7b7662c811234c5f6692c4cb6b086e87335d7d4", "title": "On Inheritance in Knowledge Representation", "authors": ["Mark S. Fox"], "date": 1979, "abstract": "This paper examines the problem of inheritance in Knowledge representation. Research in the formellteton of Knowledge has resulted in a small number of Knowledge classes and associated inheritance relations, e.g., INSTANCE IS-A, DEROTHERC, PERSPECTIVE, Virtual-Copy, etc. (Brachman, 1977; Fahlman, 1977; Hayes, 1977; Levesque & Mylopoloue, 1978). The process of inheritance is defined by the procedures that access these inheritance relationa. This paper proposes that: 1) in some cases inheritance between concepts is idiosyncratic and does not fit predefined inheritance relations, 2) learning and discovery systems require information on how and why one concept wes derived from another, which again Is not represented in standard inheritance relations, and 3) current methods of specifying inheritance modification and similarity mappings are complex to specify and understand. Consequently, e declarative approach to inheritance and similarity specification is presented as a solution to the above problems.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b8d0db8a1728e34ab45c9fe896f57c39a8d4dbaa", "title": "Exploiting the properties of functions to control search", "authors": ["Alan Bundy"], "date": 1977, "abstract": "In the first half (sections 1-5) of the paper we examine the tradeoff between representing knowledge using functions and using relations. The properties of functions turn out to be indispensable, but to be a major contribution to the combinatorial explosion. In the second half ; (sections 6-12) an examination of these properties suggests incorporating them in the inference mechanism, where they can be used to great advantage in controlling search and reducing the combinatorial explosion. This incorporation is seen as the first step in the desin of a computational logic attuned to the demards of automatic reasoning. The purpose of the first half is introductory and may be omitted by those steeped in the traditions of Resolution theorem proving.", "references": ["b8f87dffef810076750018a3b72c617b95d254ff", "b62608c716819965f2755759ce3a7edb8a93829f", "38a7fc8802962d1f91e439c28dc13a1dd5465ece"], "page_rank": 9.852216748768472e-05}, {"id": "711c569996524f6fedbc67216d12723a4417d03e", "title": "Analysing Mathematical Proofs (Or Reading Between the Lines)", "authors": ["Alan Bundy"], "date": 1975, "abstract": "He study equation solving and analyse the solutions of experienced mathematicians. We find that traditional theorem proving methods are inadequate to explain the directness of these solutions, and that the well known algorithms for polynomials etc. are inadequate to explain the wide variety of equations solved. Our analysis reveals a system of high-level descriptions, strategies and goals, which can be used to guide the search through an explosively large search space. A few of these strategies will be investigated in detail. It is hoped that this analysis will eventually be incorporated into a computer program that solves equations.", "references": ["67fa3b4575f2fe24582b001fb5839f7f41c44671", "f52867d64e6d1a07d24661320a0df79d7f39a45f", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "69c6c392117d3ebaed85ffee143a5b5bb2b81068", "a2751e87f245446e4b0e09e31c0efa6f33e9bd4d", "bbc3ea3a3bdb50dbc4c902ffb44fd43e87b82b72", "e6622c4fa6bd5aa15ad864c54684870ed516ab0e"], "page_rank": 9.852216748768472e-05}, {"id": "d9dd5f497c6831ee93f1515dfda728bb82f3ec87", "title": "Default Reasoning and Inheritance Mechanisms on Type Hierarchies", "authors": ["Jaime G. Carbonell"], "date": 1980, "abstract": "Type hierarchies abound in Artificial Intelligence, Data Bases and Programming Languages. Although their size, use and complexity differs, all share a central inference mechanism: Inheritance of information, their raison d'etre. This paper discusses various types of type hierarchies and inheritance mechanisms, concluding with a proposed generalized inheritance mapping approach to resolve issues of lateral and upward inheritance (to augment the traditional downward approach), as well as default reasoning and limited non-monotonic inference.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "9f1fbbf387f2cfd6b01ce85f80e7a3a5fe5214a4", "title": "The semantics of PASCAL in LCF.", "authors": ["Luigia Carlucci Aiello", "Mario Aiello", "Richard W. Weyhrauch"], "date": 1974, "abstract": "We define a semantics for the arithmetic part of PASCAL by giving it an interpretation in LCF, a language based on the typed $\\lambda$-calculus. Programs are represented in terms of their abstract syntax. We show sample proofs, using LCF, of some general properties of PASCAL and the correctness of some particular programs. A program implementing the McCarthy Airline reservation system is proved correct.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "46d615ef27229e99493b5ea470d26816987afbf1", "title": "A Structural Paradigm for Representing Knowledge.", "authors": ["Ronald J. Brachman"], "date": 1978, "abstract": "Abstract : This report presents on associative network formalism for representing conceptual knowledge. While many similar formalisms have been developed since the introduction of the semantic network in 1966, they have often suffered from inconsistent interpretation of their links, lack of appropriate structure in their nodes, and general expressive inadequacy. In this paper, we take a detailed look at the history of these semantic nets and begin to understand their inadequacies by examining closely what their representational pieces have been intended to model. Based on this analysis, a new type of network is presented - the Structured Inheritance Network (SI-NET) - designed to circumvent common expressive shortcomings.", "references": [], "page_rank": 0.00039956212370005466}, {"id": "a044c62299ddb831c53c5a1a30a40ba043ae4b03", "title": "Language and Memory", "authors": ["Roger C. Schank"], "date": 1980, "abstract": "This paper outlines some of the issues and basic philosophy that have guided my work and that of my students in the last ten years. It describes the progression of conceptual representational theories developed during that time, as well as some of the research models built to implement those theories. The paper concludes with a discussion of my most recent work in the area of modelling memory. It presents a theory of MOPs (Memory Organization Packets), which serve as both processors and organizers of information in memory. This enables effective categorization of experiences in episodic memory, which in turn enables better predictive understanding of new experiences.", "references": ["8171abcaf87952fceb7c992dd4517c0cb7c0c9a7", "ec08947b7384d1d333b266928df0a3bf9e82fce1", "e695f6e94f8f77058dbfc7d2126373351823a2f6", "d68065263e5bd3d00ca34525c43745d09c73bade", "09d350ebf108eae4564e0510ca5f7ffc9e8022fa", "760e3a7fdb95b8d9a9a6cbe71134939334a2421e", "b6103c72edb55beb36571f8e724aaabfea87ea4a", "7c57e3f5cfe4b550e470fdce4d73b07baa0a4ad3", "8d487fd63f715640f118cb66f2eab23ec0379ae9", "e86f41c73b3ee96b8d75614c0537edeb55719211"], "page_rank": 0.00013683634373289543}, {"id": "b762d1701e32feeff7a60a66ca2d019bd1415c65", "title": "Degrees of translatability and canonical forms in program schemas: Part I", "authors": ["Ashok K. Chandra"], "date": 1974, "abstract": "We define a measure of the generality of the control structure of a program schema. This imposes a partial ordering on program schemas, and leads to a concept of the \u201cdifficulty\u201d of a programming problem. In this sense there exists a \u201chardest\u201d flowchart program, recursive program etc. Some earlier proofs can also be simplified and/or clarified by this approach.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "54b052229eb9ae76546c7674e1137c9ce140ebc8", "title": "An algebraic theory of recursive definitions and recursive languages", "authors": ["Eric G. Wagner"], "date": 1971, "abstract": "This is the introductory paper in a series devoted to a general algebraic theory of \u201crecursive definitions\u201d and \u201crecursive languages\u201d. In this paper we present the fundamental concepts and theorems concerning the basic structure (basic syntax), the semantics and the combination and manipulation of \u201crecursive definitions\u201d and the closure properties of \u201crecursive languages\u201d. The development is carried out within the framework of category theory and lattice theory. To illustrate the generality of the approach and our results we show how they apply directly to the specific examples of \u201crecursive languages\u201d of (generalized) context-free grammars, Turing machines, and flowcharts.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a5e82e5dba72ca4347597cb99ce6b9a1f9b9a812", "title": "PROPERTIES OF PROGRAMS AND PARTIAL FUNCTION LOGIC", "authors": ["Zohar Manna", "John McCarthy"], "date": 1969, "abstract": "Abstract : Recursive definitions are considered which consist of Algol-like conditional expressions. By specifying a computation rule for evaluating such recursive definition, it determines a partial function. However, for different computation rules, the same recursive definition may determine different partial functions. Two types of computation rules are distinguished: sequential and parallel. The purpose of the paper is to formalize properties (such as termination, correctness and equivalence) of these partial functions by means of the satisfiability or validity of certain formulas in partial function logic.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "d8723388c2b838704b00e6cce886c132c2fcbd9d", "title": "Metaphor : an inescapable phenomenon in natural language comprehension", "authors": ["Jaime G. Carbonell"], "date": 1981, "abstract": "Abstract : Interpreting metaphors is an integral and inescapable process in human understanding of natural language. Part I of this paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings. Each generalized metaphor contains a recognition network, a basic mapping, additional transfer mappings, and an implicit intention component. It is argued that this method reduces metaphor interpretation from a reconstruction to a recognition task. Steps towards automating certain aspects of language learning are also discussed. Part II analyzes analogical mappings underlying metaphors and implications for inference and memory organization. Regularities have been observed indicating that certain types of conceptual relations are much more apt to remain invariant in analogical mappings than other relations, resulting in an induced invariance hierarchy. The central thesis is that human inference processes are governed by the same analogical mappings manifest as metaphors in language. (Author)", "references": ["97ceb096cc1ca6773771bdb98cd144be0261cbac", "bc1393b5cd02cc4b866770675aef3ee3ae070410", "3110035e3168aef74a7916be91065e502ea54185", "c33136bbd76f56bb9dddc56eec981f55e1c64c47", "6b23612ba807333922bf7ff9befa1143e93c5903", "d96dcbc2aad6e2e2da8d00be8941ccc555ba3242", "2e6e24fa4acbd27506c30fec75284506a6536551", "a044c62299ddb831c53c5a1a30a40ba043ae4b03", "f3f09d77332979d8315c775c1e6654323ff661cd", "50895529ef63c8dd40c186d07748639abae65f81"], "page_rank": 8.210180623973726e-05}, {"id": "427dd6f76ec119aa185f1e2ac82f040082e7d2ff", "title": "PROGRAMS AND THEIR PROOFS: AN ALGEBRAIC APPROACH,", "authors": ["Rodney Martineau Burstall", "P. J. Landin"], "date": 1968, "abstract": "Abstract : Results are presented of some applications of universal algebra and automata theory to programming problems. A method of defining some functions as homomorphisms instead of giving recursive or iterative definitions is explained. As a demonstration of the use of this approach, a proof of the correctness of a simple compiler for expressions is given. The method of description is closely related to the methods of proving theorems about the objects described. The report includes a section on basic algebraic concepts as background for the applications, and a section in which functions commonly encountered in programming are expressed as homomorphisms. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9697b3978c639f9043acb29e4fd5b53aa9131fdd", "title": "A Formal Description of a Subset of Algol", "authors": ["John McCarthy"], "date": 1964, "abstract": "Abstract : The author describes Microalgol, a trivial subset of ALGOL, by means of an interpreter. The notions of abstract syntax and of state of the computation permit a compact description of both syntax and semantics. The author advocates an extension of this technique as a general way of describing programming languages.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "2769c203102a875c10bc11affc161891472176d1", "title": "The Mechanical Evaluation of Expressions", "authors": ["P. J. Landin"], "date": 1964, "abstract": "This paper is a contribution to the \"theory\" of the activity of using computers. It shows how some forms of expression used in current programming languages can be modelled in Church's X-notation, and then describes a way of \"interpreting\" such expressions. This suggests a method, of analyzing the things computer users write, that applies to many different problem orientations and to different phases of the activity of using a computer. Also a technique is introduced by which the various composite information structures involved can be formally characterized in their essentials, without commitment to specific written or other representations.", "references": ["b443e18512181514b19363cd54dd3309c70be20e", "83f054294ba2726d02aa03e471da773c3383b146", "d88f9058b1c3277e5266b9f76cbb067c07d2f363", "99338adde12a235040c93333fec78765b0541880", "52b99d29c931d9aaf1b3d6f48b31577affef0208", "9c6e618fe404c84ecb2fcca1dba505e040460f51"], "page_rank": 9.852216748768472e-05}, {"id": "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "title": "A Data Definition Facility for Programming Languages", "authors": ["Thomas A. Standish"], "date": 1967, "abstract": "This dissertation presents a descriptive notation for data structures which is embedded in a programming language in such a way that the resulting language behaves as a synthetic tool for describing data and processes in a number of application areas. A series of examples including formulae, lists, flow charts, Algol text, files, matrices, organic molecules and complex variables is presented to explore the use of this tool. In addition, a small formal treatment is given dealing with the equivalence of evaluators and their data structures.", "references": ["d97d69e68057bf43c844c799c9132937b2efcd98", "44e11083e4b15bfa1a09c9df7a5e6f7eefeaec01", "2769c203102a875c10bc11affc161891472176d1", "94d69ebef4df1fc18cb95e87a14c6d25c2dc92ea", "bfaf9f138b54a6e8f1093078672ce0f8368bc280", "83f054294ba2726d02aa03e471da773c3383b146", "d075466245c0a58a6c2c98198ae2c6d937b0af11", "e52924efad15d1c3cd464e1e27e031dbfe722a11", "b443e18512181514b19363cd54dd3309c70be20e", "71a372e5f528e0d55a64cfdf398da60c6e572d67"], "page_rank": 0.00011963406052076002}, {"id": "d4efc5c9aa688473beebf65f0937b91b12dd4d60", "title": "Programming Linguistics", "authors": ["E. B. Spratt"], "date": 1968, "abstract": "A Comparative Study of Programming LanguagesBy Bryan Higman. (Macdonald Computer Monographs.) Pp. v + 164. (Macdonald: London, 1967.) 45s.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "cee5d4d123d6d289a14d41baffa73723dcd3e9e7", "title": "Theory of Recursive Functions and Effective Computability", "authors": ["Jr. Hartley Rogers"], "date": 1969, "abstract": "Central concerns of the book are related theories of recursively enumerable sets, of degree of un-solvability and turing degrees in particular. A second group of topics has to do with generalizations of recursion theory. The third topics group mentioned is subrecursive computability and subrecursive hierarchies", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9c6e618fe404c84ecb2fcca1dba505e040460f51", "title": "The calculi of lambda-conversion", "authors": ["Alonzo Church"], "date": 1941, "abstract": "The description for this book, The Calculi of Lambda Conversion. (AM-6), will be forthcoming.", "references": [], "page_rank": 0.0001313628899835796}, {"id": "4fe9b057f81ef69687e1ae82d5aab750cbb6ad92", "title": "RLL-1: A Representation Language Language", "authors": ["Russell Greiner"], "date": 1980, "abstract": "Abstract : The field of AI is strewn with knowledge representation languages. The language designer typically designs that language with one particular application domain in mind; as subsequent types of applications are tried, what had originally been useful features are found to be undesirable limitations, and the language is overhauled or scrapped. One remedy to this bleak cycle might be to construct a representation language whose domain is the field of representational languages itself. Toward this end, we designed and implemented RLL-1, a frame-based Representation Language Language. The components of representation languages in general (such as slots and inheritance mechanisms) and of RLL-1 itself, in particular, are encoded declaratively as frames. By modifying these frames, the user can change the semantics of RLL-1's components, and significantly alter the overall character of the RLL-1 environment. Often a large Artificial Intelligence project begins by designing and implementing a high-level language in which to easily and precisely specify the nuances of the task. The language designer typically builds his Representation Language around the one particular highlighted application (such as molecular biology for Units (Stefik), or natural language understanding for KRL (Bobrow & Winograd) and OWL (Szolovits, et al.)). For this reason, his language is often inadequate for any subsequent applications, except those which can be cast in a form similar in structure to the initial task. What had originally been useful features are subsequently found to be undesirable limitations. Consider Units' explicit copying of inherited facts or KRL's sophisticated but slow matcher.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "72b74927e14404ee070f46459e796d4e99de5822", "title": "Homomorphic analysis of speech", "authors": ["Alan V. Oppenheim", "R. W. Schafer"], "date": 1968, "abstract": "Classes of systems which satisfy a generalized principle of superposition have been previously proposed and termed \"homomorphic systems,\" emphasizing their interpretation as homomorphic (i.e., algebraically linear) transformations. One such class appears suited to the separation of signals that have been convolved. In this paper, an approach to deconvolution of speech, based on these ideas, is discussed.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "043acfaf749c456f76fe010022771ce39979f196", "title": "AM, An Artificial Intelligence Approach To Discovery In Mathematics As Heuristic Search", "authors": ["Douglas B. Lenat"], "date": 1976, "abstract": "Abstract : A program called 'AM', is described which models one aspect of elementary mathematics research: developing new concepts under the guidance of a large body of heuristic rules. 'Mathematics' is considered as a type of intelligent behavior, not as a finished product. The local heuristics communicate via an agenda mechanism, a global list of tasks for the system to perform and reasons why each task is plausible. A single task might direct AM to define a new concept, or to explore some facet of an existing concept, or to examine some empirical data for regularities, etc. Repeatedly, the program selects from the agenda the task having the best supporting reasons, and then executes it. Each concept is an active, structured knowledge module. A hundred very incomplete modules are initially provided, each one corresponding to an elementary set-theoretic concept (e.g.,union). This provides a definite but immense 'space' which AM begins to explore. AM extends its knowledge base, ultimately rediscovering hundreds of common concepts (e.g., numbers) and theorems (e.g., unique factorization). This approach to plausible inference contains great powers and great limitations.", "references": ["2edc8083073837564306943aab77d6dcc19d0cdc", "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "f0bf6182a22a44dfe9263ed35d9fd53c76026301", "e5a58c6f2ba6fad0ae564a921a19536c27434cdc", "52ccf415a6a6d9d9cb87c9e94a9264470bcf97e0", "c58fd3e1da8ff5aacb40843ba0728acfa74f82b8"], "page_rank": 8.210180623973726e-05}, {"id": "8482a60d94226f6734871761ec212ed3dd22a76c", "title": "Learning by Understanding Analogies", "authors": ["Russell Greiner"], "date": 1988, "abstract": "This research describes a method for learning by analogy \u2014 i.e., for proposing new conjectures about a target analogue based on facts known about a source analogue. We formalize this process as a rule of plausible inference and present heuristics which guide this process towards efficiently postulating useful new conjectures. The most important rule involves the use of abstractions \u2014 abstract relations that encode solution methods to past problems.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ea0bc6190993e02ea017c57595df6d4a6edc0d20", "title": "OPS, A Domain-Independent Production System Language", "authors": ["Charles Forgy", "John P. McDermott"], "date": 1977, "abstract": "It has been claimed that production systems have several advantages over other representational schemes. These include the potential for general self-augmentation (i.e., learning of new behavior) and the ability to function in complex environments. The production system language, OPS, was implemented to test these claims. In this paper we explore some of the issues that bear on the design of production system languages and try to show the adequacy of OPS for its intended purpose.", "references": ["9c99fd71e874844c4d8c982b18610a4df28fba33", "8ac63e41dbf2706a7d01adb565ecc50d03202963", "d55fc396338da162550aab6b0397d69063abffa8", "c8141014edf78d36e9171d5a1627b9a0411064d5", "6961e0b84092444701c75e7eaf133aa91584853a", "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "7abed618e349427fde64d68cc98c6b6bb844abb4", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "4037b204e73b1d4fa45961edb9b9a2a42e105fa6", "209c981960eac9e5c4a260f004c6def64e6dca67"], "page_rank": 0.0004926108374384236}, {"id": "670bd553cd1db22576b54c728d520c1b16f826c6", "title": "Modalities and intensional languages", "authors": ["Ruth Barcan Marcus"], "date": 2004, "abstract": "The subject of this paper is the foundations of modal logic. By founda tions, we generally mean the underlying assumptions, the underpinnings. There is a normative sense in which it has been claimed that modal logic is without foundation. Professor Quine, in Word and Object, suggests that it was conceived in sin: the sin of confusing use and mention. The original transgressors were Russell and Whitehead. Lewis followed suit and constructed a logic in which an operator corresponding to 'necessarily' operates on sentences whereas 'is necessary' ought to be viewed as a predicate of sentences. As Professor Quine reconstructs the history of the enterprise,1 the operational use of modalities promised only one advan tage : the possibility of quantifying into modal contexts. This several of us2 were enticed into doing. But the evils of the sentential calculus were found out in the functional calculus, and with it - to quote again from Word and Object - 'the varied sorrows of modality transpose'. I do not intend to claim that modal logic is wholly without sorrows, but only that they are not those which Professor Quine describes. I do claim that modal logic is worthy of defense, for it is useful in connection with many interesting and important questions such as the analysis of causa tion, entailment, obligation and belief statements, to name only a few. If we insist on equating formal logic with strongly extensional functional", "references": [], "page_rank": 0.0004926108374384236}, {"id": "e3945122239c960401555787a4128f54c127af97", "title": "Learning Physical Descriptions From Functional Definitions, Examples, and Precedents", "authors": ["Patrick Henry Winston", "Boris Katz", "Thomas O. Binford", "Michael R. Lowry"], "date": 1983, "abstract": "Abstract : It is too hard to tell vision systems what things look like. It is easier to talk about purpose and what things are for. Consequently, we want vision systems to use functional descriptions to identify things, when necessary, and we want them to learn physical descriptions for themselves, when possible. This paper describes a theory that explains how to make such systems work. The theory is a synthesis of two sets of ideas: ideas about learning from precedents and exercises developed at MIT and ideas about physical description developed at Stanford. The strength of the synthesis is illustrated by way of representative experiments. All of these experiments have been performed with an implemented system. (Author)", "references": ["0df1aac45ff562089a3bdbcb34e2481a71478651", "5ead274e342a696d52d2bc6484a5556feb389b63", "6bfa4cb6db49463c0e1f6604d822f776f5613cd8", "eef458bd7a6284c18a72d1eeaeb48a707fe743dd"], "page_rank": 6.157635467980295e-05}, {"id": "bc1393b5cd02cc4b866770675aef3ee3ae070410", "title": "Metaphors We Live by", "authors": ["Barbara M. H. Strang", "George Lakoff", "Mark Johnson"], "date": 1982, "abstract": "Semantic Scholar extracted view of \"Metaphors We Live by\" by Barbara M. H. Strang et al.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "96b00a66f2f001ec0d5697f5d52311dffee7cffc", "title": "A Note on Truth", "authors": ["John F. Thomson"], "date": 1949, "abstract": "Semantic Scholar extracted view of \"A Note on Truth\" by John F. Thomson", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4cc4a5e1591a5a4e81f6ad52e05833b3e750f56e", "title": "A Representation Language Language", "authors": ["Russell Greiner", "Douglas B. Lenat"], "date": 1980, "abstract": "The field of AI is strewn with knowledge representation languages. The language designer typically has one particular application domain in mind; as subsequent types of applications are tried, what had originally been useful features become undesirable limitations, and the language is overhauled or scrapped. One remedy to this bleak cycle might be to construct a representational scheme whose domain is the field of representational languages itself. Toward this end, we designed and implemented RLL, a frame-based Representation Languange Language. The components of representation languages in general (such as slots and inheritance mechanisms) and of RLL itself are encoded declaratively as frames. Modifying these frames can change the semantics of RLL, by radically altering the character of the RLL environment.", "references": ["2dd80a919ac78321fbdb4596969a40f87bdfaa26", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "95075b1b971e970be5aad3a15011f9042c72f873", "7a524d034949e44547cbbcc24454a2ef5eed10f0", "18cf70ed5f8f242fcc045234c94e151d2c25a36e", "534177394b6b3ed4b0a8ba42c0e63d66f43029d9", "7c46d6fe1c78ffa46664ee9f47f1f1063ce16879", "ddce6792562bae511ff07fb362cec31a6dc884f9"], "page_rank": 0.0001984126984126984}, {"id": "d169a71a085681efe4edf3521081fea95b20e780", "title": "A Complete Theory of Natural, Rational, and Real Numbers", "authors": ["John R. Myhill"], "date": 1950, "abstract": "Semantic Scholar extracted view of \"A Complete Theory of Natural, Rational, and Real Numbers\" by John R. Myhill", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "c511446de2dbd77f5a5fd14809d17e6ce5fec16f", "title": "Knoesphere: Building Expert Systems With Encyclopedic Knowledge", "authors": ["Douglas B. Lenat", "Alan Borning", "David W. McDonald", "Craig Taylor", "Steven Weyer"], "date": 1983, "abstract": "The Knoesphere project is an attempt to build an expert system that is encyclopedic, in the breadth of coverage of its knowledge base, and in the degree of integration of that knowledge. The primary issue is how to aid users in searching complex bodies of knowledge. Our approach is to frame the system more as a museum than a set of tomes, and to have the user take more or less guided tours of the exhibits therein. The impact of such a system on everyday life - entertainment and, eventually, education -- is clear. We discuss its potential for progress in Al as well: a testbed for representation, speech understanding, natural language understanding and generation, automatic story generation and animation, learning, user modelling, and planning. Having an immensely broad and moderately deep knowledge base, the system may also serve as a useful testbed for exploiting analogy and metaphor as a source of power. The work is in its early stages, hence much of what we present is the design for this system, not finished results. We do calculate the magnitude of the tasks involved in such an ambitious endeavor, and give scenarios of its use.", "references": ["2f4ba7aa63ac4bcceb1bc745183b2f7d6bd83685", "f07c5ed1ce55f49a9a4d4109279aef80e52e8fdc", "4cc4a5e1591a5a4e81f6ad52e05833b3e750f56e", "1cf6628cabc2f899222db78249a3a25d75c9e872", "75d19f772424b29826896ff3b7bf1d10b70c5453", "46f41bcaf5d69e3586571c6b8c91f525096726f5", "b1abc47a7a0a18a76884d59ad848d7caa32f5cc7"], "page_rank": 6.157635467980295e-05}, {"id": "2c95d7a9b6713eb0e92e22015274769e39129afe", "title": "Some expert systems need common sense.", "authors": ["Jeff McCarthy"], "date": 1984, "abstract": "An expert system is a computer program intended to embody the knowledge and ability of an expert in a certain domain. The ideas behind them and several examples have been described in other lectures in this symposium. Their performance in their specialized domains are often very impressive. Nevertheless, hardly any of them have certain common sense knowledge and ability possessed by any non-feeble-minded human. This lack makes them ``brittle''. By this is meant that they are difficult to extend beyond the scope originally contemplated by their designers, and they usually don't recognize their own limitations. Many important applications will require common sense abilities. The object of this lecture is to describe common sense abilities and the problems that require them. Common sense facts and methods are only very partially understood today, and extending this understanding is the key problem facing artificial intelligence. This isn't exactly a new point of view. I have been advocating ``Computer Programs with Common Sense''since I wrote a paper with that title in 1958. Studying common sense capability has sometimes been popular and sometimes unpopular among AI researchers. At present it's popular, perhaps because new AI knowledge offers new hope of progress. Certainly AI researchers today know a lot more about what common sense is than I knew in 1958 -- or in 1969 when I wrote another paper on the subject. However, expressing common sense knowledge in formal terms has proved very difficult, and the number of scientists working in the area is still far too small. One of the best known expert systems is MYCIN (Shortliffe 1976; Davis, Buchanan and Shortliffe 1977), a program for advising physicians on treating bacterial infections of the blood and meningitis. It does reasonably well without common sense, provided the user has common sense and understands the program's limitations. MYCIN conducts a question and answer dialog. After asking basic facts about the patient such as name, sex and age, MYCIN asks about suspected bacterial organisms, suspected sites of infection, the presence of specific symptoms (e.g. fever, headache) relevant to diagnosis, the outcome of laboratory tests, and some others. It then recommends a certain course of antibiotics. While the dialog is in English, MYCIN avoids having to understand freely written English by controlling the dialog. It outputs sentences, but the user types only single words or standard phrases. Its major innovations over many previous expert systems were that it uses measures of uncertainty (not probabilities) for its diagnoses and the fact that it is prepared to explain its reasoning to the physician, so he can decide whether to accept it. Our discussion of MYCIN begins with its ontology. The ontology of a program is the set of entities that its variables range over. Essentially this is what it can have information about. MYCIN's ontology includes bacteria, symptoms, tests, possible sites of infection, antibiotics and treatments. Doctors, hospitals, illness and death are absent. Even patients are not really part of the ontology, although MYCIN asks for many facts about the specific patient. This is because patients aren't values of variables, and MYCIN never compares the infections of two different patients. It would therefore be difficult to modify MYCIN to learn from its experience. MYCIN's program, written in a general scheme called EMYCIN, is a so-called production system. A production system is a collection of rules, each of which has two parts -- a pattern part and an action part. When a rule is activated, MYCIN tests whether the pattern part matches the database. If so this results in the variables in the pattern being matched to whatever entities are required for the match of the database. If not the pattern fails and MYCIN tries another. If the match is successful, then MYCIN performs the action part of the pattern using the values of the variables determined by the pattern part. The whole process of questioning and recommending is built up out of productions. The production formalism turned out to be suitable for representing a large amount of information about the diagnosis and treatment of bacterial infections. When MYCIN is used in its intended manner it scores better than medical students or interns or practicing physicians and on a par with experts in bacterial diseases when the latter are asked to perform in the same way. However, MYCIN has not been put into production use, and the reasons given by experts in the area varied when I asked whether it would be appropriate to sell MYCIN cassettes to doctors wanting to put it on their micro-computers. Some said it would be ok if there were a means of keeping MYCIN's database current with new discoveries in the field, i.e. with new tests, new theories, new diagnoses and new antibiotics. For example, MYCIN would have to be told about Legionnaire's disease and the associated Legionnella bacteria which became understood only after MYCIN was finished. (MYCIN is very stubborn about new bacteria, and simply replies ``unrecognized response''.) Others say that MYCIN is not even close to usable except experimentally, because it doesn't know its own limitations. I suppose this is partly a question of whether the doctor using MYCIN is trusted to understand the documentation about its limitations. Programmers always develop the idea that the users of their programs are idiots, so the opinion that doctors aren't smart enough not to be misled by MYCIN's limitations may be at least partly a consequence of this ideology. An example of MYCIN not knowing its limitations can be excited by telling MYCIN that the patient has Cholerae Vibrio in his intestines. MYCIN will cheerfully recommend two weeks of tetracycline and nothing else. Presumably this would indeed kill the bacteria, but most likely the patient will be dead of cholera long before that. However, the physician will presumably know that the diarrhea has to be treated and look elsewhere for how to do it. On the other hand it may be really true that some measure of common sense is required for usefulness even in this narrow domain. We'll list some areas of common sense knowledge and reasoning ability and also apply the criteria to MYCIN and other hypothetical programs operating in MYCIN's domain.", "references": ["494aedf82da4755badc1fe74e4d21cf5fc029e9d", "cf60718f4bec5ded5abd61655ce712be37b54770", "ea11b93aca36bc18638d8f646eab73c153ee910d"], "page_rank": 6.157635467980295e-05}, {"id": "3e865b8767daec19dceb3acdcda5b4e1e8cb52b1", "title": "Modality and Description", "authors": ["Arthur Francis Smullyan"], "date": 1948, "abstract": "There are logicians who maintain that modal logic violates Leibniz's principle that if x and y are identical, then y has every property of x . The alleged difficulty is illustrated in the following example due to Quine. 1 A. It is logically necessary that 9 is less than 10. B. 9 = the number of the planets. C. Therefore, it is logically necessary that the number of the planets is less than 10.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "decda49b97ff3846fae8accf238750eb2fed20c3", "title": "Recursive Functions of Symbolic Expressions and their Computation by by machine", "authors": ["J. D. Carthy"], "date": 1960, "abstract": "A three component achromatic microscope objective having a magnification of 2.5X and a numerical aperture of 0.07.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "ba584e40e69f55bdeaad144e7ef3e6f92102b933", "title": "Definability and Decision Problems in Arithmetic", "authors": ["Julia Robinson"], "date": 1949, "abstract": "In this paper, we are concerned with the arithmetical definability of certain notions of integers and rationals in terms of other notions. The results derived will be applied to obtain a negative solution of corresponding decision problems. In Section 1, we show that addition of positive integers can be defined arithmetically in terms of multiplication and the unary operation of successor S (where Sa = a + 1). Also, it is shown that both addition and multiplication can be defined arithmetically in terms of successor and the relation of divisibility | (where x|y means x divides y ).", "references": ["392746142b0b67726be2fa16611586f32bd95630", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3", "c3f7ad4b2af9d9888be85a8a376c61e7a612acc2", "4ec9c76a327e38958378a5a7be88639678d2edef", "626d07872e6e795dd4861908c58d4bed0fe6c98e", "2fcec1760208cc955cbcad4ec5c66d2f3189da55"], "page_rank": 7.037297677691766e-05}, {"id": "99383f03f6698eafe37cf0b646b0483fe651723b", "title": "A Formal System of Logic", "authors": ["Hao Wang"], "date": 1950, "abstract": "The main purpose of this paper is to present a formal system P in which we enjoy a smooth-running technique and which countenances a universe of classes which is symmetrical as between large and small. More exactly, P is a system which differs from the inconsistent system of [ 1 ] only in the introduction of a rather natural new restrictive condition on the defining formulas of the elements (sets, membership-eligible classes). It will be proved that if the weaker system of [ 2 ] is consistent, then P is also consistent. After the discovery of paradoxes, it may be recalled, Russell and Zermelo in the same year proposed two different ways of safeguarding logic against contradictions (see [ 3 ], [ 4 ]). Since then various simplifications and refinements of these systems have been made. However, in the resulting systems of Zermelo set theory, generation of classes still tends to be laborious and uncertain; and in the systems of Russell's theory of types, complications in the matter of reduplication of classes and meaningfulness of formulas remain. In [ 2 ], Quine introduced a system which seems to be free from all these complications. But later it was found out that in it there appears to be an unavoidable difficulty connected with mathematical induction. Indeed, we encounter the curious situation that although we can prove in it the existence of a class V of all classes, and we can also prove particular existence theorems for each of infinitely many classes, nobody has so far contrived to prove in it that V is an infinite class or that there exists an infinite class at all.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "a964c10db88ffadcc907aaa4277297cd57182085", "title": "A Set of Axioms for Logic", "authors": ["Theodore Hailperin"], "date": 1944, "abstract": "One of the preeminent problems confronting logicians is that of constructing a system of logic which will be adequate for mathematics. By a system's being adequate for mathematics, we mean that all mathematical theorems in general use can be deduced within the system. Several distinct logical systems, all having this end in view, have been proposed. Among these perhaps the best known are the systems referred to as \u201cPrincipia Mathematica\u201d and \u201cset theory.\u201d In both of these systems (we refer to the revised and simplified versions) there is a nucleus of propositions which can be derived by using only the axioms and rules of the restricted predicate calculus. However, if anything like adequacy for mathematics is to be expected, additional primitives and axioms must be added to the restricted predicate calculus. It is in their treatment of the additional primitive e, denoting class or set membership, that the above-mentioned systems differ. In addition to these two, a third and a stronger system has been proposed by W. V. Quine in his paper New foundations for mathematical logic . It is with this system of Quine's that our work is concerned and of which we now give a brief description.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e6622c4fa6bd5aa15ad864c54684870ed516ab0e", "title": "Some problems and non-problems in representation theory", "authors": ["Patrick J. Hayes"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Some problems and non-problems in representation theory\" by Patrick J. Hayes", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7c57e3f5cfe4b550e470fdce4d73b07baa0a4ad3", "title": "On some supposed contributions of artificial intelligence to the scientific study of language", "authors": ["B. Elan Dresher", "Norbert Hornstein"], "date": 1976, "abstract": "The earliest attempts to apply computers to the problem of understanding natural language, although entered into with great optimism, did not meet with much success. The ultimate failure of the various machine translation projects, chronicled by Bar Hillel (1964) showed that the basic approach of these projects was incorrect. This approach viewed translation between natural languages as a fairly trivial process: it was hoped that if the computer were only supplied with dictionaries of the two languages and a certain amount of grammatical information then it would be able to produce fairly high quality translations. The nature of this failure led to the belief that a more sophisticated view of language, involving not only a dictionary and grammar but also full semantics and total knowledge of the world, was required if computers were to deal with natural language with any success at all. The problem of applying computers to natural language, which had formerly seemed quite tractable, if not trivial, now appeared to be all but impossible. Thus it was to the considerable surprise and delight of many that Joseph Weizenbaum\u2019s ELIZA program was able to simulate a sort of Rogerian psychiatrist with a degree of success which surpassed all expectations and which even aroused a certain amount of interest in psychoanalytical circles. Since then, less outwardly striking but more influential work, such as Terry Winograd\u2019s SHRDLU program, have contributed to a renewed feeling of optimism that computers can be made to understand human language\u2019.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "a2751e87f245446e4b0e09e31c0efa6f33e9bd4d", "title": "Mathematical discovery : on understanding, learning, and teaching problem solving", "authors": ["George P{\\'o}lya"], "date": 1962, "abstract": "PATTERNS. The Pattern of Two Loci. The Cartesian Pattern. Recursion. Superposition. TOWARD A GENERAL METHOD. Problems. Widening the Scope. Solutions. Appendix. Bibliography. Index.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b6103c72edb55beb36571f8e724aaabfea87ea4a", "title": "Finding the Conceptual Content and Intention in an Utterance in Natural Language Conversation", "authors": ["Roger C. Schank"], "date": 1971, "abstract": "The conceptual dependency analyzer described in the first IJCAI (8) has been modified so as to function more conceptually with less reliance on syntactic rules. In order to have an analyzer be conceptually driven, it is necessary for the system to know what it is looking for. That is, it must make predictions as to what can follow conceptually at any point in the analysis. This paper discusses the extension of conceptual prediction to include predictions based on context and the structure of the memory model that operates with the analyzer. Such predictions make use of relations between conceptual actions and the implications of those actions. This enables the conceptual analyzer to discover not only the conceptual content of an utterance but also the intention of that utterance in context. We are concerned with the extraction of the conceptual content both explicit and implicit in an utterance in order to analyze effectively in an interactive conversational situation.", "references": ["cf2c04bbf503c01379d0fd59d943dcbc4013f01f", "3398fd64b4bcc984fbd96de71eecb6188eece7ad", "ff5cec60c25b0133da5d00d732299a90c4159779"], "page_rank": 4.926108374384236e-05}, {"id": "760e3a7fdb95b8d9a9a6cbe71134939334a2421e", "title": "Computer simulation of change in personal belief systems.", "authors": ["Kenneth Mark Colby"], "date": 1967, "abstract": "This paper deals with the following questions and answers: Are models the same as theories? No. Can a computer program serve as a model of human belief systems? Yes. How do humans change their minds? We know little about it. How can models of belief systems be corroborated? By engaging the person whose belief system is being modelled in repeated on-line dialogues. Is it dehumanizing to use a machine such as a computer as an agent for mental change? No.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "bbc3ea3a3bdb50dbc4c902ffb44fd43e87b82b72", "title": "Building in equational theories", "authors": ["Gordon Plotkin"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Building in equational theories\" by Gordon Plotkin", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "d68065263e5bd3d00ca34525c43745d09c73bade", "title": "Some effects of grammatical transformations on the recall of english sentences", "authors": ["Jacques Mehler"], "date": 1963, "abstract": "Summary The recall of English sentences varying systematically in syntactic structure was studied by the method of prompted recall with 80 S s. Analysis of the errors indicated that most of them were due to syntactical confusions. The hypothesis is advanced that S s analyze the sentences into a semantic component plus syntactic corrections when they learn them, and that this separation of semantic content from syntactic form is one reason that the general meaning of a message is generally so much easier to recall than its exact wording.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e695f6e94f8f77058dbfc7d2126373351823a2f6", "title": "Fact retrieval and the paradox of interference", "authors": ["Edward E. Smith", "Nancy Adams", "Dennis Schorr"], "date": 1978, "abstract": "Many theories of memory incorporate the notion of retrieval interference, which says that facts learned about the same concept can interfere with one another during retrieval. This notion implies that the more facts learned about a concept the greater the retrieval interference. But this makes it paradoxical how one ever becomes knowledgeable about a topic. To resolve this paradox, we argue that one can overcome interference by using relevant world knowledge to integrate various facts learned about a concept. \n \nThree experiments demonstrate that integration can overcome interference. All used a recognition paradigm developed by Anderson and Bower 1973. Subjects first memorized a set of facts about particular people (e.g., Marly, the banker), and then decided whether or not test sentences were from the memorized set. Experiments 1 and 2 manipulated (a) the number of facts learned about a particular person and (b) whether or not they were integrated. With unintegrated facts, the time to recognize a test sentence increased with the number of facts learned about the person mentioned in that sentence; with integrated facts, no such increase was found. Experiment 3 further showed that subjects had difficulty rejecting distractors that were thematically related to integrated facts. To account for these effects, two hypotheses were considered, one an alteration of Anderson and Bower's HAM model, and the other an elaboration of Schank and Abelson's 1977 script analysis.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "d96dcbc2aad6e2e2da8d00be8941ccc555ba3242", "title": "Knowledge Structures and Language Boundaries", "authors": ["Yorick Wilks"], "date": 1977, "abstract": "The paper discusses the incorporation of richer semantic structures into the Preference Semantics system: they are called pseudo-texts and capture something of the information expressed in one type of frame proposed by Minsky (q.v.). However, they are in a format, and subject to rules of inference, consistent with earlier accounts of this system of language analysis and understanding. Their use is discussed in connection with the phen\u00ad omenon of extended use: sentences where the semantic preferences are broken. It is argued that such situations are the norm and not the exception in normal language use, and that a language under\u00ad standing system must give some general treatment of them.", "references": ["ff79d5d1282aea51195afcb7de898dca2c879a97", "a4a7f3d5db8fb6f2f6380e5f913a289f5130a898", "44fe38d52ba55f4d9ee6bc12eaf4aea4dfefd7c4"], "page_rank": 5.473453749315818e-05}, {"id": "2e6e24fa4acbd27506c30fec75284506a6536551", "title": "Retrieval and organizational strategies in conceptual memory: a computer model", "authors": ["Janet L. Kolodner"], "date": 1980, "abstract": "Abstract : People effortlessly recall past events and episodes in their lives many times in the course of a normal day. A reasonable goal in the design of computer programs is to construct a memory with that same capability. To facilitate human-like retrieval of events from a computer memory, we must first specify a reasonable memory organization. We must then design updating and retrieval processes to build up and access that information. This thesis will present such a theory, and will describe a computer program called CYRUS which implements that theory. CYRUS (Computerized Yale Retrieval and Updating System) stores and retrieves episodes in the lives of Secretaries of State Cyrus Vance and Edmund Muskie. When new events are added to its memory, CYRUS integrates them into memory along with the events it already knows about. CYRUS can then answer questions posed to it in English about the events it stores. The algorithms and memory organization used in CYRUS have been developed by examining the way people answer questions requiring extensive memory search. Its reconstructive processes include instantiation strategies, which construct and elaborate on contexts for search, and search strategies, which direct construction.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "6b23612ba807333922bf7ff9befa1143e93c5903", "title": "Generalization and memory in an integrated understanding system", "authors": ["Michael Lebowitz"], "date": 1980, "abstract": "Abstract : Generalization and memory are part of natural language understanding. As people read stories describing various situations they are able to recall similar episodes form memory and use them as a basis to form generalizations about the way such situations normally occur. This thesis describes an integrated system for language understanding IPP (Integrated Partial Parser), that encompasses the ability to generalize and record information in long-term memory as well as conceptual analysis. IPP is a program that learns about the world by reading stories taken from newspapers and th UPI news wire, adding information from these stories to memory, and making generalizations that describe specific situations. It uses the generalizations that it has made to help in understanding future stories. As it reads stories, IPP adds them to its permanent memory. If it locates similar stories in memory as it does this, then it attempts to make generalizations that describe the similarities among the events. Such generalizations form the basis for organizing events in memory and understanding later stories. IPP also includes a procedure for confirming generalizations as further stories are read. In order to analyze the text that it reads, IPP makes extensive use of top-down, predictive processing. As it processes a story, IPP accesses memory in an attempt to identify generalizations describing stereotypical situations that can provide predictions to be used in understanding. Such use of memory to provide top-down context results in a robust and efficient understanding system. (Author)", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "71a372e5f528e0d55a64cfdf398da60c6e572d67", "title": "A definition of forumla ALGOL", "authors": ["Alan J. Perlis", "Renato Iturriaga", "Thomas A. Standish"], "date": 1966, "abstract": "Formula ALGOL is an extension of ALGOL 60 incorporating formula manipulation and list processing. The extension is ac1 complished by adding two new types of data structures: formulas and list structures with an appropriate set of processes to manipulate them. The control structure of ALGOL 60 is inherited without change. Algorithms may construct formulas and list structures at run time; in fact, ALGOL 60 is contained within the language as a subset. Operations are available which alter or combine formulas and list structures, and which access arbitrary subexpressions. Formulas may be evaluated, substituting numerical or logical values for occurrences of variables contained within. They may be subjected to substitution processes causing the replacement of occurrences of variables by designated formulas. They may be subjected to transformations defined by sets of rules akin to Markov algorithms. Predicates are available to determine precisely the structure and composition of any formula or list structure constructable, and mechanisms are provided to extract subexpressions of a formula or sublists of a list provided its structure is known. Numerical, logical and formula values may be stored as elements in list structures, and retrieval mechanisms exist to select them for use as constituents in other processes. Description lists composed of attributes with associated lists of values may be attached to formulas and to list structures and may be associated with identifiers naming variables. Processes exist for retrieving value lists and for creating, altering and deleting attribute-value list pairs. Pushdown stacks of arbitrary depth are available for the storage of all types of data structures, and generators are provided in the form of new types of for statements which assign to controlled variables the elements of list structures for use in an arbitrary process. Several standard procedures create names at run-time, provide the current size of the available space list, differentiate a formula with respect to a given variable, erase list structures and so on. Finally, both arrays and procedures may be defined having formulas or list structures as values. The system is in operation on the CDC G-21 at Carnegie Institute of Technology.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c33136bbd76f56bb9dddc56eec981f55e1c64c47", "title": "CONSIDERATIONS OF SOME PROBLEMS OF COMPREHENSION", "authors": ["John D. Bransford", "Marcia K. Johnson"], "date": 1973, "abstract": "Publisher Summary This chapter discusses some of the contributions made by listeners while comprehending and remembering. The ability to understand linguistic symbols is based not only on the comprehender's knowledge of his language but also on his general knowledge of the world. Much of the extralinguistic knowledge affecting comprehension and memory may come from visually presented information. The chapter presents a number of studies that illustrate some of the interplay between linguistic inputs and extralinguistic knowledge. It highlights various implications of these studies with respect to the problem of characterizing the thought processes involved in comprehending language, and of characterizing the role of comprehension factors in learning and memory. The results of the studies reported do not dictate a detailed model of comprehension, but they suggest a general orientation toward the problem of linguistic comprehension that places it squarely within the domain of cognitive psychology, and that generates questions for future research. The aspects of the comprehension process may involve mental operations on knowledge structures and the realization of the implications of these operations. Information about the consequences of such operations\u2014rather than information only about the input itself\u2014may be necessary for comprehending subsequent inputs and may be an important part of what is available in memory tasks.", "references": [], "page_rank": 0.00010946907498631636}, {"id": "e52924efad15d1c3cd464e1e27e031dbfe722a11", "title": "A programmer's description of L6", "authors": ["Kenneth C. Knowlton"], "date": 1966, "abstract": "Bell Telephone Laboratories' Low-Level Linked List Language L6 (pronounced \u201cL-six\u201d) is a new programming language for list structure manipulations. It contains many of the facilities which underlie such list processors as IPL, LISP, COMIT and SNOBOL, but permits the user to get much closer to machine code in order to write faster-running programs, to use storage more efficiently and to build a wider variety of linked data structures.", "references": ["30901b8eb11da71262fd343114efcb42c5c486fa"], "page_rank": 4.926108374384236e-05}, {"id": "d97d69e68057bf43c844c799c9132937b2efcd98", "title": "AN EXPERIMENTAL SYNTAX-DIRECTED DATA STRUCTURE LANGUAGE,", "authors": ["Robert K. Lindsay", "Terrence W. Pratt", "Kenneth M. Shavor"], "date": 1965, "abstract": "Abstract : Programmers developing systems of the complexity required in artificial intelligence research are frequently hindered by the rigid programming languages available and the time-consuming task of implementing new languages. AMOS (for associative memory organizing system) provides a flexible means to structure data and experiment with the syntactic forms of program statements while lessening the implementation bottleneck. AMOS is a syntax-directed compiler used to define languages for constructing a variety of data organizations of which Fortran-like arrays and IPL-like list structues are special cases. This research explores the use of syntactic descriptions which are not Backus Normal Form grammars and provides means for defining two-demensional languages as well as the usual linear type. In order to facilitate implementation, the system may be conveniently imbedded in any monitor system of common design; AMOS operations are manipulations within high-speed storage only. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "44e11083e4b15bfa1a09c9df7a5e6f7eefeaec01", "title": "A Descriptive Language for Symbol Manipulation", "authors": ["Robert W. Floyd"], "date": 1961, "abstract": "The algebraic command languages (ALGOL, IT, FORTRAN, UNICODE), although useful in preparing numerical algorithms, have not in the author's opinion proven themselves useful for symbol manipulation algorithms, particularly compilers. List processors, in fact, have been designed primarily to fill this gap. Analogously, the traditional flowchart serves well as a descriptive language for numerical algorithms, but does not lend itself to description of symbol manipulation algorithms in such a way that the intent of the process is clear. I t will be the purpose of this paper to present a more suitable notation for description of compilers and other complicated symbol manipulation algorithms. The algorithms used in formula translation consist principally of the following elements: (1) A set of linguistic transformations upon the input string, together with conditions determining the applicability of each transformation. (2) A set of actions, such as the generation of machine language coding, associated with each transformation. (3) A rule for transfering the attention of the translator from one portion of the input string to another. The notation presented here greatly simplifies the representation of the first and third elements. For illustrative purposes, a compilation process for a small subset of ALGOL is described below. The subset consists of assignment statements constructed from identifiers, the five binary arithmetic operators ( T, \u00d7 , / , 4 , ), the two unary arithmetic operators (-{-, --), the replacement operator ( : = ) , parentheses, and the library functions of one variable (sin, exp, sqrt, etc.). The assignment statement Z to be translated is initially taken in the augmented form ~ A2~ ~ , where the characters ~and ~ serve as termination symbols and a is a pointer which indicates the portion of the statement where the translator's attention is currently focused. The following productions and the associated generation rules respectively decompose the original statement in accordance with its structure, and simultaneously create coding to implement the statement. Coding will be represented by ALGOL statements with at most one operator, to avoid reference to particular computers.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "bfaf9f138b54a6e8f1093078672ce0f8368bc280", "title": "Computer-drawn flowcharts", "authors": ["Donald E. Knuth"], "date": 1963, "abstract": "To meet the need for improved documentation of written computer programs, a simple system for effective communication is presented, which has shown great promise. The programmer describes his program in a simple format, and the computer prepares flow charts and other cross-referenced listings from this input. The description can be kept up-to-date easily, and the final output clearly explains the original program. The system has also proved to be a valuable debugging and coding aid.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "52ccf415a6a6d9d9cb87c9e94a9264470bcf97e0", "title": "Teaching children to be mathematicians vs", "authors": ["Seymour Papert"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Teaching children to be mathematicians vs\" by Seymour Papert", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "c58fd3e1da8ff5aacb40843ba0728acfa74f82b8", "title": "Toward a general science of viable systems", "authors": ["Arthur S. Iberall", "John W. Brewer"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Toward a general science of viable systems\" by Arthur S. Iberall et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "209c981960eac9e5c4a260f004c6def64e6dca67", "title": "Designing a rule system that searches for scientific discoveries", "authors": ["Douglas B. Lenat", "Gregory Harris"], "date": 1977, "abstract": "Some scientific inference tasks (including mass spectrum identification DerzdrczL, medical diagnosis Mycin, and math theory development AM) have been successfully modelled as rule-directed search processes. These rule systems are designed quite differently from \"pure production systems\". By concentrating upon the design of one program (AM), we shall show how 13 kinds of design deviations arise from (i) the level of sophistication of the task that the system is designed to perform, (ii) the inherent nature of the task, and (iii) the designer's view of the task. The limitations of AM suggest even more radical departures from traditional rule system architecture. All these modifications are then collected into a new, complicated set of constraints on the form of the data structures, the rules, the interpreter, and the distribution of knowledge between rules and data structures. These new policies sacrifice uniformity in the interests of clarity, efficiency and power derivable from a thorough characterization of the task. Rule systems whose architectures conform to the new design principles will be more awkward for many tasks than would \"pure\" systems. Nevertheless, the new architecture should be significantly more powerful and natural for building rule systems that do scientific discovery tasks.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "94d69ebef4df1fc18cb95e87a14c6d25c2dc92ea", "title": "A FORMAL SEMANTICS FOR COMPUTER-ORIENTED LANGUAGES", "authors": ["Jerome A. Feldman"], "date": 1964, "abstract": "Abstract : This dissertation presents a number of results attained in a study of the formalization of certain properties of computer-oriented languages. The presentation of the results in the thesis is based on the structure of this program, called the compiler-compiler. Although there are several sections devoted to theoretical questions, these are set off from the main development. A more detailed introduction to the paper is given and some of the philosophical questions raised by formalizing semantics are considered. A formal syntax language used in the compiler-compiler is discussed; relationships are established between this formalization of syntax and others appearing in the literature. A complete discussion of the Formal Semantic Language is given further. The two formal systems were combined to form the basis for a useful computer technique. The final chapter contains a discussion of the strengths and weaknesses of our system as well as several suggestions for future research. The appendices include a record of the development of a translator for one small language from a formal definition of the language to examples of resultant machine code.", "references": ["28dcfd519a2058d5deef6c9e64a1d4d5721dba45", "dafabc60fe64f5fea4d20d464d453c262d2649b4", "e5d8b62af26b9240989ddb567c11e710eb42e331", "8e412b65ead1d45cf2a8200ed3632eea12e048a4", "79fcb6fa7106e2a1d78654ce958377b8f64a5156"], "page_rank": 4.926108374384236e-05}, {"id": "5ead274e342a696d52d2bc6484a5556feb389b63", "title": "A Three-Step Procedure for Language Generation", "authors": ["Boris Katz"], "date": 1980, "abstract": "Abstract : This paper outlines a three-step plan for generation English text from any sematic representation by applying a set of syntactic transformations to a collection of kernel sentences. The paper focuses on describing a program which realizes the third step of this plan. Step One separates the given representation into groups and generates from each group a set of kernel sentences. Step Two must decide, based upon both syntactic and thematic consideration, the set of transformations that shoulld be performed upon each set of kernels. The output of the first two steps provides the 'TASK' for Step Three. Each element of the TASK corresponds to the generation of one English sentence, and in turn may be defined as a triple consisting of: a list of kernel phrase markers; a list of transformations to be performed upon the list of kernals; a 'syntactic separator' to separate or connect generated sentences. Step Three takes as input the results of Step One and Step Two. The program which implements Step Three 'reads' the TASK, executes the transformations indicated there, combines the altered kernels of each set into a sentence, performs a pronominalization process, and finally produces the appropriate English word string. This approach subdivides a hard problem into three more manageable and relatively independent pieces.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "4037b204e73b1d4fa45961edb9b9a2a42e105fa6", "title": "A Production System Monitor for Parallel Computers.", "authors": ["Charles Forgy"], "date": 1977, "abstract": "Abstract : Production systems cannot compete on an equal basis with conventional programming languages until the efficiency of their monitors is improved. The process that finds the true productions on every cycle is most in need of improvement; even in today's small systems this process is often expensive and it is likely to become more expensive if productions systems become larger. There are a number of possible ways to achieve the greater efficiency, including taking advantage of the slow rate of change of the data base, taking advantage of similarities in structure of the antecedent conditions of productions, expending a minimum of effort on false antecedent conditions, avoiding whenever possible the operations that are likely to be most time consuming, and making efficient use of hardware. Since computer power is achieved most economically today through parallelism, no algorithm can be truly efficient in its use of hardware unless it can be executed in parallel. A production system monitor has been implemented that responds in a reasonable manner to both the peculiar nature of the task and the realities of current hardware technology. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6bfa4cb6db49463c0e1f6604d822f776f5613cd8", "title": "Parsing and Generating English Using Commutative Transformations", "authors": ["Boris Katz", "Patrick Henry Winston"], "date": 1982, "abstract": "Abstract : This paper is about an implemented natural language interface that translates from English into semantic net relations and from semantic net relations back into English. The parser and Companion generator were implemented for two reasons: (a) to enable experimental work in support of a theory of learning by analogy; and (b) to demonstrate the viability of a theory of parsing and generation built on commutative transformations. (Author)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "ddce6792562bae511ff07fb362cec31a6dc884f9", "title": "A Frame-Based Production System Architecture", "authors": ["David E. Smith", "Jan E. Clayton"], "date": 1980, "abstract": "We propose a flexible frame-structured representation and agenda-based control mechanism for the construction of production-type systems. Advantages of this architecture include uniformity, control freedom, and extensibility. We also describe an experimental system, named Wheeze, that uses this formalism.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "title": "A Multi-Level Organization For Problem Solving Using Many, Diverse, Cooperating Sources Of Knowledge", "authors": ["Lee D. Erman", "Victor R. Lesser"], "date": 1975, "abstract": "An organization is presented for implementing solutions to knowledge-based AI problems. The hypothesize-and-test paradigm is used as the basis for cooperation among many diverse and independent knowledge sources (KS's). The KS's are assumed individually to be errorful and incomplete. \n \nA uniform and integrated multi-level structure, the blackboard, holds the current state of the system. Knowledge sources cooperate by creating, accessing, and modifying elements in the blackboard. The activation of a KS is data-driven, based on the occurrence of patterns in the blackboard which match templates specified by the knowledge source. \n \nEach level in the blackboard specifies a different representation of the problem space; the sequence of levels forms a loose hierarchy in which the elements at each level can approximately be described as abstractions of elements at the next lower level. This decomposition can be thought of as an a prion framework of a plan for solving the problem; each level is a generic stage in the plan. \n \nThe elements at each level in the blackboard are hypotheses about some aspect of that level. The internal structure of an hypothesis consists of a fixed set of attributes; this set is the same for hypotheses at all levels of representation in the blackboard. These attributes are selected to serve as mechanisms for implementing the data-directed hypothesize-and-test paradigm and for efficient goal-directed scheduling of KS's. Knowledge sources may create networks of structural relationships among hypotheses. These relationships, which are explicit in the blackboard, serve to represent inferences and deductions made by the KS's about the hypotheses; they also allow competing and overlapping partial solutions to be handled in an integrated manner. \n \nThe Hearsay II speech-understanding system is an implementation of this organization; it is used here as an example for descriptive purposes.", "references": ["4597f9a93809a01462ee895d524df485583aeff4", "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "e97795382386ecd24300f3a6449ed5732b200bfa", "1a4c5e53e250194a258441b791364f0a725ec9ee", "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "f027ce53a12f36f93897a2b5733549ca323c18d0", "04ffb20cbfa502d3d2611dcfe027cfa94b45a629", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "68cba25ae59c2dacef771c08c5d413872d8dd00b", "3eed9d1d505e5ad021f8e3d77689750cc3c15013"], "page_rank": 0.0001532567049808429}, {"id": "6961e0b84092444701c75e7eaf133aa91584853a", "title": "The efficiency of certain production system implementations", "authors": ["John P. McDermott", "Allen Newell", "J. Moore"], "date": 1977, "abstract": "The obvious method of determining which productions are satisfied on a given cycle involves matching productions, one at a time, against the contents of working memory. The cost of this processing is essentially linear in the product of the number of productions in production memory and the number of assertions in working memory. By augmenting a production system architecture with a mechanism that enables knowledge of similarities among productions to be precomputed and then exploited during a run, it is possible to eliminate the dependency on the size of production memory. If in addition, the architecture is augmented with a mechanism that enables knowledge of the degree to which each production is currently satisfied to be maintained across cycles, then the dependency on the size of working memory can be eliminated as well. After a particular production system architecture, PSG, is described, two sets of mechanisms that increase its efficiency are presented. To determine their effectiveness, two augmented versions of PSG are compared experimentally with each other and with the original version.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ea11b93aca36bc18638d8f646eab73c153ee910d", "title": "Computer-based medical consultations mycin", "authors": ["E H Shorthffe"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Computer-based medical consultations mycin\" by E H Shorthffe", "references": [], "page_rank": 0.00016420361247947453}, {"id": "cf60718f4bec5ded5abd61655ce712be37b54770", "title": "CIRCUMSCRIPTION \u2014 A FORM OF NONMONOTONIC REASONING", "authors": ["John McCarthy"], "date": 1979, "abstract": "Humans and intelligent computer programs must often jump to the conclusion that the objects they can determine to have certain properties or relations are the only objects that do. Circumscription formalizes such conjectural reasoning.", "references": ["93bdca51c9c0477121ba9708ffe2747855b93aef"], "page_rank": 0.00016420361247947453}, {"id": "b1abc47a7a0a18a76884d59ad848d7caa32f5cc7", "title": "The programming language aspects of thinglab, a constraint-oriented simulation laboratory", "authors": ["A. H. Boming"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"The programming language aspects of thinglab, a constraint-oriented simulation laboratory\" by A. H. Boming", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "2fcec1760208cc955cbcad4ec5c66d2f3189da55", "title": "Weak definitions of field", "authors": ["B. A. Bernstein"], "date": 1947, "abstract": "Semantic Scholar extracted view of \"Weak definitions of field\" by B. A. Bernstein", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4ec9c76a327e38958378a5a7be88639678d2edef", "title": "\u00dcber die Darstellbarkeit von Zahlen durch quadratische Formen im K\u00f6rper der rationalen Zahlen.", "authors": ["Helmut Hasse"], "date": 1940, "abstract": "l n h \u00ab l t, Seile A. E in le i tung , , 129 \u00a7 l. Fragestellung und Behandlungsmethode . . . . . . . 129 \u00a7 2. Allgemeines , 131 B. U n \u00e4 r e und b i n \u00e4 r e F o r m e n . 134 \u00a7 3. Un\u00e4re Formen , 134 \u00a7 4. Darstellbarkeit der Null in K (p) durch tern\u00e4re Formen. 134 \u00a7 5. Darstellbarkeit in K (p) durch bin\u00e4re Formen , 135 \u00a7 6. Darstellbarkeit der Null in K (l) durch tern\u00e4re Formen 135 \u00a7 7. Darstellbarkeit in K(1) durch bin\u00e4re Formen 137. C. T e r n \u00e4 r e F o r m e n 138 \u00a7 8. Darstellbarkeit der Null in K (p) durch quatern\u00e4re Formen . . . 189 \u00a7 9. Darstellbarkeit in K(p) durch tern\u00e4re Formen 141 \u00a7 10. Darstellbarkeit der Null in .ff (1) durch quatern\u00e4re Formen 142 \u00a7 11. Darstellbarkeit in JT(1) durch tern\u00e4re Formen 143 D. n-\u00e4r e F o r m e n (n > 4) , 145 \u00a7 12. Darstellbarkeit in K (p) durch w-\u00e4re Formen (n >. 4) , 145 \u00a7 13. Darstellbarkeit in jfiT(l) durch >i-\u00e4re Formen (n \u0302 4) 146 \u00a7 14. Zusammenfassung 147", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "75d19f772424b29826896ff3b7bf1d10b70c5453", "title": "Sketchpad a Man-Machine Graphical Communication System", "authors": ["Ivan E. Sutherland"], "date": 1964, "abstract": "The Sketchpad system makes it possible for a man and a computer to converse rapidly through the medium of line drawings. Heretofore, most interaction between man and computers has been slowed down by the need to reduce all communication to written statements that can be typed; in the past, we have been writing letters to, rather than conferring with, our computers. For many types of communication, such as describing the shape of a mechanical part or the connections of an electrical circuit, typed statements can prove cumbersome. The Sketchpad system, by eliminating typed statements (except for legends) in favor of line drawings, opens up a new area of man-machine communication.", "references": ["f36569f5bdd034679aa637e0d4264059f712e1b6", "17df81d03f4d738a5ed3836b2120d7e468770752", "0e3afec401a19b634ac197555d9b77f49382b26d", "c761d5ef9908c537b4794a8b5b20c585886bfeb2", "a4ed116fb6523166a2286be55bf9c14030baad61", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5"], "page_rank": 8.210180623973726e-05}, {"id": "626d07872e6e795dd4861908c58d4bed0fe6c98e", "title": "Der Wahrheitsbegriff in den formalisierten Sprachen", "authors": ["Alfred Tarski"], "date": 1935, "abstract": "Semantic Scholar extracted view of \"Der Wahrheitsbegriff in den formalisierten Sprachen\" by Alfred Tarski", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "2f4ba7aa63ac4bcceb1bc745183b2f7d6bd83685", "title": "Salience as a Simplifying Metaphor for Natural Language Generation", "authors": ["David D. McDonald", "E. Jeffrey Conklin"], "date": 1982, "abstract": "We have developed a simple yet effective technique for planning the generation of natural language texts that describe photographs of natural scenes as processed by the UMass VISIONS system. The texts follow the ordering on the scene's objects that is imposed by their visual salience \u2014 an ordering which we believe is naturally computed as a by-product of visual processing, and thus is available -- for free -- as the basis for generating simple but effective texts without requiring the complex planning machinery often applied in generation. We suggest that it should be possible to find structural analogs to visual salience in other domains and to build comparably simple generation schemes based on them. We look briefly at how one such analogy might be drawn for the task of tutoring novice PASCAL programmers.", "references": ["e0318907c179df6a0a510a84e5709f6b59cc059f", "469264ac123d7cce2154f4118c72aa168f109f33"], "page_rank": 8.210180623973726e-05}, {"id": "ff5cec60c25b0133da5d00d732299a90c4159779", "title": "THE DEFINITION OF THE WORD AND THE SENTENCE", "authors": ["Alan H. Gardiner"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"THE DEFINITION OF THE WORD AND THE SENTENCE\" by Alan H. Gardiner", "references": [], "page_rank": 0.00016420361247947453}, {"id": "c3f7ad4b2af9d9888be85a8a376c61e7a612acc2", "title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "authors": ["Kurt G{\\\"o}del"], "date": 1931, "abstract": "A jack member onto a pair of which members a vehicle can be driven and then raised to a position inclined to the horizontal. The jack members are used more particularly for stowing vehicles in containers on a number of support frames. The jack members raise the vehicles so that they are supported at an angle to the horizontal with their wheels held in wheel support cradles attached to the frames. The jack members each comprises an approach ramp and a lifting ramp pivotally connected at one of their ends on which the vehicle is initially aligned and supported prior to being elevated at one end for inclined support on the vehicle frame.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "392746142b0b67726be2fa16611586f32bd95630", "title": "Extensions of Some Theorems of G\u00f6del and Church", "authors": ["J. Barkley Rosser"], "date": 1936, "abstract": "We shall say that a logic is \u201csimply consistent\u201d if there is no formula A such that both A and \u223c A are provable. \u201c\u03c9-consistent\u201d will be used in the sense of Godel. \u201cGeneral recursive\u201d and \u201cprimitive recursive\u201d will be used in the sense of Kleene, so that what Godel calls \u201crekursiv\u201d will be called \u201cprimitive recursive.\u201d By an \u201c Entscheidungsverfahren \u201d will be meant a general recursive function \u03d5 ( n ) such that, if n is the Godel number of a provable formula, \u03d5 ( n ) = 0 and, if n is not the Godel number of a provable formula, \u03d5 ( n ) = 1. In specifying that \u03d5 must be general recursive we are following Church in identifying \u201cgeneral recursiveness\u201d and \u201ceffective calculability.\u201d First, a modification is made in Godel's proofs of his theorems, Satz VI (Godel, p. 187\u2014this is the theorem which states that \u03c9 -consistency implies the existence of undecidable propositions) and Satz XI (Godel, p. 196\u2014this is the theorem which states that simple consistency implies that the formula which states simple consistency is not provable). The modifications of the proofs make these theorems hold for a much more general class of logics. Then, by sacrificing some generality, it is proved that simple consistency implies the existence of undecidable propositions (a strengthening of Godel's Satz VI and Kleene's Theorem XIII) and that simple consistency implies the non-existence of an Entscheidungsverfahren (a strengthening of the result in the last paragraph of Church).", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "cf2c04bbf503c01379d0fd59d943dcbc4013f01f", "title": "Experiments with a search algorithm for the data base of a human belief structure", "authors": ["Kenneth Mark Colby", "Lawrence G. Tesler", "Horace Enea"], "date": 1969, "abstract": "A large data base was collected from a human informant. The data consisted of beliefs regarding parent-child relations. A variety of factors in searching the data base were manipulated in an attempt to discover which were the more important in contributing to estimates of credibility. \n \nProblems of data collection, data representation and a searching algorithm are discussed in detail.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "7c46d6fe1c78ffa46664ee9f47f1f1063ce16879", "title": "Prototypes and Production Rules: An Approach to Knowledge Representation for Hypothesis Formation", "authors": ["Janice S. Aikins"], "date": 1979, "abstract": "A system called CENTAUR has been implemented to interpret data derived from pulmonary function tests using a knowledge representation that combines the advantages of both production rules and frames. The system uses a hypothesis-directed approach to problem solving, in which hypotheses are suggested by the Initial data, further Information is acquired, and then more specific hypotheses are selected. The hypotheses are represented as PROTOTYPES, frame-like data structures each of which characterizes some pulmonary disease. The prototypes guide the invocation of the production rules and focus the search for new information. Some of the advantages afforded by representing knowledge as both prototypes and rules are also presented.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "18cf70ed5f8f242fcc045234c94e151d2c25a36e", "title": "AGE (Attempt to Generalize): A Knowledge-Based Program for Building Knowledge-Based Programs", "authors": ["H. Penny Nii", "Nelleke Aiello"], "date": 1979, "abstract": "The goal of the ACE project is to demystify and make explicit the art of knowledge engineering. It is an attempt to formulate the knowledge that knowledge engineers use in constructing knowledge-based programs and put it at the disposal of others in the form of a software laboratory. To achieve this goal, the task for ACE is divided into two main sub-tasks: (1) isolating techniques used in knowledge-based systems and programming those that are general and useful (2) building an intelligent agent to guide in the use of these techniques. Currently AGE has a facility to build programs using the Blackboard Model [8,13]", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "3398fd64b4bcc984fbd96de71eecb6188eece7ad", "title": "The case for case\" in bach", "authors": ["Charles J. Fillmore"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"The case for case\" in bach\" by Charles J. Fillmore", "references": [], "page_rank": 0.00032840722495894905}, {"id": "44fe38d52ba55f4d9ee6bc12eaf4aea4dfefd7c4", "title": "Comprehension by computer: analysis of sentences in context", "authors": ["Christopher Riesbeck", "Roger C. Schank"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"Comprehension by computer: analysis of sentences in context\" by Christopher Riesbeck et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "2dd80a919ac78321fbdb4596969a40f87bdfaa26", "title": "An Overview of OWL, A Language for Knowledge Representation.", "authors": ["Peter Szolovits", "Lowell B. Hawkinson", "William A. Martin"], "date": 1977, "abstract": "Abstract : The motivation and overall organization of the OWL language for knowledge representation is described. OWL consists of a memory of concepts in terms of which all English phrases and all knowledge of an application domain are represented, a theory of English grammar which tells how to map English phrases into concepts, a parser to perform that mapping for individual sentences, and an interpreter to carry out procedures which are written in the same representational formalism. The system has been applied to the study of interactive dialogs, explanations of its own reasoning, and question answering.", "references": ["09550accec47459a61fe1710a0a32c2ec22449bd", "4272f212b7f6967c24274cd73a5c0e1a0f48cef9", "7e5670e5e44f0745294fe688b40210d4ad56c530"], "page_rank": 0.0001688951442646024}, {"id": "95075b1b971e970be5aad3a15011f9042c72f873", "title": "An Examination of a Frame-Structured Representation System", "authors": ["Mark Stefik"], "date": 1979, "abstract": "The Unit Package is an interactive knowledge representation system with representations for individuals, classes, indefinite individuals, and abstractions. Links between the nodes are structured with explicit definitional roles, types of inheritance, defaults, and various data formats. This paper presents the general ideas of the Unit Package and compares it with other current knowledge representation languages. The Unit Package was created for a hierarchical planning application, and is now in use by several AI projects.", "references": ["2dd80a919ac78321fbdb4596969a40f87bdfaa26", "af6727621f4d5f35ec3f1b85f9038ac7c2bdaa79", "8299ef6181ed3d209bbf0816ea0fe8bb2e45192f", "534177394b6b3ed4b0a8ba42c0e63d66f43029d9", "59c0054a6dc4904dc5948c0e7c882d8d7279f31c"], "page_rank": 7.037297677691766e-05}, {"id": "d55fc396338da162550aab6b0397d69063abffa8", "title": "Production system conflict resolution strategies", "authors": ["John R. McDermott", "Charles L. Forgy"], "date": 1977, "abstract": "Production systems designed to function and grow in environments that make large numbers of different, sometimes competing, and sometimes unexpected demands require support from their interpreters that is qualitatively different from the support required by systems that can be carefully hand crafted to function in constrained environments. In this paper we explore the role of conflict resolution in providing such support. Using criteria developed in the paper, we evaluate both individual conflict resolution rules and strategies that make use of several rules.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "30901b8eb11da71262fd343114efcb42c5c486fa", "title": "SNOBOL , A String Manipulation Language", "authors": ["David J. Farber", "Ralph E. Griswold", "I. P. Polonsky"], "date": 1964, "abstract": "SNOBOL is a programming language for the manipulation of strings of symbols. A statement in the SNOBOL language consists of a rule that operates on symbolically named strings. The basic operations are string formation, pattern matching and replacement. Facilities for integer arithmetic, indirect referencing, and input-output are included. In the design of the language, emphasis has been placed on a format that is simple and intuitive. SNOBOL has been implemented for the IBM 7090.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "c8141014edf78d36e9171d5a1627b9a0411064d5", "title": "An Overview of Production Systems", "authors": ["Randall Davis", "Jonathan J. King"], "date": 1975, "abstract": "Abstract : Since production systems were first proposed in 1943 as a general computational mechanism, the methodology has seen a great deal of development and has been applied to a diverse collection of problems. Despite the wide scope of goals and perspectives demonstrated by the various systems, there appear to be many recurrent themes. This paper is an attempt to provide an analysis and overview of those themes, as well as a conceptual framework by which many of the seemingly disparate efforts can be viewed, both in relation to each other, and to other methodologies. Accordingly, the authors use the term 'production system' in a broad sense, and attempt to show how most systems which have used the term can be fit into the framework. The comparison to other methodologies is intended to provide a view of PS characteristics in a broader context, with primary reference to procedurally-based techniques, but with reference also to some of the current developments in programming and the organization of data and knowledge bases.", "references": ["7abed618e349427fde64d68cc98c6b6bb844abb4", "f027ce53a12f36f93897a2b5733549ca323c18d0", "a94d16ab1f71ff8fbf247a959133a87b87029934", "b63c307d0c74426dd4cf78091ade7a5defd6e61a", "1c62bce34da5568429c2cf05415ca139eaa3e1f2", "e5d8b62af26b9240989ddb567c11e710eb42e331", "c35980a49350197aea411f9d7926d86e2a05905f"], "page_rank": 4.926108374384236e-05}, {"id": "99338adde12a235040c93333fec78765b0541880", "title": "THE ELEMENTS OF MATHEMATICAL LOGIC", "authors": ["I. S. Gradshte\u012dn"], "date": 1963, "abstract": "This chapter discusses the elements of mathematical logic. Mathematical logic developed as a result of the application of mathematical methods to the problems of formal logic and as a discipline serving the ends of the foundations of mathematics. Mathematical logic has received diverse technical applications. Contemporary mathematical logic is connected with automation, with machine mathematics and problems of automatic translation from one language to another, with information theory, and with cybernetics. The methods of mathematical logic find wide applications in the theory of electrical networks with switching action. In algebra, numbers\u2014the objects of the study of arithmetic\u2014are denoted by letters. The object of that part of mathematical logic that is known as the propositional calculus is the study of propositions. The propositional calculus is the most elementary part of mathematical logic. Mathematical logic distinguishes carefully between the different ways of using variables and fixes them with the aid of a special symbolism.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f0bf6182a22a44dfe9263ed35d9fd53c76026301", "title": "An Introduction to the Theory of Numbers", "authors": ["Gordon H. Hardy", "Elizabeth Cox Wright"], "date": 1938, "abstract": "This is the fifth edition of a work (first published in 1938) which has become the standard introduction to the subject. The book has grown out of lectures delivered by the authors at Oxford, Cambridge, Aberdeen, and other universities. It is neither a systematic treatise on the theory of numbers nor a 'popular' book for non-mathematical readers. It contains short accounts of the elements of many different sides of the theory, not usually combined in a single volume; and, although it is written for mathematicians, the range of mathematical knowledge presupposed is not greater than that of an intelligent first-year student. In this edition, the main changes are in the notes at the end of each chapter. Sir Edward Wright seeks to provide up-to-date references for the reader who wishes to pursue a particular topic further and to present, both in the notes and in the text, a reasonably accurate account of the present state of knowledge.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "d88f9058b1c3277e5266b9f76cbb067c07d2f363", "title": "An Abstract Computer with a Lisp-Like Machine Language Without a Label Operator", "authors": ["Paul C. Gilmore"], "date": 1959, "abstract": "Publisher Summary This chapter discusses the Lisp-Like machine language. One of the operators that must be assumed to be primitive in Lisp, independently of any class F, is a conditional operator. The new conditional operator is also defined and it is shown how the Lisp conditional operator can be defined in terms of it whenever there is an element representing the truth value truth in the universe U. The quote operator of Lisp is used to create names for members of U that are symbols, in particular for the symbol T used to represent the truth value truth; the value of the quote operator for a symbol is a name for the symbol. Important purposes can be served by defining the semantics of a programming language by defining an abstract computer for which the programming language is the machine language. These purposes include both a better understanding of the language, as well as of the difficulties that are likely to be faced in implementing it. The chapter explains how a recursive definition of a function can be given to the computer in its machine language and how the computer evaluates such a function for a given argument.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "79fcb6fa7106e2a1d78654ce958377b8f64a5156", "title": "Introduction to mathematical logic: Vol. I, by Alonzo Church. 376 pages, 6 \u00d7 9 in. Princeton, Princeton University Press, 1956. Price, $7.50", "authors": ["Haskell B. Curry"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"Introduction to mathematical logic: Vol. I, by Alonzo Church. 376 pages, 6 \u00d7 9 in. Princeton, Princeton University Press, 1956. Price, $7.50\" by Haskell B. Curry", "references": [], "page_rank": 0.0001231527093596059}, {"id": "e5a58c6f2ba6fad0ae564a921a19536c27434cdc", "title": "Viewing Control Structures as Patterns of Passing Messages", "authors": ["Carl Hewitt"], "date": 1977, "abstract": "A light sensing apparatus is described which employs a GaAsP MOS light-receiving element to which a potential is applied for creating a depletion region. Upon exposure, minority carriers are generated and trapped. Light levels are sensed by a charge injection technique which results in a characteristic charge or voltage, the magnitude of which is proportional to the total exposure during the sensing period.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "e5d8b62af26b9240989ddb567c11e710eb42e331", "title": "An ALGOL 60 Compiler", "authors": ["Arthur Evans"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"An ALGOL 60 Compiler\" by Arthur Evans", "references": [], "page_rank": 0.00019352568613652358}, {"id": "b443e18512181514b19363cd54dd3309c70be20e", "title": "Recursive functions of symbolic expressions and their computation by machine, Part I", "authors": ["John McCarthy"], "date": 1960, "abstract": "A programming system called LISP (for LISt Processor) has been developed for the IBM 704 computer by the Artificial Intelligence group at M.I.T. The system was designed to facilitate experiments with a proposed system called the Advice Taker, whereby a machine could be instructed to handle declarative as well as imperative sentences and could exhibit \u201ccommon sense\u201d in carrying out its instructions. The original proposal [1] for the Advice Taker was made in November 1958. The main requirement was a programming system for manipulating expressions representing formalized declarative and imperative sentences so that the Advice Taker system could make deductions. In the course of its development the LISP system went through several stages of simplification and eventually came to be based on a scheme for representing the partial recursive functions of a certain class of symbolic expressions. This representation is independent of the IBM 704 computer, or of any other electronic computer, and it now seems expedient to expound the system by starting with the class of expressions called S-expressions and the functions called S-functions.", "references": [], "page_rank": 0.00018062397372742197}, {"id": "8e412b65ead1d45cf2a8200ed3632eea12e048a4", "title": "Formal properties of grammars", "authors": ["Noam Chomsky"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Formal properties of grammars\" by Noam Chomsky", "references": [], "page_rank": 0.0001231527093596059}, {"id": "28dcfd519a2058d5deef6c9e64a1d4d5721dba45", "title": "A syntax controlled generator of formal language processors", "authors": ["J{\\\"u}rgen Eickel", "Manfred Paul", "Friedrich L. Bauer", "Klaus Samelson"], "date": 1963, "abstract": "In recent years formal languages have become a subject of wide interest--from a theoretical point of view in connection with symbolic logic and automata theory, from a practical point of view as input languages for information processing systems. In both cases some processing of these languages enters into the question, generally as a translation into another formal language. Some examples illustrate the possible variation of the circumstances with regard to the origin and meaning of formM languages and to their syntactical structure: languages for describing theorems, sequential circuits, differential equations systems or numerical algorithms. However, actual processors constructed for languages whose syntax can be described in the same metalanguage, e.g. Baekus notation [2], have shown remarkable similarities. In our own experience an important general principle in processing formal languages was the cellar principle introduced in 1957 by F. L. Bauer and K. Samelson [4, 5] in the design of a formal language-controlled computer. This principle has been described in [19, 20]. The essential features are as follows: The incoming information is analyzed sequentially, the meaning of each symbol already being established insofar as it can be determined from previous history. Corresponding information is stored into a state pushdown store, the \"cellar.\" In this way the momentary top levels of the cellar always reflect previous histo~T to the extent necessary to analyze the next incoming symbol. This principle proved to be applicable in translating programs from the algorithmic language ALGOL [3, 17] into machine code (pilot ALGOL 58 translator of the ZMMD group for the Zuse Z22, ERMETH and PERM computer) or into a computer-oriented, macro instruction language (logical plans of the ALCOR group in 1.959 and 1960). The principle was found and used independently elsewhere. Processors for other formal languages have been built, using a similar technique [7, 18].", "references": [], "page_rank": 0.0001231527093596059}, {"id": "3eed9d1d505e5ad021f8e3d77689750cc3c15013", "title": "Scientific Discovery and the Psychology of Problem Solving", "authors": ["Herbert A. Simon"], "date": 1977, "abstract": "In the previous chapter a theory of human problem solving was put forward with references to some of the evidence for its validity. The theory has been formalized and tested by incorporating it in programs for digital computers and studying the behavior of these programs when they are confronted with problem-solving tasks.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "3110035e3168aef74a7916be91065e502ea54185", "title": "The Structure of Analogical Models in Science.", "authors": ["Dedre Gentner"], "date": 1980, "abstract": "Abstract : Analogical models can be powerful aids to reasoning, as when light is explained in terms of water waves; or they can be misleading, as when chemical processes are thought of in terms of life processes such as putrefaction. This paper proposes a structural characterization of good science analogy using a theoretical approach in which complex metaphors and analogies are treated as structure-mappings between domains. To delineate good from poor science analogy, a series of comparisons is made. First, metaphor and analogy are contrasted with literal similarity; then, explanatory-predictive analogy is contrasted with expressive metaphor; finally, within science, good explanatory analogy is contrasted with poor explanatory analogy. Analogies of historical importance are analyzed and empirical findings are discussed. (Author)", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "97ceb096cc1ca6773771bdb98cd144be0261cbac", "title": "Metaphor, Metaphor Schemata, and Selective Inferencing", "authors": ["Jerry R. Hobbs"], "date": 1979, "abstract": "Abstract : This paper demonstrates the importance of spatial and other metaphors. An approach to handling metaphors in a computational framework is described, based on the idea of selective inferencing. Three types of metaphor are examined in detail in this light: a simple metaphor, a spatial metaphor schema, and a novel metaphor. Finally, the author discusses the analogical processes that underlie the metaphor in this approach, and what the approach says about several classical questions about the metaphor.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "68cba25ae59c2dacef771c08c5d413872d8dd00b", "title": "System Organizations for Speech Understanding: Implications of Network and Multiprocessor Computer Architectures for AI", "authors": ["Lee D. Erman", "Richard D. Fennell", "Victor R. Lesser", "Raj Reddy"], "date": 1973, "abstract": "This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented: control of processes, interprocess communication and data sharing, resource allocation, and debugging are discussed.1", "references": ["4597f9a93809a01462ee895d524df485583aeff4", "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "be2ccd19a3906c54172027d17cfc2c017c978572", "70e69265cb050a5050e75ccb308e00b6d9571ab0", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "5444939ad7935c0302f414c136c0d4f09c202e67", "4448051cadba1a198f1fe1d705780cb43c8ca302", "f027ce53a12f36f93897a2b5733549ca323c18d0", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "73ed12a7b3019f266adca7beef47163b8b737402"], "page_rank": 4.926108374384236e-05}, {"id": "1a4c5e53e250194a258441b791364f0a725ec9ee", "title": "The Theory of Problem Solving", "authors": ["Herbert A. Simon"], "date": 1971, "abstract": "It is now about fifteen years since the first computer programs were written and tested that used the method of heuristic search to solve problems. Dozens of such programs, some designed for specific task domains, others claiming various degrees of generality, have now been described in the literature, and many experiments with their performance have been reported and analysed. It is an appropriate time to ask what has been learned from these experiments about the general theory of problem solving, and to try to summarize the present state of that theory.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "04ffb20cbfa502d3d2611dcfe027cfa94b45a629", "title": "The Hearsay-I Speech Understanding System: An Example of the Recognition Process", "authors": ["Raj Reddy", "Lee D. Erman", "Richard D. Fennell", "Richard B. Neely"], "date": 1973, "abstract": "This paper describes the structure and operation of the Hearsay-I1speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its operation in a particular task situation: Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.", "references": ["96a9cfa8a396fd8967b327dc675a8ce88f16a18f", "f027ce53a12f36f93897a2b5733549ca323c18d0", "1ff661af7f909f8a8644a0b5d445216c357f8f76", "190f432914ca9f9925860139c88d9664787a5939", "6a69a48ababc4d2e1b6132d5a2fc1de365942c9e", "62184a540db0f8c4712c7fe6c42272be3a7ed96e"], "page_rank": 4.926108374384236e-05}, {"id": "469264ac123d7cce2154f4118c72aa168f109f33", "title": "Computer generation of multiparagraph text", "authors": ["William C. Mann", "Joi L. Moore"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"Computer generation of multiparagraph text\" by William C. Mann et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "c761d5ef9908c537b4794a8b5b20c585886bfeb2", "title": "On-Line Man-Computer Communication", "authors": ["J. C. R. Licklider", "Welden E. Clark}"], "date": 1940, "abstract": "An improved optical fiber cathode ray tube is provided which enables the recording of images on a recording medium with a high resolution even if the recording medium is disposed with an increased gap between the tube and the recording medium to prevent the mechanical contact therebetween. The tube is provided with a light transparent thin layer between the non-lens-like optical fiber face plate and the phosphor layer. The light transparent thin layer is a transparent plate and may include a spatial gap.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4597f9a93809a01462ee895d524df485583aeff4", "title": "Parallelism in AI Problem Solving: A Case Study of Hearsay 2", "authors": ["Richard D. Fennell", "Victor R. Lesser"], "date": 1975, "abstract": "Abstract : The Hearsay II speech-understanding system (HSII) (Lesser, et al., 1974; Fennell, 1975; Erman and Lesser, 1975) is an implementation of a knowledge-based multiprocessing AI problem-solving organization. HSII is intended to represent a problem-solving organization which is applicable for implementation in a multiprocessing environment, and is, in particular, currently being implemented on the C.mmp multiprocessor system (Bell, et al., 1971) at Carnegie-Mellon University. The object of this paper is to explore several of the ramifications of such a problem-solving organization by examining the mechanisms and policies underlying HSII which are necessary for supporting its organization as a multiprocessing problem-solving system. First, an abstract description of a class of problem-solving systems is given using the Production System model of Newell (1973). Then, the HSII problem-solving organization is described in terms of this model. The various decisions made during the course of design necessitated the introduction of various multiprocessing mechanisms (e.g., mechanisms for maintaining data localization and data integrity), and these mechanisms are discussed. Finally, a simulation study is presented which details the effects of actually implementing such a problem-solving organization for use in a particular application area, that of speech understanding.", "references": ["13b55dfb568050d86191b755c4d9d6e5bd09ac78", "f027ce53a12f36f93897a2b5733549ca323c18d0", "006521402572c1d0fcded90f8d16803b640803cd", "320cefa46a705ef20ace952267a34eaa0d2c929f", "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "73ed12a7b3019f266adca7beef47163b8b737402", "f1a93cd7f9302e93462c7d694a84527ae23bab7f", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8"], "page_rank": 9.852216748768472e-05}, {"id": "93bdca51c9c0477121ba9708ffe2747855b93aef", "title": "A Logic for Default Reasoning", "authors": ["Raymond Reiter"], "date": 1980, "abstract": "The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults. \n \nIn this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.", "references": ["f78390e72ac8f4cdb34a2f953a586b11a6cd67bd", "2255db1a8ada12287fb175f52805f4c5bac26873", "0eaf153cf0e613845e44d4543a7e1c12916f8afe", "06162f180234f4e1e190784236157cc52c79a0b5", "f8ddb251bf94e4b055c6f520f21816e403c30e2a", "b35aa4d90f7368fefaf05ca94f76edf03134eee7", "b8938098d8c8bb2594cb6d02a55f5f8f203f4af4", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "da6eb135261dc3b3f7df8ff741796688898b5cd4", "3430d7929dc9c0c9278dca858e785ee3d89ce2b0"], "page_rank": 0.0004926108374384236}, {"id": "a4ed116fb6523166a2286be55bf9c14030baad61", "title": "Man-Computer Symbiosis", "authors": ["J. C. R. Licklider"], "date": 1960, "abstract": "Man-computer symbiosis is an expected development in cooperative interaction between men and electronic computers. It will involve very close coupling between the human and the electronic members of the partnership. The main aims are 1) to let computers facilitate formulative thinking as they now facilitate the solution of formulated problems, and 2) to enable men and computers to cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs. In the anticipated symbiotic partnership, men will set the goals, formulate the hypotheses, determine the criteria, and perform the evaluations. Computing machines will do the routinizable work that must be done to prepare the way for insights and decisions in technical and scientific thinking. Preliminary analyses indicate that the symbiotic partnership will perform intellectual operations much more effectively than man alone can perform them. Prerequisites for the achievement of the effective, cooperative association include developments in computer time sharing, in memory components, in memory organization, in programming languages, and in input and output equipment.", "references": ["1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1", "dea6a942effe558ed7a134ee9c8eadd953c5a5b5", "2edc8083073837564306943aab77d6dcc19d0cdc", "09c2bea7aba02571609c56fa4d27f77dcf7cd600", "316961b569563051f5bee4bd318e057fadca71b7", "879650714f3dce59b666346ccf63fd73250259d6", "c68b11f2aa5eef2e5fa3690a973cdaf3041f7b7a", "b349928abf6733c3028c23f6fc3cb6d09826277e", "e07a333e39c65b906f4284f192d9653f90c7d712", "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69"], "page_rank": 9.852216748768472e-05}, {"id": "0e3afec401a19b634ac197555d9b77f49382b26d", "title": "Sketchpad III, three dimensional graphical communication with a digital computer", "authors": ["Timothy E. Johnson"], "date": 1963, "abstract": "Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 1963.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "17df81d03f4d738a5ed3836b2120d7e468770752", "title": "Theoretical foundations for the computer-aided design system", "authors": ["Douglas T. Ross", "Jorge E. Rodriguez"], "date": 1963, "abstract": "A Computer-Aided Design System for general use must have a unique and powerful organization. Even the simplest of design problems involves the exercise of many disciplines and the carrying out of many types of activity. Since the area of applicability of the design system is to be essentially unlimited, we know from the beginning that the system itself must be very large and complex. Even though only a few of its features may be exercised on any given design problem, there is no way of predicting which portions of the system will be required nor how they will be used. Furthermore the designer or engineer who is using the system cannot be expected to be a computer programmer, and it must be possible for him to carry out his design function in a way which is natural to him, and without his being aware that the statements and actions that he performs are in fact constructing and executing large numbers of highly complex computer programs. Although to be sure the user must learn and become facile with the basic vocabulary and manipulations of the system, the system must be so designed that he finds his normal thought processes aided, augmented, and stimulated by the use of the system in such a way that he is able to think almost entirely at the concept level within his own field of interest, while at the same time carrying out data processing activities of extreme complexity.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "ec08947b7384d1d333b266928df0a3bf9e82fce1", "title": "Spinoza II: Conceptual Case-Based Natural Language Analysis.", "authors": ["Roger C. Schank"], "date": 1970, "abstract": "Abstract : The paper presents the theoretical changes that have developed in conceptual dependency theory and their ramifications in computer analysis of natural language. The major items of concern are: the elimination of reliance on 'grammar rules' for parsing with the emphasis given to conceptual rule based parsing; the development of a conceptual case system to account for the power of conceptualizations; the categorization of ACT's based on permissible conceptual cases and other criteria. These items are developed and discussed in the context of a more powerful conceptual parser and a theory of language understanding. (Author)", "references": ["9b3ea2ad8a2d0f1cfc327578f2481d422176aca2", "9ef210f0ceeacbd9d6e65b9708bbfca18d73979d", "3398fd64b4bcc984fbd96de71eecb6188eece7ad"], "page_rank": 4.926108374384236e-05}, {"id": "f36569f5bdd034679aa637e0d4264059f712e1b6", "title": "Man-machine console facilities for computer-aided design", "authors": ["Robert H. Stotz"], "date": 1963, "abstract": "The backbone of the man-machine communication link in Computer-Aided Design is a console whose principal components are the display scope and the light pen. The display scope is an ordinary cathode ray tube which is controlled by the computer by means of program instructions. It allows the computer to output to the man rapidly in easily interpreted graphical form. The data displayed can be textual, pictorial, or a combination of the two. The light pen is a photosensitive device which responds to the light generated by an intensified point on the scope face and which amplifies, shapes and transmits this response back to the computer where it can be tested by the program and used as a branch condition. The display scope and light pen form, so to speak, the paper and pencil of the designer, but they possess some extremely useful additional properties which open a whole new expressive medium. The following papers describe some of the fascinating and invaluable facilities which are provided by this basically simple hardware. In this paper we present some basic information about the hardware itself and describe some of the sophistications appropriate to the Computer-Aided Design problem.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "8171abcaf87952fceb7c992dd4517c0cb7c0c9a7", "title": "Inference and the Computer Understanding of Natural Language", "authors": ["Roger C. Schank", "Charles J. Rieger"], "date": 1974, "abstract": "The notion of computer understanding of natural language is examined relative to inference mechanisms designed to function in a language-free deep conceptual base (Conceptual Dependency). The conceptual analysis of a natural language sentence into this conceptual base, and the nature of the memory which stores and operates upon these conceptual structures are described from both theoretical and practical standpoints. The various types of inferences which can be made during and after the conceptual analysis of a sentence are defined, and a functioning program which performs these inference tasks is described. Actual computer output is included.", "references": ["0fae95253749e05975fc34036ae71f36ae2efc70", "99c1ef49842f72ad07b5283ec41b924ddc4e6959", "c82330fd123cc50c6dd0c5add0786d253727f383", "d6190336549fff6d971aecc373c195464788a52a", "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "d53cfca137c38f82361878a67ea47002e9940e34", "f8a2e2932af3b09ee9b3837319a32248de5498eb"], "page_rank": 4.926108374384236e-05}, {"id": "b62608c716819965f2755759ce3a7edb8a93829f", "title": "Plane Geometry Theorem Proving Using Forward Chaining", "authors": ["Arthur J. Nevins"], "date": 1975, "abstract": "A computer program is described which operates on a subset of plane geometry. Its performance not only compares favorably with previous computer programs, but within its limited problem domain (e.g., no curved lines nor introduction of new points), it also invites comparison with the best human theorem provers. The program employs a combination of forward and backward chaining with the forward component playing the more important role. This, together with a deeper use of diagrammatic information, allows the program to dispense with the diagram filter in contrast with its central role in previous programs. An important aspect of human problem solving may be the ability to structure a problem space so that forward chaining techniques can be used effectively.", "references": [], "page_rank": 0.0002134646962233169}, {"id": "59c0054a6dc4904dc5948c0e7c882d8d7279f31c", "title": "A formalism for modelling", "authors": ["Hector J. Levesque", "John Mylopoulos", "Gordon I. McCalla", "Luca Melli", "John K. Tsotsos"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"A formalism for modelling\" by Hector J. Levesque et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "38a7fc8802962d1f91e439c28dc13a1dd5465ece", "title": "STRIPS: A New Approach", "authors": ["Richard Fikes", "Nils J. Nilsson"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"STRIPS: A New Approach\" by Richard Fikes et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "7e5670e5e44f0745294fe688b40210d4ad56c530", "title": "An Experimental Parsing System for Transition Network Grammars", "authors": ["William A. Woods"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"An Experimental Parsing System for Transition Network Grammars\" by William A. Woods", "references": [], "page_rank": 0.0003448275862068965}, {"id": "69c6c392117d3ebaed85ffee143a5b5bb2b81068", "title": "The MACSYMA system", "authors": ["William A. Martin", "Richard J. Fateman"], "date": 1971, "abstract": "MACSYMA is a system for symbolic manipulation of algebraic expressions which is being developed at Project MAC, M.I.T. This paper discusses its philosophy, goals, and current achievements.\n MACSYMA makes extensive use of the power of its rational function subsystem. The facilities derived from this are discussed in considerable detail.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "1c62bce34da5568429c2cf05415ca139eaa3e1f2", "title": "Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system.", "authors": ["Edward H. Shortliffe", "Rich Davis", "Stanton G. Axline", "Bruce G. Buchanan", "Clarence C. Green", "Sanford N. Cohen"], "date": 1975, "abstract": "Abstract This report describes progress in the development of an interactive computer program, termed MYCIN, that uses the clinical decision criteria of experts to advise physicans who request advice regarding selection of appropriate antimicrobial therapy for hospital patients with bacterial infections. Since patients with infectious diseases often require therapy before complete information about the organism becomes available, infectious disease experts have identified clinical and historical criteria that aid in the early selection of antimicrobial therapy. MYCIN gives advice in this area by means of three subprograms: (1) A Consultation System that uses information provided by the physician, together with its own knowledge base, to choose an appropriate drug or combination of drugs; (2) An Explanation System that understands simple English questions and answers them in order to justify its decisions or instruct the user; and (3) A Rule Acquisition System that acquires decision criteria during interactions with an expert and codes them for use during future consultation sessions. A variety of human engineering capabilities have been included to heighten the program's acceptability to the physicians who will use it. Early experience indicates that a sample knowledge base of 200 decision criteria can be used by MYCIN to give appropriate advice for many patients with bacteremia. The system will be made available for evaluation in the clinical setting after its reliability has been shown to approach that of infectious disease experts.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "f52867d64e6d1a07d24661320a0df79d7f39a45f", "title": "REDUCE: a user-oriented interactive system for algebraic simplification", "authors": ["Anthony C. Hearn"], "date": 1967, "abstract": "Many of the day-to-day problems which confront applied mathematicians involve extensive algebraic or nonnumerical calculation. Such problems may range from the evaluation of analytical solutions to complicated differential or integral equations on the one hand, to the calculation of coefficients in a power series expansion or the computation of the derivative of a complicated function on the other. The difference between these two classes of problems is obvious; in the former case, no straightforward algorithm exists which will guarantee a solution, and indeed, an analytic form for the solution may not even exist. On the other hand, algorithms do exist for the solution of problems such as series expansion and differentiation, and therefore a correct answer may always be found provided that the researcher possesses sufficient time, perseverance, and accuracy to carry the more complicated problems through free of error. Many examples of this type of problem may be found in physics and engineering. Calculations of general relativistic effects in planetary motion, structural design calculations, and many of the calculations associated with elementary particle physics experiments at high energy accelerators, to name a few, may demand many man-months or even years of work before a useful and error free answer can be found, even though the operations involved are quite straightforward.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4272f212b7f6967c24274cd73a5c0e1a0f48cef9", "title": "Diagnosis as a notion of grammar", "authors": ["Mitchell Marcus"], "date": 1975, "abstract": "This paper will sketch an approach to natural language parsing based on a new conception of what makes up a recognition grammar for syntactic analysis and how such a grammar should be structured. This theory of syntactic analysis formalizes a notion very much like the psychologist's notion of \"perceptual strategies\" [Bever \"70] and makes this formalized notion which will be called the notion of wait-and-see diagnostics a central and integral part of a theory of what one knows about the structure of language. By recognition grammar, we mean here what a speaker of a language knows about that language that allows him to assign grammatical structure to the word strings that make up utterances in that language.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "af6727621f4d5f35ec3f1b85f9038ac7c2bdaa79", "title": "Knowledge Base Management for Experiment Planning in Molecular Genetics", "authors": ["Nancy Martin", "Peter Friedland", "Jonathan King", "Mark Stefik"], "date": 1977, "abstract": "The use of a representation language involving schemata and associated derived models has been extended to include all aspects of domain knowledge and strategy and heuristic problem solving knowledge. This uniform representation will allow the extension of knowledge base management techniques for acquisition and retrieval of procedural knowledge.", "references": ["f51b42090496ab8b338840e593c78b25452986e4"], "page_rank": 9.852216748768472e-05}, {"id": "b63c307d0c74426dd4cf78091ade7a5defd6e61a", "title": "The Symbolic Nature of Visual Imagery", "authors": ["Thomas P. Moran"], "date": 1973, "abstract": "The issue is whether human visual imagery can be represented by symbolic structures and processes. Protocols of the task of imagining a path in space were analyzed using a production system interpreter. A detailed simulation of the subject's behavior within the confines of a symbolic short-term memory (STM) demonstrates that symbol structures are sufficient to explain imaging behavior. The experience of programming with productions and a homogeneous STM has brought out the importance of a control language when using an unstructured rule system.", "references": ["9aba81a93203f0f92f7cc9cf38b2d5b54e346c49"], "page_rank": 7.037297677691766e-05}, {"id": "a94d16ab1f71ff8fbf247a959133a87b87029934", "title": "The LISP7O Pattern Matching System", "authors": ["Lawrence G. Tesler", "Horace Enea", "David Canfield Smith"], "date": 1973, "abstract": "LISP70 is a descendant of LISP which emphasizes pattern-directed computation and extensibility. A function can be defined by a set of pattern rewrite rules as well as by the normal LAMBDA method. New rewrite rules can be added to a previously defined function; thus a LISP70 function is said to be \"extensible\". It is possible to have new rules merged in automatically such that special cases are checked before general cases. Some of the facilities of the rewrite system are described and a variety of applications are demonstrated.", "references": ["85827cf800d963c44edee1c79d9431cf46fdeef8", "b443e18512181514b19363cd54dd3309c70be20e", "2bce48117d8589af931d1610338fa3b2599bf2d1", "2ff0eeb5cceb4bb9698e470521e04d19e03dbcc1", "e274e2cb0aaf7ad18f70e938ef58e01b6a590030", "91a1b89eb1b286e6edd3aeba3562a9e195d6fb11", "5fed09060c4b19a30ef4ae2a9aeddaa3011f23f7", "f354d8ce5cd2ead9301498aa1c16b5b2a0e4e0ec", "25a13868653f786ba00043be5b34cb12c0854976", "a761f26f8239acd88fc83787f28a7f2d2ff9ea22"], "page_rank": 7.037297677691766e-05}, {"id": "b8f87dffef810076750018a3b72c617b95d254ff", "title": "Doing Arithmetic with Diagrams", "authors": ["Alan Bundy"], "date": 1973, "abstract": "A theorem prover for part of arithmetic in described which proves theorems by representing them in the form of a diagram or network. The nodes of this network represent 'ideal integers', i.e. objects which have all the properties of integers, without being any particular intoger. The links in the network represent relationships between 'ideal integers'. The procedures which draw these diagrams make elementary deductions based on their built-in knowledge of the functions and predicates of arithmetic. This theorem prover is intended as a model of some kinds of human problem-solving behaviour.", "references": ["d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "a56524d3d5a847a54e547d07cb713a54f3cb4b68", "816e9b69c6adb73205e841be8e69b3651d3bfbbf", "5e9d825d198820be858a5ebb898087a0d52c9c78"], "page_rank": 0.00016420361247947453}, {"id": "172c6ad67c02b6d22a1f23a678583d547cffd4e8", "title": "Conceptual Structures: Information Processing in Mind and Machine", "authors": ["John F. Sowa"], "date": 1983, "abstract": "A new cultivar of Saintpaulia ionantha distinguished by its continuous and profuse production of vivid and well defined bi-color flowers, borne upright above the foliage on erect and rigid peduncles and exhibiting clean white petals each narrowly bordered by a bright blue margin.", "references": [], "page_rank": 0.0001778872468527641}, {"id": "ac3b4d70f5cced40be6a07967c4cf8aa8a460ec6", "title": "Natural language inputs to a simulation programming system", "authors": ["George E. Heidom"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"Natural language inputs to a simulation programming system\" by George E. Heidom", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b5b01207c035b655fd764fb35d282d07467bb279", "title": "Towards a Discourse Analysis Using Conceptual Graphs", "authors": ["Hiroshi Maruyama"], "date": 1985, "abstract": "Direct pressure leaching of zinc and iron containing mineral sulphides in dilute H2SO4 is carried out in a two-stage countercurrent process in which finely divided sulphides are leached in a first leaching stage with solution from the second leaching stage to produce a first stage leach solution containing a high zinc concentration and low iron and H2SO4 concentrations. The first stage solution, after suitable purification treatment, may be treated for zinc recovery by conventional electrowinning. The residue from the first stage leach is pressure leached with return electrolyte from the electrowinning operation with the leach end solution being passed to the first stage leach as already stated. The acid and sulphides concentrations in each of the first and second leaching stage is controlled such that (1) sufficient acid and sulphides enter the leaching circuit to ensure that the final leach solution exiting from the first leaching stage will contain sufficient dissolved zinc, e.g., 140-180 g.p.l., to make it suitable for treatment by electrolysis for recovery of zinc therefrom, (2) the quantity of sulphides entering the first stage leach is in excess of the quantity required to react with all available acid in the solution recycled from the second leaching stage and (3) the quantity of free acid entering the second leaching stage is in excess of the quantity required to react stoichiometrically with all acid reactive constituents in the residue passed from the first leaching stage to the second leaching stage.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "bc587390dcf5198671fb0fa9304f52dea5afdacb", "title": "Expanding the Utility of Semantic Networks Through Partitioning", "authors": ["Gary G. Hendrix"], "date": 1975, "abstract": "An augmentation of semantic networks is presented in which the various nodes and arcs are partitioned into \"net spaces.\" These net spaces delimit the scopes of quantified variables, distinguish hypothetical and imaginary situations from reality, encode alternative worlds considered in planning, and focus attention at particular levels of detail.", "references": ["05b2ce14226ac47418954b2e48887aa4612c8e32", "071e316fc7de5cac931900ae390ab1d13fb099e9"], "page_rank": 0.00010399562123700055}, {"id": "0f3edff08ee069948840610d9d58cc5a1225a118", "title": "Augmented phrase structure grammars", "authors": ["George E. Heidorn"], "date": 1975, "abstract": "Augmented phrase structure grammars consist of phrase structure rules with embedded conditions and structure-building actions written in a specially developed language. An attribute-value, record-oriented information structure is an integral part of the theory.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "5e52e74fa3dbeeffd47a079a53c90c1bda523592", "title": "Conceptual Graphs for Semantics and Knowledge Processing", "authors": ["Jean Fargues", "Marie-Claude Landau", "Anne Dugourd", "Laurent Catach"], "date": 1986, "abstract": "This paper discusses the representational and algorithmic power of the conceptual graph model for natural language semantics and knowledge processing. Also described is a Prolog-like resolution method for conceptual graphs, which allows to perform deduction on very large semantic domains. The interpreter developed is similar to a Prolog interpreter in which the terms are any conceptual graphs and in which the unification algorithm is replaced by a specialized algorithm for conceptual graphs.", "references": ["ae931723114d0ad1fbee600a3dc1a7a1dcfe0e15", "92ba24e7fbce40bd1d3e9275651b3a84f409a02e", "a677f2d52d1c0c3e762f0942c7ddac886462d24a", "03380e7083807d3264472871dc0582036cf79479", "94f6e007afe8e25005dd08cd2728d5a1849700c6", "172c6ad67c02b6d22a1f23a678583d547cffd4e8", "d53cfca137c38f82361878a67ea47002e9940e34"], "page_rank": 5.473453749315818e-05}, {"id": "d2e32e2acf3cc906f69e8340497567bce1a852a7", "title": "THE SNePS SEMANTIC NETWORK PROCESSING SYSTEM", "authors": ["Stuart C. Shapiro"], "date": 1979, "abstract": "Publisher Summary This chapter describes the SNePS semantic network processing system, which is a direct descendent of MENTAL. SNePS is currently implemented in ALISP and runs interactively on the CDC CYBER 173 at the State University of New York at Buffalo. There are three levels of SNePS: (1) the abstract graph level, (2) the pictorial level, and (3) the linear symbolic level. For the latter, the SNePS User Language, SNePSUL, is used. Semantic networks have been used as representations of knowledge since the mid-1960s. There are four levels at which semantic networks can be discussed: (1) an abstract graph level, (2) a two-dimensional pictorial level, (3) a one-dimensional symbolic level, and (4) a computer implementation level. These levels, though related, are independent in the sense that two semantic networks can differ on one or more levels and be the same on the other levels.", "references": [], "page_rank": 0.00030103995621237}, {"id": "73ed12a7b3019f266adca7beef47163b8b737402", "title": "The HEARSAY Speech Understanding System", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard D. Fennell", "Bruce T. Lowerre", "Richard B. Neely"], "date": 1974, "abstract": "This talk describes the present state of performance of the HEARSAY system. [For more complete descriptions of the system see D. R. Reddy, L. D. Erman, and R. D. Neely, \u201cA Model and a System for Machine Recognition of Speech,\u201d IEEE Trans. Audio Electroacoust. AU\u201021, 229\u2013238 (1973) and D. R. Reddy, L. D. Erman, R. D. Fennell, and R. B. Neely, \u201cThe HEARSAY Speech Understanding System : An Example of the Recognition Process,\u201d Proc. 3rd Int. Joint Conf. on Artificial Intelligence (Aug. 1973)]. The system uses task and context\u2010dependent information to help in the recognition of the utterance; this system consists of a set of cooperating parallel processes, each representing a different source of knowledge (e.g., acoustic\u2010phonetic, syntactic, semantic). The knowledge is used either to predict what may appear in a given context or to verify an hypothesis resulting from a previous prediction. Performance data of the system on several tasks (e.g., medical diagnosis, news retrieval, chess, and programming) will be ...", "references": [], "page_rank": 0.0001532567049808429}, {"id": "4448051cadba1a198f1fe1d705780cb43c8ca302", "title": "Analytic models of time-shared computing systems: new results, validations, and uses", "authors": ["Jr. John William Mccredie"], "date": 1972, "abstract": "Abstract : The goals of the research discussed in the report are: To create new models of time-shared computer systems which include important features commonly found in real systems; to insure that the formulations of, and solutions to, these models are relatively simple so that they may be used by designers and computer center managers; to compare the behavior of these models with the behavior of more complex systems through simulation studies and empirical performance investigations of operational computers; and to indicate some of the ways these models may be used to aid in the design, evaluation, and control of time-shared computers.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7c9a4334c1ca95e5d495c5d4e0e0a0edd4acc203", "title": "Property norms for typical and atypical items from 17 categories: A description and discussion", "authors": ["Mark H. Ashcraft"], "date": 1978, "abstract": "A description is presented of normative data for property responses to 121 words\u201417 category labels, three typical and three atypical members of each category, and the words \u201cplant\u201d and \u201canimal.\u201d The production frequency of properties is considered a measure of property dominance or semantic relatedness, and has been validated for the present data as a significant predictor of reaction time to property statements. Additional data include measures that support definitions of typicality in terms of property overlap between member and category, criteriality or dominance of the superordinate term, and the average number of properties generated to the category member. In reverse order, these three variables provide the best prediction of rated typicality. Average number of properties and superordinate dominance were the more important variables in this prediction, were virtually independent statistically, and were approximately equal in their contribution. Implications for semantic memory models are discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "62184a540db0f8c4712c7fe6c42272be3a7ed96e", "title": "Speech Recognition: Prospects of the Seventies", "authors": ["Raj Reddy"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Speech Recognition: Prospects of the Seventies\" by Raj Reddy", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "6a69a48ababc4d2e1b6132d5a2fc1de365942c9e", "title": "On the Motor Theory of Speech Perception", "authors": ["Peter B. Denes"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"On the Motor Theory of Speech Perception\" by Peter B. Denes", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "96a9cfa8a396fd8967b327dc675a8ce88f16a18f", "title": "Working Papers in Speech Recognition - II,", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard B. Neely"], "date": 1972, "abstract": "Abstract : The report is a collection of Working Papers in Speech Recognition on the following topics: Organization of the HEARSAY II speech understanding system; The DRAGON system -- an overview; Parameter-independent machine segmentation and labeling; A new time-domain analysis of fricatives and stop consonants; Sub-lexical levels in the HEARSAY II speech understanding system; Inference and use of simple predictive grammars; Real-time linear predictive coding of speech on the SPS-41 microprogrammed triple-processor system; A 16-bit A-D-A conversion system for high-fidelity audio research.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "190f432914ca9f9925860139c88d9664787a5939", "title": "The Technology Chess Program", "authors": ["James J. Gillogly"], "date": 1972, "abstract": "A chess program has been developed which plays good chess (for a program) using a very simple structure. It is based on a brute force search of the move tree with no forward pruning, using material as the only terminal evaluation function, and using a limited positional analysis at the top level for a tiebreak between moves which are materially equal. Because of the transparent structure, this program is proposed as a technological benchmark for chess programs which will continue to improve as computer technology increases.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f78390e72ac8f4cdb34a2f953a586b11a6cd67bd", "title": "On reasoning by default", "authors": ["Raymond Reiter"], "date": 1978, "abstract": "This paper surveys a number of kinds of default reasoning in Artificial Intelligence, specifically, default assignments to variables, the closed world assumption, the frame default for causal worlds, exceptions as defaults, and negation in Artificial Intelligence programming languages. Some of these defaults provide clear representational and computaional advantanges over their corresponding first order theories. Finally, the paper discusses various difficulties associated with default theories.", "references": ["2255db1a8ada12287fb175f52805f4c5bac26873", "4e895eed5fb7ded7baffd36f8aff455603787676"], "page_rank": 5.473453749315818e-05}, {"id": "da6eb135261dc3b3f7df8ff741796688898b5cd4", "title": "Extracting Information from Resolution Proof Trees", "authors": ["David C. Luckham", "Nils J. Nilsson"], "date": 1971, "abstract": "Procedures for generating proofs within a logical inference system can be applied to many information-retrieval and automatic problem-solving tasks. These applications require additional procedures for extracting information from the proofs when they are found. We present an extraction procedure for proofs generated by the resolution principle. The procedure uses a given proof to find solutions for existential quantifiers in the statement proved in terms of known quantities in the initial data. This procedure relies heavily on basic subfunctions in the resolution program, so that it requires relatively little additional programming. The correctness of the procedure is proved, and examples are given to illustrate how it operates and to show that it cannot be simplified at certain points without loss of generality.", "references": ["dd15a1958d5a7bfa45548eb101bef4186acf2299", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "ffe701d533cffe8b81d1d2921b07a72e93648fa3", "c8ed24d86755095c263a2f031752f817e668da3e", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "f516d9dd73b117eca287bd3a07c2f3d26523b765"], "page_rank": 5.473453749315818e-05}, {"id": "b349928abf6733c3028c23f6fc3cb6d09826277e", "title": "Automatic Recognition of Spoken Digits", "authors": ["Carma D. Forgie", "M. L. Groves", "Frederick Cowing Frick"], "date": 1958, "abstract": "No necessary\u2010and\u2010sufficient stimulus, or stimuli, has been discovered for the identification of any given word, yet people can reliably recognize speech under a wide variety of conditions One implication of this fact might be that speech recognition involves a complex data processing which takes advantage of linguistic redundancy. Such a system could make possible a reliable identification on the basis of a number of discriminating conditions, no one of which was in itself dependable. An attempt was made to realize such a data processing system in a program written for an experimental computer at Lincoln Laboratory. The program input consisted of the spoken digits processed through the Haskins Laboratories' resonance vocoder. Ninety\u2010eight percent recognition was achieved on a sample of ten voices, mixed male and female. [The research reported in this talk was supported jointly by the U. S. Army, Navy, and Air Force under contract with the Massachusetts Institute of Technology.]", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "e07a333e39c65b906f4284f192d9653f90c7d712", "title": "Results Obtained from a Vowel Recognition Computer Program", "authors": ["James W. Forgie", "Carma D. Forgie"], "date": 1959, "abstract": "As a first step toward a general speech recognition computer program, a program has been developed to recognize ten Eng.ish vowels in isolated words of the form /b/\u2014bowel\u2014/t/. Input to the computer was real time spectral data obtained through the input system described at these meetings in May, 1958, by Forgie and Hughes. The program was developed from the study of spectral data from five speakers. The program was tried on new speakers, the errors analyzed, and the program modified. The program, as finally evolved by this process, first determines the rough location of the first two formants. The remaining confusions are resolved by combinations of the following, as required: (1) position of F3, (2) the slopes of F1 and F2, (3) an estimate of pitch, and (4) the energy in the frequency band between F1 and F2. Whenever the over\u2010all power is above an arbitrary threshold, each 6\u2010msec scan of the filtered data is classified as one of the ten possible vowels. A tally at the end of the word determines the final ...", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9ef210f0ceeacbd9d6e65b9708bbfca18d73979d", "title": "A NOTE ON TWO BASIC SEMANTIC DISTINCTIONS", "authors": ["Marianne Celce", "Robert M. Schwarcz"], "date": 1969, "abstract": "Abstract : The paper discusses the nature of two basic semantic distinctions--intensional/extensional, and mental/physical (mental/physical being similar to abstract/concrete but more explicit)--and how aan understanding of their interaction is an essential preliminary to writing a semantically motivated grammar of English. (Author)", "references": [], "page_rank": 0.00016420361247947453}, {"id": "c68b11f2aa5eef2e5fa3690a973cdaf3041f7b7a", "title": "Programming the logic theory machine", "authors": ["Allen Newell", "J. C. Shaw"], "date": 1957, "abstract": "A companion paper has discussed a system, called the Logic Theory Machine (LT), that discovers proofs for theorems in symbolic logic in much the same way as a human does. It manipulates symbols, it tries different methods, and it modifies some of its processes in the light of experience.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d6190336549fff6d971aecc373c195464788a52a", "title": "Recognition memory for syntactic and semantic aspects of connected discourse", "authors": ["Jacqueline Sachs"], "date": 1967, "abstract": "This study investigates the pattern of retention of syntactic and semantic information shortly after comprehension of connected discourse. Ninety-six Ss listened to 24 taped passages and, after each passage, heard one recognition test sentence which was either identical to a sentence that had occurred in the passage, or was changed in some slight way. The Ss responded \u201cidentical\u201d or \u201cchanged,\u201d rated their confidence, and classified changes as \u201cmeaning\u201d or \u201cform.\u201d Two independent variables were manipulated: (1) The relationship between the original sentence in the passage and the test sentence. The test sentence was (a) semantically changed, (b) changed from active to passive voice or vice versa, (c) formally changed in other ways that did not affect the meaning, or (d) unchanged. Each sentence appeared in all change types. (2) The amount of interpolated material between the original and test sentences was zero, 80, or 160 syllables of connected discourse which was a continuation of the passage. Each S heard passages representing all levels of each variable. All combinations of particular passages, relationship of original and test sentence, and amount of interpolated material were tested.When the test sentence was heard immediately after the original, retention was high for all test types. But after 80\u00bd160 syllables, recognition for syntactic changes had dropped to near chance levels while remaining high for semantic changes. Even when the meaning of a sentence was remembered, formal properties that were not necessary for that meaning were forgotten very quickly. The results suggest that the original form of the sentence is stored only for the short time necessary for comprehension to occur. When a semantic interpretation has been made, the meaning is stored. Thus the memory of the meaning is not dependent on memory of the original form of the sentence.", "references": [], "page_rank": 0.00015247478301665492}, {"id": "879650714f3dce59b666346ccf63fd73250259d6", "title": "Simulation of self-organizing systems by digital computer", "authors": ["B. G. Farley", "W. A. Clark"], "date": 1954, "abstract": "A general discussion of ideas and definitions relating to self-organizing systems and their synthesis is given, together with remarks concerning their simulation by digital computer. Synthesis and simulation of an actual system is then described. This system, initially randomly organized within wide limits, organizes itself to perform a simple prescribed task.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9b3ea2ad8a2d0f1cfc327578f2481d422176aca2", "title": "Linguistic processes in deductive reasoning.", "authors": ["Herbert H. Clark"], "date": 1969, "abstract": "The present paper develops a theory to specify in part how a person stores and searches through information retained from sentences. The theory states that (a) functional relations, like the abstract subject-predicate relation which underlies sentences, are more available from memory than other, less basic kinds of information; (6) certain \"positive\" adjectives, like long, are stored in memory in a less complex and more accessible form than their opposites, like short; and (c) listeners can only retrieve, from memory, information which is congruent at a deep level to the information they are searching for. The present theory, unlike previous ones, correctly predicts the principal differences in the solution times of 8 types of two-term series problems and 32 types of three-term series problems (e.g., // John isn't as bad as Pete, and Dick isn't as good as Pete, then who is worst?). It also accounts for previous observations on children solving these problems and explains other phenomena in deductive reasoning. Deductive reasoning has often been studied in particular types of reasoning problems. The strategies suggested for their solution have therefore often been of limited generality : they apply in one kind of problem and that kind alone. The present paper proposes, instead, that reasoning is accomplished mainly through certain very general linguistic processes, the same mental operations", "references": ["3e467357faa9e737a71b4119a1ffadc795a8ac34", "e027ffaa9a27d1e5d8768307337f9a7d6fcb2a60", "5e2adb8a8b78d0a0d5803a1326302cf81a21cb66", "8471715ef5fad0ae204295cb5146330acd7e9410", "3aa7ab76543ff484a250acd674369648d05c8158", "7c2ce7737fd5a56ce82040957dd66a5637b9e040", "59ca51807e51584afc7bb1cf2d48edc9fdee6edf", "5fe2437b4015cbea1013709471b7d384108a32e5", "3b0bd66785193d9d5b29f85c5cd04d7169de893d"], "page_rank": 0.00034404566424270854}, {"id": "09c2bea7aba02571609c56fa4d27f77dcf7cd600", "title": "The chess machine: an example of dealing with a complex task by adaptation", "authors": ["Allen Newell"], "date": 1955, "abstract": "The modern general-purpose computer can be characterized as the embodiment of a three-point philosophy: (1) There shall exist a way of computing anything computable; (2) The computer shall be so fast that it does not matter how complicated the way is; and (3) Man shall be so intelligent that he will be able to discern the way and instruct the computer.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "dea6a942effe558ed7a134ee9c8eadd953c5a5b5", "title": "A command structure for complex information processing", "authors": ["J. C. Shaw", "Allen Newell", "Herbert A. Simon", "T. O. Ellis"], "date": 1958, "abstract": "The general purpose digital computer, by virtue of its large capacity and general-purpose nature, has opened the possibility of research, into the nature of complex mechanisms per se. The challenge is obvious: humans carry out information processing of a complexity that is truly baffling. Given the urge to understand either how humans do it, or alternatively, what kinds of mechanisms might accomplish the same tasks, the computer is turned to as a basic research tool. The varieties of complex information processing will be understood when they can be synthesized: when mechanisms can be created that perform the same processes.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "316961b569563051f5bee4bd318e057fadca71b7", "title": "Trie memory", "authors": ["Edward Fredkin"], "date": 1960, "abstract": "Trie memory is a way of storing and retrieving information. ~ It is applicable to information that consists of function-argument (or item-term) pairs--information conventionally stored in unordered lists, ordered lists, or pigeonholes. The main advantages of trie memory over the other memoIw plans just mentioned are shorter access time, greater ease of addition or up-dating, greater convenience in handling arguments of diverse lengths, and the ability to take advantage of redundancies in the information stored. The main disadvantage is relative inefficiency in using storage space, but this inefficiency is not great when the store is large. In this paper several paradigms of trie memory are described and compared with other memory paradigms, their advantages and disadvantages are examined in detail, and applications are discussed. Many essential features of trie memory were mentioned by de la Briandais [1] in a paper presented to the Western Joint Computer Conference in 1959. The present development is essentially independent of his, having been described in memorandum form in January 1959 [2], and it is fuller in that it considers additional paradigms (finitedimensional trie memories) and includes experimental results bearing on the efficiency of utilization of storage space.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b8938098d8c8bb2594cb6d02a55f5f8f203f4af4", "title": "On Closed World Data Bases", "authors": ["Raymond Reiter"], "date": 1977, "abstract": "Deductive question-answering system generally evaluate queries under one of two possible assumptions which we in this paper refer to as the open and closed world assumptions. The open world assumption corresponds to the usual first order approach to query evaluation: Given a data base DB and a query Q, the only answers to Q are those which obtain from proofs of Q given DB as hypotheses. Under the closed world assumption, certain answers are admitted as a result of failure to find a proof. More specifically, if no proof of a positive ground literal exists, then the negation of that literal is assumed true. In this paper, we show that closed world evaluation of an arbitrary query may be reduced to open world evaluation of so-called atomic queries. We then show that the closed world assumption can lead to inconsistencies, but for Horn data bases no such inconsistencies can arise. Presented at the Workshop on Logic and Data Bases, Toulouse, France, November 16-18, 1977.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "06162f180234f4e1e190784236157cc52c79a0b5", "title": "The Logic of Frames", "authors": ["Patrick J. Hayes"], "date": 1981, "abstract": "Publisher Summary Minsky introduced the terminology of frames to unify and denote a loose collection of related ideas on knowledge representation. Frames are sometimes understood at the metaphysical level. One aspect of frame reasoning which is often considered to lie outside of logic is the idea of a default value: a value that is taken to be the slot filler in the absence of explicit information to the contrary. A close analysis of what defaults mean shows that they are intimately connected with the idea of observations: additions of fresh knowledge into a data-base. Their role in inference \u2014 the drawing of consequences of assumptions \u2014 is readily expressible in logic, but their interaction with observation requires that the role of the state of the system's own knowledge is made explicit. This requires not a new logic but an unusual ontology and some new primitive relations. One needs to be able to talk about the system itself, in its own language, and to involve assumptions about itself in its own processes of reasoning. An emphasis on the analysis of such processes of reflexive reasoning is one of the few positive suggestions which the frames movement has produced.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "f8ddb251bf94e4b055c6f520f21816e403c30e2a", "title": "Truth Maintenance Systems for Problem Solving", "authors": ["Jon Doyle"], "date": 1977, "abstract": "Abstract : The thesis developed in this paper is that reasoning programs which take care to record the logical justifications for program beliefs can apply several powerful, but simple, domain-independent algorithms to: (1) maintain the consistency of program beliefs; (2) realize substantial search efficiencies; and (3) automatically summarize explanations of program beliefs. This report describes techniques for representing, recording, maintaining, and using justifications for beliefs. Also presented is an annotated implementation of a domain-independent program.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "2255db1a8ada12287fb175f52805f4c5bac26873", "title": "A Note on Deduction Rules with Negative Premises", "authors": ["Ivan Kramosil"], "date": 1975, "abstract": "The so called frame axioms problem or frame problem is well-known to anybody who is interested in automatic problem solving.Some information concerning this problem can be found in (1) or (2).In (2) the author proposes to eliminate the frame axioms in such a way that they are replaced by a new deduction rule denoted as UNLESS-operator.In fact,this operator is a rule enabling to derive that something concerning the environment is valid at the present situation supposing it was valid in a past situation and it is not provable that a change concerning the validity of this statement has occur-ed.For our reasona only the formally logical aspect of this solution is interesting es it leais immediately to a new and very interesting modification of the notion of formalized theory. The aspect just mentioned consists in in the fact that in (2) the author formalizes his UNL_.SS-operator in the following form of a deduction rule: If a formula A is deducible and a formula B is not deducible ; then a formula C is de-ducible.Written in symbols (1) where,of course.\"not\" and \" \" are symbols of an appropriate metatheory,not the investigated theory itself. The use of a deduction rule of this type may involve some doubts whether we are justified to do so.The first objection arises from the fact that any deduction rule,including those of the type (1), is an integral part of the definition of the notion \"deducible formula\",ao this notion itself.neither in its negative form \"not l-B\",is not allowed to occur in a deduction rule.It is a matter of fact that in \"usual\" formalized theories the deduction rules are also written in the form using the symbol ,e.g. if the modus ponens rule is considered. However,it is a well-known fact that in this case the symbol can be eliminated so that the definition of the notion \"provable formula\" should be correct.lt is the g08l of this paper to investigate the deduction rules of the type (1) in order to see,wheher a set of deduction rules,containing at least one rule of the type (1) and considered together with a recursive set of axioms defines unambiguously and correctly e set of de-ducible formulas.The second objection concerning the deduction rules of the type (1) arises from the fact that when a proof is constructed we have at our disposal in every step only a finite number of formulas proved already to be theorems,so \u2026", "references": [], "page_rank": 0.0006513409961685822}, {"id": "0eaf153cf0e613845e44d4543a7e1c12916f8afe", "title": "Non-Monotonic Logic I", "authors": ["Drew McDermott", "Jon Doyle"], "date": 1980, "abstract": "Abstract \u2018Non-monotonic\u2019 logical systems are logics in which the introduction of new axioms can invalidate old theorems. Such logics are very important in modeling the beliefs of active processes which, acting in the presence of incomplete information, must make and subsequently revise assumptions in light of new observations. We present the motivation and history of such logics. We develop model and proof theories, a proof procedure, and applications for one non-monotonic logic. In particular, we prove the completeness of the non-monotonic predicate calculus and the decidability of the non-monotonic sentential calculus. We also discuss characteristic properties of this logic and its relationship to stronger logics, logics of incomplete information, and truth maintenance systems.", "references": ["2255db1a8ada12287fb175f52805f4c5bac26873", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "b48d67c1e4e43f6ec2fdd3dc91a20a4efdc97ddc", "68ff263250279e00572fc43a050fa348838f34a1", "182c99e5bc08303fb84035dc23ce54033f4701ae", "f08f699374a27cdbc2c1ecf050ae285b01bda723", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "67a9db95296d2b2008d3c350690b67edd48c23b3", "07b82b58e1fd76540cf2217ed4537136855685d5"], "page_rank": 5.473453749315818e-05}, {"id": "25a13868653f786ba00043be5b34cb12c0854976", "title": "Heuristic methods for computer understanding of natural language in context-restricted on-line dialogues", "authors": ["Kenneth Mark Colby", "Horace Enea"], "date": 1967, "abstract": "Abstract This computer program accepts expressions in natural language as on-line input. It searches each expression for syntactic and semantic patterns. When a pattern match is discovered an appropriate reply is typed out in natural language so that a continuing dialogue develops between person and program. The dialogue is restricted to the context of interpersonal relations such as occurs in a psychiatric interview. The program is an interpreter/supervisor written in SUBALGOL and runs on a 32K IBM 7090 connected via a direct-data device to a PDP-1 and a Philco console.", "references": [], "page_rank": 0.00011963406052076002}, {"id": "e274e2cb0aaf7ad18f70e938ef58e01b6a590030", "title": "The ECL programming system", "authors": ["Ben Wegbreit"], "date": 1971, "abstract": "ECL is a programming language system currently being implemented as a research project at Harvard University. Its goal is an environment which will significantly facilitate the production of programs. In this paper, we describe the motivation for this project, present the approach taken in its design, and sketch the resulting ECL system. Detailed treatment of specific aspects of the system are found elsewhere.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "f354d8ce5cd2ead9301498aa1c16b5b2a0e4e0ec", "title": "Levels of language for portable software", "authors": ["Peter J. Brown"], "date": 1972, "abstract": "An increasing amount of software is being implemented in a portable form. A popular way of accomplishing this is to encode the software in a specially designed machine-independent language and then to map this language, often using a macro processor, into the assembly language of each desired object machine. The design of the machine-independent language is the key factor in this operation. This paper discusses the relative merits of pitching this language at a high level or a low level, and presents some comparative results.", "references": ["31bb1c092dba2b1a692b87cd2ff859bb7ce735f7"], "page_rank": 4.926108374384236e-05}, {"id": "5fed09060c4b19a30ef4ae2a9aeddaa3011f23f7", "title": "Toward a Programming Laboratory", "authors": ["Warren Teitelman"], "date": 1969, "abstract": "This paper discusses and feasibility and desirability of constructing a \"programming laboratory \" which would cooperate with the user in the development of his programs, freeing him to concentrate more fully on the conceptual diffculties of the problem he wishes to solve. Experience with similar systems in other fields indicates that such systems would significantly increase the programmer's productivity. \n \nThe PILOT system, implemented within the interactive BBN LISP system, is a step in the direction of a programming laboratory. PILOT operates as an interface between the user and his programs, monitoring both the requests of the user and operation of his programs. For example, if PILOT detects an error during the execution of a program, it takes appropriate correactive action based on previous Instructions from the user. Similarly, the user can give directions to PILOT about the operation of his programs, even while they are runing, and PILOT by instructing it about its own operation, anf thus develop his own language and conventions for interacting with PILOT. \n \nSeveral examples are presented.", "references": ["d836012048d706c484bbd9291f6c66d72c5cc508", "68f89c663b8dc91869640a11f7336f89b5e0f19c", "47f2261792cef069622607ba74bf7e3178651667", "d61f2b03eeff55581e83a3d6572ceb2d1478aeba", "4204f84ce3356ea0ce07a6b4600689220ddf7acf", "222e4515024210cfd4a42e88ef46462bdefec90a", "fed99fce695fdb08054722291f9ece37f581128b"], "page_rank": 4.926108374384236e-05}, {"id": "071e316fc7de5cac931900ae390ab1d13fb099e9", "title": "Partitioned networks for the mathematical modeling of natural language semantics.", "authors": ["Gary G. Hendrix"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"Partitioned networks for the mathematical modeling of natural language semantics.\" by Gary G. Hendrix", "references": [], "page_rank": 0.0002463054187192118}, {"id": "05b2ce14226ac47418954b2e48887aa4612c8e32", "title": "Active Semantic Networks as a Model of Human Memory", "authors": ["David E. Rumelhart", "Donald A. Norman"], "date": 1973, "abstract": "A g e n e r a l system to s i m u l a t e human c o g n i t i v e p r o cesses is d e s c r i b e d . The f o u r p a r t system compr ises a nodespace to s t o r e t he ne twork s t r u c t u r e ; a s u p e r v i s o r ; a t r a n s i t i o n network p a r s e r ; and an i n t e r p r e t e r . The method by wh ich noun phrases ope ra te and t h e p rocess f o r t he d e t e r m i n e r \" t h e \" i s p r e s e n t e d . A n a n a l y s i s o f ve rb s t r u c t u r e s i l l u s t r a t e s how network s t r u c t u r e s can b e c o n s t r u c t e d f rom p r i m i t i v e ve rb d e f i n i t i o n s t h a t ge t a t t h e u n d e r l y i n g s t r u c t u r e s o f p a r t i c u l a r v e r b s . The paper conc ludes w i t h an i l l u s t r a t i o n o f a p rob lem i n q u e s t i o n a s k i n g . A Model of Human Memory We have c o n s t r u c t e d a l a r g e g e n e r a l s i m u l a t i o n of human language and l o n g t e r m memory on t h e p remise t h a t t h e s t u d y o f t he i n t e r r e l a t i o n s h i p s among p s y c h o l o g i c a l p rocesses w i l l l ead t o more i n s i g h t i n t o human cog n i t i o n and memory. The g e n e r a l i m p l e m e n t a t i o n i s ba s i c a l l y c o m p l e t e , and a v a r i e t y o f use rs a r e s t a r t i n g t o s t u d y s p e c i f i c p s y c h o l o g i c a l t a s k s ( language under s t a n d i n g ; c h i l d r e n ' s development o f l anguage ; p r i m i t i v e v e r b s t r u c t u r e ; r e a d i n g ; i n f e r e n c e ; game p l a y i n g G o and Gomoku; v i s u a l r e p r e s e n t a t i o n and memory; l e a r n i n g ; and q u e s t i o n a n s w e r i n g ) . I t i s s t i l l too e a r l y t o r e p o r t o n t h e r e s u l t s o f t h e p s y c h o l o g i c a l i n v e s t i g a t i o n . . T h e r e f o r e , t h i s paper i s a p r o g r e s s r e p o r t o n t h e s y s tem and t h e u n d e r l y i n g p s y c h o l o g i c a l p r i n c i p l e s . The ma jo r g u i d e l i n e s have come f rom our a t t e m p t s to r e p r e s e n t l o n g t e r m memory s t r u c t u r e s . We know t h a t peop le r a p i d l y f o r g e t t h e d e t a i l s about t h e s u r f a c e s t r u c t u r e o f a n e x p e r i e n c e b u t r e t a i n t h e meaning o r i n t e r p r e t a t i o n o f t h a t e x p e r i e n c e i n d e f i n i t e l y . W e a l so know t h a t r e t r i e v a l o f an e x p e r i e n c e f rom memory i s u s u a l l y a r e c o n s t r u c t i o n wh ich i s h e a v i l y b i a s e d b y t h e p e r s o n ' s g e n e r a l knowledge o f t h e w o r l d . Thus , g e n e r a l w o r l d knowledge shou ld i n t e r a c t w i t h s p e c i f i c event knowledge in such a way t h a t d i s t i n c t i o n between the two i s n o t p o s s i b l e . The r e p r e s e n t a t i o n shou ld a l l o w p a r a p h r a s e . F i n a l l y , t h e l i m i t a t i o n s o f human w o r k i n g s t o r a g e ( o r s h o r t t e r m memory) p r o b a b l y compr i se a f u n damenta l p r o p e r t y o f t h e sys tem, one t h a t shou ld be v iewed as an e s s e n t i a l , p o s i t i v e component , n o t as s imp l y a per fo rmance l i m i t a t i o n .", "references": [], "page_rank": 0.0002463054187192118}, {"id": "a761f26f8239acd88fc83787f28a7f2d2ff9ea22", "title": "Procedural Embedding of knowledge in Planner", "authors": ["Carl Hewitt"], "date": 1971, "abstract": "Since the last IJCAI, the PLANNER problem solving formalism has continued to develop. Our eplstemolgy for the foundations for problem solving has been extended. An overview of the formalism Is given from an Information processing view point. A simple example Is explained using snapshots of the state of the problem solving as the example Is worked, finally, current applications for the formalism are listed.", "references": ["e91acb2dd6054614bc254376f2cdb464fe3ba073"], "page_rank": 0.00011963406052076002}, {"id": "5e9d825d198820be858a5ebb898087a0d52c9c78", "title": "The metatheory of the elementary equation calculus.", "authors": ["Alan Bundy"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"The metatheory of the elementary equation calculus.\" by Alan Bundy", "references": [], "page_rank": 0.00016420361247947453}, {"id": "94f6e007afe8e25005dd08cd2728d5a1849700c6", "title": "Prolog in Ten Figures", "authors": ["Alain Colmerauer"], "date": 1983, "abstract": "Prolog is presented in a rigourous way, through 10 easi ly understandable f i gu res . I t s theoret ica l model is completly rewrought. After introducing i n f i n i t e trees and i nequa l i t i es , t h i s paper puts fo r th the minimal set of concepts necessary to give Prolog an autonomous existence, independent of lengthy considerations about f i r s t order logic and inference ru les . Mystery is sacr i f i ced in favor of c l a r i t y .", "references": [], "page_rank": 0.0001231527093596059}, {"id": "816e9b69c6adb73205e841be8e69b3651d3bfbbf", "title": "Planner: a language for manipulating models and proving theorems in a robot\" ijcai-69", "authors": ["Carl Hewitt"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Planner: a language for manipulating models and proving theorems in a robot\" ijcai-69\" by Carl Hewitt", "references": [], "page_rank": 0.00022577996715927748}, {"id": "a677f2d52d1c0c3e762f0942c7ddac886462d24a", "title": "Problems in Logical Form", "authors": ["Robert C. Moore"], "date": 1981, "abstract": "Abstract : Most current theories of natural-language processing propose that the assimilation of an utterance involves producing an expression or structure that in some sense represents the literal meaning of the utterance. It is often maintained that understanding what an utterance literally means consists in being able to recover such a representation. In philosophy and linguistics this sort of representation is usually said to display the \"logical form\" of an utterance. This paper surveys some of the key problems that arise in defining a system of representation for the logical forms of English sentences and suggests possible approaches to their solution. The author first looks at some general issues relating to the notion of logical form, explaining why it makes sense to define such a notion only for sentences in context, not in isolation, and then discusses the relationship between research on logical form and work on knowledge representation in artificial intelligence. The rest of the paper is devoted to examining specific problems in logical form. These include the following: quantifiers; events, actions and processes; time and space; collective entities and substances; propositional attitudes and modalities; and questions and imperatives.", "references": ["a9952695e1c107f24a1f6f36b287139f2db00a90", "a0adea7988254f3d0740b587334c8ca6357cdd8b", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "e40e4a3f4cffa628e98097107f16be69dcf81262", "e8ff7d065cb2eed432348c8143bbfd02245246ae"], "page_rank": 0.00022167487684729062}, {"id": "a56524d3d5a847a54e547d07cb713a54f3cb4b68", "title": "Interactions Between Philosophy and Artificial Intelligence: The Role of Intuition and Non-Logical Reasoning in Intelligence", "authors": ["Aaron Sloman"], "date": 1971, "abstract": "Abstract This paper echoes, from a philosophical standpoint, the claim of McCarthy and Hayes that Philosophy and Artificial Intelligence have important relations. Philosophical problems about the use of \u201cintuition\u201d in reasoning are related, via a concept of anlogical representation, to problems in the simulation of perception, problem-solving and the generation of useful sets of possibilities in considering how to act. The requirements for intelligent decision-making proposed by McCarthy and Hayes are criticised as too narrow, and more general requirements are suggested instead.", "references": ["eb35108a8903c75b96c426b420144353f7f16c7a", "bea20de7b908ee518194e7247ce00f098991883b"], "page_rank": 0.00016420361247947453}, {"id": "2bce48117d8589af931d1610338fa3b2599bf2d1", "title": "ETC: an extendible macro-based compiler", "authors": ["B. N. Dickman"], "date": 1971, "abstract": "ETC (ExTendible Compiler) is a high level language compiler that allows the programmer to produce very efficient code when necessary, getting as close to the machine as he desires, and yet to write in machine independent statements when the production of optimized code is not necessary. The programmer may also easily extend ETC, creating new data types and operations either from previous extensions or from the machine operations (or both).", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "2ff0eeb5cceb4bb9698e470521e04d19e03dbcc1", "title": "Requirements for advanced programming systems for list processing", "authors": ["Daniel G. Bobrow"], "date": 1972, "abstract": "List processing systems should be designed to facilitate production of large programs to manipulate large complex symbolic data stores. This paper presents an overview of a number of system features which the author feels are important to improve the productivity of programmers working in such domains. A systems view is taken, rather than focusing just on language features, since algorithms must be not only coded in a language form, but debugged, modified, made efficient, and run on data. Because of this general framework, the requirements specified are applicable to the design of advanced programming systems for a wide range of applications.\nThree aspects of programming systems are highlighted: good interactive facilities, programmable control structures, and sophisticated data communication mechanisms. Interactive features are described to facilitate program composition, entry, testing, debugging, editing, optimization, and packaging. Implementation of a generalized environment structure model specified would allow programming of various control regimes including multiprocesses, coroutines and backtracking. Alternative methods of procedure invocation required include invocation by pattern and by monitoring condition. The need for extended data forms, storage management, and extensibility are stressed, as is the duality of data retrieval and function evaluation. Syntax directed input and output of data would facilitate use of complex data stores.", "references": [], "page_rank": 0.00014778325123152708}, {"id": "f51b42090496ab8b338840e593c78b25452986e4", "title": "Knowledge Representation Language", "authors": ["Daniel G. Bobrow", "Terry Winograd"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"Knowledge Representation Language\" by Daniel G. Bobrow et al.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "9aba81a93203f0f92f7cc9cf38b2d5b54e346c49", "title": "Program and Protocol Analysis on a Mental Imagery Task", "authors": ["George W. Baylor"], "date": 1971, "abstract": "This paper presents a LISP 1.5 program and parts of a protocol analysis that model aspects of the behavior on one adult subject (S) solving a set of Guilford's block visualization problems. The model derives from a detailed analysis of S's thinking aloud protocol on four of these problems. Two problem spaces are postulated to account for S's internal representation of the task. Nine operators, evoked by a production system cum goal stack, are used to describe his encoding and problem solving. The program lags the protocol analysis: It respects the two problem spaces but incorporates only five of the nine operators, and these are evoked by hand. The principal problem-solving operators in the image space, Process Block and Tally, both programmed, are described, and the behavior they generate is put in correspondence with, and serves as a set of psychological hypotheses for, S's behavior. Their plausibility rests primarily on the closeness of simulation to S's performance, while their generality has scarcely been tested.", "references": ["76c86f58da0fc1270eb9655cc0b08468e4120cdd", "fc06e32be22ec9fa295986ed41e63b9b5e710fba", "d1c8dd5bfcd1270af88f9fb151a377a6051ad2a3", "354d338bbe84b3bd361eccc8e3368b54b52eea9e", "a9388c1101f83f4a882aa6bf7aa96ae9cab1900e", "47f2261792cef069622607ba74bf7e3178651667", "a2af0d367f13565cb5676aa70f49b1ee68fb69e3", "f3be4120894ba32a8c75d503d6c6687d93277c55", "382c8aee17ed963bae6c3b5f4f2047773093b261"], "page_rank": 0.0004926108374384236}, {"id": "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "title": "An ALGOL-based associative language", "authors": ["Jerome A. Feldman", "Paul Rovner"], "date": 1969, "abstract": "A high level programming language for large, complex associative structures has been designed and implemented. The underlying data structure has been implemented using a hash-coding technique. The discussion includes a comparison with other work and examples of applications of the language.", "references": ["4aac001b7741dc79056c86e9aaa4a2e20841b68e", "412f7940a8edea0b52c2fcd019a9d7cdaf7f14e9"], "page_rank": 0.00028051450465243566}, {"id": "f1a93cd7f9302e93462c7d694a84527ae23bab7f", "title": "Eyes and Ears for Computers", "authors": ["Raj Reddy"], "date": 1973, "abstract": "Visual and speech perception tasks, which can be performed with no apparent effort by people, have proved to be difficult for machines. This may be in part due to the absence of cognitive models of perception of the type proposed above by Jakobson. In this paper we attempt to give a unified view of the research in machine perception of speech and vision in the hope that a clear appreciation of similarities and differences may lead to better information processing models of perception. Being active in research in both computer vision and speech, we have found it useful to look at the problems that have arisen in one domain and anticipate corresponding problems in the other (Reddy, 1969). Thus, this paper represents a comparitive study of the issues, systems and unsolved problems that are, at present, of interest to visual and speech recognition research.", "references": ["8ef1568b4377fce96f9d350d6d46a619d71a1462", "73ed12a7b3019f266adca7beef47163b8b737402", "79b72cbd43608729524b5e41384094949a6064ce", "1374222f975da69b26b0c34d4fac6d139a065e4d", "236dcee7c12e0a19ed200a02e95e55c561266347", "36421f1bcc37d6b46c8a4d80c588569decfce59e", "fda841df3b0620c9322514d4434ca1dcccddd1f8", "e3c63e00a1bc1042406ef08775b6482ccda0fa1b", "c2e41b4445a5988f59136c62fc061958c088e731", "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a"], "page_rank": 5.473453749315818e-05}, {"id": "85827cf800d963c44edee1c79d9431cf46fdeef8", "title": "The LISP 2 programming language and system", "authors": ["Paul W. Abrahams", "Jeffrey A. Barnett", "Erwin Book", "Donna Firth", "Stanley L. Kameny", "Clark Weissman", "Lowell Hawkinson", "Michael I. Levin", "Robert A. Saunders"], "date": 1966, "abstract": "LISP 2 is a new programming language designed for use in problems that require manipulation of highly complex data structures as well as lengthy arithmetic operations. Presently implemented on the AN/FSQ--32V computer at the System Development Corporation in Santa Monica, California, LISP 2 has two components: the language itself, and the programming system in which it is embedded. The system programs that define the language are accessible to and modifiable by the user; thus the user has an unparalleled ability to shape the language to suit his own needs and to utilize parts of the system as building blocks in constructing his own programs.", "references": [], "page_rank": 0.000541871921182266}, {"id": "006521402572c1d0fcded90f8d16803b640803cd", "title": "A new minicomputer/multiprocessor for the ARPA network", "authors": ["Frank E. Heart", "Severo M. Ornstein", "William R. Crowther", "W. B. Barker"], "date": 1973, "abstract": "Since the early years of the digital computer era, there has been a continuing attempt to gain processing power by organizing hardware processors so as to achieve some form of parallel operation. One important thread has been the use of an array of processors to allow a single control stream to operate simultaneously on a multiplicity of data streams; the most ambitious effort in this direction has been the ILLIAC IV project. Another important thread has been the partitioning of problems so that several control streams can operate in parallel. Often functions have been unloaded from a central processor onto various specialized processors; examples include data channels, display processors, front-end communication processors, on-line data preprocessors---in fact, I/O processors of all sorts. Similarly, dual processor systems have been used to provide load sharing and increased reliability. Still another thread has been the construction of pipeline systems in which sub-pieces of a single (generally large) processor work in parallel on successive phases of a problem. In some of these pipeline approaches the parallelism is \"hidden\" and the user considers only a single control stream.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "320cefa46a705ef20ace952267a34eaa0d2c929f", "title": "Recent developments in SAIL: an ALGOL-based language for artificial intelligence", "authors": ["Jerome A. Feldman", "James R. Low", "D. C. Swinehart", "R. H. Taylor"], "date": 1972, "abstract": "Progress in Artificial Intelligence has traditionally been accompanied by advances in special purpose programming techniques and languages. Virtually all of this development has been concentrated in languages and systems oriented to list processing. As the efforts of Artificial Intelligence researchers began to turn from purely symbolic problems toward interaction with the real world, certain features of algebraic languages became desirable. There were several attempts (notably LISP2 and FORMULA ALGOL) to combine the best features of both kinds of language. At the same time, designers of algebraic languages began to include features for non-numerical computation. No new general purpose language without some sort of list processing facility has been suggested for several years. We have followed a tack somewhat different from either of these in the design of SAIL and in its subsequent modifications.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "70e69265cb050a5050e75ccb308e00b6d9571ab0", "title": "System Support for the Stanford Hand-Eye System", "authors": ["Jerome A. Feldman", "Robert F. Sproull"], "date": 1971, "abstract": "The Stanford hand-eye system is implemented as several separate tasks, each executing under a timesharing executive. Development of a programming language (SAIL) and augmentation of the timesharing system were required to provide the necessary data sharing and control flow among the tasks. The SAIL language provides facilities for \"associative processing,\" and is extended to serve the data sharing and communication needs of the hand-eye system. Several user facilities are designed to aid running and debugging the system.", "references": ["cf4570f4801181a2f8e1b9aac77c180d0206834e", "c2e41b4445a5988f59136c62fc061958c088e731", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8"], "page_rank": 0.00011083743842364531}, {"id": "5444939ad7935c0302f414c136c0d4f09c202e67", "title": "Computer network development to achieve resource sharing", "authors": ["Lawrence G. Roberts", "Barry D. Wessler"], "date": 1970, "abstract": "In this paper a computer network is defined to be a set of autonomous, independent computer systems, interconnected so as to permit interactive resource sharing between any pair of systems. An overview of the need for a computer network, the requirements of a computer communication system, a description of the properties of the communication system chosen, and the potential uses of such a network are described in this paper.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "be2ccd19a3906c54172027d17cfc2c017c978572", "title": "Dynamic control structures and their use in emulation", "authors": ["Victor R. Lesser"], "date": 1973, "abstract": "This thesis describes an architecture for a parallel microcomputer system that permits a systematic and flexible approach to the emulation of a wide variety of complex sequential and parallel intermediate machine languages in a dynamically varying Processor-Memory-Switch (PKS) environment. This architecture is based on the view that complex emulators can be best structured in terms of a set of microprocessors that interact in a highly structure manner. These highly structured interaction patterns are defined through the concept of a virtual PMS environment. This concept embodies the capability for reconfiguring both the internal and the external environment of a microcomputer system: the number of internal working registers of each microprocessor; the structure of memory, e.g., its size and word length; and the number of microprocessors and functional units, and their interconnection and interaction patterns. The virtual PMS is implemented in the microcomputer architecture by adding a new global level of hardware control. A particular virtual PMS is dynamically defined by modifying the syntax (i.e., the number of data elements and their relationship) of the data structure for control used by this global hardware control level. The representational capabilities of this architecture have been examined through the microprogramming of an emulator for a sophisticated parallel machine language, Adams\u2019 Graph Machine Language. The emulator of this machine language has demonstrated the versatility and usefulness of the concept of a virtual PMS by requiring less than 600 64-bit microinstructions to be programmed, while at the same time being able to exploit fully the implicit parallelism of a graph machine program. In addition, the dynamic execution characteristics of this architecture have been studied through the use of a detailed simulator of a hardware organization for this microcomputer architecture. The simulator has been used to verify quantitatively that this organization permits parallel activity on the virtual PMS to be mapped without significant overhead onto the physical PMS. In particular, the simulation results indicate that where sufficient parallel activity exists, the addition of microprocessors to the PMS configuration will reduce in a linear way the time it takes to execute the computation. The simulation results have also indicated that the logical hardware design, with the appropriate PMS configuration, can efficiently handle sustained parallel activity, involving highly structured interaction patterns, of greater than sixteen microprocessors. . . . 111", "references": ["9193f366e993ed27c2605eceaa18f13b1439035e", "96e741a8139bb6f06fa9903f830d5d4f23029487", "b44dbeb6e8dba0558c4c02b2786897d771d33d8f", "fceac668a164d18128e8f1e06965b3c7ad5203fd", "456ac66abf2d97b6cf8ebacd85107f69e7a97c08", "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "5198b80c4037a9ff9f84e6081c5567888b1ac0fb", "21678be84430f56942cf5172c281b1861b9ac7a0", "5ee22f744331bf35bc45c3538000790e9483b967", "0930f4d5637eb6f25e922f08ae09e03847a9404d"], "page_rank": 4.926108374384236e-05}, {"id": "3b0bd66785193d9d5b29f85c5cd04d7169de893d", "title": "Underlying and Superficial Linguistic Structure", "authors": ["Paul Martin Postal"], "date": 1964, "abstract": "The following remarks based on examples from English are a rather informal discussion of some of the kinds of results and implications of linguistic research being done in the conceptual framework which has come to be called 'generative grammar.'", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "title": "The working set model for program behavior", "authors": ["Peter J. Denning"], "date": 1968, "abstract": "Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate model for program behavior. In this paper a new model, the \u201cworking set model,\u201d is developed. The working set of pages associated with a process, defined to be the collection of its most recently used pages, provides knowledge vital to the dynamic management of paged memories. \u201cProcess\u201d and \u201cworking set\u201d are shown to be manifestations of the same ongoing computational activity; then \u201cprocessor demand\u201d and \u201cmemory demand\u201d are defined; and resource allocation is formulated as the problem of balancing demands against available equipment.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "5fe2437b4015cbea1013709471b7d384108a32e5", "title": "Response to affirmative and negative binary statements.", "authors": ["Peter Cathcart Wason"], "date": 1961, "abstract": "A previous experiment showed consistent and significant differences in the times taken to complete true affirmative, false affirmative, true negative and false negative statements in relation to given situations. In that task only one pattern of stimuli could have made an affirmative true and a negative false, but more than one could have made an affirmative false and a negative true. The present investigation equates the specificity of all statements, and hence eliminates the disjunctive implication of false affirmatives and true negatives by utilizing the mutually exclusive classes of odd and even numbers as material. Two tasks were used, (a) \u2018verification\u2019, i.e. determining whether a statement about a number is true or false, and (b) \u2018construction\u2019, i.e. stating a number to make a statement either true or false. In both tasks the form of a statement (affirmative or negative) was significant at the 0\u00b7001 level at the end of practice. In the construction task the truth value of a statement (true or false) was also significant at the 0\u00b7001 level, but in the verification task it was not significant. These results were obtained from two groups who performed the tasks in a different order. The cognitive processes involved in the tasks are discussed.", "references": [], "page_rank": 0.00031746031746031746}, {"id": "59ca51807e51584afc7bb1cf2d48edc9fdee6edf", "title": "Grammatical relations as determinants of sentence similarity", "authors": ["Charles Clifton", "Ida Kurcz", "James J. Jenkins"], "date": 1965, "abstract": "Generalization of a motor response among the kernel, passive, negative, and passivenegative forms of a number of sentences was investigated. The generalization among such constructions, relative to generalization among unrelated sentences, was found to be highly significant. A distance metric was used to compare the generalization decrements obtained between the various constructions. It was found that the distance between sentences related by the passive transformation was less than the distance between sentences related by the negative transformation, or by the combination of the negative and passive transformations. The distance between sentences related by the negative transformation was not consistently less than the distance between sentences related by the combination of passive and negative transformations. However, when the possible orders of application of the transformations were taken into account, it appeared that the distance between sentences related by the combination of passive and negative transformations equalled the sum of the distance between sentences related by the passive transformation and the distance between sentences related by the negative transformation. The implications of current linguistic analyses calling into question the transformational nature of the relationships among the sentences were briefly considered.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "5e2adb8a8b78d0a0d5803a1326302cf81a21cb66", "title": "STUDIES IN THE DEVELOPMENT OF REASONING IN SCHOOL CHILDREN", "authors": ["Dorothy V. Wheeler"], "date": 1958, "abstract": "Summary. Earlier researches produced a number of conflicting hypotheses as to the nature and development of the reasoning processes; but of late the whole problem has been singularly neglected. The following investigations attempt to decide between the rival theories by the combined use of experimental, statistical, and introspective procedures. The main conclusions are: (i) that the solution of problems by reasoning involves, in addition to the general factor of intelligence, a number of specific factors (e.g. factors for the apprehension of relations, for the combination of relations, and for both analytic and intuitive procedures); (ii) that, contrary to the view put forward by Hall and Piaget, most of the elementary schemata necessary for valid reasoning are already within the capacity of the average child of seven, so that subsequent development consists chiefly in a more experienced use of these capacities and in an increasing complexity of the problems which the child is able to solve.", "references": [], "page_rank": 0.00012510751427007583}, {"id": "3aa7ab76543ff484a250acd674369648d05c8158", "title": "Judgment and Reasoning in the Child", "authors": [""], "date": 1940, "abstract": "AN excellent book. Dr. Piaget gives a very detailed and comprehensive account of investigation into judgment and reasoning as shown by young children. The book forms a supplement to \u201cLanguage and Thought of the Child.\u201d The logical and reasoning powers of children are not simply elementary forms of adult logic and reasoning; they are something different. The logic of the child is almost entirely ego-centric; it is more closely allied to the autistic or dereistic type of thinking, a conception which we owe to the psychoanalytic school. The child's powers of reasoning are very limited, and it is not until the age of 11-12 years that anything approaching sound formal reasoning appears.Judgment and Reasoning in the Child.Prof.JeanPiagetBy, in collaboration with Miles. E. Cartalis, S. Escher, A. Hanhart, L. Hannloser, O. Matthes, S. Perret, and M. Roud. Translated by Marjorie Warden. (International Library of Psychology, Philosophy and Scientific Method.) Pp. viii + 260. (London: Kegan Paul and Co., Ltd.; New York: Harcourt, Brace and Co., 1928.) 10s. 6d. net.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "e027ffaa9a27d1e5d8768307337f9a7d6fcb2a60", "title": "THE USE OF SYNTAX IN UNDERSTANDING SENTENCES", "authors": ["Herbert H. Clark", "Jeffrey S. Begun"], "date": 1968, "abstract": "An English speaker appears to understand a sentence while he is hearing it. This observation, taken naively, suggests that people process a sentence for its semantic information in a strict left-to-right order. Recent linguistic work on generative grammars (Chomsky, 1957, 1965), however, implies quite a different process. People must process a sentence for its deep structure-its underlying functional relationsin order to understand it, and this processing is necessarily hierarchical in character. The present paper describes two experiments which uncover some of the psychological properties of this hierarchical processing, and another two experiments which show that this hierarchical processing is independent of a distinct, but real, left-to-right processing of semantic content. In the spirit of generative grammars, speakers must, at some point, understand sentences in terms of elementary functional relations. These relations, extracted by speakers from the sentences they hear, are contained in the kernel strings (as yet unformed basic sentences) which underlie the sentences. Consider The clown amused the children. This sentence contains the functional relations of: subject of the sentence (the clown), predicate of the sentence (amused the children), main verb (amuse), object of the verb (children), and so on (Chomsky, 1965). For the present study we singled out kernel strings with transitive verbs, choosing sentences like The clown amused the children and The umpire was hit by the bat. The functional relations of interest in such sentences were named, for brevity, subject (clown, bat), verb (amuse, hit) and object (children, umpire). The main question we asked was: How do people process sentences for understanding in terms of these relations? To answer it, we gave participants of Expt. I partly anomalous active sentences to judge for sensibleness. This confirmed that people do process sentences hierarchically. In Expt. II we asked other students to alter the sentences so that they made more sense. These students, while reflecting the hierarchical structure of Expt. I, also seemed to show a left-to-right process. In Expt. III, therefore, we required other students to judge the sensibleness of both actives and passives. Here it was found that the subject, verb and object were pro", "references": ["da30054297eaee23a57271f7f9c4d7655841c537", "166c9f94ba1ffd1eb27444608cfe78fce77366e6", "ce47461b286a333b2131743370fcf05f9ea067e7", "e77c7042063d7439927b605b449c1d642d552f6c", "af5fb38672c7ff11d7ae28db64e5c01c2287db52", "8fcf060606495f711e98ac4acb1eb1a7def35c9b", "22ba823c095224796a06631634350f6694e8f6d2", "acc68f38ab9c5a636d764c95bd8e92ce67516063"], "page_rank": 0.0002345765892563922}, {"id": "7c2ce7737fd5a56ce82040957dd66a5637b9e040", "title": "Grammatical structure and the immediate recall of english sentences", "authors": ["Harris B. Savin", "Ellen Perchonock"], "date": 1965, "abstract": "Various grammatical features of English sentences\u2014negative and passive transformations, and others\u2014are encoded in immediate memory apart from one another, and apart from the rest of the sentence. The evidence for this claim is that sentences having these features require a larger part of the capacity of immediate memory than do otherwise identical sentences lacking these same features. The amount of memory capacity required by each sentence was measured by seeing how much additional material could be remembered along with it.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "8471715ef5fad0ae204295cb5146330acd7e9410", "title": "Grading, A Study in Semantics", "authors": ["Edward Sapir"], "date": 1944, "abstract": "The first thing to realize about grading as a psychological process is that it precedes measurement and counting. Judgments of the type \"A is larger than B\" or \"This can contains less milk than that\" are made long before it is possible to say, e.g., \"A is twice as large as B\" or \"A has a volume of 25 cubic feet, B a volume of 20 cubic feet, therefore A is larger than B by 5 cubic feet,\" or \"This can contains a quart of milk, that one 3 quarts of milk, therefore the former has less milk in it.\" In other words, judgments of quantity in terms of units of measure or in terms of number always presuppose, explicitly or implicitly, preliminary judgments of grading. The term four means something only when it is known to refer to a number which is \"less than\" certain others, say five, six, seven, arranged in an ordered series of relative mores and lesses, and \"more than\" certain others, say one, two, three, arranged in an ordered series of relative mores and lesses. Similarly, a foot as a unit of linear measure has no meaning whatever unless it is known to be more than some other stretch, say an inch, and less than a third stretch, say a yard. Judgments of \"more than\" and \"less than\" may be said to be based on perceptions of \"envelopment.\" If A can be \"enveloped by\" B, contained by it, so placed in contact with B, either actually or by the imagination, as to seem to be held within its compass instead of extending beyond it, it is judged to be \"less than\" B, while B is judged to be \"more than\" A. With only two existents of the same class, A and B, the judgments \"A is less than B\" and \"B is more than A\" can be translated into the form \"A is small\" and \"B is large.\" In the case of the two cans of milk, we may say \"There is little milk in this can\" and \"There is much milk in that can.\" Again, if there are three men in one room and seven in another, we may either say \"The first room has fewer men in it than the second\" and \"The second room has more men in it than the first\" or, if we prefer, \"The first room has few men in it\" and \"The second room has many men in it.\"1 Such contrasts as small and large, little and much, few and many, give us a deceptive feeling of absolute values within the field of quantity comparable to such qualitative differences as red and green within the field of color perception. This feeling is an illusion, however, which is largely due to the linguistic fact that the grading which is implicit in these terms is not formally indicated, whereas it is made explicit in such judgments as \"There were fewer people there than here\" or \"He has more milk than I.\" In other words, many, to take but one example, embodies no class of judgments clustering about a given quantity norm which is applicable to every type of experience, in the sense in which red or green is applicable to every experience in which color can have a place, but is, properly speaking, a purely relative term which loses all significance when deprived of its conno-", "references": [], "page_rank": 0.00012510751427007583}, {"id": "67a9db95296d2b2008d3c350690b67edd48c23b3", "title": "Some extensions of a system for inferencing on partial information", "authors": ["Aravind K. Joshi"], "date": 1977, "abstract": "Some extensions of a system for inferencing on partial information, which uses production rules, are described. Basically, the system consists of RULES, an active set of rules (a subset of potentially large set of rules), partially ordered by specificity, and FACTS, a small active set of facts (a subset of potentially large set of data base facts). The critical feature of the inference method is that only a partial match of the antecedent of a rule is needed. Some selected extensions of the system are presented. These concern some approaches to the problems of selecting from an ambiguous response and, more importantly, transforming or dynamic clustering of FACTS and RULES. This latter problem is important because partial match is defined over the sets RULES and FACTS, and unless these sets are reasonably small, partial match can be an unmanageable operation. Several issues concerning the use of this inference system in certain applications are also briefly discussed.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "182c99e5bc08303fb84035dc23ce54033f4701ae", "title": "Unprovability of Consistency", "authors": ["J. Donald Monk"], "date": 1976, "abstract": "We shall prove in this chapter that in a strong theory \u0413, some statements which naturally assert the consistency of \u0413 cannot be proved within \u0413. This famous result of Godel shows that our ordinary first-order languages have a severe limitation as far as any project for a thorough-going check on the consistency of mathematics is concerned. Historically, the theorem caused a major change of emphasis in foundational research away from a preoccupation with consistency proofs.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "4204f84ce3356ea0ce07a6b4600689220ddf7acf", "title": "THE BBN-LISP SYSTEM", "authors": ["Daniel G. Bobrow", "D. Lucille Darley", "Daniel L. Murphy", "Cynthia J. Solomon", "Warren Teitelman"], "date": 1966, "abstract": "Abstract : The report describes in detail the BBN-LISP system. This LISP system has a number of unique features; most notably, it has a small core memory, and utilizes a drum for storage of list structure. The paging techniques described allow utilization of this large, but slow, drum memory with a surprisingly small time penalty. These techniques are applicable to the design of efficient list processing systems embedded in time-sharing systems using paging for memory allocation.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "222e4515024210cfd4a42e88ef46462bdefec90a", "title": "Symbolic mathematical laboratory.", "authors": ["William A. Martin"], "date": 1967, "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1967. Ph.D.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "fed99fce695fdb08054722291f9ece37f581128b", "title": "Sketchpad III, a Computer Program for Drawing in Three Dimensions", "authors": ["Timothy E. Johnson}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Sketchpad III, a Computer Program for Drawing in Three Dimensions\" by Timothy E. Johnson", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "f08f699374a27cdbc2c1ecf050ae285b01bda723", "title": "A Truth Maintenance System", "authors": ["Jon Doyle"], "date": 1979, "abstract": "Abstract To choose their actions, reasoning programs must be able to make assumptions and subsequently revise their beliefs when discoveries contradict these assumptions. The Truth Maintenance System (TMS) is a problem solver subsystem for performing these functions by recording and maintaining the reasons for program beliefs. Such recorded reasons are useful in constructing explanations of program actions and in guiding the course of action of a problem solver. This paper describes (1) the representations and structure of the tms , (2) the mechanisms used to revise the current set of beliefs, (3) how dependency-directed backtracking changes the current set of assumptions, (4) techniques for summarizing explanations of beliefs, (5) how to organize problem solvers into \u201cdialectically arguing\u201d modules, (6) how to revise models of the belief systems of others, and (7) methods for embedding control structures in patterns of assumptions. We stress the need of problem solvers to choose between alternative systems of beliefs, and outline a mechanism by which a problem solver can employ rules guiding choices of what to believe, what to want, and what to do.", "references": ["68ff263250279e00572fc43a050fa348838f34a1", "f8ddb251bf94e4b055c6f520f21816e403c30e2a", "8c813f13be97f2c0d2114a5a6a05afdff744676c", "0f6c7ea83494f4c921758115a925ecf55ea5ec70", "d7d927cda381864aeaf4f4d2aa15d5ec05ffcdee", "da9fc17631b3b3a05ac98f3af39dfcff5e895823", "2255db1a8ada12287fb175f52805f4c5bac26873", "884c316ffab19b1ae8cfad412b0306512b8f5c0b", "d9c80173ba244764128f49e62476a9065f5a5404", "0cd96a1be99dd3c2c515a622d63378ed90b52253"], "page_rank": 5.473453749315818e-05}, {"id": "d61f2b03eeff55581e83a3d6572ceb2d1478aeba", "title": "Computer experiments in finite algebra", "authors": ["Ward Douglas Maurer"], "date": 1966, "abstract": "A medium-scale programming system is written in MAD and FAP on the IBM 7094 to manipulate some of the objects of modern algebra: finite groups, maps and sets of maps, subsets and sets of subsets, constant integers and truth-values. Designed to operate in a time-sharing environment, the system can serve as a teacher's aid to the undergraduate student of modern algebra, as well as for the working scientist or engineer wishing to familiarize himself with the subject.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "68f89c663b8dc91869640a11f7336f89b5e0f19c", "title": "Design and implementation of FLIP, a LISP format directed list processor", "authors": ["Warren Teitelman"], "date": 1967, "abstract": "Abstract : The paper discusses some of the considerations involved in designing and implementing a pattern matching or 'COMIT' feature inside of LISP. The programming language FLIP is presented here as a paradigm for such a feature. The design and implementation of FLIP discussed below emphasizes compact notation and efficiency of operation. In addition, FLIP is a modular language and can be readily extended and generalized to include features found in other pattern driven languages such as CONVERT and SNOBOL. This makes it extremely versatile. The development of this paper proceeds from abstract considerations to specific details. The syntax and semantics of FLIP are presented first, followed by a discussion of the implementation with especial attention devoted to techniques used for reducing the number of conses required as well as improving search strategy. Finally FLIP is treated as a working system and viewed from the user's standpoint. Here we present some of the additions and extensions to FLIP that have evolved out of almost two years of experimentation. These transform it from a notational system into a practical and useful programming system.", "references": ["555cf8b5e761a26bc14157946adfb8cf047c9df8", "58a84cbec5be4f2b1b6399473d9af085d837a027", "dc668c1c80a7bc533ad7b20ead1f737e953f79f9", "f687f588bf8c0063fb93b7217d098946f9d31b85", "ceed36b2ff0ad9efa12401d8639275b4606b287f", "15c5be51fd35dc5445e163ad5eb2540cfa80144d"], "page_rank": 7.037297677691766e-05}, {"id": "e91acb2dd6054614bc254376f2cdb464fe3ba073", "title": "The Future", "authors": ["Judith Scott Clayton"], "date": 1943, "abstract": "ALL this development, both on the transmitting and receiving side, has taken place well inside ten years, and is not solely due to the War, What will be the future of short-wave broadcasting ? Will people still go on listening and searching to see what they can find coming from distant countries ? There is not much doubt that for some time they will, since news must be foremost in everybody's mind for many years to come. The question remains what will happen when and if news becomes a matter of lees pressing importance to the whole world ? Is shortwave broadcasting capable of further development, purely as a means of recreation and enjoyment ? Whether the results obtainable by this means will-ever be equalled by an ordinary direct listener in his home would at first sight seem doubtful, but if the progress in the next ten years is anything like that in the last ten years, we may look forward to the day when reception from far-off countries is almost as good as from the local station, and a few years after that we may even see the addition of pictures.", "references": ["9eed17dd2ad338a2d4ccace774e4ac767284a2d7"], "page_rank": 0.0004926108374384236}, {"id": "a9952695e1c107f24a1f6f36b287139f2db00a90", "title": "On the Spatial Uses of Prepositions", "authors": ["Annette Herskovits"], "date": 1980, "abstract": "At first glance, the spatial uses of prepositions seem to constitute a good semantic domain for a computational approach. One expects such uses will refer more or less strictly to a closed, explicit, and precise chunk of world knowledge. Such an attitude is expressed in the following statement:\"Given descriptions of the shape of two objects, given their location (for example, by means or coordinates in some system of reference), and, in some cases, the location of an observer, one can select an appropriate preposition.\"This paper shows the fallacy of this claim. It addresses the problem of interpreting and generating \"locative predications\" (expressions made up of two noun-phrases governed by a preposition used spatially). It identifies and describes a number of object characteristics beyond shape (section 1) and contextual factors (section 2) which bear on these processes. Drawing on these descriptions, the third section proposes core meanings for two categories of prepositions, and describes some of the transformations these core meanings are subject to in context. The last section outlines the main directions of inquiry suggested by the examples and observations in the paper.", "references": ["fd1ff6b009f621f71d751c4c51dfa3c4f1f0a357", "8bdaba4632ed343863a4369b554908dca5540c1e", "a0af7809cb6a1fee05494b2c0c61dc560dfd0089"], "page_rank": 0.00022167487684729062}, {"id": "31bb1c092dba2b1a692b87cd2ff859bb7ce735f7", "title": "The mobile programming system: STAGE2", "authors": ["William M. Waite"], "date": 1970, "abstract": "STAGE2 is the second level of a bootstrap sequence which is easily implemented on any computer. It is a flexible, powerful macro processor designed specifically as a tool for constructing machine-independent software. In this paper the features provided by STAGE2 are summarized, and the implementation techniques which have made it possible to have STAGE2 running on a new machine with less than one man-week of effort are discussed. The approach has been successful on over 15 machines of widely varying characteristics.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "d836012048d706c484bbd9291f6c66d72c5cc508", "title": "PILOT: A STEP TOWARDS MAN-COMPUTER SYMBIOSIS", "authors": ["Warren Teitelman"], "date": 1966, "abstract": "PILOT is a programming system constructed in LISP. It is designed to facilitate the development of programs by easing the familiar sequence: write some code, run the program, make some changes, write some more code, run the program again, make some changes, write some more code, run the program again, etc. As a program becomes more complex, making these changes becomes harder and harder because the implications of changes are harder to anticipate. In the PILOT system, the computer plays an active role in this evolutionary process by providing the means whereby changes can be effected immediately, and in ways that seem natural to the user. The user of PILOT feels that he is giving advice, or making suggestions, to the computer about the operation of his programs, and that the system then performs the work necessary. The PILOT system is thus an interface between the user and his program, monitoring both the requests of the user and the operation of his program. The user may easily modify the PILOT system itself by giving it advice about its own operation. This allows him to develop his own language and to shift gradually onto PILOT the burden of performing routine but increasingly complicated tasks. In this way, he can concentrate on the menial tasks of editing rewriting, or adding to his programs. Two detailed examples are presented. PILOT is a first step toward computer systems that will help man to formulate problems in the same way they now help him to solve them. Experience with it supports the claim that such \"symbiotic systems\" allow the programmer to attack and solve more difficult problems.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "e8ff7d065cb2eed432348c8143bbfd02245246ae", "title": "Definite reference and mutual knowledge", "authors": ["Herbert H. Clark", "Charles R. Marshall"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"Definite reference and mutual knowledge\" by Herbert H. Clark et al.", "references": [], "page_rank": 0.0001231527093596059}, {"id": "382c8aee17ed963bae6c3b5f4f2047773093b261", "title": "Semantic Information Processing", "authors": ["Marvin Minsky"], "date": 1968, "abstract": "Most of the chapters are slightly edited Ph.D thesis directed toward making intelligent machines. Each solves different problems like resolving ambiguities in word meanings, finding analogies between things, making logical and nonlogical inference, resolving inconsistency in information engaging in coherent discourse with a person and more", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "f3be4120894ba32a8c75d503d6c6687d93277c55", "title": "Decomposition of a visual scene into three-dimensional bodies", "authors": ["Adolfo Guzm{\\'a}n"], "date": 1968, "abstract": "We consider visual scenes composed by the optical image of a group of bodies. When such a scene is \"seen\" by a computer through a film spot scanner, image dissector, or similar device, it can be treated as a two-dimensional array of numbers, or as a function of two variables.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a9388c1101f83f4a882aa6bf7aa96ae9cab1900e", "title": "A Heuristic Program that Solves Symbolic Integration Problems in Freshman Calculus", "authors": ["James R. Slagle"], "date": 1963, "abstract": "A large high-speed general-purpose digital computer (IBM 7090) was programmed to solve elementary symbolic integration problems at approximately the level of a good college freshman. The program is called SAINT, an acronym for \"Symbolic Automatic INTegrator.\" This paper discusses the SAINT program and its performance. SAINT performs indefinite integration. I t also performs definite and multiple integration when these are trivial extensions of indefinite integration. I t uses many of the methods and heuristics of students attacking the same problems. SAINT took an average of two minutes each to solve 52 of the 54 attempted problems taken from the Massachusetts Institute of Technology freshman calculus final examinations. Based on this and other experiments with SAINT, some conclusions concerning computer solution of such problems are: (1) Pattern recognition is of fundamental importance. (2) Great benefit would have been derived from a larger memory and more convenient symbol manipulating facilities. (3) The solution of a symbolic integration problem by a commercially available computer is far cheaper and faster than by man.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "eb35108a8903c75b96c426b420144353f7f16c7a", "title": "Form and Content in Computer Science (1970 ACM turing lecture)", "authors": ["Marvin Minsky"], "date": 1970, "abstract": "The trouble with computer science today is an obsessive concern with form instead of content. No, that is the wrong way to begin. By any previous standard the vitality of computer science is enormous; what other intellectual area ever advanced so far in twenty years? Besides, the theory of computation perhaps encloses, in some way, the science of form, so that the concern is not so badly misplaced. Still, I will argue that an excessive preoccupation with formalism is impeding our development. Before entering the discussion proper, I want to record the satisfaction my colleagues, students, and I derive from this Turing award. The cluster of questions, once philosophical but now scientific, surrounding the understanding of intelligence was of paramount concern to Alan Turing, and he along with a few other thinkers--notably Warren S. McCulloch and his young associate, Walter P i t t s made many of the early analyses tha t led both to the computer itself and to the new technology of artificial intelligence. In recognizing this area, this award should focus attention on other work of my own scientific family--especially Ray Solomonoff, Oliver Selfridge, John McCarthy, Allen Newell, Herbert Simon, and Seymour Papert, my closest associates in a decade of work. Papert 's views pervade this essay. This essay has three parts, suggesting form-content confusion in theory of computation, in programming languages, and in education.", "references": ["96bbe96f8586e16a9f55f9dee37139fd3b793b87", "ef2aa11a9e5dac4577a90b65978f894fa3f4b193"], "page_rank": 0.0002463054187192118}, {"id": "bea20de7b908ee518194e7247ce00f098991883b", "title": "Translations from the Philosophical Writings of Gottlob Frege", "authors": ["James S. Albertson"], "date": 1953, "abstract": "Semantic Scholar extracted view of \"Translations from the Philosophical Writings of Gottlob Frege\" by James S. Albertson", "references": [], "page_rank": 0.0002463054187192118}, {"id": "e3c63e00a1bc1042406ef08775b6482ccda0fa1b", "title": "COMPUTER ANALYSIS OF VISUAL PROPERTIES OF CURVED OBJECTS", "authors": ["Lawrence J. Krakauer"], "date": 1971, "abstract": "A method is presented for the visual analysis of objects by computer. It is particularly well suited for opaque objects with smoothly curved surfaces. The method extracts information about the object''s surface properties, including measures of its specularity, texture, and regularity. It also aids in determining the object''s shape. The application of this method to a simple recognition task, (the recognition of fruit) is discussed. The results on a more complex smoothly curved object, a human face, are also considered.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c2e41b4445a5988f59136c62fc061958c088e731", "title": "The Use of Vision and Manipulation to Solve the \"Instant Insanity\" Puzzle", "authors": ["Jerome A. Feldman", "Karl K. Pingle", "Thomas O. Binford", "Gilbert Falk", "Alan C. Kay", "R. Paul", "Robert F. Sproull", "Jay M. Tenenbaum"], "date": 1971, "abstract": "This paper describes a system which solves the puzzle \"Instant Insanity\". The puzzle consists of four multicolored cubes. The solution involves arranging the cubes in a tower so that no side of the tower reveals more than one face of a given color. Our system, which runs as eight (multitask) jobs under the PDP-10 time-sharing system, uses a TV camera to locate four objects and, having verified that they are cubes, to find the color of each face. A mechanical arm turns the cubes over to expose all faces to the TV. Having found the solution, the arm then stacks the cubes into a tower to demonstrate it.", "references": ["18836cbf140d542022ec5f04805fa47d9d0772f9", "c3576b28daab769c8ece29fed47e6f95f83f4777", "9957429e56e0ac0a906b769476c0518732051261", "70e69265cb050a5050e75ccb308e00b6d9571ab0", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "ae12520c730c4588f79f1751f04b9675fd10e3d9", "10eae4058bd6a68600afc4a068e32eaf741f1e74", "e979c7bf95e924f71ed6697da3cb5bac3ea926b7"], "page_rank": 0.0002134646962233169}, {"id": "236dcee7c12e0a19ed200a02e95e55c561266347", "title": "An information-processing explanation of some perceptual phenomena.", "authors": ["Herbert A. Simon"], "date": 1967, "abstract": "An information-processing system that scans stimuli serially, part-by-part, and attempts \u2018simple\u2019 interpretations of the parts would experience a number of the well-known perceptual illusions that human subjects report. The hypothesized system has the same basic characteristics as systems which have been used to explain a wide range of cognitive phenomena. The description of the system is proposed as an explanation of some of the mechanisms for human perceptual processing.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "36421f1bcc37d6b46c8a4d80c588569decfce59e", "title": "Computer recognition of connected speech.", "authors": ["D. Raj Reddy"], "date": 1967, "abstract": "A system for obtaining a phonemic transcription from a connected speech sample entered into the computer by a microphone and an analog\u2010to\u2010digital converter is described. A feature\u2010extraction program divides the speech utterance into segments approximately corresponding to phonemes, determines pitch periods of those segments where pitch analysis is appropriate, and computes a list of parameters for each segment. A classification program assigns a phoneme\u2010group label (vowellike segment, fricativelike segment, etc.) to each segment, determines whether a segment should be classified as a phoneme or whether it represents a phoneme boundary between two phonemes, and then assigns phoneme label to each segment that is not rejected as being a phoneme boundary. About 30 utterances of 1\u20132 sec duration were analyzed using the above programs on an interconnected IBM 7090\u2010PDP1 system. Correct identification of many vowel and consonantal phonemes was achieved for a single speaker using the same speech material that was ...", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0930f4d5637eb6f25e922f08ae09e03847a9404d", "title": "A new architecture for mini-computers: the DEC PDP-11", "authors": ["Gordon Bell", "R. Cady", "H. McFarland", "Bruce Delagi", "J. O'Laughlin", "R. Noonan", "William A. Wulf"], "date": 1970, "abstract": "The mini-computer has a wide variety of uses: communications controller; instrument controller; large-system pre-processor; real-time data acquisition systems...; desk calculator. Historically, Digital Equipment Corporation's PDP-8 Family, with 6,000 installations has been the archetype of these minicomputers.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "cf4570f4801181a2f8e1b9aac77c180d0206834e", "title": "The structure of the \u201cTHE\u201d-multiprogramming system", "authors": ["Edsger W. Dijkstra"], "date": 1968, "abstract": "A multiprogramming system is described in which all activities are divided over a number of sequential processes. These sequential processes are placed at various hierarchical levels, in each of which one or more independent abstractions have been implemented. The hierarchical structure proved to be vital for the verification of the logical soundness of the design and the correctness of its implementation.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5198b80c4037a9ff9f84e6081c5567888b1ac0fb", "title": "Microprogramming: principles and practices", "authors": ["S. S. Husson", "P. Schneider"], "date": 1971, "abstract": "details how one can use a four-state cell to store logic which has been ex'and-not' and 'exclusive-or' functions as well as the standard sum of products ity to recognize these additional functions further reduces the size of required Th i s reduction is illustrated in detail using both Karnaugh maps and Roolean examples including increment and parity are also included. one typographical error in section 1.3. The number of words required for the 2-state associative memory and an operand width of 'n' bits should be", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a", "title": "A Mobile Automaton: An Application of Artificial Intelligence Techniques", "authors": ["Nils J. Nilsson"], "date": 1969, "abstract": "A research pro ject applying a r t i f i c i a l i n t e l l igence techniques to the development of i n t e grated robot systems Is descr ibed. The experiment a l f a c i l i t y consists of an SDS-940 computer and associated programs c o n t r o l l i n g a wheeled vehic le that ca r r i es a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the veh ic le , fo r s to r i ng relevant in format ion about the environment, and for planning the sequence of motor act ions necessary to accomplish tasks in the environment. A t yp i ca l task performed by our present system requires the robot vehic le to r e arrange (by pushing) simple objects in i t s env i ronment .", "references": [], "page_rank": 0.00011083743842364531}, {"id": "5ee22f744331bf35bc45c3538000790e9483b967", "title": "Properties of a model for parallel computations: determinacy", "authors": ["Richard M. Karp", "Raymond E. Miller"], "date": 1966, "abstract": "This paper gives a graph-theoretic model for the description and analysis of parallel computations. Within the model, computation steps correspond to nodes of a graph, and dependency between computation steps is represented by branches with which queues of data are associated. First, it is shown that each such computation graphGrepresents a unique computation, determined independently of operation times. Next, methods of determining whether such a computation terminates and of finding the number of performances of each computation step are developed. The maximal strongly connected subgraphs of G and the loops within these subgraphs play aooutnal role in this analysis. For example, use is made of the result that either every computation step within a strongly connected snbgroph of G is performed an infinite number of times, or none is. Finally, necessary and sufficient conditions for the lengths of data queues to remain bounded are derived.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "456ac66abf2d97b6cf8ebacd85107f69e7a97c08", "title": "A multi-level computer organization designed to separate data-accessing from the computation", "authors": ["Victor R. Lesser"], "date": 1968, "abstract": "The computer organization to be described in this paper has been developed to overcome the inflexibility of computers designed around a few fixed data structures, and only binary operations. This has been accomplished by separating the data-accessing procedures from the computational algorithm. By this separation, a new and different language may be used to express data-accessing procedures. The new language has been designed to allow the programmer to define the procedures for generating the names of the operands for each computation, and locating the value of an operand given its name.", "references": ["cb67fe0cd45ad16001206773ffde93140f80a1a2", "be501e26e4d0037a7e78879d2b1c12524d9c7fd3", "115b5c9c97d75c93c827a167fb2bf4fcf7f0f96c", "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "c597c14d8428cbbecab42d3c385064f1174937da", "7f9154f6ab93a935acccb6b3fdf83d011e44d175", "f06bb30589a0554b19f1544af0e018b50b974bf7"], "page_rank": 0.00011083743842364531}, {"id": "fceac668a164d18128e8f1e06965b3c7ad5203fd", "title": "C.ai: A Computing Environment for AI Research. Overview, PMS and Operating System Considerations", "authors": ["Gregory Bruce Bell", "Peter Freeman", "Mario Barbacci", "Subhash Bhatia", "W. Broadley"], "date": 1971, "abstract": "Abstract : A computer for artificial intelligence research is examined. The design is based on a large, straightforward primary memory facility (about 8 million 74 bit words). Access to the memory is via at least 16 ports which are hardware protected; there is dynamic assignment of the memory to the ports. The maximum port bandwidth is 8,600 million bits/sec. Processors for languages (e. g., LISP) and specialized terminals (e.g., video input/output) can be reliably connected to the system during its operation. The approach is evolutionary in that high performance processors, such as the Standford AI Processor, can be connected to the memory structure, giving an overall power of at least 100 times a PDP-10 (and 200 to 300 times a PDP-10 for list processing languages) for 10 processors -- although 20 processors can be attached.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "21678be84430f56942cf5172c281b1861b9ac7a0", "title": "Programming semantics for multiprogrammed computations", "authors": ["Jack B. Dennis", "Earl C. Van Horn"], "date": 1966, "abstract": "The semantics are defined for a number of meta-instructions which perform operations essential to the writing of programs in multiprogrammed computer systems. These meta-instructions relate to parallel processing, protecting of separate computations, program debugging, and the sharing among users of memory segments and other computing objects, the names of which are hierarchically structured. The language sophistication contemplated is midway between an assembly language and an advanced algebraic language.", "references": ["3171063dc13f6ec66e70b6418e3e353e811ae912", "604c4f34b7d71703ac0b67d0d2f8ee4fbc328c71"], "page_rank": 4.926108374384236e-05}, {"id": "96e741a8139bb6f06fa9903f830d5d4f23029487", "title": "DIRECT EMULATION OF CONTROL STRUCTURES BY A PARALLEL MICRO-COMPUTER.", "authors": ["Victor R. Lesser"], "date": 1970, "abstract": "This paper is a preliminary investigation of the organization of a parallel micro-computer designed to emulate a wide variety of sequential and parallel computers. This micro-computer allows tailoring of the control structure of an emulator so that it directly emulates (mirrors) the control structure of the computer to be emulated. An emulated control structure is implemented through a tree type data structure which is dynamically generated and manipulated by six primitive (built-in) operators D This data structure for control is used as a syntactic framework within which particular implementations of control concepts, such as iteration, recursion, co-routines, parallelism, interrupts, etc., can be easily expressed. The major features of the control data structure and the primitive operators are: 1) once the fixed control and data linkages among processes have been defined, they need not be rebuilt on subsequent executions of the control structure; 2) micro-programs may be written so that they execute independently of the number of physical processors present and still take advantage of available processors; 3) control structures for I/O processes, dataaccessing processes, and computational processes are expressed in a single uniform framework. This method of emulating control structures is in sharp contrast with the usual method of micro-programming control structures which handles control instructions in the same manner as other types of instructions, e.g., subroutines of micro-instructions, and provides a unifying method for the efficient emulation of a wide variety of sequential and parallel computers.", "references": ["456ac66abf2d97b6cf8ebacd85107f69e7a97c08", "f1ebc2e485e806113a46d3c4b5a98ecf51815713", "e46faa52ff86efb77b35497699d0910fed41dcde", "0519d8bf92fbb5424ee8b2655d6d0262c1f82ed3", "8854e412a9367a76deb2168407bb3aa065009abd", "de813b713e5bc2af02ace1af51f9922c7f0c0bcf", "fb28000e5d5a504beb4007aa22e664a3d803c52e", "ac84a075cfc1fa64979526d81ec2ac4442155f69"], "page_rank": 4.926108374384236e-05}, {"id": "b44dbeb6e8dba0558c4c02b2786897d771d33d8f", "title": "Control Mechanisms for Parallelism in Programs.", "authors": ["Harvey W. Bingham", "Earl W. Reigel", "David A. Fisher"], "date": 1968, "abstract": "Abstract : Relations between routines in procedure oriented languages are investigated. In parallel or multiprocessing machines, these relations are associated with the path of control rather than with the routines themselves. Subroutines, coroutines, and parallel routines are defined in terms of the primitive operations: create, suspend, resume, delete. Two systems organizations capable of controlling parallel processing are described. Both organizations permit automatic exploitation of parallelism within expressions. One organization uses compile time detected parallelism between statements, whereas the other provides this detection at run time. Macroparallelism among routines and for repeat statements is also developed. Block oriented random access memory (BORAM) is considered as a way to provide information bandwidth and accessibility sufficient for demands of parallel processing. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e77c7042063d7439927b605b449c1d642d552f6c", "title": "Response strength of single words as an influence in sentence behavior", "authors": ["Joan L. Prentice"], "date": 1966, "abstract": "Active or passive sentences were learned in response to nouns. In one set of either construction the first noun was elicited as primary word associate to a stimulus noun, and in re-paired sets the second noun was so elicited. Comparison groups learned the same sentences in response to unrelated stimulus nouns. Results indicated that sentences beginning with high response strength (RS) nouns are easier to learn than sentences ending with high RS nouns. By inference, word order in natural speech is a function of RS, where verbal operants at high RS tend to be emitted early. Error analysis provided weak evidence for change in word order attributable to RS. The active construction appeared easier to learn than the passive construction, and syntactic errors tended to be in the direction of the simpler construction.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ce47461b286a333b2131743370fcf05f9ea067e7", "title": "Sytatic position and rated meaning", "authors": ["Michael G. Johnson"], "date": 1967, "abstract": "Does the position which a word occupies in a sentence affect the meaning of that word? Nonsense syllables were used in the subject and object positions of simple active and passive sentences. The referential meaning of these sentences was controlled. Semantic Differential ratings of the nonsense syllables showed that active subjects are more active and potent (animate) than active objects. Passive subjects were rated more animate than passive objects, but ratings for the passive positions were more neutral than those for active positions. The variability of the ratings showed consistent differences, according to the position rated. It was concluded that these differences in ratings represent structural meanings. The results are interpreted as reflecting the distributional properties of English.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "af5fb38672c7ff11d7ae28db64e5c01c2287db52", "title": "Focus of attention in recall of active and passive sentences", "authors": ["Elizabeth Ann Turner", "Ragnar Rommetveit"], "date": 1968, "abstract": "An attempt was made to manipulate the voice in which sentences were remembered by varying the focus of the child's attention both at the time of sentence storage and at the time of sentence retrieval. The storage and retrieval pictures were pictures of the actor element, the acted-upon element, and a picture of the total sentence content. The pictures, both those presented at storage and those presented at retrieval, tended to be effective in manipulating the voice of the sentence recalled. The retrieval-picture effect, however, tended to be stronger than the storage-picture effect. When the picture on which the child focused his attention was congruent with the subject of the original stimulus sentence, correct recall was facilitated; on the other hand, when the picture on which the child focused was incongruent with the sentence subject (but congruent with the object), sentences tended to be transformed into the opposite voice in recall.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "166c9f94ba1ffd1eb27444608cfe78fce77366e6", "title": "The choice of the passive voice in a communicative task.", "authors": ["Philip N. Johnson-Laird"], "date": 1968, "abstract": "Subjects had to rank-order four logically equivalent but syntactically different sentences in terms of their appropriateness as descriptions of one diagram in contrast to another. A prediction of this rank order was derived on the assumption that subjects would attempt to emphasize the larger areas of colour in the stimuli, and that passive sentences emphasize the logical object to a greater extent than active sentences. Four groups were run in a 2 times 2 design. One variable was whether the stimulus to be described was symmetrically or asymmetrically divided into two colours. When one type of stimulus was to be described the other type served as a contrast. The other variable was whether the larger area of the asymmetrical stimulus was denoted by the logical subject or the logical object of the sentences. These two variables gave rise to a subsidiary prediction concerning the degree of correlation between the predicted rank orders and the actual performance of the groups. Both predictions were confirmed.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "acc68f38ab9c5a636d764c95bd8e92ce67516063", "title": "Notes on transitivity and theme in english", "authors": ["Michael Halliday"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Notes on transitivity and theme in english\" by Michael Halliday", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "da30054297eaee23a57271f7f9c4d7655841c537", "title": "Some structural properties of simple active and passive sentences", "authors": ["Herbert H. Clark"], "date": 1965, "abstract": "The present experiment studied the diversity and covariation of the words used by S s as the actor, verb, and object of simple active and passive sentences. Two groups of 60 S s were given sentence-frames and asked to generate sentences. Informational uncertainty of the words S s used in these sentence-frames was used as a measure of diversity. The results were: (a) In the active sentence, the actor had much less uncertainty than the verb and object. The actor varied relatively independently of the verb and object, which covaried more as a unit. (b) In the passive sentence, the object (the first sentence part and traditional \u201csubject\u201d), verb, and actor did not show noticeably different uncertainties. The object and verb constrained each other more than the actor constrained either of them; however, the actor was constrained considerably by the verb. (c) The S s did not treat the passive sentence-frame simply as a transformed active sentence-frame, since the pattern of uncertainty in the actor, verb, and object and the use of animate nouns as actors and objects differed consistently in the two grammatical forms. These results present negative evidence for the notion that S s generate passive sentences simply by imposing a transformation on active sentences, as Miller's (1962) transformational model would imply; instead, the results argue for a sequential left-to-right generation of sentences.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0cd96a1be99dd3c2c515a622d63378ed90b52253", "title": "A system for incrementally designing and verifying programs.", "authors": ["Mark Moriconi"], "date": 1977, "abstract": "Abstract : SID (System for Incremental Development) is a computer system for incrementally designing and verifying large, complex programs. It executes commands, proposes actions, answers questions, and accepts and reasons about new or changed information. SID has three main, distinctive characteristics: (1) it provides several useful incremental capabilities, including the ability to respond to changes by ensuring that the final problem solution is consistent and by keeping intact still-valid work without complete reprocessing; (2) its user interface has the ability to guide the user through the design and verification and to engage in an interactive English dialog about the potential effects of changes; (3) it supports a substantial programming language which includes features for generating run-time checks, stating concurrent processes and shared data, and developing data abstractions. SID has been used to completely design and verify several programs. Volume 2 (appendix) contains a transcript of a session with SID in which a simple message switching network that allows secure, asynchronous message transfer among a fixed number of users in incrementally developed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "22ba823c095224796a06631634350f6694e8f6d2", "title": "Cognitive Psychology: Classic Edition", "authors": ["Ulric Neisser"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Cognitive Psychology: Classic Edition\" by Ulric Neisser", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "15c5be51fd35dc5445e163ad5eb2540cfa80144d", "title": "METEOR, A LISP Interpreter for String Transformations", "authors": ["Daniel G. Bobrow"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"METEOR, A LISP Interpreter for String Transformations\" by Daniel G. Bobrow", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "9193f366e993ed27c2605eceaa18f13b1439035e", "title": "An Introduction to the Direct Emulation of Control Structures by a Parallel Microcomputer", "authors": ["Victor R. Lesser"], "date": 1971, "abstract": "This paper is an investigation of the organization of a parallel microcomputer designed to emulate a wide variety of sequential and parallel computers. This microcomputer allows tailoring of its control structure so that it is appropriate for the particular computer to be emulated. The control structure of this microcomputer is dynamically modified by changing the organization of its data structure for control. The microcomputer contains six primitive operators that dynamically manipulate and generate a tree-type data structure for control. This data structure for control is used as a syntactic framework within which particular implementations of control concepts, such as iteration, recursion, co-routines, parallelism, interrupts, etc., can be easily expressed. The major features of the control data structure and the primitive operators are: 1) once the fixed control and data linkages among microprocesses have been defined, they need not be rebuilt on subsequent executions of the control structure; 2) microprograms may be written so that they execute independently of the number of physical processors present and still take advantage of available processors; 3) control structures for I/O processes, data-accessing processes, and computational processes are expressed in a single uniform framework. An emulator programmed on this microcomputer works as an iterative two-step process similar to the process of dynamic compilation or run time macroexpansion. This data structure approach to emulation differs considerably from the conventional approach to emulation and provides a unifying approach to the emulation of a wide variety of sequential and parallel computers.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ceed36b2ff0ad9efa12401d8639275b4606b287f", "title": "Department of Commerce. for Sale to the General Public", "authors": ["Moflltof Contract", "Voelker Thomas", "Griffiths", "Lab0\u00abat0llin", "R Nitored", "I- r Swanson", "Ivey Tl}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"Department of Commerce. for Sale to the General Public\" by Moflltof Contract et al.", "references": ["3a4a561b6fe2888fdf7263b603fbeda1c0336328", "fb00017e3e070016b7bf9651a7241117e4708dad", "ca4d6ae23b5ed54faa7516d2b27946c7abb6c5ab", "4d478cf13be6e2e810ba51a293825082a6f33da6", "6be47b5305379c52961c15d80781f55849d9f758", "bf6fa65a1c0a9e780cd2ac615b41f9ef7785526a", "7b7bb5c2b9ca333681418bc3f9a99c9bd2a60f07"], "page_rank": 8.210180623973726e-05}, {"id": "8bdaba4632ed343863a4369b554908dca5540c1e", "title": "How near is near: a near specialist", "authors": ["M. E. Denofsky"], "date": 1976, "abstract": "Semantic Scholar extracted view of \"How near is near: a near specialist\" by M. E. Denofsky", "references": [], "page_rank": 0.00016420361247947453}, {"id": "fd1ff6b009f621f71d751c4c51dfa3c4f1f0a357", "title": "Computational interpretation of english spatial prepositions.", "authors": ["Lois C. Boggess"], "date": 1978, "abstract": "Abstract : It seems clear to anyone who pays attention to the use of prepositions in language that any one preposition, when used to describe the spatial relationship between different objects can produce strikingly different mental models for different objects. The mental model produced by the description 'a bowl on a table' seems to be somewhat different from that produced by 'a poster on a wall' which in turn is somewhat different from 'a shelf on a wall' which again is different from 'a fly on a ceiling'. It is the contention of this paper that the preposition in conjunction with a small set of features of the objects (mostly perceptual features) can account for such variations in spatial relations. The thesis discusses a means of taking English-language descriptions involving prepositions and their semantic subjects and objects and deriving a three-dimensional model of the spatial relationships of the subject and object. The program takes extended descriptions involving many objects each of which is incorporated into the overall model. Once an object has been described, it is possible to interrogate the model about the relation of the object to any other in the model, without recourse to inference rules of the following kind: 'if A is on B and B is in C then A is (probably) in C'.", "references": [], "page_rank": 0.0002627257799671592}, {"id": "58a84cbec5be4f2b1b6399473d9af085d837a027", "title": "List Processing and Extension of Language Facility by Embedding", "authors": ["Daniel G. Bobrow", "Joseph Weizenbaum"], "date": 1964, "abstract": "There are two distinct (though not necessarily disjoint) types of language extension. The first is a syntactic extension, in which the class of expressions interpretable within a language is widened. The second is an extension of the functional domain in which operations not previously performable are added to the language. These extensions may be implemented by completely reprogramming the basic language processor (the compiler or interpreter), but such extensions may often be achieved at far less cost by embedding. The authors give an example of each of these two types of extensions by embedding, adding list-processing facilities to the FORTRAN language, and extending the list processing language LISP to facilitate expression and interpretation of string transformations. As an aid to those unfamiliar with list processing, there is a section which reviews this subject. Included are descriptions and explanations of list structures, the various notations used for lists, the basic processes of list processing, and the different notations (machine language, functional, and prototype) used for expressing processing of lists.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a0af7809cb6a1fee05494b2c0c61dc560dfd0089", "title": "Logic and conversation", "authors": ["H. Paul Grice"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"Logic and conversation\" by H. Paul Grice", "references": [], "page_rank": 0.0004105090311986863}, {"id": "9eed17dd2ad338a2d4ccace774e4ac767284a2d7", "title": "The United States Department of Agriculture", "authors": ["Philip S. Derfler"], "date": 1940, "abstract": "THE absence of a Department of Agriculture from the complicated scheme of British Government offices leads us to inquire whether it is possible for such a Department in the United States to publish annually eleven or twelve hundred pages of matter useful to the agricultural community, and whether those publications have any considerable circulation in the country.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "555cf8b5e761a26bc14157946adfb8cf047c9df8", "title": "Format-directed list processing in LISP", "authors": ["Daniel G. Bobrow", "Warren Teitelman"], "date": 1966, "abstract": "This article describes a notation and a programming language for expressing, from within a LISP system, string transformations such as those performed in COMIT or SNOBOL. A simple transformation (or transformation rule) is specified by providing a pattern which must match the structure to be transformed and a format which specifies how to construct a new structure according to the segmentation specified by the pattern. The patterns and formats are greatly generalized versions of the left-half and right-half rules of COMIT and SNOBOL. For example, elementary patterns and formats can be variable names, results of computations, disjunctive sets, or repeating subpatterns; predicates can be associated with elementary patterns which check relationships among separated elements of the match; it is no longer necessary to restrict the operations to linear strings since elementary patterns can themselves match structures. The FLIP language has been implemented in LISP 1.5, and has been successfully used in such disparate tasks as editing LISP functions and parsing Kleene regular expressions.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f687f588bf8c0063fb93b7217d098946f9d31b85", "title": "Structure of a LISP system using two-level storage", "authors": ["Daniel G. Bobrow", "Daniel L. Murphy"], "date": 1967, "abstract": "In an ideal list-processing system there would be enough core memory to contain all the data and programs. Described in this paper are a number of techniques that have been used to build a LISP system utilizing a drum for its principal storage medium, with a surprisingly low time penalty for use of this slow storage device. The techniques include careful segmentation of system programs, allocation of virtual memory to allow address arithmetic for type determination, and a special algorithm for building reasonably linearized lists. A scheme for binding variables is described which is good in this environment and allows for complete compatibility between compiled and interpreted programs with no special declarations.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "ef2aa11a9e5dac4577a90b65978f894fa3f4b193", "title": "A Universal Turing Machine with Two Internal States", "authors": ["Claude E. Shannon"], "date": 1956, "abstract": "Semantic Scholar extracted view of \"A Universal Turing Machine with Two Internal States\" by Claude E. Shannon", "references": [], "page_rank": 0.0002463054187192118}, {"id": "d9c80173ba244764128f49e62476a9065f5a5404", "title": "Dependency Directed Reasoning for Complex Program Understanding", "authors": ["Howard E. Shrobe"], "date": 1979, "abstract": "Abstract : Artificial Intelligence research involves the creation of extremely complex programs which must possess the capability to introspect, learn, and improve their expertise. Any truly intelligent program must be able to create procedures and to modify them as it gathers information from its experience. A crucial stepping stone in AI research is the development of a system which can understand complex programs well enough to modify them. The Programmer's Apprentice Project is attempting to develop an interactive programming tool which will help expert programmers deal with the complexity involved in engineering a large software system. This report describes REASON, the deductive component of the programmer's apprentice. REASON is intended to help expert programmers in the process of evolutionary program design. REASON utilizes the engineering techniques of modelling, decomposition, and analysis by inspection to determine how modules interact to achieve the desired overall behavior of a program. REASON coordinates its various sources of knowledge by using a dependency-directed structure which records the justification for each deduction it makes. Once a program has been analyzed these justifications can be summarized into a teleological structure called a plan which helps the system understand the impact of a proposed program modification.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "10eae4058bd6a68600afc4a068e32eaf741f1e74", "title": "Computer control of a mechanical arm through visual input", "authors": ["Karl K. Pingle", "Jonathan A. Singer", "William M. Wichman"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"Computer control of a mechanical arm through visual input\" by Karl K. Pingle et al.", "references": [], "page_rank": 0.00030788177339901473}, {"id": "e979c7bf95e924f71ed6697da3cb5bac3ea926b7", "title": "Machine Perception of 3-Dimensional Solids", "authors": ["Lawrence G. Roberts"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Machine Perception of 3-Dimensional Solids\" by Lawrence G. Roberts", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ae12520c730c4588f79f1751f04b9675fd10e3d9", "title": "A Laboratory for Hand-Eye Research", "authors": ["Jay M. Tenenbaum", "Alan C. Kay", "Thomas O. Binford", "Gilbert Falk", "Jerome A. Feldman", "G. Grape", "R. Paul", "Karl K. Pingle", "Irwin Sobel", "Robert F. Sproull"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"A Laboratory for Hand-Eye Research\" by Jay M. Tenenbaum et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "9957429e56e0ac0a906b769476c0518732051261", "title": "An Accommodating Edge Follower", "authors": ["Karl K. Pingle", "Jay M. Tenenbaum"], "date": 1971, "abstract": "We describe an operational program that locates objects in a television image and traces their edges. The program accommodates the television camera, maximizing dynamic range during acquisition and sensitivity during tracing, to obtain the most appropriate image for each phase. If the trace routine loses an edge, various heuristics diagnose the difficulty and tune both the camera and software to recover contrast at the point of difficulty. Experimental evidence of the effectiveness of accommodation is provided.", "references": ["0357413d2005fc1da140a3332f0a353354d4b99f", "10eae4058bd6a68600afc4a068e32eaf741f1e74"], "page_rank": 6.157635467980295e-05}, {"id": "884c316ffab19b1ae8cfad412b0306512b8f5c0b", "title": "Plan Recognition Using A Hypothesize and Revise Paradigm", "authors": ["Charles F. Schmidt", "N. S. Sridharan"], "date": 1977, "abstract": "Problem-Solving: Schmidt linearly ordered and unbounded in time. Plans are bounded, hierarchical, and nonlinearly ordered based on the logical connection between an enduring outcome of one action and the precondition of some subsequent action. Further, a plan is well-formed with reference to the planner's beliefs about the world rather than with respect to the actual state of the world. For these and similar reasons, we have argued that any process of plan recognition must use meta-knowledge about plans and the psychological constraints which define a well-formed plan. Knowledge of only the physical constraints on action does not provide sufficient information to recognize plans [8]. In this paper we present, by means of an example, those aspects of a plan recognition process that have been implemented in BELIEVER. The properties of this process have been motivated primarily by our desire to incorporate within this process certain psychological assumptions about characteristics of the human plan recognition process. The two major assumptions are that the human process is: (1) a general non-specialized process; and (2) is based on a hypothesize and revise strategy. The remainder of this paper is concerned with conveying the meaning of these labels. However, it will be useful to first explicate these ideas in an informal way. Because of interest in application or for methodological reasons, much of the recent work in AI has been concerned wxth the representation of expert knowledge within a relatively narrow domain. Chess, restaurants, flush toilets and particular electronic circuits are well-known examples of this approach. An expert's knowledge is, by definition, highly specialized and customized to a particular domain. This has led researchers to encode this knowledge in similarly specialized and customized forms which are often generically referred to as scripts [3). The use of script-like knowledge is an important aspect of the human information processing capability. A great deal of research has focused on the problem of how to generate a plan that satifies a given goal [2]. The problem of plan recognition is to take as input the sequence of actions performed by an actor and to identify the goal pursued by the actor and also to organize the action sequence in terms of a plan structure. This plan structure explicitly describes the goal-subgoal relations among its component units. Our concern with plan recognition has arisen from our research [4,5,6,7] in the development of a theory of how persons understand the \u2026", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c3576b28daab769c8ece29fed47e6f95f83f4777", "title": "A computer with hands, eyes, and ears", "authors": ["J. Michael McCarthy", "L. D. Earnest", "Raj Reddy", "Pierre J. Vicens"], "date": 1968, "abstract": "The anthropomorphic terms of the title may suggest an interest in machines that look or act like men. To this extent it is misleading. Our interest is in extending the range of tasks to which machines can be applied to include those that, when performed by a human, require coordination between perceptual and motor processes. We attempt to suppress the egocentric idea that man performs these tasks in the best of all possible ways.", "references": [], "page_rank": 0.0001319493314567206}, {"id": "3171063dc13f6ec66e70b6418e3e353e811ae912", "title": "A design for a multiple user multiprocessing system", "authors": ["James D. McCullough", "Kermith H. Speierman", "F. W. Zurcher"], "date": 1965, "abstract": "The B8500 system is designed to deal with the following situation. A large number of active programs requiring various services are present in the system and their current status and required service are recorded. When some component of the system becomes available, e.g., processor, memory space, peripheral device, it is assigned to the active job of highest priority that requires this service. The important concept is that no component of the system belongs to any program but rather provides a service and then goes on to service another program. The main function of the executive scheduling program is to keep track of the services required by programs and to schedule the services when equipment becomes available.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "96bbe96f8586e16a9f55f9dee37139fd3b793b87", "title": "The development of the space-time view of quantum electrodynamics", "authors": ["Richard Phillips Feynman"], "date": 1966, "abstract": "We have a habit in writing articles published in scientific journals to make the work as finished as possible, to cover up all the tracks, to not worry about the blind alleys or to describe how you had the wrong idea first, and so on. So there isn't any place to \npublish, in a dignified manner, what you actually did in order to get to do the work, although there has been, in these days, some interest in this kind of thing. Since winning the prize is a personal thing, I thought I could be excused in this particular situation if I were to talk personally about my relationship to quantum electrodynamics, rather than to discuss the subject itself in a refined and finished fashion. Furthermore, since there are three people who have won \nthe prize in physics, if they are all going to be talking about quantum electrodynamics itself, one might become bored with the subject. So, what I would like to tell you about today are \nthe sequence of events, really the sequence of ideas, which occurred, and by which I finally came out the other end with an unsolved problem for \nwhich I ultimately received a prize.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "da9fc17631b3b3a05ac98f3af39dfcff5e895823", "title": "Deductive Retrieval Mechanisms for State Description Models", "authors": ["Richard Fikes"], "date": 1975, "abstract": "This paper presents some programming facilities for modeling the semantics of a task domain and for describing the situations that occur in that domain as a task is being carried out. Each such description models a \"state\" of the task environment, and any given state can be transformed into a new state by the occurrence of an event that aIters the environment. Such modeling systems are vital in many Al systems, particularly those that do question answering and those that do automatic generation and execution monitoring of plans. The modeling mechanisms described are basically extensions and modifications of facilities typically found in Al programming languages such as PLANNER, CONN1VER, and QA4. In particular, we discuss our use of a 3 valued logic, generator functions to deduce answers to model queries, the saving and maintaining of derived results, and new facilities for modeling stntc changes produced by the occurrence of events.", "references": ["fb4b11202c03ff7855af3e23cf166a2a28c62f26", "351bdc21bd5e67e8d41549f9d89e4fcd84438f0f", "a761f26f8239acd88fc83787f28a7f2d2ff9ea22", "1863d22c8a45373a6ba26ff04b2f2db780ad5901", "141ea38bea3ecac14fff05cfd80c4b9b30b73f6a", "97e0e7161f9d0e6762c396d9714f9043b8079b48", "cef155786903a97bb61dfd02af9ee797072a4a3d"], "page_rank": 9.852216748768472e-05}, {"id": "604c4f34b7d71703ac0b67d0d2f8ee4fbc328c71", "title": "A Multiprocessor System Design", "authors": ["Melvin E. Conway}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"A Multiprocessor System Design\" by Melvin E. Conway", "references": [], "page_rank": 0.0002463054187192118}, {"id": "f06bb30589a0554b19f1544af0e018b50b974bf7", "title": "Language directed computer design", "authors": ["William M. McKeeman"], "date": 1967, "abstract": "It is an accident that digital computers are organized like desk calculators--with somewhat worse luck we might have taken the Turing machine as our model. And someone would have been unenlightened enough to prove that, under certain (actually untrue) assumptions, it made no difference. All general purpose machines can compute the same functions, given sufficient time.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "18836cbf140d542022ec5f04805fa47d9d0772f9", "title": "Camera Models And Machine Perception", "authors": ["Irwin Sobel"], "date": 1970, "abstract": "Abstract : The report developes a parametric model for a computer-controlled moveable camera on a pan-tilt head. The model expresses the transform relating object space to image space as a function of the control variables of the camera. We constructed a calibration system for measuring the model parameters which has a demonstrated accuracy more than adequate for our present needs. We have also identified the major source of error in model measurement to be undesired image motion and have developed means of measuring and compensating for some of it and eliminating other parts of it. The system can measure systematic image distortions if they become the major accuracy limitation. It has been shown how to generalize the model to handle small systematic errors due to aspects of pan-tilt head geometry not presently accounted for. The report demonstrates the model's application in stereo vision and have shown how it can be applied as a predictive device in locating objects of interest and centering them in an image. (Author)", "references": ["f8276f400d412a401986aabfec3b2ee19ced5113", "f9b3e5785bcf1743b3d7212122cc0f61ed3f49f3", "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a", "7fa23280d121ea05e88157b268c476c6dbc6800f", "203bc34514447f0228f368006eef2821945e7ac1", "bf47f891e889c39edd799eb1802a033bf7445412", "70e712cf863cd84963257b927bd57773cbcc0d11", "368c9d8c89036aee75bf4159ea3e4d5851d3c6c4"], "page_rank": 6.157635467980295e-05}, {"id": "ac84a075cfc1fa64979526d81ec2ac4442155f69", "title": "Structuring Complex Processes", "authors": ["James J. Horning", "Brian Randell"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Structuring Complex Processes\" by James J. Horning et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "c597c14d8428cbbecab42d3c385064f1174937da", "title": "Highly Parallel Information Processing Systems", "authors": ["John C. Murtha"], "date": 1966, "abstract": "Publisher Summary This chapter discusses the main body of work in the organization of highly parallel information processing systems. This work is very broad in scope, ranging from memory organized systems to various network systems and multiple computer systems. The chapter discusses the general objectives of the research, the types of systems that are being developed, and the general problems faced. It also describes parallel networks, distributed control networks, limited application parallel processors, and multiple instruction stream or multiple function systems. There are three primary design objectives being pursued in research in highly parallel processors. Probably the most significant of these is to obtain a radical increase in computing power, perhaps several orders of magnitude, within the existing component technology. This increase is expected to be obtained from using more components and obtaining better utilization of these components. Costs are expected to be held down because cheaper parts and manufacturing techniques can be used. Parallel languages, translation techniques, and algorithms are discussed.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "7f9154f6ab93a935acccb6b3fdf83d011e44d175", "title": "A universal computer capable of executing an arbitrary number of sub-programs simultaneously", "authors": ["John H. Holland"], "date": 1959, "abstract": "This paper describes a universal computer capable of simultaneously executing an arbitrary number of sub-programs, the number of such sub-programs varying as a function of time under program control or as directed by input to the computer. Three features of the computer are:\n (1) The structure of the computer is a 2-dimensional modular (or iterative) network so that, if it were constructed, efficient use could be made of the high element density and \"template\" techniques now being considered in research on microminiature elements.\n (2) Sub-programs can be spatially organized and can act simultaneously, thus facilitating the simulation or direct control of \"highly-parallel\" systems with many points or parts interacting simultaneously (e.g. magneto-hydrodynamic problems or pattern recognition).\n (3) The computer's structure and behavior can, with simple generalizations, be formulated in a way that provides a formal basis for theoretical study of automata with changing structure (cf. the relation between Turing machines and computable numbers).", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "115b5c9c97d75c93c827a167fb2bf4fcf7f0f96c", "title": "The introduction of definitional facilities into higher level programming languages", "authors": ["Thomas E. Cheatham"], "date": 1966, "abstract": "The purpose of this paper is to present a scheme for employing definitional or \"macro\" features in a higher level programming language. The emphasis will not be on defining the syntactic augments and precise interpretation of such features in any particular programming language and/or operating environment but, rather, on developing the compiler mechanisms for handling the definition and call of such macros and then indicating the kinds of extensions one might propose to current programming languages in order to usefully employ these kinds of facilities.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "de813b713e5bc2af02ace1af51f9922c7f0c0bcf", "title": "A fourth-generation computer organization", "authors": ["Stanley E. Lass"], "date": 1968, "abstract": "A single processor's performance is limited by its organizational efficiency and the technology available. Paralleling of processors and/or improving the organizational efficiency are the ways of obtaining greater performance with a given technology. Much research has been done on multiple processors and single processors which perform operations on vectors in parallel.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "fb28000e5d5a504beb4007aa22e664a3d803c52e", "title": "An apl machine", "authors": ["Philip S. Abrams"], "date": 1970, "abstract": "Semantic Scholar extracted view of \"An apl machine\" by Philip S. Abrams", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "8854e412a9367a76deb2168407bb3aa065009abd", "title": "SIMULA: an ALGOL-based simulation language", "authors": ["Ole-Johan Dahl", "Kristen Nygaard"], "date": 1966, "abstract": "This paper is an introduction to SIMULA, a programming language designed to provide a systems analyst with unified concepts which facilitate the concise description of discrete event systems. A system description also serves as a source language simulation program. SIMULA is an extension of ALGOL 60 in which the most important new concept is that of quasi-parallel processing.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0519d8bf92fbb5424ee8b2655d6d0262c1f82ed3", "title": "A Small Computer for the Direct Processing of FORTRAN Statements", "authors": ["Alan J. Melbourne", "John Matthew Pugmire"], "date": 1965, "abstract": "(ii) The computer must be readily available. The nature of scientific work makes accurate forecasting of computer time difficult. For this reason, renting time on a large installation is not entirely satisfactory because of the tight jobscheduling involved. A small computer locally installed seems preferable if provided with a compiler. Compilation, however, is a timeconsuming process and may take longer than running the final compiled program. It should be reduced to a minimum.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e46faa52ff86efb77b35497699d0910fed41dcde", "title": "System Design of a Dynamic Microprocessor", "authors": ["Robert W. Cook", "Michael J. Flynn"], "date": 1970, "abstract": "Dynamic microprogramnming (i. e., utilizing a READ/ WRITE microstorage) allows the structure of a computer to be altered to suit a problem at hand and results in major efficiencies (an order of magnitude) in running time for nonarithmetic programs (e. g., compilers).", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "7b7bb5c2b9ca333681418bc3f9a99c9bd2a60f07", "title": "Turing machine recognizers for general rewriting systems", "authors": ["Thomas V. Griffiths"], "date": 1964, "abstract": "The notion of structural descriptions of sentences generated by context-free grammars is generalized to include structural descriptions of derivations in general rewriting systems (GRS's). A structural description is represented by a canonical member of the set of derivations having the same structural description.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "f1ebc2e485e806113a46d3c4b5a98ecf51815713", "title": "Parallelism exposure and exploitation in digital computing systems", "authors": ["Earl W. Reigel"], "date": 1969, "abstract": "Abstract : Techniques are presented for the exposure and exploitation of parallelism within programs. Two algorithms, based on input/output set comparisons, are given for the automatic detection of parallelism extant in serially written programs. Representation and segmentation of the partial order control information resulting from program analysis are discussed. Language constructs are suggested that provide explicit indication of parallelism at the task level (routines and repeat statements). Concepts for efficient exploitation of parallelism are investigated. A parallel processing system is described and various related system considerations are discussed. Information flow is studied in terms of memory hierarchy and inter-unit communication. Motivations for the study of parallelism are given and several levels of parallelism are defined. Multiple computer systems are examined and compared based on homogeneity and inter-unit communication. Three basic approaches for highly parallel processing systems - array, associative, and pipeline - are reviewed. (Author)", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "be501e26e4d0037a7e78879d2b1c12524d9c7fd3", "title": "A new approach to the functional design of a digital computer", "authors": ["Robert S. Barton"], "date": 1961, "abstract": "The present methods of determining the functional design of computers are critically reviewed and a new approach proposed. This is illustrated by explaining, in abstracted form, part of the control organization of a new and different machine based, in part, on the ALGOL 60 language. The concepts of expression and procedure lead directly to use of a Polish string program. A new arrangement of control registers results, which provides for automatic allocation of temporary storage within expressions and procedures, and a generalized subroutine linkage.\n The simplicity and power of these notions suggests that there is much room for improvement in present machines and that more attention should be given to control functions in new designs.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4d478cf13be6e2e810ba51a293825082a6f33da6", "title": "Preservation of unambiguity and inherent ambiguity in context-free languages", "authors": ["Seymour Ginsburg", "Joseph S. Ullian"], "date": 1966, "abstract": "Various elementary operations are studied to find whether they preserve on ambiguity and inherent ambiguity of language (\u201clanguage\u201d means \u201ccontext-free language\u201d) The following results are established:If L is an unambiguous language and S is a generalized sequential machine, then (a) S(L) is an unambiguous language if S is one-to-one on L, and (b) S-1(L) is an unambiguous language.\nInherent ambiguity is preserved by every generalized sequential machine which is one-to-one on the set of all words.\nThe product (either left or right) of a language and a word preserves both unambiguity and inherent ambiguity.\nNeither unambiguity nor inherent ambiguity is preserved by any of the following language preserving operations: (a) one state complete sequential machine; (b) product by a two-element set; (c) Init(L) = [u \u2260 dur in L for some v]; (d) Subw(L) = [w \u2260 durr in L for some u, v].\n", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "6be47b5305379c52961c15d80781f55849d9f758", "title": "The Mathematical Theory Of Context Free Languages", "authors": ["Seymour Ginsburg"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"The Mathematical Theory Of Context Free Languages\" by Seymour Ginsburg", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "bf6fa65a1c0a9e780cd2ac615b41f9ef7785526a", "title": "The theory and application of pushdown store machines", "authors": ["R. James Evey"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"The theory and application of pushdown store machines\" by R. James Evey", "references": [], "page_rank": 0.00015247478301665492}, {"id": "ca4d6ae23b5ed54faa7516d2b27946c7abb6c5ab", "title": "Operations Which Preserve Definability in Languages", "authors": ["Seymour Ginsburg", "Gene F. Rose"], "date": 1963, "abstract": "To better understand the syntax of problem-oriented languages, the equational form used in defining ALGOL [1] was recently abstracted [6]. On varying the coefficients in the abstract equational form two families of languages arose. [A language is viewed as a set of strings, called words, of symbols from a finite, fixed alphabet.] The two types of languages, called definable and sequentially definable, were then investigated and a number of properties discovered. In particular, it was shown that the definable languages were identical to the type-two phrase structure languages introduced by Chomsky [4]. The present paper deals with operations T (on languages) which preserve definability ~nd frequently sequential definability. All operations considered have the additional property of preserving regularity [7]--the interest in regular sets stemming from the fact that they have been considered as \"finite state languages\" [3]. Two basic results are proved. The first (Theorem 2.1) permits derivation of (sequential) definability-preserving operations from other (sequential) definability-preserving operations. The second (Theorem 3.1) asserts that a sequential machine always changes a definable set into a definable set (but not necessarily a sequentially definable set into a sequentially definable set). Using these two results, a large number of specific operations-many occurring in data processing--arc shown to preserve definability and, depending on the operation, sequential definability. In addition to the two main results there occur in appendices A and B necessary conditions for a set to be sequentiMly definable and definable respectively. The former is the first known necessary condition for sequentially definable sets which differs from those for definable sets. It is anticipated that many of the operations considered here will be useful in later studies of problem oriented languages. 1. Pre l im inar i e s The n o t a t i o n a n d basic concep t s to be used t h r o u g h o u t t he p a p e r are p resen ted here. M u c h of th is m a t e r i a l is d r a w n f rom [6], to which the reader is referred for mot iva t ion a n d fu r the r de ta i l s . Notation. L e t Z = In, b, . . } be a finite, n o n e m p t y a l p h a b e t of symbols or letters. Le t 0 ( E ) , or 0 for shor t , deno te t he set of a l l words, i.e. s tr ings, fo rmed from the e l emen t s in ~, inc lud ing the e m p t y word e. Consider func t ions f(~(1), \" \" , ~(~)) wh ich a re cons t ruc ted f rom a finite number of se t va r i ab le s ~(1), . . . , $(~), each ~(~) ranging ove r a l l subsets of 0, and a finite n u m b e r of subse t s of 0 (ca l led coeff icients) , us ing the opera t ions of \" + \" ( a d d i t i o n or set un ion ) and \" . \" ( m u l t i p l i c a t i o n or complex p roduc t ) 1 a * Received January, 1962; revised June, 1962. Let A~ , . . . , A, , be a sequence of sets of words. The (complex) product A1.A~ . . . . . A,~ , or A~ . . . A,,~ for short, is the set of words {x~ . . x,, I each x~ in A~}, x~ . . . x,~ being the word formed from the concatenation of the words x~ in the given order: If one or more of the A~, say Aj(~) , \u2022 \u2022 \u2022 , As(,) consist of just a single word, say aja) , \u2022 \" , a/(~) respectively; then aj(~) is writ ten instead of A/(~) at each occurrence. For example, aA is written instead of {a}A, and e instead of {~}.", "references": ["dafabc60fe64f5fea4d20d464d453c262d2649b4"], "page_rank": 0.00015247478301665492}, {"id": "fb00017e3e070016b7bf9651a7241117e4708dad", "title": "Mappings which Preserve Context Sensitive Languages", "authors": ["Seymour Ginsburg", "Sheila A. Greibach"], "date": 1966, "abstract": "A basic result which gives a condition under which a (possibly length-decreasing) homomorphism preserves a contest. sensitive language is presented. Using this result, conditions under which pushdown transducers and linear bounded transducers preserve contest sensitive languages are given. The basic result is also applied to show that certain rewriting systems generate context sensitive languages instead of arbitrary recursively enumerable sets. Of special interest is the result that if each rule in a rewriting system has a terminal letter on its right side, then the language generated is context free.", "references": ["ca4d6ae23b5ed54faa7516d2b27946c7abb6c5ab", "e3d442b3881e75eb4c8d60bfc8591108ebcd9d20", "dafabc60fe64f5fea4d20d464d453c262d2649b4", "3a8143965c0c15028a7959c4a017651e3b50ddac", "2a71434371bb4a11a5fc78d7e5c3723c273fff39", "8a29d437f9d45aed60e3052dd60c93129292ead4", "bf6fa65a1c0a9e780cd2ac615b41f9ef7785526a"], "page_rank": 7.037297677691766e-05}, {"id": "3a4a561b6fe2888fdf7263b603fbeda1c0336328", "title": "A New Normal-Form Theorem for Context-Free Phrase Structure Grammars", "authors": ["Sheila A. Greibach"], "date": 1965, "abstract": "A context>free phrase structure general~or is in..~landard jb~'m if and only if alt of its rules are of the form: Z-, aY~, ... , Y,~ where Z and Yi are intermediate symbels and a is a l~erminM symbol, so that one input., symbol is processed at each step. Standard form is eonvenien(~ for computer manipulation of eontext-free languages. A proof is given that every context-free phrase structure generator is strongly equivalent to one in standard form; it, is in the form of an algorithm now being prograrr~med, and offers an independent proo[' of a variant of the Chomsky-Setditzenberger 1tortoni form theorem.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "8c813f13be97f2c0d2114a5a6a05afdff744676c", "title": "Representation of Actions That Have Side-Effects", "authors": ["N. S. Sridharan", "Frank M. Hawrusik"], "date": 1977, "abstract": "Systems that reason about actions, whether they do plan generation [1] or plan recognition [2] typically model the effects of actions that occur in a plan. Simple declarative schemata allow the specification of assertions to be added/deleted to model the primary effects of actions. Side-effects of actions are those that are conditional on properties of the state in which the action is taken. When representing actions which have side-effects, conventional wisdom suggests adopting a procedural representation for they allow detailed specification of side-effects. However the procedural representation hides this knowledge from other parts of the system, thereby hindering the system in reasoning about side-effects. We use a STRIPS like declarative schema for actions that has parameters, preconditions, assertional forms for goal and outcomes and investigate three methods of representing knowledge about side-effects and discuss how the system computes side-effects without running into severe combinatorics. The following discussion and examples deal with knowledge representation as implemented in the AIMDS system which forms the AI framework for the BELIEVER project. Examples Consider a normal input of the form \"John walked from the office to the bus station\" interpreted in a world model where \"John is at the office\" is true. The conclusions drawn include \"John is at the bus station\". This can be handled using an act schema with three variables P, FL and TL as shown below. (Each WALK act has (agent [a PERSON [refer: P]]) (fromloc [a LOCATION (refer: FL]]) (toloc [a LOCATION [refer: TL]]) (goal (PROPOSITION [P loc TL])) (precond (PROPOSITION [P loc FL)))) We have extended the interpreter to deal with some simple cases of partial act instance descriptions and incomplete world models. The incomplete description \"John walked to the bus station\" can be filled in using the world model, so that the system now can conclude \"John walked from the office\". Similarly, if in the world model \"John's location is unknown\" then from the normal input \"John walked from the office to the bus station\" the system concludes \"John was at the office before walking to the bus station\". Dealing with side-effects requires additional knowledge. For example, consider the world model \"John is at the office; John is holding a package; The package is at the office\". To update the world model properly, the location of the package must be changed to the same location as John. We explain three methods in which this knowledge about side-effects \u2026", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0f6c7ea83494f4c921758115a925ecf55ea5ec70", "title": "Plausible inference: A multi-valued logic for problem solving", "authors": ["Leonard Friedman"], "date": 1979, "abstract": "A new logic is developed which permits continuously variable strength of belief in the truth of assertions. Four inference rules result, instead of the two of formal logic, with formal logic as a limiting case. Quantification of belief is defined using the methods introduced by Shortliffe and Buchanan. Propagation of belief to linked assertions results from dependency-based techniques of truth maintenance so that local consistency is achieved or contradiction discovered in problem solving. Rules for combining, confirming, or disconfirming a beliefs are given, and several heuristics are suggested that apply to revising already formed beliefs in the light of new evidence. The strength of belief that results in such revisions based on conflicting evidence appears to be a highly subjective phenomenon. Nevertheless, certain quantification rules r^pear to reflect an orderliness in the subjectivity. Several examples of reasoning by Plausible Inference (PI) are given, including a legal example and one from robot learning. Propagation of belief takes place in directions forbidden in formal logic and this results in conclusions becoming possible for a given set of assertions that are not reachable by formal logic.", "references": ["54f35b4edba6ddee8ce2eac489bde78308e3e708", "ad9ac8938d230ef41cf2aa6a795743c8b1520200"], "page_rank": 4.926108374384236e-05}, {"id": "cb67fe0cd45ad16001206773ffde93140f80a1a2", "title": "A programming language", "authors": ["Kenneth E. Iverson"], "date": 1962, "abstract": "The paper describes a succinct problem-oriented programming language. The language is broad in scope, having been developed for, and applied effectively in, such diverse areas as microprogramming, switching theory, operations research, information retrieval, sorting theory, structure of compilers, search procedures, and language translation. The language permits a high degree of useful formalism. It relies heavily on a systematic extension of a small set of basic operations to vectors, matrices, and trees, and on a family of flexible selection operations controlled by logical vectors. Illustrations are drawn from a variety of applications.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "79b72cbd43608729524b5e41384094949a6064ce", "title": "The Automatic Speech Recognition System for Conversational Sound", "authors": ["Toshiyuki Sakai", "Shuji Doshita"], "date": 1963, "abstract": "This paper describes the method and the system investigated to solve the problem encountered in the automatic recognition of speech sound. From research in the automatic analyzer of speech sound, a monosyllable recognition system was constructed in which the phoneme is used as the basic recognition unit. Recently this system has been developed to accept the conversational speech sound with unlimited vocabulary. The mechanical recognition of conversational speech sound requires two basic operations. One is the segmentation of the continuous speech sound into several discrete intervals (or segments), each of which may be thought to correspond to a phoneme, and the other is the pattern recognition of such segments. For segmentation, by defining two criteria, ``stability'' and ``distance,'' the properties of the time pattern obtained by the analysis of input speech sound may be examined. The principle of the recognition is based on the mechanism of the articulation in our speech organ. Corresponding to this, the machine has the functions called phoneme classification, vowel analysis and consonant analysis. A conversational speech recognition system with the phonetic contextual approach is also applied to the vowel recognition where the time pattern of input speech is matched with the stored standard patterns in which the phonetic contextual effects are taken into consideration. The time pattern which has great variety may be effectively expressed by the new representation of ``sequential pattern'' and ``weighting pattern.''", "references": ["5f87bb3dc34be8c90498b31c3bdbf43f539f1a8f"], "page_rank": 4.926108374384236e-05}, {"id": "d7d927cda381864aeaf4f4d2aa15d5ec05ffcdee", "title": "Planning and Acting", "authors": ["Drew McDermott"], "date": 1978, "abstract": "A new theory of problem solving is presented, which embeds problem solving in the theory of action; in this theory, a problem is just a difficult action. Making this work requires a sophisticated language for talking about plans and their execution. This language allows a broad range of types of action, and can also be used to express rules for choosing and scheduling plans. To ensure flexibility, the problem solver consists of an interpreter driven by a theorem prover which actually manipulates formulas of the language. Many examples of the use of the system are given, including an extended treatment of the world of blocks. Limitations and extensions of the system are discussed at length. It is concluded that a rule-based problem solver is necessary and feasible, but that much more work remains to be done on the underlying theory of planning and acting.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1374222f975da69b26b0c34d4fac6d139a065e4d", "title": "Aspects of speech recognition by computer", "authors": ["Pierre J. Vicens"], "date": 1969, "abstract": "Abstract : The paper describes techniques and methodology which are useful in achieving close to real-time recognition of speech by a computer. To analyze connected speech utterances, any speech recognition system must perform the following processes: preprocessing, segmentation, segment classification, recognition of words, recognition of sentences. The paper presents implemented solutions to each of these problems which achieved accurate recognition in all the trial cases. (Author)", "references": [], "page_rank": 0.00011963406052076002}, {"id": "0357413d2005fc1da140a3332f0a353354d4b99f", "title": "An Operator Which Locates Edges in Digitized Pictures", "authors": ["Manfred H. Hueckel"], "date": 1971, "abstract": "Because of the fundamental importance of edges as primitives of pictures, automatic edge finding is set as goal. A set of requirements which should be met by a local edge recognizer is formulated. Their main concerns are fast and reliable recognition in the presence of noise. A unique optimal solution for an edge operator results. The operator obtains the best fit of an ideal edge element to any empirically obtained edge element. Proof of this is given. A reliability assessment accompanies every recognition process.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "4aac001b7741dc79056c86e9aaa4a2e20841b68e", "title": "Programming Techniques: ASP\u2014a ring implemented associative structure package", "authors": ["C. A. Lang", "J. C. Gray"], "date": 1968, "abstract": "ASP is a general purpose Associative Data Structure Package in which an arbitrary number of data items and an arbitrary number of the relationships between these data items may be represented. A special picture language is described which has proved very useful for drawing ASP structures on paper. ASP structures are built and manipulated by means of a series of macro calls, which are outlined in the Appendix. Emphasis is on the philosophy of the system rather than a particular implementation, though sufficient information is included to enable the reader to produce his own implementation of ASP.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "cef155786903a97bb61dfd02af9ee797072a4a3d", "title": "A preliminary qlisp manual", "authors": ["Ren{\\'e} Reboh", "Earl D. Sacerdoti"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"A preliminary qlisp manual\" by Ren\u00e9 Reboh et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "354d338bbe84b3bd361eccc8e3368b54b52eea9e", "title": "Simulation of Human Problem-Solving Method", "authors": ["D. L. Johnson", "A.D.C. Holden"], "date": 1964, "abstract": "One of the most intriguing and potentially important applications of the digital computer is in the simula tion of human thought processes. The objects of thought when suitably coded can be transformed, classified and stored within the computer at will. The predominant charactertistic of human thought processes is their generality. To make any progress in the simulation of such processes, it is first necessary to isolate a small region and examine it in isolation. As progress is made in one region, it can be ex panded and generalized to include a wider field, and it is likely that many of the methods used in a re stricted field will be susceptible to modification for use in a larger area. It is desirable to work initially in a region in which the relationships between the ob jects of thought are clearly defined. This report deals with the problem of finding se quences of transformations which constitute proofs of trigonometric identities. The method described need not be confined to this particular problem, but could easily be used in other fields if the allowable transformations are known. The behaviour of a ma chine which has been programmed to carry out this process is described in detail and its responses, when several identities were presented to it for proof, are given. No attempt is made to use the method of finding proofs by exhaustive search, even though in this case such a search is quite feasible, since the number of \"basic\" trigonometric transformations which can be applied to a given function is relatively small. It is considered here that such repetitive \"trial and error\" methods are less interesting than the methods which we shall discuss. It is desirable to decide on the best transformation for a particular problem by compar ing the characteristics of the problem with the prop erties of each transformation in such a way that the machine's performance will improve with experi ence. Exhaustive search methods become useless when the number of possible decisions at each step becomes large.", "references": ["97876c2195ad9c7a4be010d5cb4ba6af3547421c"], "page_rank": 5.473453749315818e-05}, {"id": "68ff263250279e00572fc43a050fa348838f34a1", "title": "A Three Valued Truth Maintenance System.", "authors": ["David A. McAllester"], "date": 1978, "abstract": "Abstract : Truth maintenace systems have been used in recently developed problem solving systems. A truth maintenance system (TMS) is designed to be used by deductive systems to maintain the logical relations among the beliefs which those systems manipulate. These relations are used to incrementally modify the belief structure when premises are changed, giving a more flexible context mechanism than has been present in earlier artificial intelligence systems. The relations among beliefs can also be used to directly trace the source of contradictions or failures, resulting in far more efficient backtracking. In this paper a new approach is taken to truth maintenance algorithms. Each belief, or proposition, can be in any one of three truth states, true, false, or unknown. The relations among propositions are represented in disjunctive classes. By representing an implication in a clause the same algorithm that is used to deduce its consequent can be used to deduce the negation of antecedents that would lead to contradictions. A simple approach is also taken to the handling of assumptions and back tracking which does not involve the non-monotonic dependency structures present in other truth maintenance systems. (Author)", "references": [], "page_rank": 0.00010399562123700055}, {"id": "1863d22c8a45373a6ba26ff04b2f2db780ad5901", "title": "A Model for Control Structures for Artificial Intelligence Programming Languages", "authors": ["Daniel G. Bobrow", "Ben Wegbreit"], "date": 1973, "abstract": "Newer programming languages for artificial intelligence extend the class of available control regimes beyond simple hierarchical control. In so doing, a key issue is using a model that clearly exhibits the relation between modules, processes, access environments, and control environments. This paper presents a model which is applicable to diverse languages and presents a set of control primitives which provide a concise basis on which one can define almost all known regimes of control.", "references": ["4436c947845aa1ee9041bc41a03c612d57494923", "2ff0eeb5cceb4bb9698e470521e04d19e03dbcc1", "07ee4a3d6e78326acdc77ea24ba61466eea31e19", "91a1b89eb1b286e6edd3aeba3562a9e195d6fb11", "539e079e1c7a23b69f3c336103f9ad51769898b8"], "page_rank": 7.037297677691766e-05}, {"id": "412f7940a8edea0b52c2fcd019a9d7cdaf7f14e9", "title": "Revised report on the algorithmic language ALGOL 60", "authors": ["John W. Backus", "Friedrich L. Bauer", "Julien Green", "C. Katz", "John McCarthy", "Peter Naur", "Alan J. Perlis", "Heinz Rutishauser", "Klaus Samelson", "Bernard Vauquois", "J. H. Wegstein", "Adriaan van Wijngaarden", "Michael Woodger"], "date": 1963, "abstract": "@article{m:lisp, author = {McCarthy, J.}, title = {Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I}, journal = {Communications of the ACM}, volume = {3}, number = {4}, pages = {184-195}, year = 1960 } Summary: McCarthy introduces LISP. LISP deviates from other contemporary languages such as ALGOL as its design revolves around the idea that partial recursive functions of symbolic expressions can be an effective programming language. Mc-Carthy gives meaning to LISP through a meta-circular \" defunctionalized \" interpreter and discusses the implementation of LISP's runtime. Evaluation: This is a seminal paper for multiple reasons. First, it introduces a new programming model that affected the design of programming languages in the decades to come. Second, LISP pioneers many ideas that have evolved nowadays into staple features of most programming languages such as anonymous first-class functions (closures), reflection and garbage collection. Third, the way McCarthy presents LISP foreshadows the formal definition of programming languages with abstract syntax and meta-circular interpreters.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "70e712cf863cd84963257b927bd57773cbcc0d11", "title": "An operator which locates edges in digitized pictures", "authors": ["H. Heuckel"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"An operator which locates edges in digitized pictures\" by H. Heuckel", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "368c9d8c89036aee75bf4159ea3e4d5851d3c6c4", "title": "Automatic interpretation and classification of images", "authors": ["Antonio Grasselli"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Automatic interpretation and classification of images\" by Antonio Grasselli", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "b48d67c1e4e43f6ec2fdd3dc91a20a4efdc97ddc", "title": "Semantical Analysis of Intuitionistic Logic I", "authors": ["Saul Kripke"], "date": 1965, "abstract": "Publisher Summary The chapter discusses a semantical analysis of intuitionistic logic I. The chapter presents a semantical model theory for Heyting's intuitionist predicate logic and proves the completeness of that system relative to the modeling. The semantics for modal logic that is announced and developed together with the known mappings of intuitionistic logic into the modal system, S4, inspired the present semantics for intuitionist logic. It is important to develop the semantics of intuitionistic logic independently of that of S4; this procedure helps to obtain somewhat more information about intuitionistic logic, including the mapping into S4 as a consequence thereof. In addition to giving a simple decision procedure for Heyting's propositional calculus, the chapter presents the undecidability of monadic intuitionistic quantification theory. The proof is based on the semantics previously developed. Beth semantic tableaux for intuitionistic logic is developed in the chapter. The chapter describes consistency property: in a standard formalization of Heyting's predicate calculus, the axioms are all valid, and the rules preserve validity.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "203bc34514447f0228f368006eef2821945e7ac1", "title": "Algorithms for finding zeros and extrema of functions without calculating derivatives", "authors": ["Richard P. Brent"], "date": 1971, "abstract": "Abstract : Theorems are given concerning the order (i.e., rate) of convergence of a successive interpolation process for finding simple zeros of a function or its derivatives, using only function evaluations. Special cases include the successive linear interpolation process for finding zeros, and a parabolic interpolation process for finding turning points. Results on interpolation and finite differences include weakening the hypotheses of a theorem of Ralston on the derivative of the error in Lagrangian interpolation. The theoretical results are applied to given algorithms for finding zeros or local minima of functions of one variable, in the presence of rounding errors. The algorithms are guaranteed to converge nearly as fast as would bisection or Fibonacci search, and in most practical cases convergence is superlinear, and much faster than for bisection or Fibonacci search.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f8276f400d412a401986aabfec3b2ee19ced5113", "title": "THE COMPUTER REPRESENTATION OF SIMPLY DESCRIBED SCENES", "authors": ["Richard P. Paul", "Gil Falk", "Jerome A. Feldman"], "date": 1969, "abstract": "Abstract : The paper describes the computer representation of scenes consisting of a number of simple three-dimensional objects. One method of representing such scenes is a space oriented representation where information about a region of space is accessed by its coordinates. Another approach is to access the information by object, where, by giving the object name, its description and position are returned. As the description of an object is lengthy, it is desirable to group similar objects. Groups of similar objects can be represented in terms of a common part and a number of individual parts. If it is necessary to simulate moving an object then only the individual information need be saved. (Author)", "references": [], "page_rank": 0.0001319493314567206}, {"id": "bf47f891e889c39edd799eb1802a033bf7445412", "title": "The Perception of the Visual World", "authors": ["Ralph Hetherington"], "date": 1952, "abstract": "Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd the perception of the visual world to read.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "7fa23280d121ea05e88157b268c476c6dbc6800f", "title": "The Stanford Hand-Eye Project", "authors": ["Jerome A. Feldman", "Gerald M. Feldman", "Gilbert Falk", "G. Grape", "J. Pearlman", "Irwin Sobel", "Jay M. Tenenbaum"], "date": 1969, "abstract": "There is a large continuing project at Stanford Artificial Intelligence Laboratory aimed towards the development of a system capable of interesting perceptual-motor behavior. This paper presents a brief outline of the currently active efforts and suggests references for more detailed information. A more thorough discussion of the effort to organize a visual perception system is presented.", "references": ["c3576b28daab769c8ece29fed47e6f95f83f4777", "313225cd9b6729c1a239a3af40d2dfeaf6946a52", "06600e1ca9451d81e89d078a924184683ace9196", "f8276f400d412a401986aabfec3b2ee19ced5113", "07f4f7c0e01282f4852cd1c13d97048c1c64106f", "1374222f975da69b26b0c34d4fac6d139a065e4d", "c0956c94191cebf50e3ec32e14897cbc6bde9119", "25a13868653f786ba00043be5b34cb12c0854976", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5"], "page_rank": 6.157635467980295e-05}, {"id": "a5d795cc08672f117debad8edd94c4b7ca593301", "title": "Efficient Processing of Interactive Relational Data Base Queries expressed in Logic", "authors": ["David H. D. Warren"], "date": 1981, "abstract": "Relational database retrieval is viewed as a special case of deduction in logic. It is argued that expressing a query in logic clarifies the problems involved in processing it efficiently (\"query optimisationn). The paper describes a simple but effective strategy for planning a query so that it can be efficiently executed by the elementary deductive mechanism provided in the programming language Prolog. This planning algorithm has been implemented as part of a natural language question answering system, called Chat-80. The Chat-80 method of query planning and execution is compared with the strategies used in other relational database systems, particularly Ingres and System R.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "351bdc21bd5e67e8d41549f9d89e4fcd84438f0f", "title": "Learning and Executing Generalized Robot Plans", "authors": ["Richard Fikes", "Peter E. Hart", "Nils J. Nilsson"], "date": 1972, "abstract": "In this paper we describe some major new additions to the STRIPS robot problem-solving system. The first addition is a process for generalizing a plan produced by STRIPS so that problem-specific constants appearing in the plan are replaced by problem-independent parameters. \n \nThe generalized plan, stored in a convenient format called a triangle table, has two important functions. The more obvious function is as a single macro action that can be used by STRIPS\u2014either in whole or in part\u2014during the solution of a subsequent problem. Perhaps less obviously, the generalized plan also plays a central part in the process that monitors the real-world execution of a plan, and allows the robot to react \"intelligently: to unexpected consequences of actions. \n \nWe conclude with a discussion of experiments with the system on several example problems.", "references": ["62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "72b3682e166faa5c5ced5873934b20dd8f4c4d21"], "page_rank": 7.037297677691766e-05}, {"id": "3e467357faa9e737a71b4119a1ffadc795a8ac34", "title": "Influence of language on solving three-term series problems.", "authors": ["Homer H. Clark"], "date": 1969, "abstract": "Three linguistic processes have been previously proposed to account for the influence of language on deductive reasoning. To test for these processes, 100 5\"s were each asked to solve 64 different forms of the three-term series problem (e.g., \"If John isn't as bad as Bill, and Pete isn't as good as Bill, then who is worst?\"). The 6\"s were allowed only 10 sec. per problem. The kind and relative number of errors SB made support the existence of an implicit problem solving strategy containing the three proposed linguistic processes. The strategy consists of (a) interpreting the propositions and question of a problem in terms of their deep structure relations, (b) retaining the information in this form, and (c) later searching memory for information that is congruent in deep structure with the information asked for in the question. For some problems, this strategy leads to longer solution times, hence more errors. For many problems, it also leads to preliminary solutions that show up as specific errors.", "references": ["9b3ea2ad8a2d0f1cfc327578f2481d422176aca2", "6ace6c437c501b94be29cfea029025f4cd0f87e2", "04c32d0420d32bf144e1a2f99dd350178c64a3e8", "d6190336549fff6d971aecc373c195464788a52a", "e027ffaa9a27d1e5d8768307337f9a7d6fcb2a60", "ea78b2ce88b7dca417aa7c96b02690a0428c1d98", "5e2adb8a8b78d0a0d5803a1326302cf81a21cb66", "451fa303b16a007106d52061d37269822ed9c705", "8471715ef5fad0ae204295cb5146330acd7e9410", "f1cb4346fd84d15169c57828fd62de64ea2fa0fb"], "page_rank": 5.473453749315818e-05}, {"id": "0dd38898c35ba7364f10f10935e50ebb498ca91d", "title": "The Lexical Base for Semantic Interpretation in a Prolog Parser", "authors": ["Michael C. McCord"], "date": 1985, "abstract": "An electrical power supply arrangement for a guidable stock selector truck for use in low temperature (e.g. -10 DEG F) environments having a storage cart, operator platform, enclosure walls, and heating members. The power arrangement comprises a generally longitudinally elongated guide rail spaced in close proximity to a floor surface and laterally spaced from storage racks by a distance sufficient to accommodate longitudinally extending bus bars surrounded by an insulated guide member and a protected pivotal electrical contactor device connected to the truck having contact birds adapted for continuously engaging and transferring the electrical energy from the bus bars to the truck. Inclined converging lead-ins positioned at ends of the bus bars serve to direct the contact birds to respective contact channel surfaces defined by respective bus bars before the contact birds enter respective contact channels. In a preferred embodiment, the selector truck is provided with an enclosed working area for the platform and within which the heating members allow a selector to be in a more comfortable working temperature.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9a66cd464abbebd99df2ba7e62f761a11696d18a", "title": "Taxonomic Reasoning", "authors": ["Josh D. Tenenberg"], "date": 1985, "abstract": "In formalizing knowledge for common sense reasoning, one often needs to partition some domain. An instance of this from the Blocks World is the statement \"All blocks are either held, on the table, or on another block.\" Although we can write this axiom in predicate calculus or in clause form for input to a theorem prover, such representations are highly space inefficient. In this paper we present a generalized clause form that allows for the compact representation of arbitrary partitions, along with a set of corresponding inference rules. Additionally, a theorem prover implementing these rules is described that demonstrates their utility with certain kinds of common sense rule bases.", "references": ["652670fa8e275a9a6fe399becd5194b63af4da6f", "e1a214d7efdaa03bfc593c02e6cf72fb1dd54859", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "885194cadd5989d077f5f25b3dc0c42d0d55ce40", "d2e32e2acf3cc906f69e8340497567bce1a852a7"], "page_rank": 0.0002134646962233169}, {"id": "ba3ac4cc3d15144bf856ae1ae72476e1a1f9fbdd", "title": "Using language and context in the analysis of text", "authors": ["Yigal Arena"], "date": 1981, "abstract": "We describe a theory of natural language understanding within which we identify two separate components, a language centered one and a context centered one. xn5 rdrme*1 component uses a knowledge Dase consisting of pairings of phrases with the concepts associated with them to determine the meaning of utterances. The latter component clarifies the meaning found by the first one and makes it more specific by attempting to reconcile it with the context of the utterance. \n \nWe have constructed a program called PHRAN (PHRasal ANalyzer) which performs the task of the language centered component.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "3709d6c71d1003f47fce2989134173bf477802ac", "title": "An Efficient Context-Free Parsing Algorithm for Natural Languages", "authors": ["Masaru Tomita"], "date": 1985, "abstract": "This thesis introduces an efficient context-free parsing algorithm and emphasizes its practical value in natural language processing. In the theoretical worst case analysis, the parsing algorithm occasionally takes more than O(n('3)) time with kinds of context-free grammars which are very unlikely to appear in natural languages. As far as practical natural language processing is concerned, on the other hand, the parsing algorithm seems more efficient than any existing algorithms including Earley's algorithm. Experiments with several English grammars and sample sentences show that our algorithm is 5 to 10 times faster than Earley's standard algorithm. \nThe parsing algorithm can be viewed as an extended LR parsing algorithm which embodies the concept of a \"graph-structured stack.\" Unlike the standard LR, the algorithm is capable of handling arbitrary non-cyclic context-free grammars including ambiguous grammars, with little loss of LR efficiency. In particular, if its grammar is \"close\" to LR, most of the LR parsing efficiency can be preserved. Natural language grammars are, fortunately, considerably \"close\" to LR, compared with other general context-free grammars. \nThe algorithm is an all-path parsing algorithm; it produces all possible parse trees (a parse forest) in an efficient representation called a \"shared-packed forest.\" This thesis also shows that Earley's forest representation has a defect and his representation cannot be used in natural language processing. \nThe last chapters of the thesis suggest practical applications of the algorithm. A concept of left-to-right on-line parsing is introduced, taking advantage of the fact that our algorithm parses a sentence strictly from left to right. Several benefits of on-line parsing are described, and its application to user-friendly natural language interface is discussed. This thesis also proposes a technique to disambiguate a sentence out of the shared-packed forest representation by asking the user questions interactively. Finally, a personal/interactive machine translation system is suggested.", "references": ["68961538c5d2d8593f3d4ae3f61fe09b76d008e1", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "7b57a3f720c52b26b7437e6b1099eaade9b222b8", "eed11c13813396a6198ea13a8fc54260d12074d5", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "c10ef511683b0ed9f03927f64dbc81f575bccb9a", "40dbb25a15b63af3faccb81c8e64a3f5d659e07e", "5e32836ba775c74315b3d215a7c4fa81f97700a9", "9274889f719b8308a0e389b2251d2a2b64c03e84", "c3313782db9fd777feae94cf12d2fd2170b2cab0"], "page_rank": 4.926108374384236e-05}, {"id": "83efd499e7223be947edbaf72d8ec59c31070ee4", "title": "Kind Types in Knowledge Representation", "authors": ["Kathleen Dahlgren", "Joyce P. McDowell"], "date": 1986, "abstract": "This paper describes Kind Types (KT), a system which uses commonsense knowledge to reason about natural language text. KT encodes some of the knowledge underlying natural language understanding, including category distinctions and descriptions differentiating real-world objects, states and events. It embeds an ontology reflecting the ordinary person's top-level cognitive model of real-world distinctions and a database of prototype descriptions of real-world entities. KT is transportable, empirically-based and constrained for efficient reasoning in ways similar to human reasoning processes.", "references": ["9a66cd464abbebd99df2ba7e62f761a11696d18a", "e1a214d7efdaa03bfc593c02e6cf72fb1dd54859", "2aa3668a5b01e1e0986f9352c4bfbd80c078326a", "97e21875955ccc0b18a8a89b518ffdf73a0da410"], "page_rank": 4.926108374384236e-05}, {"id": "7476a53192144d3de768c54d4832acf759ea2654", "title": "Toward a Detailed Model of Processing for Language Describing the Physical World", "authors": ["David L. Waltz"], "date": 1981, "abstract": "This paper explores the problem of judging whether or not an English sentence could correspond to a real world situation or event which is literally, physically plausible, and the related problem of representing the different possible physical situations. The judgement of plausibility can be made at a high level by checking semantic marker restrictions on verb case frame constituents. Often, however, plausibility judgement can only be based on the results of an attempt to construct (imagine) a scene that corresponds to the sentence, and which does not violate \"common sense\" (i.e. relevant physical laws and expected, stereotyped behavior). Methods are presented for constructing representations for different scenes which could correspond to a sentence. These methods incorporate (1) \"subscripts\" (sequences of scenes which comprise an event, with attached preconditions and postconditions) to express different verb senses, (2) object representations which express properties such as shape, size, weight, strength, and behavior under common conditions; (5) physical laws, encoded as constraints on behavior; (4) representation of context; and (5) robot problem solving-like methods to fit all this material together.", "references": ["92ae36972df50e48db4adf30cc5eb369b3991709", "0da1f9475c7a380600de167f9aedcba99aa679da", "2fa8570ae5ac3950a1bc6d9ef5c6af9126824d23", "fd1ff6b009f621f71d751c4c51dfa3c4f1f0a357", "a9952695e1c107f24a1f6f36b287139f2db00a90", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "e369643dd7e4bde48a850b451baaec7d5fee42a0", "ed57a5b47a16ed1e0d6f0b3d061a4af24dd5675f", "157e750a1259d0a5f839bb5cb8779ccb9d7702d6", "c3d04a4e021fd0b90529f89c9cf2c43e4c436b58"], "page_rank": 4.926108374384236e-05}, {"id": "988db1837c5323ff7e3f9d8631f9e74cda09b90f", "title": "Predicting garden path sentences", "authors": ["Robert William Milne"], "date": 1982, "abstract": "This work is an investigation into part of the human sentence parsing mechanism (HSPM). The major test of the psychological validity of any model of the HSPM is that it fail on precisely those sentences that humans find to be garden paths. It is hypothesized that the HSPM consists of at least two processes. We call the first process the syntactic processor, and the second will be known as the semantic processor. It is hypothesized that the syntactic processor is unconscious, deterministic and fast, but limited. While most ambiguities are resolved on the basis of syntactic information, when the syntactic processor can no longer guarantee a correct analysis, semantic information is used to help resolve the ambiguity. This model leads to a better prediction and explanation of which sentences will cause people to garden path.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a19ea03ab4645dcce3e87719e4b7ffaf565bf526", "title": "From English to Logic: Context-Free Computation of 'Conventional' Logical Translation", "authors": ["Lenhart K. Schubert", "Francis Jeffry Pelletier"], "date": 1982, "abstract": "We describe an approach to parsing and logical translation that was inspired by Gazdar's work on context-free grammar for English. Each grammar rule consists of a syntactic part that specifies an acceptable fragment of a parse tree, and a semantic part that specifies how the logical formulas corresponding to the constituents of the fragment are to be combined to yield the formula for the fragment. However, we have sought to reformulate Gazdar's semantic rules so as to obtain more or less 'conventional' logical translations of English sentences, avoiding the interpretation of NPs as property sets and the use of intensional functors other than certain propositional operators. The reformulated semantic rules often turn out to be slightly simpler than Gazdar's. Moreover, by using a semantically ambiguous logical syntax for the preliminary translations, we can account for quantifier and coordinator scope ambiguities in syntactically unambiguous sentences without recourse to multiple semantic rules, and are able to separate the disambiguation process from the operation of the parser-translator. We have implemented simple recursive descent and left-corner parsers to demonstrate the practicality of our approach.", "references": ["49231bc48d77378c4b1c8cdb576d043b0b10be7f", "ae931723114d0ad1fbee600a3dc1a7a1dcfe0e15", "99c1ef49842f72ad07b5283ec41b924ddc4e6959", "4239dd1dedad10f871bd1139f2eccc1d72d51185", "678f1a5e770260205a4762861fb158c2981d2fc3", "006cce6ef401f84f39aeed27d5196acdb990c7da", "7736f8a436bf8ab8b1df712074a6df7ebfed6b6f", "5d124e2eba8166dd22084dfb25eedfcea800d468", "98165270bacb2e0c584f5d7571cce46248046755", "8120c0a67f38b9727bdd15dbe47090391b9ccb42"], "page_rank": 8.210180623973726e-05}, {"id": "2a71434371bb4a11a5fc78d7e5c3723c273fff39", "title": "Classes of Languages and Linear-Bounded Automata", "authors": ["S.-Y. Kuroda"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Classes of Languages and Linear-Bounded Automata\" by S.-Y. Kuroda", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "2c05d131576d01668ef01156ac73e52c3152384a", "title": "Basic objects in natural categories", "authors": ["Eleanor Rosch", "Carolyn B. Mervis", "Wayne D. Gray", "David M. Johnson", "Penny Boyes-Braem"], "date": 1976, "abstract": "Abstract Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language.", "references": ["e6584eb10d5b8ff0a0a3630d04a9d1ba9c015379", "2bca55f23d377c66dbaa7361cbb8ab6b21f8ae31", "64b6e5fe61f22988557f08e1612b51d2a8c46b0c", "135b7a71eb5f3eb6f11b69511e1763f78080786c", "4140e7481c2599604b14fcd04625274022583631", "026aef2a937cb0ed2ab0d3a433061d1bd3e73c3e", "6c7e40aaa700ba03133bee8028f34b5d88e148c8", "8e04eb816b68f288b768621523fcb367a3ffdc32", "fcd1453f7dda96718b4092354f2a19069041b3a8"], "page_rank": 4.926108374384236e-05}, {"id": "5f87bb3dc34be8c90498b31c3bdbf43f539f1a8f", "title": "New Instruments and Methods for Speech Analysis", "authors": ["Toshiyuki Sakai", "Sei\u2010ichi Inoue"], "date": 1960, "abstract": "Some important features involved in the zero\u2010crossing waves of the signal generated by the human voice are described. The method of analyzing the vocal sound by a Sonagraph using the natural wave forms is compared with our new method using the zero\u2010crossing waves. Next, three kinds of devices are described which have been developed by the authors to analyze speech sounds. These include devices to analyze automatically the zero\u2010crossing intervals, to display the zero\u2010crossing waves in three\u2010dimensional form, and to make a visible pattern of the zero\u2010crossing waves. In this paper, as examples of an important application of these devices, the following are described: the results of analysis of vowels in the Japanese language, some characteristics of a number of groups of consonants (after the separation of the consonant and the vowel parts), and the discrimination of the nasal consonants, [m] and [n].", "references": [], "page_rank": 0.0004926108374384236}, {"id": "3a8143965c0c15028a7959c4a017651e3b50ddac", "title": "Three Theorems on Phrase Structure Grammars of Type 1", "authors": ["Peter S. Landweber"], "date": 1963, "abstract": "It is shown that the class of languages generated by type 1 phrase structure grammars is not enlarged by allowing end markers, that this class is closed under the operation of intersection, and that those languages representable by linear bounded automata belong to this class.", "references": ["8a29d437f9d45aed60e3052dd60c93129292ead4"], "page_rank": 0.0002463054187192118}, {"id": "8a29d437f9d45aed60e3052dd60c93129292ead4", "title": "On formal properties of simple phrase structure grammars", "authors": ["Yehoshua Bar-Hillel", "Micha A. Perles", "Eliahu Shamir"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"On formal properties of simple phrase structure grammars\" by Yehoshua Bar-Hillel et al.", "references": [], "page_rank": 0.0007389162561576354}, {"id": "ad9ac8938d230ef41cf2aa6a795743c8b1520200", "title": "A model of inexact reasoning in medicine", "authors": ["Edward H. Shortliffe", "Bruce G. Buchanan"], "date": 1975, "abstract": "Abstract Medical science often suffers from having so few data and so much imperfect knowledge that a rigorous probabilistic analysis, the ideal standard by which to judge the rationality of a physician's decision, is seldom possible. Physicians nevertheless seem to have developed an ill-defined mechanism for reaching decisions despite a lack of formal knowledge regarding the interrelationships of all the variables that they are considering. This report proposes a quantification scheme which attempts to model the inexact reasoning processes of medical experts. The numerical conventions provide what is essentially an approximation to conditional probability, but offer advantages over Bayesian analysis when they are utilized in a rule-based computer diagnostic system. One such system, a clinical consultation program named mycin , is described in the context of the proposed model of inexact reasoning.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "e3d442b3881e75eb4c8d60bfc8591108ebcd9d20", "title": "Preservation of Languages by Transducers", "authors": ["Seymour Ginsburg", "Gene F. Rose"], "date": 1966, "abstract": "Abstract : A linear bounded transducer (pushdown transducer) is a linear bounded automaton (pushdown automaton) with outputs. Answers to the following two problems are derived: (1) If S is a linear bounded transducer or a pushdown transducer, and L is context sensitive, context free, or regular; is S(L) some well known type of set (2) Does there exist a decision procedure to determine for arbitrary sets L sub 1 and L sub 2, both context sensitive or both context free, whether there exists a linear bounded transducer (pushdown transducer) which maps (a) L sub 1 onto L sub 2 or (b) L sub 1 into L sub 2 nontrivially. (Author)", "references": ["735503201ac1ed817039f9d052892d83e6d48783", "3a8143965c0c15028a7959c4a017651e3b50ddac", "8a29d437f9d45aed60e3052dd60c93129292ead4"], "page_rank": 8.210180623973726e-05}, {"id": "54f35b4edba6ddee8ce2eac489bde78308e3e708", "title": "Forward Reasoning and Dependency-Directed Backtracking in a System for Computer-Aided Circuit Analysis", "authors": ["Richard M. Stallman", "Gerald J. Sussman"], "date": 1977, "abstract": "Abstract We present a rule-based system for computer-aided circuit analysis. The set of rules, called EL, is written in a rule language called ARS. Rules are implemented by ARS as pattern-directed invocation demons monitoring an associative data base. Deductions are performed in an antecedent manner, giving EL's analysis a catch-as-catch-can flavour suggestive of the behavior of expert circuit analyzers. We call this style of circuit analysis propagation of constraints. The system threads deduced facts with justifications which mention the antecedent facts and the rule used. These justifications may be examined by the user to gain insight into the operation of the set of rules as they apply to a problem. The same justifications are used by the system to determine the currently active data-base context for reasoning in hypothetical situations. They are also used by the system in the analysis of failures to reduce the search space. This leads to effective control of combinatorial search which we call dependency-directed backtracking.", "references": ["01da4c5f9486e850cb4f112a9c059a87ad16d3ed", "83f054294ba2726d02aa03e471da773c3383b146", "567a07403759d35092263bbc437f6cd59c4e66a1", "da9fc17631b3b3a05ac98f3af39dfcff5e895823", "e97795382386ecd24300f3a6449ed5732b200bfa", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "2cfe1adca4b3fd0b20eb37260d7013303f112111", "a0adea7988254f3d0740b587334c8ca6357cdd8b", "b62608c716819965f2755759ce3a7edb8a93829f", "694d6894a9223144e967f2511d327260632b6995"], "page_rank": 0.0002463054187192118}, {"id": "37a5b98e31d8f1d0b6739c5f506df570d3c3536e", "title": "Seven principles of surface structure parsing in natural language", "authors": ["John Kimball"], "date": 1973, "abstract": "Abstract In generative grammar there is a traditional distinction between sentence acceptability, having to do with performance, and sentence grammaticality, having to do with competence. The attempt of this paper is to provide a characterization of the notion \u2018acceptable sentence\u2019 in English, with some suggestions as to how this characterization might be made universal. The procedure is to outline a set of procedures which are conjectured to be operative in the assignment of a surface structure tree to an input sentence. To some extent, these principles of parsing are modeled on certain parsing techniques formulated by computer scientists for computer languages. These principles account for the high acceptability of right branching structures, outline the role of grammatical function words in sentence perception, describe what seems to be a fixed limit on short-term memory in linguistic processing, and hypothesize the structure of the internal syntactic processing devices. The operation of various classes of transformations with regard to preparing deep structures for input to parsing procedures such as those outlined in the paper is discussed.", "references": [], "page_rank": 0.00015247478301665492}, {"id": "b78b881dec3334abf5b8c6390c40a2d93a177294", "title": "Sentence Disambiguation by a Shift-Reduce Parsing Technique", "authors": ["Stuart Merrill Shieber"], "date": 1983, "abstract": "Native speakers of English show definite and consistent preferences for certain readings of syntactically ambiguous sentences. A user of a natural-language-processing system would naturally expect it to reflect the same preferences. Thus, such systems must model in some way the linguistic performance as well as the linguistic competence of the native speaker. We have developed a parsing algorithm--a variant of the LALR(I) shift-reduce algorithm--that models the preference behavior of native speakers for a range of syntactic preference phenomena reported in the psycholinguistic literature, including the recent data on lexical preferences. The algorithm yields the preferred parse deterministically, without building multiple parse trees and choosing among them. As a side effect, it displays appropriate behavior in processing the much discussed garden-path sentences. The parsing algorithm has been implemented and has confirmed the feasibility of our approach to the modeling of these phenomena.", "references": ["37a5b98e31d8f1d0b6739c5f506df570d3c3536e", "aa1bf0c2777c84ca10ceb252cd83b9e88699ff0b", "006cce6ef401f84f39aeed27d5196acdb990c7da", "21baf3290e4a4dff364ac5526a07984fe9c5d05f", "00d975899150b364784a7fc55560d789dd87aa25", "5e32836ba775c74315b3d215a7c4fa81f97700a9", "deaf8fe4bef6b6e026bb6cef75c65a030dc0ce84", "d4c7c0e13d98cc3ea23158379768a5f9bd3b55d3"], "page_rank": 8.210180623973726e-05}, {"id": "bd459cc59b09e612eeec5327d0690d1508ffe362", "title": "A theory of syntactic recognition for natural language", "authors": ["Mitchell P. Marcus"], "date": 1978, "abstract": "Abstract : Assume that the syntax of natural language can be parsed by a left-to-right deterministic mechanism without facilities for parallelism or backup. It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter. (Author)", "references": ["4deb324dca009d4a0ee096f6238bfc9b608594e1", "328e4ac472b433de5e249939644383dd49ca6999", "10f7507b8408bf35125b8e04254ad890c8d45e1d", "09550accec47459a61fe1710a0a32c2ec22449bd", "7aa9a62943c594ca94289c610b55566d404f8320", "170b042592c2fadf717e25fe2cbfe02a593288d9", "16c762445f11fa2020994918dc4f93e76264df17", "b85a31b0a5ff247c74ad53df67d27043a54c760d", "a0e3dd2480d2a3e90e0f04b6362a676cb047b85a", "7c57e3f5cfe4b550e470fdce4d73b07baa0a4ad3"], "page_rank": 8.210180623973726e-05}, {"id": "100f9867f10f15274d0fb168b02bc0929245f817", "title": "Kindersprache, Aphasie und allgemeine Lautgesetze", "authors": ["Rom{\\'a}n Jakobson"], "date": 1942, "abstract": "Roman Jakobson hat sich in seinem in deutscher Sprache verfassten und zuerst in Uppsala 1941 erschienenen Werk \u201eKindersprache, Aphasie und allgemeine Lautgesetze\u201c zum Ziel gesetzt zu beweisen, dass Kindersprache und Aphasie in die vergleichende Sprachwissenschaft einbezogen werden m\u00fcssen, da ihnen die gleichen Gesetze zugrunde liegen wie der Lautgeschichte aller V\u00f6lkersprachen. Es handelt sich also nicht um eine allgemeine Einf\u00fchrung in die Linguistik, sondern um eine spezielle Thematik.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "00d975899150b364784a7fc55560d789dd87aa25", "title": "The ATN and the sausage machine: Which one is baloney?", "authors": ["Eric Wanner"], "date": 1980, "abstract": "In a recent issue of Cognition, Lyn Frazier and Janet Dean Fodor proposed a new two-stage parsing model, dubbed the Sausage Machine (Frazier and Fodor, 1978). One of the major results which Frazier and Fodor bring forward in support of their proposal concerns a parsing strategy which, following Kimball (1973), they call Right Association. The center-piece of their argument concerns an interaction between this parsing strategy and another one, which they call Minimal Attachment. Frazier and Fodor (henceforth FF) provide interesting evidence that the language user makes tacit use of both strategies to resolve temporary syntactic ambiguities that arise during parsing. FF then proceed to argue that the existence of these strategies, as well as the apparent interaction between them, can be fully explained if we assume that the language user\u2019s parsing system is configured along the lines of the Sausage Machine. In FF\u2019s view, the Augmented Transition Network (ATN) runs a very poor second to the Sausage Machine, for according to FF\u2019s argument, it is impossible even to describe the two parsing strategies within the ATN framework. In effect then, FF are claiming that the Sausage Machine achieves explanation adequacy in this case while the ATN fails to reach the level of descriptive adequacy. These are strong and potentially important claims. If correct, they obviously provide grounds for pursuing parsing models built along the lines of the Sausage Machine rather than the ATN. However, when FF\u2019s arguments are examined at close range, the comparison between parsing systems comes out rather differently than they claim. In particular, it appears that the Sausage Machine explanation of Right Association and its interaction with Minimal Attachment is empirically incorrect. The inadequacy of this explanation completely cancels the Sausage Machine\u2019s ability to describe the interaction between strategies that FF have observed. This follows because", "references": [], "page_rank": 0.00015247478301665492}, {"id": "539e079e1c7a23b69f3c336103f9ad51769898b8", "title": "GEDANKEN\u2014a simple typeless language based on the principle of completeness and the reference concept", "authors": ["John C. Reynolds"], "date": 1970, "abstract": "GEDANKEN is an experimental programming language with the following characteristics. (1) Any value which is permitted in some context of the language is permissible in any other meaningful context. In particular, functions and labels are permissible results of functions and values of variables. (2) Assignment and indirect addressing are formalized by introducing values, called references, which in turn possess other values. The assignment operation always affects the relation between some reference and its value. (3) All compound data structures are treated as functions. (4) Type declarations are not permitted.\nThe functional approach to data structures and the use of references insure that any process which accepts some data structure will accept any logically equivalent structure, regardless of its internal representation. More generally, any data structure may be implicit; i.e. it may be specified by giving an arbitrary algorithm for computing or accessing its components. The existence of label variables permits the construction of co-routines, quasi-parallel processes, and other unorthodox control mechanisms.\nA variety of programming examples illustrates the generality of the language. Limitations and possible extensions are discussed briefly.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "07ee4a3d6e78326acdc77ea24ba61466eea31e19", "title": "Data structure models for programming languages", "authors": ["Peter Wegner"], "date": 1971, "abstract": "This paper introduces a class of models (information structure models) for characterizing computations in terms of the data structures to which they give rise during execution, shows how such models can be used to characterize automata, digital computers and programming languages, considers in some detail the data structures generated during the execution of programs in block structure languages, develops a model for a non-block structure language (SNOBOL 4) and indicates how information structure models may be used in the semantic definition and formal characterization of programming languages. Sections 1 and 2 discuss the reasons for studying the relation between data structures and programming languages, section 3 introduces the notion of an information structure model and considers the classification of interpreters, and section 4 shows how automata, computers and programming languages may be characterized as sequential information structure models. Section 5 underlines the importance of introducing cells and references as semantic primitives of computational models. Section 6 develops models of implementation of block structure languages, section 7 considers the limitations of stack structure, and section 8 considers the hardware realization of block structure implementation of the Burroughs B6500. Section 9 develops an information structure model for the non-block structure language SNOBOL 4, while section 10 briefly discusses information structure models of language definition and the use of information structure models in proofs that programs have certain property. A final subsection considers the relative merits of axiomatic definition versus implementation-dependent definition of programming languages.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "0e1f03b13cce8bd604ff76fcd325b548a26a5e40", "title": "A phonetic study of West African languages", "authors": ["Peter Ladefoged"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"A phonetic study of West African languages\" by Peter Ladefoged", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4436c947845aa1ee9041bc41a03c612d57494923", "title": "An implementation of backtracking for programming languages", "authors": ["Charles J. Prenner", "Jay M. Spitzen", "Ben Wegbreit"], "date": 1972, "abstract": "Primitive operations for backtracking and a technique for their implementation are described. The technique is applicable to all programming languages whose control structure is strictly hierarchical. A general stack model for such languages is presented and a realization of the backtracking primitives is described in terms of this model. The primitives give the user control over which assignments to variables will be 'undone' when backtracking. Moreover, their realization, at each point of choice, does not save the entire current machine state but only a relatively small amount of information sufficient to later reconstruct that machine state. Consequently, the implementation makes efficient use of storage and makes backtracking a practical programming device for dealing with complex problems.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "982368726d6b27fbb8ffffda7f1a06ea7bd1610c", "title": "Problems of phonemic interpretation I. Nasalized sounds in Yoruba", "authors": ["Berthe Siertsema"], "date": 1957, "abstract": "Abstract After an inquiry into the phonemic status of the nasalized consonants r , w , j as opposed to oral r , w , j the great number of Yoruba nasalized vowels is inspected. The expression that a certain oral vowel also \u201coccurs nasalized\u201d is shown to be dangerous for a clear insight into a system of nasalized vowels. To establish the number of nasalized vowel phonemes it is not enough to find minimal pairs in which an oral vowel is contrasted with its nasalized variety; the point is to find minimal pairs with mutually contrastive nasalized vowels. Thus Yoruba with its 7 oral vowel phonemes is shown to have not more than 4 nasalized ones.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "7a126228141a3467f2ea13cefd362c4956101133", "title": "Some controversial questions in phonological theory", "authors": ["Noam Chomsky", "Morris Halle"], "date": 1965, "abstract": "In the first issue of this journal, Fred W. Householder discussed two papers of ours which he found defective in various respects. We feel that the issues involved are important and deserve the fullest clarification. We will therefore discuss Householder's objections and the underlying issues in some detail, reiterating points that have been made in the aforementioned papers and elsewhere and making no attempt to avoid redundancy if this can contribute to clarity.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "4126239a4f3b89207c64a3d875fb41eb43d040b2", "title": "The Phoneme: Its Nature and Use", "authors": ["David Oldham. Jones"], "date": 1950, "abstract": "The Phoneme, first published in 1950, is now not only a standard work on its subject but comprises Daniel Jone's major and final expression of his theoretical attitudes. This is a reissue of the third edition (1967), which includes the appendix on the history and meaning of the term phoneme. It sets out in detail the author's conclusions about the nature and function of the phoneme, illustrating them from over 40 languages. The book continues to represent for the historian of language an important landmark in the development of linguistics.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "6c74e63edfd78486826a628ef40e09ab94de6380", "title": "The treatment of data types in EL1", "authors": ["Ben Wegbreit"], "date": 1974, "abstract": "In constructing a general purpose programming language, a key issue is providing a sufficient set of data types and associated operations in a manner that permits both natural problem-oriented notation and efficient implementation. The EL1 language contains a number of features specifically designed to simultaneously satisfy both requirements. The resulting treatment of data types includes provision for programmer-defined data types and generaic routines, programmer control over type conversion, and very flexible data type behavior, in a context that allows efficient compiled code and compact data representation.", "references": ["85827cf800d963c44edee1c79d9431cf46fdeef8"], "page_rank": 5.473453749315818e-05}, {"id": "07f4f7c0e01282f4852cd1c13d97048c1c64106f", "title": "The Modeling of Simple Analogic and Inductive Processes in a Semantic Memory System", "authors": ["Joseph D. Becker"], "date": 1969, "abstract": "In this paper w. present a general data structure for a semantic memory, and we give a definition of \"analogy\" between items of semantic information. We then construct an inductive process in which general laws are formulated and verified on the basis of observations of individual cases.", "references": ["20eb54d6d0159bb257564e632727aed369c9fd1c"], "page_rank": 7.037297677691766e-05}, {"id": "72b3682e166faa5c5ced5873934b20dd8f4c4d21", "title": "Research and applications: Artificial intelligence", "authors": ["Bertram Raphael", "Richard O. Duda", "Richard Fikes", "Peter E. Hart", "Nils J. Nilsson", "Perry W. Thorndyke", "B. Michael Wilber"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Research and applications: Artificial intelligence\" by Bertram Raphael et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "652670fa8e275a9a6fe399becd5194b63af4da6f", "title": "Reasoning Using Exclusion. An Extension of Clausal Form.", "authors": ["Josh D. Tenenberg"], "date": 1985, "abstract": "Abstract : In formalizing knowledge for common sense reasoning, one often needs to partition some domain. An instance of this from the block's world is the statement 'All blocks are one of held, on the table, or on another block.' Although we can write such an axiom in predicate calculus using the standard logical connectives, or in clause form as input to a resolution theorem prover, such representations are highly space inefficient. A generalized clause form is presented that allows for the compact representation of arbitrary partitions, along with a set of corresponding inference rules whose soundness and refutation completeness are proven. Additionally, an implementation of a subset of these rules is described that demonstrates their utility with certain kinds of common sense rule bases. (Author)", "references": [], "page_rank": 0.0002463054187192118}, {"id": "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "title": "The magical number seven plus or minus two: some limits on our capacity for processing information.", "authors": ["George Abram Miller"], "date": 1956, "abstract": "First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.", "references": ["2e8136581d934e18eb17d124f0224a32b01272d5", "10b9c3b2923e952df0b7bd68e52d1bb56081a27b", "28628e1a8bf31402113cef0e87fef9c23c1c4d3e", "a546750bd366343354b745de9476d3cc52025a93", "5322a485d71098e245e4c7c36cfdb7f5fce2858c", "352a7df7e022c1760b496b6b3e1d39a8369ad5c4", "5eec4e79df13c612a02b01185e1f72143db81ae5", "6bda9f1be3714d2a9866d4714b7f20e15c5a6e74", "5e73f775ea677849942c2b299dffa75f69cfca42"], "page_rank": 0.0001313628899835796}, {"id": "451fa303b16a007106d52061d37269822ed9c705", "title": "More on the English comparative", "authors": ["Rodney Huddleston"], "date": 1967, "abstract": "i. In the last few years there have been three papers published devoted wholly or largely to a study of the English comparative construction: Lees (I96I); Smith (I96I); and Pilch (i965)1 The focus of attention has been the structure of what, following Pilch, I shall call the comparative expansion (the constituent introduced by than or as), rather than the morphology of the compared adjective or adverb. It is not surprising that so much interest should be shown in this construction at a time when one of the main concerns of linguists is to make their syntactic descriptions maximally explicit. For as this was not the aim of traditional grammarians, they had relatively little to say on the structure of the comparative expansion, implicitly assuming doubtless that such ungrammaticalities as (i) *John bought a bigger car than Mary went home yesterday can be ruled out by commonsense, semantics, or logic. Nor is it surprising that two of the papers, those by Lees and Smith, should have been written from the standpoint of transformational grammar. It is here that the greatest emphasis has been placed on the need to distinguish between representations of the deep grammar on the one hand and of the surface grammar on the other, and the comparative construction provides clear and compelling evidence for such a distinction witness the many, and familiar, ambiguities of the type (2) John likes Peter more than Bill, where in one interpretation the comparison is between how much John likes Peter and how much he likes Bill, and in another between how much John likes Peter and how much Bill likes Peter. In pursuing here the description of English comparatives, I shall, obviously, draw on what my predecessors have written especially Lees and Smith. Pilch, who makes no reference to the work of Lees, I find somewhat less helpful. This is partly because some rather important distinctions in his classification are based on highly questionable decisions as to grammaticality. Thus he assigns (3) and (4) to different primary classes on the grounds that more . . . than allegedly commutes with as ... as and less. . . than in (3) but not in (4):", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "9cf159a7e67143dcda48637391f96ee0d6f7ab36", "title": "Toward a mathematical semantics for computer languages", "authors": ["Dana S. Scott", "Christopher Strachey"], "date": 1971, "abstract": "Compilers for high-level languages aTe generally constructed to give the complete translation of the programs into machme language. As machines merely juggle bit patterns, the concepts of the original language may be lost or at least obscured during this passage. The purpose of a mathematical semantics is to give a correct and meaningful correspondence between programs and mathematical entities in a way that is entirely independent of an implementation. This plan is illustrated in a very elementary way in the introduction. The first section connects the general method wi th the usual idea of state transformations. The next section shows why the mathematics of functions has to be modified to accommodate recursive commands. Section 3 explains the modifi\u00ad cation. Section 4 introduces the environments for handling variables and identifiers and shows how the semantical equations define equivalence of programs. Section 5 gives an exposition of the new type of mathematical function spaces that are required fOl the semantics of procedures when these are allowed in assignment state\u00ad ments. The conclusion traces some of the background of the project and points the way to future work.", "references": ["a3ffc76d9d3e310f8ae4248d4b43d60253f2067c"], "page_rank": 5.473453749315818e-05}, {"id": "97e21875955ccc0b18a8a89b518ffdf73a0da410", "title": "Thoughts about Animate and Inanimate Objects", "authors": ["Rochel Gelman", "Elizabeth S. Spelke"], "date": 1981, "abstract": "In a plasma display panel, of a conventional type or of the planar type, a drive system consists of an array of paired, parallel drive lines, both of which may be on the same side of the enclosed gas atmosphere, as with the planar panel, or disposed on opposite sides of the enclosed gas atmosphere, and a second array, orthogonal to the first, of paired parallel write/erase trigger lines. The parallel drive lines are driven continuously with a sustain voltage drive signal. The trigger lines are selectively driven in conjunction with a selected pair of the paired drive lines to write or erase a selected display element.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "f1cb4346fd84d15169c57828fd62de64ea2fa0fb", "title": "Grammatical transformations and sentence comprehension in childhood and adulthood", "authors": ["Dan Isaac Slobin"], "date": 1966, "abstract": "Children and adults verified sentences of four grammatical types\u2014\u201ckernel,\u201d passive, negative, and passive negative\u2014with respect to pictures. The pictures presented situations which were either reversible, in that the object of action could also serve as the subject, or nonreversible, in that the object could not normally serve as the subject. Chomsky's syntactic competence model correctly predicted that passives would take more time to evaluate than kernels, and passive negatives more time than negatives; but semantic and psychological factors are required to explain the finding that syntactically simple negatives took more time than relatively more complex passives. Making sentences nonreversible largely washed out the difference in syntactic complexity between active and passive sentences, making passives about as easy as kernels, and passive negatives about as easy as negatives. It is argued that nonreversibility facilitates comprehension of passive (both affirmative and negative) sentences in that, although the normal subject-object order is reversed, it is still clear which of the two nouns is subject and which object. The syntactic theory also does not account for an obtained interaction between truth value and affirmation-negation. All of the factors considered\u2014syntactic, semantic, and pragmatic\u2014are important in accounting for the performance of S s as young as six.", "references": [], "page_rank": 0.00012510751427007583}, {"id": "06600e1ca9451d81e89d078a924184683ace9196", "title": "MH-1, a computer-operated mechanical hand", "authors": ["Heinrich A. Ernst"], "date": 1962, "abstract": "MH-1 is a motorized and sensitized servomanipulator operated by the TX-O computer at the Massachusetts Institute of Technology. It serves as an experimental vehicle to explore the feasibility of direct relations between a digital computer and the physical world with which this computer is concerned. Usually, a human interpreter stands between the computer and the physical world. Instead, the TX-O computer in the MH-1 system is programmed to perform by itself some of the functions normally assigned to the human intermediary; namely, to perceive the world, to appreciate it, and to determine a reasonable course of action after a goal has been specified for the hand. The data processing tools used are, rather than numerical operations on quantitative signals, pattern recognition and simulation of higher cognitive processes such as awareness and understanding. This paper describes some of the experiments performed with MH-1 and the mechanisms upon which the capabilities of MH-1 are based.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "c3d04a4e021fd0b90529f89c9cf2c43e4c436b58", "title": "The clowns microworld", "authors": ["Robert F. Simmons"], "date": 1975, "abstract": "About fifteen years of active research in natural language question-answering systems has provided reasonably concise and elegant formulations of computational semantics for understanding English sentences and questions about various microworlds. These include the Woods Lunar Data Base, the Winograd world of a pictured hand and blocks, the Heidorn world of a fueling station, the Hendrix, Slocum, Thompson world of transactions, John Seely Brown's power circuit and Schank's sketches of motivated humans. (See Woods et al 1972, Winograd 1972, Hendrix et al 1973, Heidorn 1972, Schank 1975 and Brown et al 1974.) In each of these worlds, a natural language processor is able to understand an ordinary subset of English and use it conversationally to accept data and to respond to commands and questions.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "8120c0a67f38b9727bdd15dbe47090391b9ccb42", "title": "Some Transformational Extensions of Montague Grammar", "authors": ["Barbara H. Partee"], "date": 1973, "abstract": "Richard Montague\u2019s work on English, as represented in Montague (1970a), (1970b), (1972), represents the first systematic attempt to apply the logician\u2019s methods of formal syntax and semantics to natural language. With few exceptions,1 linguists and logicians had previously been agreed, although for different reasons, that the apparatus developed by logicians for treating the syntax and semantics of artificially constructed formal languages, while obviously fruitful within its restricted domain, was not in any direct way applicable to the analysis of natural languages. Logicians seem to have felt that natural languages were too unsystematic, too full of vagueness and ambiguity, to be amenable to their rigorous methods, or if susceptible to formal treatment, only at great cost.2 Linguists, on the other hand, emphasize their own concern for psychological reality, and the logicians\u2019 lack of it, in eschewing the logicians\u2019 approach: linguists, at least those of the Chomskyan school, are searching for a characterization of the class of possible human languages, hoping to gain thereby some insight into the structure of the mind, and the formal languages constructed by logicians appear to depart radically from the structures common to actual natural languages.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "5e32836ba775c74315b3d215a7c4fa81f97700a9", "title": "LR Parsing", "authors": ["Alfred V. Aho", "Stephen C. Johnson"], "date": 1974, "abstract": "The LR syntax analysis method is a useful and versatile technique for parsing deterministic context-free languages in compiling applications. This paper provides an informal exposition of LR parsing techniques emphasizing the mechanical generation of efficient LR parsers for context-free grammars. Particular attentmn is given to extending the parser generation techniques to apply to ambiguous grammars.", "references": [], "page_rank": 0.00019352568613652358}, {"id": "98165270bacb2e0c584f5d7571cce46248046755", "title": "Translating English into Logical Form", "authors": ["Stanley J. Rosenschein", "Stuart M. Shieber"], "date": 1982, "abstract": "A scheme for syntax-directed translation that mirrors compositional model-theoretic semantics is discussed. The scheme is the basis for an English translation system called PATR and was used to specify a semantically interesting fragment of English, including such constructs as tense, aspect, modals, and various lexically controlled verb complement structures. PATR was embedded in a question-answering system that replied appropriately to questions requiring the computation of logical entailments.", "references": ["eb707d747ccaf5136e2b77d18fe275ac7faf0375", "a677f2d52d1c0c3e762f0942c7ddac886462d24a", "8ec35513fabc515f6e5b46859458a339f4a45cc6", "4145eea44469b42682cc1feb2665c20bfbbf8ec2", "885194cadd5989d077f5f25b3dc0c42d0d55ce40", "c402b73ea2295eac620d41442a9a10858e315020"], "page_rank": 6.157635467980295e-05}, {"id": "5d124e2eba8166dd22084dfb25eedfcea800d468", "title": "A Formalism for the Description of Question Answering Systems", "authors": ["Camilla Schwind"], "date": 1978, "abstract": "The following article presents a formalism for the description of a natural language based intelligent system. The meaning of natural language texts is to be represented by a state logic. This is an extension of predicate logic by special operators, which are applied to formulae and make their truth value dependent on the state of the world in which the formula is evaluated. The extension of the non-logical symbols depends also on the state of the world and it may change when a state changes. Natural language texts are described syntactically by a formal grammar, which is an extension of a CHOMSKY-grammar. The alphabet consists of complex symbols and the structure of these symbols is given by special rules. The derivation rules of our grammar are applied to symbols in different way which constitutes an extension of the usual method. The application of a rule is governed by the structure of the symbols and on applying one rule, we can derive a set of sentences. Natural language texts are translated into state logic formulae by special functions which are associated with the production rules. These functions depend on the syntactic structure of the sentences and on the world in which the sentences are evaluated. We will give a detailed example for the application of the whole formalism.", "references": ["bb20f121c979b535bbeade5ac06676d627d4ad7d", "5a31aac1e2bbe12be2e2534eff2c4f8017ea0f48", "09550accec47459a61fe1710a0a32c2ec22449bd", "eb233638ad0b99e3c66c636bef814ae94131ab5f", "fbcd27ad3515d71b4622ee24e1d644a77646f102", "1efcd7be3b52e46de800e06e17268ce7d535d9a7", "9fc77941297522cc420ce9292193dd04ed2ed1af"], "page_rank": 6.157635467980295e-05}, {"id": "fcd1453f7dda96718b4092354f2a19069041b3a8", "title": "Cognitive representations of semantic categories.", "authors": ["Eleanor Rosch"], "date": 1975, "abstract": "Semantic Scholar extracted view of \"Cognitive representations of semantic categories.\" by Eleanor Rosch", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "694d6894a9223144e967f2511d327260632b6995", "title": "A PROGRAM FOR THE DESIGN OF PROCUREMENT SYSTEMS", "authors": ["Michael Bosyj"], "date": 1976, "abstract": "Computer technology has had a limited success in producing useful business applications. Management systems seldom meet users'' requirements, are often inappropriate to an application, and are frequently abandoned. But why? Business lacks expertise in the application of computers. Managers who are expert in solving business problems find it difficult to specify formal procedures for the solution of these problems. It is not surprising that programmers who work from poorly defined specifications produce poorly written software. The computer industry has provided only limited support in business applications. Application packages are seldom appropriate to a problem and are often misapplied. Misapplication is the principle reason for their failure in practice situations. Improvements are certainly possible. The manager could be supplied with a system that assists in the design of an application. This system could help the manager specify his requirements by providing him with a framework for thinking about issues relevant to the design of a particular application. The manager might then be better equipped to select a commercial package or to guide in the design of his implementation. This thesis describes a prototype version of such a system. PROCTOR is a program that assists in the design of a hierarchical planning and control system for a procurement firm. PROCTOR is implemented as an \"unstructured\" questionnaire. It guides the user in investigating various aspects of a problem while giving him complete freedom in deciding how and when to supply answers to questions. It allows him to change and skip answers whenever he desires. PROCTOR is implemented in OWL, a system for representing and processing conceptual knowledge. It uses the OWL data base to represent procedures for the questionnaire and to store data accumulated during the iteration. This representation makes possible the presentation of an English-like problem description, various evaluations and the reasons for the evaluations.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "8e04eb816b68f288b768621523fcb367a3ffdc32", "title": "A First Language", "authors": ["R. W. Brown"], "date": 1973, "abstract": "Semantic Scholar extracted view of \"A First Language\" by R. W. Brown", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "026aef2a937cb0ed2ab0d3a433061d1bd3e73c3e", "title": "General Principles of Classification and Nomenclature in Folk Biology", "authors": ["Dennis Eugene. Breedlove", "Peter H. Raven"], "date": 1973, "abstract": "Since about 1954, modern field research has been carried out by a number of ethnographers and biologists in an effort to understand more fully the nature of folk biological classification. Much of this work has been devoted to studies dealing with the naming and classification of plants and animals in non-Western societies. It has now become apparent that several important and far reaching generalizations can be formulated which promise to throw considerable light on prescientific man's understanding of his biological universe.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "6c7e40aaa700ba03133bee8028f34b5d88e148c8", "title": "The Processing of Information and Structure", "authors": ["Wendell R. Garner"], "date": 1974, "abstract": "A process for forming an optical sound track which comprises applying a nitrogen containing heterocyclic compound in which at least one of the nitrogen atoms is connected to a group having 6 or more carbon atoms to form a quaternary salt to a sound track area of a multilayer color photographic material, after color development but prior to a bleaching step, whereby an optical sound track can be easily prepared on a conventional multilayer color photographic material.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "title": "Flexibility and Efficiency in a Computer Program for Designing Circuits", "authors": ["D. McDermott"], "date": 1976, "abstract": "Abstract : This report is concerned with the problem of achieving flexibility (additivity, modularity) and efficiency (performance, expertise) simultaneously in one AI program. It deals with the domain of elementary electronic circuit design. The proposed solution is to provide a deduction-driven problem solver with built-in control-structure concepts. This problem solver and its knowledge base in the application areas of design and electronics are described. The program embodying it is being used to explore the solution of some modest problems in circuit design. It is concluded that shallow reasoning about problem-solver plans is necessary for flexibility, and can be implemented with reasonable efficiency. (Author)", "references": [], "page_rank": 0.00020935960591133003}, {"id": "d4c7c0e13d98cc3ea23158379768a5f9bd3b55d3", "title": "Is the human sentence parsing mechanism an ATN?", "authors": ["Janet D. Fodor", "Lyn Frazier"], "date": 1980, "abstract": "Semantic Scholar extracted view of \"Is the human sentence parsing mechanism an ATN?\" by Janet D. Fodor et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "21baf3290e4a4dff364ac5526a07984fe9c5d05f", "title": "Natural language parsing: A new characterization of attachment preferences", "authors": ["Fernando Carlos Neves Pereira"], "date": 1985, "abstract": "Abstract : Several authors have tried to model attachment preferences for structurally ambiguous sentences that cannot be disambiguated from semantic information. These models lack rigor and have been widely criticized. By starting from a precise choice of parsing model, it is possible to give a simple and rigorous description of Minimal Attachment and Right Association that avoids some of the problems of other models.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "170b042592c2fadf717e25fe2cbfe02a593288d9", "title": "semantic categories of nominals for conceptual dependency analysis of natural language.", "authors": ["Sylvia Weber Russell"], "date": 1972, "abstract": "A system for the semantic categorization of conceptual objects (nominals) is provided. The system is intended to aid computer understanding of natural language. Specific implementations for \"noun-pairs\" and prepositional phrases are offered.", "references": ["27845db178cf77d366230209452a6f7a99b6dc0c", "e6ae33d983b56ca4a692e52f387e54a913b946f5"], "page_rank": 6.157635467980295e-05}, {"id": "deaf8fe4bef6b6e026bb6cef75c65a030dc0ce84", "title": "A competence-based theory of syntactic closure", "authors": ["Marilyn Ford", "Joan Bresnan", "Ronald M. Kaplan"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"A competence-based theory of syntactic closure\" by Marilyn Ford et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "328e4ac472b433de5e249939644383dd49ca6999", "title": "Wait-and-See Strategies for Parsing Natural Language", "authors": ["Mitchell P. Marcus"], "date": 1974, "abstract": "The intent of this paper is to convey one idea central to the structure of a natural language parser currently under development, the notion of wait-and-see strategies. This notion will hopefully allow the recognition of the structure of natural language input by a process that is deterministic and \"backupless\", that can have strong expectations but .still be immediately responsive to the actual structure of the input. The notion is also discussed as a peradigm for recognition processes in general. Working Papers are informal papers intended for internal use.", "references": ["bb20f121c979b535bbeade5ac06676d627d4ad7d", "cf98020a89f7edcba554dd8c2e3ad7b092cdc4ae", "e1596adff54c958816e5a10d33e69f13a4d8afc5", "547a664cf042af7ce4f171a65577441833ba673e", "0c86cc944de3b822f02d7004803347f9728888d0", "c35980a49350197aea411f9d7926d86e2a05905f", "c90f0ebc7f74bb5ca6d7e79a138ab697c3740a89", "a7cb5868c2142aea345cb0dcb57ae0dd421cc3aa"], "page_rank": 6.157635467980295e-05}, {"id": "b85a31b0a5ff247c74ad53df67d27043a54c760d", "title": "LINGOL: a progress repor", "authors": ["Vaughan R. Pratt"], "date": 1975, "abstract": "A new parsing algorithm is described. It is intended for use with advice-taking (or augmented) phrase structure grammars of the type used by Woods, Simmons. Heidorn and the author. It has the property that it is guaranteed not to propose a phrase unless there exists a continuation of the sentence seen thus far, in which the phrase plays a role in some surface structure of that sentence. The context in which this algorithm constitutes a contribution to current issues in parsing methodology is discussed, and we present a case for reversing the current trend to ever more complex control structures in natural language systems.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "7aa9a62943c594ca94289c610b55566d404f8320", "title": "Augmented Transition Networks as Psychological Models of Sentence Comprehension", "authors": ["Ronald M. Kaplan"], "date": 1971, "abstract": "This paper describes the operation of an augmented recursive transition network parser and demonstrates the natural way in which perceptual strategies, based on the results of psycholinguistic experimentation, can be represented in the transition network grammatical notation. Several illustrative networks are given, and it is argued that such grammars are empirically justified and conceptually productive models of the psychological processes of sentence comprehension.", "references": ["09550accec47459a61fe1710a0a32c2ec22449bd", "c718246790a35cd01c09859be9f36d31aa8df37b", "27e16f13ce627631900f6354ab04a71c07219326", "4cf962d84d2483a238c5c6a61e24eda48b3a11eb", "733986b7f0798a69e8b535d0e1140785830aef50", "5c0f99e72bd539171a4dedcffd0e0a424ad3aad6", "c9cfc85a98014a8c638b1bc6608f5405b4b8c726", "126eeb7ed23c564673c093b9c1a9d1730bb48b2e", "ccea36c0c514eb4cbd1e7e7572cb454feaad1f40", "49a3da48d0e6def2245985145cc2a28eb3ed49d5"], "page_rank": 6.157635467980295e-05}, {"id": "735503201ac1ed817039f9d052892d83e6d48783", "title": "Some Recursively Unsolvable Problems in ALGOL-Like Languages", "authors": ["Seymour Ginsburg", "Gene F. Rose"], "date": 1963, "abstract": "[ntroduct~ion In [5] the method of generation of the constituent parts of the algorithmic language ALGoI~ was abstracted. This gave rise to a family of \"ALGoL-like\" languages and their constituent parts, the latter called \"definable\" sets. A par-ticutar subclass of the definable sets, the \"sequentially definable\" sets, was then introduced, and a number of results about the definable and the sequentially definable sets proved. (For example, it was shown that the definable sets are identical to the context free phrase structure languages of Chomsky.) In [6] the effect of a number of operations on definable and sequentially definable sets was studied. Among other facts it was demonstrated that both complete sequential machines and generalized sequential machines transform definable sets to definable sets (the companion result for sequentially definable sets being false). In [1] several questions about definable sets were proved recursively unsolvable, that is, there are no algorithms for deciding the answers to these questions. The purpose of the present paper is to prove that two \"natural\" questions about definable and sequentially definable sets are recursively unsolvable. More precisely, we shall show that each of the following questions is recursively un-solvable. (1) Given a definable set, is it sequentially definable? (2) Given two (sequentially) definable sets L1 and L2 ~, (a) does there exist a complete sequential machine which maps L1 onto L2 (into L2)? (b) does there exist a generalized sequential machine which maps L~ onto L~ (into L2 so that the image of L~ is infinite if L~ is infinite)? The reeursive unsolvability of (2b) may be interpreted as saying that if a generalized sequential machine is a faithful model of a translating program and if definable sets are the constituent parts of all possible programming languages, then there is no mechanical procedure for deciding whether of two given programming languages there is a translation program converting one language into the other in a nontrivial way. The basic material and notation for definable and sequentially definable sets are now presented. The reader is referred to [5] for additional details as well as motivation.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "567a07403759d35092263bbc437f6cd59c4e66a1", "title": "Heuristic techniques in computer-aided circuit analysis", "authors": ["Gerald J. Sussman", "Richard M. Stallman"], "date": 1975, "abstract": "A new kind of circuit analysis program, EL, is presented. Whereas other circuit analysis systems rely on classical, formal, analysis techniques, EL employs heuristic \"inspection\" methods to solve rather complex dc bias circuits. These techniques also give EL the ability to explain any result in terms of its own qualitative reasoning processes. EL's reasoning is based on the concept of a \"local one-step deduction\" augmented by various \"teleological\" principles and by the concept of a \"macro-element.\" Several annotated examples of EL in operation and an explanation of how it works are presented. Also how EL can be extended in several directions, including sinusoidal steady-state analysis is shown. Finally, the possible implications of this work for engineering education and computer-aided design technology are discussed briefly. EL is significant not only as a novel approach to circuit analysis but also as an application of Artificial Intelligence techniques to a new and interesting domain.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "64b6e5fe61f22988557f08e1612b51d2a8c46b0c", "title": "PROCESSING OF THE STIMULUS IN IMAGERY AND PERCEPTION", "authors": ["Sydney Joelson Segal"], "date": 1971, "abstract": "Publisher Summary This chapter explains the processing of the stimulus in imagery and perception. Perception is defined as an experience that occurs in response to a physical stimulus; however, an image or hallucination is defined as a qualitatively similar subjective experience that occurs when there is no physical stimulus. There is sharpness in the perception of edges and boundary changes, which gives the qualities of clarity and focus to perception. Attempts to compare imagery to perception have focused on the extreme situations. If an image is the same as a perception, then it should show the after effects. Many psychologists assumed that if an image was a weaker replica of a perception, it should have been possible to reinstate a total perceptual experience through imagery. Researchers have believed that hypnotized subjects could be induced to perform astounding feats. Attempts to induce an illusion by asking the subjects to image the inducing background around a real test figure, or vice versa, are sometimes successful.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "135b7a71eb5f3eb6f11b69511e1763f78080786c", "title": "Influence of imaged pictures and sounds on detection of visual and auditory signals.", "authors": ["Sheldon J. Segal", "V Fusella"], "date": 1970, "abstract": "The present study compared sensitivity for auditory and visual signals in a simple detection task and in a related task in which ,9 was also imaging mental pictures and sounds. Sensitivity (rf') was reduced during imagery; within the imaging conditions, it was smaller when image and signal were both auditory or both visual than for cross-modal conditions and smaller with unfamiliar than familiar images. Likelihood ratio (Lx) was also smaller in the isomodal imaging conditions, as there were more visual false alarms during visual imagery and more auditory false alarms during auditory imagery. The data are not consistent with the assumption that d' is lower during imagery due to distraction; they do not entirely fit a channel competition model, but suggest that imagery functions as an internal signal which is confused with the external signal. Perky's (1910) effect has been difficult to explain: she found that if Os were asked to describe their images of common objects while dim facsimiles of the objects were presented before them, they reported only an \"imagery,\" not a \"perceptual,\" experience. This finding seemed paradoxical: in ordinary situations, imagery can be distinguished from real stimuli virtually 100% of the time; yet Perky's Os confused external stimuli with the images they were describing and seemed unable to discriminate the real physical signals. It is possible to explain the seeming inconsistency between Perky's experiment and everyday experience by inferring that the two events are at antipodal points on a continuum. The continuum would represent a class of conscious events characterized by activity in the sensory pathways and some central expectancies and memories, encompassing both", "references": ["571e9d8b9e05ea748151c9eef857060146bfa695", "933c7dbeb31a4c0eec99dd35eead8c341440f29b", "f7ac83c788b6c4562db4d89813a007c58d8541da", "faf2933887410cb110cd7c68487cb0dd81d363ee", "df8da64b592a223c02a05286eb0795621eda511d", "cd7385c514f137948c3075f2c740006f056615ef", "49d84484014b2b1d50d99e96ff3fb450fe5ccf9e", "fbd7345a8f96da9fc6cd7b1a7bf301ed620e262b"], "page_rank": 5.473453749315818e-05}, {"id": "01da4c5f9486e850cb4f112a9c059a87ad16d3ed", "title": "Initial Report on a Lisp Programmer's Apprentice", "authors": ["Charles Rich", "Howard E. Shrobe"], "date": 1978, "abstract": "This paper reports on the initial design and partial implementation of an interactive programming environment to be used by expert programmers. The system is based on three forms of program description: 1) definition of structured data objects, their parts, properties, and relations between them, 2) input\u2013output specification of the behavior of program segments, and 3) a hierarchical representation of the internal structure of programs (plans). The plan representation is of major theoretical interest because it includes not only data flow and control flow relationships between subsegments of a program, but also goal-subgoal, prerequisite, and other logical dependencies between the specifications of the subsegments. Plans are utilized both for describing particular programs and in the compilation of a knowledge base of more abstract knowledge about programming, such as the concept of a loop and various specializations, such as enumeration loops and search loops. We also describe a deductive system which can verify the correctness of plans involving side effects on complex data with structure sharing.", "references": ["006ae42f9e23ae43afff97082a484de2443c616f", "d22000e27a064baa027fa685353abb216b0d06da", "bf15ce3d1575d124527496cb249dc1249eee0acb", "7d06bf84338e89456f609896de4e41f61086d98e", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "47330daa19a95f093b7628e325bee238439abc40", "23174bbbc77874d0ebee7b8c2719037d86993170", "6321686427c86b87e1071497ffd633b71aad6fb6", "17fe58e6115711ce4d5ceef941c60eb6d6898dcf", "92dfbf02fc68df56b7ae3b27188b0314797d072e"], "page_rank": 4.926108374384236e-05}, {"id": "4140e7481c2599604b14fcd04625274022583631", "title": "Availability: A heuristic for judging frequency and probability\u2606\u2606\u2606", "authors": ["Amos Tversky", "Daniel Kahneman"], "date": 1973, "abstract": "Abstract This paper explores a judgmental heuristic in which a person evaluates the frequency of classes or the probability of events by availability, i.e., by the ease with which relevant instances come to mind. In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatorial outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. The effects of the availability of incidents and scenarios on subjective probability are discussed.", "references": ["188022ac15cc7e9d375b5b019af8c6ff119f80a8", "85978718f87a0299b6b3fbbc3e8c40210d21942b", "0267084651d780ff7a78ebcc1d8b466c26141fa0", "437a161c0a878655c9da19400c076d6a6aec6f7e", "a00176470d841ea1bd2c3a13dd2991c008ac2e55", "9825e92285bb5d1025894d66d104aaa65240f529", "0db44ff7d6277d79c5e723d43ae8d837208f18a9", "4ee237d215c41374cb94227a44d1a51af7d3873f", "96e7f6f23285e4e23cf2c7ee3fd3a41941931ff7", "3b5879ca275da0c82ca4759ae338c3c6263405ae"], "page_rank": 0.00010946907498631636}, {"id": "20eb54d6d0159bb257564e632727aed369c9fd1c", "title": "The teachable language comprehender", "authors": ["M. Ross Quillian"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"The teachable language comprehender\" by M. Ross Quillian", "references": [], "page_rank": 0.0004926108374384236}, {"id": "7736f8a436bf8ab8b1df712074a6df7ebfed6b6f", "title": "Nominalization and Montague Grammar: A semantics without types for natural languages", "authors": ["G. Chierchia"], "date": 1982, "abstract": "ConclusionsWe started from the fact that type theory, in the way it was implemented in IL, makes it costly to deal with nominalization processes. We have also argued that the type hierarchy as such doesn't play any real role in a grammar; the classification it provides for different semantic objects is already contained, in some sense, in the categorial structure of the grammar itself. So, on the basis of a theory of properties (Cocchiarella's HST*) we have tried to build a language (IL*) whose syntax does not contain any explicit typing of expressions. Some of the consequences that this move brings about in the overall organization of the grammar can be summarized as follows:(a)it allows for a simple treatment of infinitives, gerunds, factives and, in general, all those phenomena which might be analyzed as cases of nominalization;(b)it provides a simpler and more constrained semantics than IL, since IL* doesn't go beyond second order, and its non-modal basis is axiomatizable;(c)it suggests that the role of logical form in a theory of grammar could be that of a family of theories of semantic objects;(d)it eliminates the extrinsic limitations of a type hierarchy on the choice of the system of syntactic categories.\nI think that it is interesting to notice how having an explicit semantic framework helps to provide a sense in which it is legitimate to regard the syntax of a language as \u2018autonomous\u2019.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "e6584eb10d5b8ff0a0a3630d04a9d1ba9c015379", "title": "THE CLASSIFICATION OF INSTANCES OF FOUR COMMON CLASS CONCEPTS BY CHILDREN AND ADULTS", "authors": ["Marian Annett"], "date": 1959, "abstract": "Summary. Dispositional views of conceptual thinking suggest the need for a descriptive study of the ways in which normal subjects of various ages classify familiar objects. The sortings and explanations of 303 children, aged 5\u201311 years, and forty-two adults, aged 18\u201373 years, in grouping pictures of common objects, were analysed. The explanations were classified for both form and content. Five methods of explanation were distinguished, only two of which are recognised in logic. There were four main types of content, the most numerous at all ages being those concerned with spatial position and activity. The main developmental changes were found in methods of explanation. Older adults perform more like children than younger adults.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "5e73f775ea677849942c2b299dffa75f69cfca42", "title": "Absolute judgments as a function of stimulus range and number of stimulus and response categories.", "authors": ["Charles W. Eriksen", "Harold W. Hake"], "date": 1955, "abstract": "Semantic Scholar extracted view of \"Absolute judgments as a function of stimulus range and number of stimulus and response categories.\" by Charles W. Eriksen et al.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "006cce6ef401f84f39aeed27d5196acdb990c7da", "title": "Unbounded Dependencies and Coordinate Structure", "authors": ["Gerald Gazdar"], "date": 1981, "abstract": "Consider eliminating the transformational component of a generative grammar. In particular, consider the elimination of all movement rules, whether bounded or unbounded, and all rules making reference to identity of indices. Suppose, in fact, that the permitted class of generative grammars constituted a subset of those phrase structure grammars capable only of generating context-free languages. Such a move would have two important metatheoretical consequences, one having to do with learnability, the other with processability. In the first place, we would be imposing a rather dramatic restriction on the class of grammars that the language acquisition device needs to consider as candidates for the language being learned. And in the second place, we would have the beginnings of an explanation for the obvious, but largely ignored, fact that humans process the utterances they hear very rapidly.1 Sentences of a context-free language are provably parsable in a time which is proportional to the cube of the length of the sentence or less (Younger (1967), Earley (1970)). But no such restrictive result holds for the recursive or recursively enumerable sets potentially generable by grammars which include a transformational component.", "references": ["678f1a5e770260205a4762861fb158c2981d2fc3", "6ce17190b047f813237bea498da4e1dc08220aa2", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "398b4734b145e048b55e5d8b0fa0865e81d864f2", "aa1bf0c2777c84ca10ceb252cd83b9e88699ff0b", "aec0dedafa9e255c7bfd843c64c58868a9577b49", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "5e001e5257deeb0de7fa49456a67c754c1f87aad", "81dda0a818c758889399593886a33315b585a519", "32e673a4fd39cf30c6c4767e967d6c7b76b647b7"], "page_rank": 0.00023047149894440532}, {"id": "a3ffc76d9d3e310f8ae4248d4b43d60253f2067c", "title": "Fixpoint induction and proofs of program properties", "authors": ["David Michael Ritchie Park"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Fixpoint induction and proofs of program properties\" by David Michael Ritchie Park", "references": [], "page_rank": 0.0004926108374384236}, {"id": "6bda9f1be3714d2a9866d4714b7f20e15c5a6e74", "title": "The effect of presenting various numbers of discrete steps on scale reading accuracy.", "authors": ["Harold W. Hake", "William R. Garner"], "date": 1951, "abstract": "Abstract : An experiment was conducted to determine the accuracy with which a subject can determine the value of a continuous variable when only discrete values representing ranges of the continuous variable are presented.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "5eec4e79df13c612a02b01185e1f72143db81ae5", "title": "The Occurrence of Clustering in the Recall of Randomly Arranged Words of Different Frequencies-Of-Usage", "authors": ["Weston A. Bousfield", "Burton H. Cohen"], "date": 1955, "abstract": "Abstract : The study is closely related to one previously reported in that both deal with the influence of reinforcement on the incidence of clustering in the recall of randomly arranged words. Both employ the same hypotheses. They differ, however, in the techniques used for controlling reinforcement. The present study deals with differences in reinforcement during a pre-experimental phase whereas the earlier study undertook to manipulate reinforcement during the experimental phase. (Author)", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "2bca55f23d377c66dbaa7361cbb8ab6b21f8ae31", "title": "Pattern recognition and categorization", "authors": ["Stephen K. Reed"], "date": 1972, "abstract": "Four experiments are reported which attempt to determine how people make classifications when categories are defined by sets of exemplars and not by logical rules. College students classified schematic faces into one of two categories each composed of five faces. One probability model and three distance models were tested. The predominant strategy, as revealed by successful models, was to abstract a prototype representing each category and to compare the distance of novel patterns to each prototype, emphasizing those features which best discriminated the two categories.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "352a7df7e022c1760b496b6b3e1d39a8369ad5c4", "title": "Chromaticity-Confusion Contours in a Complex Viewing Situation*", "authors": ["Rita M. Halsey", "Alphonse Chapanis"], "date": 1954, "abstract": "This experiment measured confusion contours for 58 standard colors distributed throughout the CIE constant-luminance diagram. Matches were made to these standards from an assortment of 342 heterogeneous colors arranged in a display which presented 171 of them simultaneously. The viewing conditions approximate those found in certain complex display situations. Twenty subjects were used in the experiment. Each subject was required to indicate those colors on the display board which provided satisfactory matches for each of the standard colors. Contours on the CIE diagram are drawn to show the percentage of times various chromaticities were confused. From these contours we have selected colors which can be discriminated with high levels of accuracy and so are suitable for coding qualitative and quantitative information. In general, the contours follow trends suggested by extrapolation from precise threshold data.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "5322a485d71098e245e4c7c36cfdb7f5fce2858c", "title": "Multidimensional stimulus differences and accuracy of discrimination.", "authors": ["Charles W. Eriksen", "Harold W. Hake"], "date": 1955, "abstract": "Determined discrimination accuracy using the method of absolute judgment for a series of stimuli varying along the single dimensions of size, hue and brightness. These measures were compared with measures obtained when the stimuli varied on several dimensions simultaneously. It was found that discriminability for a multidimensional series of stimuli was considerably greater than that obtained for any of the compounding dimensions used alone. Also showed that the discrimination accuracy for a compounded series of stimuli could be predicted with reasonable accuracy if the discrimination accuracy of the compounding dimensions is known. Language: en", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "c402b73ea2295eac620d41442a9a10858e315020", "title": "Morphological analysis of finnish by computer", "authors": ["Lauri Karttunen", "Rebecca Root", "Hans Uszkoreit"], "date": 1981, "abstract": "Semantic Scholar extracted view of \"Morphological analysis of finnish by computer\" by Lauri Karttunen et al.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "fbcd27ad3515d71b4622ee24e1d644a77646f102", "title": "RITA - An Experimental Man-Computer System On A Natural Language Basis", "authors": ["A. Ershov", "A. Nariniany", "I. Mel'chuk"], "date": 1975, "abstract": "The report presents an experimental (Pictorial) Representation - Information - Text - Author system intended for work with texts in natural language and simple geometrical compositions. The general principles of the system operation, its architecture, and basic problems are discussed.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "28628e1a8bf31402113cef0e87fef9c23c1c4d3e", "title": "The Information of Elementary Multidimensional Auditory Displays", "authors": ["Irwin Pollack", "Lawrence Ficks"], "date": 1954, "abstract": "A given amount of information may be incorporated within elementary auditory displays by utilizing a small number of finely subdivided stimulus dimensions or by utilizing a relatively larger number of crudely subdivided stimulus dimensions. Experimental interest to date has been confined largely to the former type of display; the present study considers the latter type. Specifically, the informational transmission with elementary auditory displays of a large number (6\u20138) stimulus dimensions was investigated. In general, nearly perfect identification was obtained with skilled listeners when each dimension was crudely subdivided into two alternative states. Finer subdivision of each dimension does not produce a proportional gain in information transmission with the display.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "8ec35513fabc515f6e5b46859458a339f4a45cc6", "title": "PSG: A Simple Phrase Structure Parser", "authors": ["John Bear", "Lauri Karttunen"], "date": 1979, "abstract": "Programme d'ordinateur utilisant une grammaire syntagmatique pour l'analyse des phrases de l'anglais.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "eb233638ad0b99e3c66c636bef814ae94131ab5f", "title": "A natural language compiler for on-line data management", "authors": ["Charles Kellogg"], "date": 1968, "abstract": "During the past few years there has been a rapid advance in the technology of time-sharing systems and software to permit quick access to large files of structured data. This has led to a growing interest in communicating with computer files directly in a natural language such as English. The natural language systems described in the literature are largely small-scale research vehicles dealing with small data bases of restricted subject scope. Giuliano (1965), among others, has questioned the generalization of these systems to wider universes of discourse. Developments in this area have been reviewed by Simmons (1966), and by Bobrow, Fraser and Quillan (1967). In contrast, the work in on-line data management has been more concerned with the efficient organization of structured data to allow for quick access and maintenance of large volumes of formatted information [see the reviews by Kellogg (1967), Climenson (1966), and Minker and Sable (1967)].", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "4145eea44469b42682cc1feb2665c20bfbbf8ec2", "title": "Capturing Linguistic Generalizations with Metarules in an Annotated Phrase-Structure Grammar", "authors": ["Kurt Konolige"], "date": 1980, "abstract": "1. I n t r o d u c t i o n Compu ta t i ona l models employed by cu r ren t na tu ra l language unders tand ing systems re ly on p h r a s e s t r u c t u r e rep resen ta t i ons o f syn tax . Whether imp lemen ted as augmented t rans i t i on nets, BNF grammars, anno ta ted phrase-structure grammars, or s imi la r methods, a phrase-structure representation makes the pars ing p rob lem c o m p u t a t l o n a l l y t r a c t a b l e [ 7 ] . H o w e v e r , p h r a s e s t r u c t u r e rep resen ta t i ons have been open to the c r i t i c i s m tha t they do not cap tu re l i ngu i s t i c gene ra l i za t i ons t h a t are easi ly expressed in t r a n s f o r m a t i o n a l g rammars .", "references": ["006cce6ef401f84f39aeed27d5196acdb990c7da", "7b773624063af98bf0bfb75c705e489f736aa7f8", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "9192ee319d6a5425bcfb80b46a058d1df8e49c10", "9ad5d4001d5b4850be93251810b58092799c3dd0", "7e5670e5e44f0745294fe688b40210d4ad56c530"], "page_rank": 9.852216748768472e-05}, {"id": "10b9c3b2923e952df0b7bd68e52d1bb56081a27b", "title": "An analysis of perceptual confusions among some English consonants", "authors": ["George A. Miller", "Patricia E. Nicely"], "date": 1955, "abstract": "Sixteen English consonants were spoken over voice communication systems with frequency distortion and with random masking noise. The listeners were forced to guess at every sound and a count was made of all the different errors that resulted when one sound was confused with another. With noise or low\u2010pass filtering the confusions fall into consistent patterns, but with high\u2010pass filtering the errors are scattered quite randomly. An articulatory analysis of these 16 consonants provides a system of five articulatory features or \u201cdimensions\u201d that serve to characterize and distinguish the different phonemes: voicing, nasality, affrication, duration, and place of articulation. The data indicate that voicing and nasality are little affected and that place is severely affected by low\u2010pass and noisy systems. The indications are that the perception of any one of these five features is relatively independent of the perception of the others, so that it is as if five separate, simple channels were involved rather tha...", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "eb707d747ccaf5136e2b77d18fe275ac7faf0375", "title": "Quantification and Syntactic Theory", "authors": ["Robin Cooper"], "date": 1983, "abstract": "I: Syntax and Model-Theoretic Semantics.- II: A Fragment of English.- III: Quantifier Storage.- IV: Storage and wh-Phenomena.- V: wh-Phenomena and the Theory of Grammar.- VI: Presupposition and Quantification.- VII: Gender Agreement.- Notes.- Answers to Selected Exercises.- Index of Names.- Index of Subjects.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "678f1a5e770260205a4762861fb158c2981d2fc3", "title": "An extension of classical transformational gram-mar", "authors": ["Eric Bach"], "date": 1976, "abstract": "0. Introductory remarks. I assume that every serious theory of language must give some explicit account of the relationship between expressions in the language described and expressions in some interpreted language which spells out the semantics of the language. 1 Let's call this relationship the translation relation. Theories differ as to how this relation is specified. In the Aspects theory of syntax, taken together with a Katz-Postal view of \"semantic rules\" (Chomsky 1965; Katz and Postal, 1964), it was assumed that the relation was defined on deep structures. Serious problems led to modifications of this view in several directions ('generative' and 'interpretive' semantics). Chomsky's latest papers (1976, 1975a: Ch. 3) assume that a modified 'intermediate structure' (surface structure with traces) is transformed by rules of interpretation to a level of representation called 'logical form' (which is input to further rules). Common to all of these approaches is the assumption that the translation rules are defined initially on syntactic structures of one sort or another (sometimes from several 'levels' at once). Let us call this assumption the configurational hypothesis: the translation rules all have the form: [183] I. Given a structure of such and such a form, translate the structure into an expression in the interpreted language of such and such a form. In sharp contrast to this view of the translation relation is another, in which it is assumed that the syntax and the translation go hand in hand. More precisely, the syntax builds up a syntactic structure by the application of syntactic rules which operate on syntactic structures, while for each syntactic rule there is a unique translation rule specifying the translation of the resultant expression as a function of the translation of the parts. Let us call this hypothesis the rule-to-rule assumption about the translation relation. It is characteristic of Montague grammar as well as a number of other approaches all falling within the limits of what T. Parsons (class lectures, U. Mass., 1976) has called 'recursive grammar.' Montague himself (in Montague 1973) described the translation relation as operating on structures, the socalled 'analysis trees'; but since the analysis trees are simply a record of the", "references": ["8120c0a67f38b9727bdd15dbe47090391b9ccb42", "908409527465661b5fb0ed005bfc25d65971d12e", "4239dd1dedad10f871bd1139f2eccc1d72d51185", "224ea543683298ca37115eb096376dec7ac82e5c", "aec0dedafa9e255c7bfd843c64c58868a9577b49", "b4351302cfeae8fc1dd540927248af3279c23fae", "c086c6a8b99a4ce12ea5a770dee2f9e0892bc917", "05a100eb6aac635ff976e9a3c089f95ae1dce70b", "a376f76c6029cd053db3a601f95b0f4e9b3d56fc", "ba50fcd0d29a70710587b72dd933b98c87312cb9"], "page_rank": 0.0001231527093596059}, {"id": "49231bc48d77378c4b1c8cdb576d043b0b10be7f", "title": "Processing English with a Generalized Phrase Structure Grammar", "authors": ["Jean Mark Gawron", "Jonathan King", "John Lamping", "Egon E. Loebner", "E. Anne Paulson", "Geoffrey K. Pullum", "Ivan A. Sag", "Thomas Wasow"], "date": 1982, "abstract": "This paper describes a natural language processing system implemented at Hewlett-Packard's Computer Research Center. The system's main components are: a Generalized Phrase Structure Grammar (GPSG); a top-down parser; a logic transducer that outputs a first-order logical representation; and a \"disambiguator\" that uses sortal information to convert \"normal-form\" first-order logical expressions into the query language for HIRE, a relational database hosted in the SPHERE system. We argue that theoretical developments in GPSG syntax and in Montague semantics have specific advantages to bring to this domain of computational linguistics. The syntax and semantics of the system are totally domain-independent, and thus, in principle, highly portable. We discuss the prospects for extending domain-independence to the lexical semantics as well, and thus to the logical semantic representations.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4239dd1dedad10f871bd1139f2eccc1d72d51185", "title": "MONTAGUE GRAMMAR, GENERATIVE SEMANTICS, AND INTERPRETIVE SEMANTICS", "authors": ["Robin Cooper", "Terence Parsons"], "date": 1976, "abstract": "Publisher Summary This chapter discusses Montague grammar, generative semantics, and interpretive semantics. In Universal Grammar, Montague complains that work in transformational grammar is not yet rigorous enough to merit serious consideration. As an alternative, he gives a system of grammar that generates a fragment of English syntax and assigns meanings to the sentences so generated. The grammar generates only a portion of English, but it does so by means of a theory that meets the high standards of mathematical rigor and precision he advocates. Montague generates phrases of English by a many-claused recursive definition. Familiarity with this technique is presupposed here; for an exposition. Montague characterizes the translation relation as a particular relation between disambiguated languages. The syntax of the disambiguated language whose meaningful expressions are viably indexed trees consists of the following rules from the original C syntax. Both Montague's general approach and transformational grammar are capable of generating all recursively enumerable languages.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "2fa8570ae5ac3950a1bc6d9ef5c6af9126824d23", "title": "The relation of grammar to cognition: a synopsis", "authors": ["Leonard Talmy"], "date": 1978, "abstract": "A sentence (or other portion of discourse) is taken to evoke in the listener a meaning complex, here called a \"cognitive representation\". The lexical elements of the sentence, to simplify, by and large specify the content of the cognitive representation, while the grammatical elements specify its structure. Thus, looking systematically at the actual notions specified by grammatical elements can give us a handle for ascertaining the very makeup of (linguistic-) cognitive structuring. We accordingly examine a number of grammatically specified notions, observe the categories and systems in which they pattern, and speculate on broader cognitive connections.Some provisional findings have already emerged. Grammatical specifications for structure are preponderantly relativistic or topological, and exclude the fixed or metrically Euclidean. The categories in which grammatical notions pattern include:plexity perspectival modestate of boundedness level of synthesisstate of dividedness level of exemplaritydegree of extensionality axial characteristicspattern of distribution scene-breakup\"Grammatical specification of structuring appears to be the same, in certain abstract characteristics, as the structuring of visual perception.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "27845db178cf77d366230209452a6f7a99b6dc0c", "title": "Intention, memory, and computer understanding", "authors": ["Roger C. Schank"], "date": 1971, "abstract": "Procedures are described for discovering the intention of a speaker by relating the Conceptual Dependency representation of the speaker''s utterance to the computer''s world model such that simple implications can be made. These procedures function at levels higher than that of the sentence by allowing for predictions based on context and the structure of the memory. Computer understanding of natural language is shown to consist of the following parts: assigning a conceptual representation to an input; relating that representation to the memory such as to extract the intention of the speaker; and selecting the correct response type triggered by such an utterance according to the situation.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "a7cb5868c2142aea345cb0dcb57ae0dd421cc3aa", "title": "The lunar sciences natural language information system", "authors": ["William A. Woods"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"The lunar sciences natural language information system\" by William A. Woods", "references": [], "page_rank": 0.00016420361247947453}, {"id": "e6ae33d983b56ca4a692e52f387e54a913b946f5", "title": "The language and thought of the child.", "authors": ["Samuel W. Fernberger}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"The language and thought of the child.\" by Samuel W. Fernberger", "references": [], "page_rank": 0.0002463054187192118}, {"id": "49a3da48d0e6def2245985145cc2a28eb3ed49d5", "title": "The psychological reality of linguistic segments", "authors": ["Jerry A. Fodor", "Thomas G. Bever"], "date": 1965, "abstract": "Experimentation with the subjective location of clicks heard during speech supports the following conclusions: (a) Clicks are attracted towards the nearest major syntactic boundaries in sentential material. (b) The number of correct responses is significantly higher in the case of clicks located at major segment boundaries than in the case of clicks located within segments. (c) These results are consistent with the view that the segments marked by formal constituent structure analysis in fact function as perceptual units and that the click displacement is an effect which insures the integrity of these units. (d) The distribution of acoustic pauses in the sentential material does not account for the observed distribution of errors. (e) There is a slight tendency to prepose responses to clicks in sentences. This tendency is reversed during later stages of the experimental session. Both these effects are asymmetrical for the two ears.", "references": [], "page_rank": 0.0008092892329345531}, {"id": "126eeb7ed23c564673c093b9c1a9d1730bb48b2e", "title": "The comprehension and verification of ambiguous sentences", "authors": ["Donald J. Foss", "Thomas G. Bever", "Maury Silver"], "date": 1968, "abstract": "When Ss are presented with an ambiguous sentence they tend to interpret it in only one way. If later events warrant, Ss can recover the other meaning, a process which takes time. These conclusions follow from the results of a study in which 40 undergraduate Ss verified whether or not pictures shown at the end of a sentence represented the meaning of the sentence. When ambiguous sentences were presented, the verification time (VT) was no slower than for unambiguous sentences if the picture represented the \u201cexpected\u201d meaning (as determined on a pre-test) of the ambiguity. The VT to the picture representing the \u201cunexpected\u201d meaning of the ambiguity was longer than VT to corresponding control sentences.", "references": [], "page_rank": 0.00012510751427007583}, {"id": "ccea36c0c514eb4cbd1e7e7572cb454feaad1f40", "title": "A System for Transformational Analysis", "authors": ["Susumu Kuno"], "date": 1965, "abstract": "A system is proposed here for assigning a derived P-marker to a given transformed sentence and obtaining the corresponding base P-marker at the same time. Rules of analytical phrase-structure grammar for such a system have associated with them information pertaining to the transformational histories of their own derivation. When a phrase-structure analysis of the sentence is obtained, the set of grammar rules used for the analysis contains all the information necessary for the direct mapping of the derived P-marker into the corresponding P-marker. The system can also be used for decomposing a given complex sentence into \"kernel\" sentences for the purpose of structure matching between a query sentence and stored document sentences in information retrieval. An experimental program for the proposed system has been written and is currently tested with a small sample grammar. Study is underway to see if there is any mechanical procedure for obtaining an analytical phrase structure grammar of the proposed type for a given transformational grammar.", "references": ["269b32442e864fb2529a73e44f6f670529af0d02", "5b99112ef00bbb6fb79238a081bff13091292659", "f1a2c7757d110cc9e2b114de852f629dbaad4318", "cb4ee34bd3e6340dcac617ce06da9d70ec3f4202"], "page_rank": 7.037297677691766e-05}, {"id": "27e16f13ce627631900f6354ab04a71c07219326", "title": "The mitre syntactic analysis procedure for transformational grammars", "authors": ["Arnold M. Zwicky", "Joyce Friedman", "Barbara C. Hall", "Donald E. Walker"], "date": 1965, "abstract": "A solution to the analysis problem for a class of grammars appropriate to the description of natural languages is essential to any system which involves the automatic processing of natural language inputs for purposes of man-machine communication, translation, information retrieval, or data processing. The analysis procedure for transformational grammars described in this paper was developed to explore the feasibility of using ordinary English as a computer control language.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "3b5879ca275da0c82ca4759ae338c3c6263405ae", "title": "Memory and the memory-monitoring process", "authors": ["Joseph T. Hart"], "date": 1967, "abstract": "Two experiments are reported that attempt to evaluate whether people can accurately monitor the contents of their memories when they are unable to retrieve those memories. Both experiments used recently learned paired-associates as memory materials; S s were asked to predict which unrecalled response items they would be able to recognize by referring to their feelings of knowing about the missing items. The results show that the Ss were able to make relatively accurate predictions about recognition failures and successes. The general process revealed by the findings (called the memory-monitoring or MEMO process) is discussed in relation to recall and recognition thresholds.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c9cfc85a98014a8c638b1bc6608f5405b4b8c726", "title": "The interaction of veracity and syntax in the processing of sentences1", "authors": ["Jacques Mehler", "Peter Carey"], "date": 1968, "abstract": "The reaction times of 80 Ss in judging sentences true or false with respect to pictures were analyzed, and it was noted (a) that true sentences containing an expected surface structure required less time than false sentences of the same structure, (b) that latencies to true sentences containing an unexpected surface structure were longer than latencies to the same sentences when their structure was identical to that of previous sentences, and (c) that transitive-verb constructions appeared easier to judge than predicate nominatives of the same length.", "references": ["ccbaaea4604cc0a0883dd36fee8c548d651b7616", "6e661e50a75d2fc9199d2e6c6dbbd021698f1e89", "420ff95518ebc78da882a1b61cdb807f3427fede"], "page_rank": 0.00012510751427007583}, {"id": "733986b7f0798a69e8b535d0e1140785830aef50", "title": "Semantic Determinants of Preferred Adjective Order.", "authors": ["James E. Martin"], "date": 1969, "abstract": "Syntactic attempts to account for preferred adjective order are shown to be inadequate. Data showing the relationship between preferred adjective order and nonsyntactic dimensions are presented. Two hypotheses concerning the antecedents of adjective order are proposed. In these hypotheses, semantic and syntactic aspects of the phenomenon are integrated in a psycholinguistic framework. In both views, the ordered output of the semantic component is seen as providing an ordered input to the syntactic component, which is seen as resulting in the ordering of adjective production.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "4cf962d84d2483a238c5c6a61e24eda48b3a11eb", "title": "Observations with self-embedded sentences", "authors": ["Arthur L. Blumenthal"], "date": 1966, "abstract": "The Miller-Isard hypothesis that people attempt to process multiply self-embedded sentences as recursively interrupted sub-routines was critically examined. Results of a sentence comprehension test showed that Ss perceive such sentences as ungrammatical approximations to sentences with one embedding rather than as multiply embedded structures. The Miller-Isard hypothesis appeared irrelevant to the actual processing performance of the naive Ss.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "96e7f6f23285e4e23cf2c7ee3fd3a41941931ff7", "title": "Illusory correlation in observational report.", "authors": ["Loren J. Chapman"], "date": 1967, "abstract": "Phenomena from such diverse areas as superstition, belief in primitive magic, errors in clinical observations, social prejudice, and halo effect have certain similarities. All are based on a systematic error in reports of observations of a supposed correlation between the occurrences of two classes of events. It seems likely that the same principles may account for this systematic error regardless of the subject matter being observed. The term \u201cillusory correlation\u201d is proposed for such systematic errors in correlations reported by observers. Variables affecting illusory correlation were studied using visually presented words as stimuli. The findings indicate (a) that words which have a strong associative connection are reported as correlated in their occurrence when not actually correlated and (b) that illusory correlation also occurs between distinctive stimuli.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0db44ff7d6277d79c5e723d43ae8d837208f18a9", "title": "Pronunciation and apparent frequency", "authors": ["Ronald H. Hopkins", "Richard J. Boylan", "Geri L. Lincoln"], "date": 1972, "abstract": "Four experiments were conducted to test the assumption of frequency theory that pronouncing a word increases its apparent frequency of presentation. The first two experiments required comparative judgments (selecting the more frequent member of a test pair); the remaining experiments required absolute judgments (estimating the number of times each test word was presented). The results indicated that pronouncing a word does increase its apparent frequency. However, the pronunciation effect was much smaller than the effect of an increment in actual frequency, and was observed only when a within- S manipulation of pronunciation was employed. It was suggested that pronunciation responses do not produce the same type of memorial change as increments in frequency of presentation.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "9825e92285bb5d1025894d66d104aaa65240f529", "title": "Some correlates of item repetition in free-recall learning", "authors": ["Benton J. Underwood"], "date": 1969, "abstract": "The variables in six free-recall studies were the frequency of occurrence of a word within a single presentation of a long list of words, and the schedule of occurrences, either massed (MP) or distributed (DP). Recall of the DP words was much higher than that of the MP words. This was shown not to be due to rehearsal of other words during MP presentation. Degree of spacing of DP words had little influence on recall. The MP-DP difference also occurred in recognition memory. MP words presented with exactly the same frequency as DP words were judged to have occurred with far less frequency than the DP words, and it was concluded that the MP-DP difference was due to reception failure under MP which produced learning comparable to a lower actual frequency of presentation.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0267084651d780ff7a78ebcc1d8b466c26141fa0", "title": "Multiple probability learning: Associating events with their probabilities of occurrence", "authors": ["Charles A.J. Vlek"], "date": 1970, "abstract": "In the predecisional stage of information processing subjects often revise prior probabilities of several hypotheses on the basis of sample information. Multiple probability learning, defined as learning to associate events with their probabilities of occurrence, is often crucial for a consistent utilization of both prior and sampling distributions in probabilistic inference tasks. Two-choice probability learning studies have not provided much information about the manner in which subjects learn probabilities of events mainly because results of both perceptual learning and decision processes are confounded in the subject's prediction response. Extensions of the classical 2-choice paradigm to more than 2 alternatives and even to values of a continuous variable often suffer from the same weakness although sometimes response proportions and subjective probability estimates were collected simultaneously, indicating that they are fundamentally different dependent variables. The problem of measuring subjective probabilities is briefly discussed and some suggestions are made concerning research still to be done.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "4ee237d215c41374cb94227a44d1a51af7d3873f", "title": "RECOGNITION AND RETRIEVAL PROCESSES IN FREE RECALL", "authors": ["John R. Anderson", "Gordon H. Bower"], "date": 1972, "abstract": "A model of free recall is described which identifies two processes in free recall: a retrieval process by which the subject accesses the words, and a recognition process by which the subject decides whether an implicitly retrieved word is a to-be-recalled word. Submodels for the recognition process and the retrieval process are described. The recognition model assumes that during the study phase, the subject associates \"list markers\" to the to-be-recalled words. The establishment of such associates is postulated to be an all-or-none stochastic process. In the test phase, the subject recognizes to-be-recalled words by deciding which words have relevant list markers as associates. A signal detectability model is developed for this decision process. The retrieval model is introduced as a computer program that tags associative paths between list words. In several experiments, subjects studied and were tested on a sequence of overlapping sublists sampled from a master set of common nouns. The twoprocess model predicts that the subject's ability to retrieve the words should increase as more overlapping sublists are studied, but his ability to differentiate the words on the most recent list should deteriorate. Experiments confirmed this predicted dissociation of recognition and retrieval. Further predictions derived from the free recall model were also supported. This paper has several aims: First, we offer a critique of two popular \"strength\" theories, one which relates recall and recognition to the strength of one and the same memory trace, and another which relates only recognition memory to a similar strength measure; second, we develop a particular conceptualization about recognition memory which we believe satisfies the criticisms of the traditional strength theory; third, we illustrate how that recognition mechanism could be interfaced with a retrieval mechanism so as to yield a viable theory about multilist free recall. Along with reviewing published data relevant to these points, we also shall present new 1 This research was supported by Grant MH", "references": ["b131faf6ff67a2e3000cff5deba6b47dca2c5bf6", "639c596e253589d97c94c45619684e0fbdf1e6be", "609e3f429cf8024d2d47fe26dccdbf44adb3e4e4", "fa25ea935d8d3bdfa273950ba00594e0a66beb67", "5e2d37ea860e1c99385717d08be226071f4325ba", "42f8f7a256c0668b6f6739190781f377ca20ec29", "a7e6efb049b364c579945d4286d331b9503fe2a3", "caf677b63e29ee338db2f7db6b0449a82d38540e", "6ea79c63b58ee0b03b94aabb0d38500556896f7b", "fad9814dee74c786e70b45a951874f6a4f26deaf"], "page_rank": 4.926108374384236e-05}, {"id": "437a161c0a878655c9da19400c076d6a6aec6f7e", "title": "Recall and judged frequency of implicitly occurring words", "authors": ["Kenneth L. Leicht"], "date": 1968, "abstract": "Associative relatedness may facilitate free recall because associated words have occurred as implicit associational responses (IARs) to one another, their frequency of occurrence thereby being increased. This interpretation predicts that both recall and judged frequency of a word will increase with the number of times the word is an associate of other list words. Both predictions were confirmed. Additional results were as follows: (a) recall and judged frequency increased with increasing presentation frequency; (b) frequency discrimination was poorer at high than at low levels of presentation frequency; and, (c) discrimination of presentation frequency was poorer at 24 hr than at 0 hr.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "81dda0a818c758889399593886a33315b585a519", "title": "On strategies for processing relative clauses: A comparison of children and adults", "authors": ["Amy Sheldon"], "date": 1977, "abstract": "Adults were tested for the way in which they process four types of subject and object relative clauses. The results support an anti-interruption and anti-rearrangement constraint that has been proposed by Slobin. The reason why interruption and rearrangment of linguistic units is hard for adults is explained in terms of language-processing strategies that they are hypothesized to be using, in particular the Adjacency strategy. Adult behavior is compared to the performance of 4- and 5-year-old children in a previous study. The results of these two studies support the claim that children and adults are following the same strategies in processing these sentences, and that the difference between them is in which strategies they rely most heavily on, and, consequently, which sentences they make the most mistakes on.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "a00176470d841ea1bd2c3a13dd2991c008ac2e55", "title": "Availability versus accessibility of information in memory for words.", "authors": ["Endel Tulving", "Zena Pearlstone"], "date": 1966, "abstract": "The S s learned, on a single trial, lists of words belonging to explicitly designated conceptual categories. Lists varied in terms of length (12, 24, and 48 words) and number of words per category (1, 2, and 4). Immediate recall was tested either in presence or absence of category names as retrieval cues. Cued recall was higher than noncued recall, the difference varying directly with list length and inversely with number of items per category. This finding was interpreted as indicating that sufficiently intact memory traces of many words not recalled under the noncued recall conditions were available in the memory storage, but not accessible for retrieval. Further analysis of the data in terms of recall of categories and recall of words within recalled categories suggested two independent retrieval processes, one concerned with the accessibility of higher-order memory units, the other with accessibility of items within higher-order units.", "references": ["3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "ab92bbf18b4862639c551043eb0286202e923085", "62edcc7d7f76dc9b12c9ed6f6864ba1045577452", "0012e189a719cc29c3fb1ac597286e9052c0ba82", "429e26d55e4fbedaea78d1f910eca164f12f23a7", "05e30356a92a2fb2d370da3a2ea76f3cf2149169", "6ab6440287db447321ca027a7e87ec4af415bbfe", "3cf2f3c6b27b552f3c969137c90d91a9ea3efea2"], "page_rank": 4.926108374384236e-05}, {"id": "5e001e5257deeb0de7fa49456a67c754c1f87aad", "title": "Children's Comprehension of Relativized English Sentences.", "authors": ["H. D. Brown"], "date": 1971, "abstract": "BROWN, H. DOUGLAS. Children's Comprehension of Relativized English Sentences. CHILD DEVELOPMENT, 1971, 42, 1923-1936. 3 factors in the syntactic structure of restrictive relative clauses, (a) embeddedness position of the clause, (b) focus of the relative pronoun, and (c) the relative pronoun itself, were incorporated into a picture-cued comprehension test administered to 3 groups of children 3, 4, and 5 years of age. Analysis of variance showed that in overall performance 3-year-olds had significantly lower scores than 4and 5-year-olds. Embeddedness was nonsignificant except in the case of an added structural ambiguity. Subject focus was significantly easier to comprehend than object focus (p < .01). In 1 phase relative pronouns were nonsignificant, but in another phase 1 pair of pronouns was significantly harder (p < .05) than another pair. Results imply that in early childhood education the language of test instructions and reading programs could be better geared to the child's linguistic competence.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "32e673a4fd39cf30c6c4767e967d6c7b76b647b7", "title": "Monitoring around the Relative Clause.", "authors": ["Ulrich Hans Frauenfelder", "Juan Pacheco Segu{\\'i}", "Jacques Mehler"], "date": 1980, "abstract": "This article reports two experiments which examined the utility of the phoneme monitoring technique for studying syntactic processing of sentences. In French, by using self-embedded relative clauses, it is possible to isolate and examine the effect of a syntactic cue while controlling the factors known to effect phoneme detection times. Monitoring within and after the relative clause led to significant differences in phoneme detection times for reversible subject and object relatives only after the clause boundary. These results demonstrate the sensitivity of the phoneme monitoring task to syntactic processing and are taken to reflect structural calculations of the underlying grammatical relations for the reversible object relatives. When lexical information was introduced with nonreversible relatives, there was no longer a difference between the detection times for subject and object relatives after the clause boundary. Thus, it appears that lexical information can be used in the attribution of underlying grammatical roles.", "references": ["5caf18bdbf00efa2e962dba79b059eaf8b91b66e", "bd9878a15afd3f3f9f6d80c8d843b1114a267720", "c5d082224239b26141d0de372bf011de15ec5b06", "0117dc91d01091257e599ba407992e5ae8ea5357", "b5d6d96b0b93cc0e1a827c24ea0c0c1bcca3f72f", "d0d6ba169089cb18b525f2f1cb4f3004364f43f5", "1f87f79b5911f6abcc2b21220b4834ae2da5a99d", "334699230f81a209d76ef3e9b4639132c6d659d1", "a6b42f24563b360b5537eef9ecd0cedaf92bfce4", "43adee17f2dbb49a5bac51b9a52ec4d7ad928593"], "page_rank": 6.157635467980295e-05}, {"id": "aec0dedafa9e255c7bfd843c64c58868a9577b49", "title": "The NP-S analysis of relative clauses and compositional semantics", "authors": ["Emmon W. Bach", "Robin Cooper"], "date": 1978, "abstract": "ConclusionsWe have sketched how it is possible to give an analysis for adjoined relative clauses which is consistent with the compositionality principle and have shown that the technique which seems necessary for this analysis can be used to provide a compositional semantics for the NP-S analysis of English relative clauses.It is unlikely that anyone working within the framework of a compositional theory would choose the NP-S analysis for English, since it is clearly much less elegant and simple, in some intuitive sense, than the alternative Nom-S analysis. Our results seem to indicate, however, that such an analysis cannot be ruled out in principle, since any constraint on the theory that would exclude the NP-S analysis would seem to exclude the Hittite analysis as well. So the arguments for the Nom-S analysis in the English case must be based on other grounds, or the Hittite analysis is also incorrect and should be ruled out, or the happy discovery of some as yet unknown principles will allow one but not the other.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "6ce17190b047f813237bea498da4e1dc08220aa2", "title": "Constraints on coordination", "authors": ["Paul Schachter"], "date": 1974, "abstract": "1. 'The fundamental aim in the linguistic analysis of a language L', according to Chomsky (1957:13), 'is to separate the GRAMMATICAL sequences which are the sentences of L from the UNGRAMMATICAL sequences which are not sentences of L, and to study the structure of the grammatical sequences.' While this aim is obviously somewhat utopian, the research results of recent years have shown that even a limited achievement of it, in connection with some small subset of the sentences of a language, may be of considerable interest both in itself and in its consequences for general linguistic theory. In the present paper, I shall try to provide further evidence to this effect through an investigation of certain determinants of grammaticalness for constructions involving coordinating conjunctions, primarily constructions involving Eng. and.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "398b4734b145e048b55e5d8b0fa0865e81d864f2", "title": "The syntax and semantics of when-questions", "authors": ["Richard Kurth Larson", "Robin Cooper"], "date": 1982, "abstract": "0. This paper presents a novel account of the syntax and semantics of questions, making use of the framework for linguistic description developed by Richard Montague (1974). Certain features of the proposal are based on work by N. Belnap (1963), L. Aqvist (1965), C. L. Baker (1968, 1970), S. Kuno and J. Robinson (1972), C. L. Hamblin (1973), E. Keenan and R. Hull (1973), J. Hintikka (1974), Lewis (1975), and D. Wunderlich (1975), but it differs from all of its predecessors in one way or another. I will start with a number of observations which provide the basis for the treatment of questions presented in the second part of the paper and conclude with a summary and a brief discussion of how the proposed description compares with recent transformational analyses.", "references": ["224ea543683298ca37115eb096376dec7ac82e5c", "a0af7809cb6a1fee05494b2c0c61dc560dfd0089"], "page_rank": 6.157635467980295e-05}, {"id": "9ad5d4001d5b4850be93251810b58092799c3dd0", "title": "A framework for a portable nl interface to large data bases", "authors": ["Kurt Konolige"], "date": 1979, "abstract": "Semantic Scholar extracted view of \"A framework for a portable nl interface to large data bases\" by Kurt Konolige", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "9192ee319d6a5425bcfb80b46a058d1df8e49c10", "title": "A Framework For Speech Understanding", "authors": ["William H. Paxton"], "date": 1977, "abstract": "Semantic Scholar extracted view of \"A Framework For Speech Understanding\" by William H. Paxton", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "92dfbf02fc68df56b7ae3b27188b0314797d072e", "title": "An Introduction to Proving the Correctness of Programs", "authors": ["Sidney L. Hantler", "James C. King"], "date": 1976, "abstract": "This paper explains, in an introductory fashion, the method of specifying the correct behavior of a program by the use of input/output assertions and describes one method for showing that the program is correct with respect to those assertions. An initial assertion characterizes conditions expected to be true upon entry to the program and a final assertion characterizes conditions expected to be true upon exit from the program. When a program contains no branches, a technique known as symbolic execution can be used to show that the truth of the initial assertion upon entry guarantees the truth of the final assertion upon exit. More generally, for a program with branches one can define a symbolic execution tree. If there is an upper bound on the number of times each loop in such a program may be executed, a proof of correctness can be given by a simple traversal of the (finite) symbolic execution tree. However, for most programs, no fixed bound on the number of times each loop is executed exists and the corresponding symbolic execution trees are infinite. In order to prove the correctness of such programs, a more general assertion structure must be provided. The symbolic execution tree of such programs must be traversed inductively rather than explicitly. This leads naturally to the use of additional assertions which are called \"inductive assertions.\"", "references": ["bf15ce3d1575d124527496cb249dc1249eee0acb"], "page_rank": 4.926108374384236e-05}, {"id": "17fe58e6115711ce4d5ceef941c60eb6d6898dcf", "title": "Abstraction mechanisms in CLU", "authors": ["Barbara Liskov", "Alan Snyder", "Russell Atkinson", "Craig Schaffert"], "date": 1977, "abstract": "CLU is a new programming language designed to support the use of abstractions in program construction. Work in programming methodology has led to the realization that three kinds of abstractions, procedural, control, and especially data abstractions, are useful in the programming process. Of these, only the procedural abstraction is supported well by conventional languages, through the procedure or subroutine. CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and control abstractions.\n This paper provides an introduction to the abstraction mechanisms in CLU. By means of programming examples, we illustrate the utility of the three kinds of abstractions in program construction and show how CLU programs may be written to use and implement abstractions. We also discuss the CLU library, which permits incremental program development with complete type-checking performed at compile-time.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7b773624063af98bf0bfb75c705e489f736aa7f8", "title": "DIAGRAM: a grammar for dialogues", "authors": ["Jane J. Robinson"], "date": 1982, "abstract": "An explanatory overview is given of DIAGRAM, a large and complex grammar used in an artificial intelligence system for interpreting English dialogue. DIAGRAM is an augmented phrase-structure grammar with rule procedures that allow phrases to inherit attributes from their constituents and to acquire attributes from the larger phrases in which they themselves are constituents. These attributes are used to set context-sensitive constraints on the acceptance of an analysis. Constraints can be imposed by conditions on dominance as well as by conditions on constituency. Rule procedures can also assign scores to an analysis to rate it as probable or unlikely. Less likely analyses can be ignored by the procedures that interpret the utterance. For every expression it analyzes, DIAGRAM provides an annotated description of the structure. The annotations supply important information for other parts of the system that interpret the expression in the context of a dialogue.\nMajor design decisions are explained and illustrated. Some contrasts with transformational grammars are pointed out and problems that motivate a plan to use metarules in the future are discussed. (Metarules derive new rules from a set of base rules to achieve the kind of generality previously captured by transformational grammars but without having to perform transformations on syntactic analyses.)", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "188022ac15cc7e9d375b5b019af8c6ff119f80a8", "title": "Subjective probability: A judgment of representativeness", "authors": ["Daniel Kahneman", "Amos Tversky"], "date": 1972, "abstract": "This paper explores a heuristic \u2014 representativeness \u2014 according to which the subjective probability of an event, or a sample, is determined by the degree to which it: (i) is similar in essential characteristics to its parent population; and (ii) reflects the salient features of the process by which it is generated. This heuristic is explicated in a series of empirical examples demonstrating predictable and systematic errors in the evaluation of uncertain events. In particular, since sample size does not represent any property of the population, it is expected to have little or no effect on judgment of likelihood. This prediction is confirmed in studies showing that subjective sampling distributions and posterior probability judgments are determined by the most salient characteristic of the sample (e.g., proportion, mean) without regard to the size of the sample. The present heuristic approach is contrasted with the normative (Bayesian) approach to the analysis of the judgment of uncertainty.", "references": ["da3b59db6a1c939796aab7800abd535a01f7ba7e", "2e55dd55b8fdedd809a90acb322696417d06de66", "894fc603f9b16e775f95045fb805b5d7e6935944", "2dd45e770ddbb18e9509212fd5a24440740870c2", "ba5b197521b5368f42c9f1d3e08220a379bd1643", "a52c440632745862ea07987a15da575f4b442263", "65ba8fd8ef9c2a70cee99d2e5cab9302d0307a1e", "c4b1cd7f68a1c9c9295a11c6a31f3e227bf0e783", "04d3cd118c395e52208327b3ab633063442d43a6", "e789b973cfc0d6a519d45109fa8a97a21a2bf07a"], "page_rank": 0.00010399562123700055}, {"id": "05a100eb6aac635ff976e9a3c089f95ae1dce70b", "title": "The syntax of crossing coreference sentences", "authors": ["Pauline Jacobson"], "date": 1977, "abstract": "The Syntax of Crossing Coreference Sentences By Pauline Ida Jacobson A.B. (University of California) 1968 M.A. (University of California) 1972 DISSERTATION Submitted in partial satisfaction of the requirements for the degree of DOCTOR OF PHILOSOPHY in Linguistics in the GRADUATE DIVISION of the UNIVERSITY OF CALIFORNIA, BERKELEY n Approved: j / {/ f \\ / Committee in Charge DEGREE CONFERRED JUNE 1 8 .1 9 7 7 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "ba50fcd0d29a70710587b72dd933b98c87312cb9", "title": "Deletion And Logical Form", "authors": ["Ivan A. Sag"], "date": 1976, "abstract": "Thesis. 1976. Ph.D.--Massachusetts Institute of Technology. Dept. of Foreign Literatures and Linguistics.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "85978718f87a0299b6b3fbbc3e8c40210d21942b", "title": "On the Psychology of Prediction.", "authors": ["Daniel Kahneman", "Amos Tversky"], "date": 1973, "abstract": "In this paper, we explore the rules that determine intuitive predictions and judgments of confidence and contrast these rules to the normative principles of statistical prediction. Two classes of prediction are discussed: category prediction and numerical prediction. In a categorical case, the prediction is given in nominal form, for example, the winner in an election, the diagnosis of a patient, or a person's future occupation. In a numerical case, the prediction is given in numerical form, for example, the future value of a particular stock or of a student's grade point average. In making predictions and judgments under uncertainty, people do not appear to follow the calculus of chance or the statistical theory of prediction. Instead, they rely on a limited number of heuristics which sometimes yield reasonable judgments and sometimes lead to severe and systematic errors (Kahneman & Tversky, 1972b, 3; Tversky & Kahneman, 1971, 2; 1973, 11). The present paper is concerned with the role of one of these heuristics \u2013 representativeness \u2013 in intuitive predictions. Given specific evidence (e.g., a personality sketch), the outcomes under consideration (e.g., occupations or levels of achievement) can be ordered by the degree to which they are representative of that evidence. The thesis of this paper is that people predict by representativeness, that is, they select or order outcomes by the degree to which the outcomes represent the essential features of the evidence.", "references": ["188022ac15cc7e9d375b5b019af8c6ff119f80a8", "894fc603f9b16e775f95045fb805b5d7e6935944", "2e55dd55b8fdedd809a90acb322696417d06de66", "4140e7481c2599604b14fcd04625274022583631", "78db21f07fb0065a97cdfc1a9e935877e36e3b4e", "be2b068a1a95fcac76aa818e3e6713bf51669cca", "414f521d2073bd257ef4853967c9fc4a3a7b2aa7", "31462baa59dfa02c5801a549fca3b2af90510369", "f81f80cafae77889ef1f42beb0077f604bd362a8"], "page_rank": 4.926108374384236e-05}, {"id": "b4351302cfeae8fc1dd540927248af3279c23fae", "title": "Opacity, coreference, and pronouns", "authors": ["Barbara H. Partee"], "date": 1970, "abstract": "The problem discussed here is to find a basis for a uniform treatment of the relation between pronouns and their antecedents, taking into account both linguists' and philosophers' approaches. The two main candidates would appear to be the linguists' notion of coreference and the philosophers' notion of pronouns as variables. The notion of coreference can be extended to many but not all cases where the antecedent is non-referential. The pronouns-as-variables approach appears to come closer to full generality, but there are some examples of \u2018pronouns of laziness\u2019 which appear to resist either of the two approaches.", "references": ["b250f8439864b9ba517566c83e674dd7c887f24f"], "page_rank": 5.473453749315818e-05}, {"id": "c086c6a8b99a4ce12ea5a770dee2f9e0892bc917", "title": "FACTIVES AND PROPOSITION LEVEL CONSTRUCTIONS IN MONTAGUE GRAMMAR", "authors": ["Enrique Buado Delacruz"], "date": 1976, "abstract": "In this paper I shall try to modify and extend Montague's treatment of a fragment of English in Montague (1973) (henceforth PTQ) to accommodate factives and to account for some of their semantic and syntactic properties. Since the salient features of this extension of PTQ are motivated largely by the Kiparskys\u2019 work on the subject, Kiparsky and Kiparsky (1971), it would be well to consider first the Kiparskys\u2019 analysis of factives.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "224ea543683298ca37115eb096376dec7ac82e5c", "title": "An integrated theory of linguistic descriptions", "authors": ["Jerrold J. Katz", "Paul M. Postal"], "date": 1964, "abstract": "The authors offer a theory concerning the nature of a linguistic description, that is, a theoretical statement about the kind of description that a linguist is able to give of a natural language. This theory seeks to integrate the generative conception of phonology and syntax developed by Chomsky and Halle, with the conception of semantics proposed by Katz and Fodor. The authors demonstrate that the integration within one theory of these conceptions of phonology, syntax, and semantics clarifies, further systematizes, and justifies each of them. They also show that such integration sheds considerable light upon the nature of linguistic universals, that is, upon the nature of language. Primary focus is placed on the relation between the syntactic and the semantic components of a linguistic description.", "references": [], "page_rank": 0.00030103995621237}, {"id": "cb4ee34bd3e6340dcac617ce06da9d70ec3f4202", "title": "Automatic language-data processing", "authors": ["David G. Hays"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"Automatic language-data processing\" by David G. Hays", "references": [], "page_rank": 0.00016420361247947453}, {"id": "908409527465661b5fb0ed005bfc25d65971d12e", "title": "Syntax and semantics of questions", "authors": ["Lauri Karttunen"], "date": 1977, "abstract": "This paper presents a novel account of the syntax and semantics of questions, making use of the framework for linguistic description developed by Richard Montague (1974). Certain features of the proposal are based on work by N. Belnap (1963), L. Aqvist (1965), C. L. Baker (1968, 1970), S. Kuno and J. Robinson (1972), C. L. Hamblin (1973), E. Keenan and R. Hull (1973), J. Hintikka (1974), Lewis (1975), and D. Wunderlich (1975), but it differs from all of its predecessors in one way or another. I will start with a number of observations which provide the basis for the treatment of questions presented in the second part of the paper and conclude with a summary and a brief discussion of how the proposed description compares with recent transformational analyses.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "6321686427c86b87e1071497ffd633b71aad6fb6", "title": "Automatic program verification I: A logical basis and its implementation", "authors": ["Shigeru Igarashi", "Ralph L. London", "David C. Luckham"], "date": 2004, "abstract": "SummaryDefining the semantics of programming languages by axioms and rules of inference yields a deduction system within which proofs may be given that programs satisfy specifications. The deduction system herein is shown to be consistent and also deduction complete with respect to Hoare's system. A subgoaler for the deduction system is described whose input is a significant subset of Pascal programs plus inductive assertions. The output is a set of verification conditions or lemmas to be proved. Several non-trivial arithmetic and sorting programs have been shown to satisfy specifications by using an interactive theorem prover to automatically generate proofs of the verification conditions. Additional components for a more powerful verification system are under construction.", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "5a19db7d827f623143e92bca8c082de5c014d59c", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "5d8056e326d4199d157a17fbeee97a7349d2824c", "a17e7dce1c39b30d22158e4a8c133d0a4b36f1ca", "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "405666eddc9d6db81880a195b26d5a66a153cc36"], "page_rank": 4.926108374384236e-05}, {"id": "23174bbbc77874d0ebee7b8c2719037d86993170", "title": "A System for Understanding Mathematical FORTRAN Programs", "authors": ["Richard C. Waters"], "date": 1976, "abstract": "Abstract : This paper proposes a system which, when implemented, will be able to understand mathematical FORTRAN programs, such as those in the IBM Scientific Subroutine Package. The system takes, as input, a program and annotation of the program. In order to understand the program, the system develops a 'plan' for it. The plan specifies the purpose of each feature of the program, and how these features cooperate in order to create the behavior exhibited by the program. The system can use its understanding of the program to answer questions about it including questions about the ramifications of a proposed modification. It is also able to aid in debugging the program by detecting errors in it, and by locating the features of the program which are responsible for an error. The system should be of significant assistance to a person who is writing a program. (Author)", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5b99112ef00bbb6fb79238a081bff13091292659", "title": "Syntactic analysis of English by computer: a survey", "authors": ["Daniel G. Bobrow"], "date": 1963, "abstract": "A statement in a spoken language may be regarded as a one-dimensional string of symbols used to communicate an idea from the speaker to a listener. The dimensionality of the statement is limited by the need for presenting words in a single time sequence. However, evidence indicates that most information and ideas are not stored by people in one-dimensional arrays isomorphic to these linear strings. This implies that a speaker must use certain complex information manipulating processes to transform the stored information to a linear output string, and that a listener, in order to \"understand\" the speaker, must use another set of processes to decode this linear string. In order for communication to take place, the information map of both the listener and the speaker must be approximately the same, at least for the universe of discourse. Most important, the decoding process of the listener must be an approximate inverse of the encoding process of the speaker.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "6e661e50a75d2fc9199d2e6c6dbbd021698f1e89", "title": "Grammatical factors in sentence retention", "authors": ["Edwin D. Martin", "Kelyn H. Roberts"], "date": 1966, "abstract": "A rationale for indexing the structural complexity of sentences was introduced and an experiment reported that demonstrated the relationship between this index and sentence retention. The proposed measure entails a phrase-structure analysis of the sentence and a counting of the grammatical commitments incurred by each word of the sentence. A word is said to be structurally embedded in a sentence to the extent that it determines the structure of those parts of the sentence that follow. In a six-trial free-learning experiment where sentence complexity and sentence kind were manipulated independently and sentence length held constant, sentences of lesser indexed complexity were recalled significantly more frequently than sentences of greater complexity. The role of sentence kind was found to affect recall, but not in the systematic way predicted by the transformation-grammar model.", "references": [], "page_rank": 0.0002189381499726327}, {"id": "47330daa19a95f093b7628e325bee238439abc40", "title": "Automatic Construction Of Algorithms And Data Structures Using A Knowledge Base Of Programming Rules", "authors": ["David R. Barstow"], "date": 1977, "abstract": "Abstract : Although large amounts of programming knowledge are available to human programmers in the form of books and articles, very little of this knowledge is available in a form suitable for use by a machine in performing programming tasks automatically. The principal goal of the research reported here is the explication of programming knowledge to a sufficient level of detail that it can be used effectively by a machine. The programming task considered in this experiment is that of constructing concrete implementations of abstract algorithms in the domain of symbolic programming. Knowledge about several aspects of symbolic programming has been expressed as a collection of four hundred refinement rules. The rules deal primarily with collections and mappings and ways of manipulating such structures, including several enumeration, sorting and searching techniques. The principal representation techniques covered include the representation of sets as linked lists and arrays (both ordered and unordered), and the representation of mappings as tables, sets of pairs, property list markings, and inverted mappings (indexed by range element). In addition to these general constructs, many low-level programming details are covered (such as the use of variables to store values).", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "269b32442e864fb2529a73e44f6f670529af0d02", "title": "Syntactic structure and ambiguity of English", "authors": ["Susumu Kuno", "Anthony G. Oettinger"], "date": 1963, "abstract": "This paper is in two parts. The first (Section 2) gives an evaluation of the performance of the multiple-path syntactic analyzer to date, with emphasis on the nature and the consequences of syntactic ambiguities in English sentences and suggestions for the refinement of the grammar. The remainder of the paper is concerned with certain concrete implications of the theoretical description of multiple-path predictive analysis provided by recent work of Evey and Greibach. A modification of the form of the current grammar is proposed which should yield a new grammar with additional intuitive appeal, a simplified version of the present analysis program, and sentence structure descriptions in the form of a generalized parenthesis-free notation readily interpretable as a tree.", "references": ["ad933a59832e17bc350f06b42827b9a05fe3cea8"], "page_rank": 0.00016420361247947453}, {"id": "420ff95518ebc78da882a1b61cdb807f3427fede", "title": "The Processing of Positive and Negative Information", "authors": ["Peter Cathcart Wason"], "date": 1959, "abstract": "An affirmative statement which is known to be false and the complementary negative statement which is known to be true, provide the same information, i.e. that something is not the case. Similarly, an affirmative statement which is known to be true and the complementary negative statement which is known to be false, both imply that something is the case. (If P is false, not-P is true and if P is true, not-P is false.) Hence there are four kinds of statement (\u201cconditions\u201d): true affirmatives, false affirmatives, true negativee and false negatives, but only two kinds of information: positive and negative. This experiment investigates the times taken to process information presented in these ways. The task was to select two alternative words which would make affirmative or negative conjunctive statements agree or conflict with given situations. The four conditions were presented six times in different serial orders, so that each occurred once in every block of four trials. The mean response times were: true affirmatives 8\u201399 sec, false affirmatives 11 19 sec., true negatives 12\u201358 sec, false negatives 15\u201317 sec. This order was the same at each of the six presentations of the conditions, the differences being significant at the 0 001 level in each case. There was a pronounced decline in errors (without knowledge of results) for three of the conditions. These results are discussed in relation to (i) the assumption of a positive set, established through a long learning process; (ii) the inferential nature of negative information in relation to experience; and (iii) the possible emotional effects of negative terms.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "title": "Toward Interactive Design of Correct Programs", "authors": ["Robert W. Floyd"], "date": 1971, "abstract": "Publisher Summary This chapter describes an imagined interaction between a computer programmer and his machine, which might be made feasible within the next decade. It focuses on an intelligent assistant approach. The chapter also presents the earliest published description of an intelligent programming assistant. It describes an imagined interaction between a computer programmer and an intelligent program verifier assistant; such systems will be feasible within the following decade. The programmer is at an interactive console, designing a program, first in its overall outline, then by successive developments in detail. The computer is, of course, serving its customary role as syntactic analyzer, code generator, program executor, prompter, and file handler. In addition, the computer is continually checking the program, at each level of specification, for consistency with the programmer's stated intentions.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "ccbaaea4604cc0a0883dd36fee8c548d651b7616", "title": "Role of surface and base structure in the perception of sentences", "authors": ["Jacques Mehler", "Peter Carey"], "date": 1967, "abstract": "The Ss in this study listened to groups of ten syntactically homogeneous sentences mixed with white noise. An anomalous test sentence followed each group of ten. Perception scores for the test sentences showed that changes in both surface structure and base structure can significantly disrupt perception. Changes in surface structure have the stronger effect. These findings provide evidence for the psychological reality of some abstract properties of stimuli.", "references": ["49a3da48d0e6def2245985145cc2a28eb3ed49d5"], "page_rank": 0.00016420361247947453}, {"id": "bf15ce3d1575d124527496cb249dc1249eee0acb", "title": "An interactive program verifier", "authors": ["L. Peter Deutsch"], "date": 1973, "abstract": "Program verification refers to the idea that the intent or effect of a program can be stated in a precise way that is not a simple \"rewording\" of the program itself, and that one can prove (in the mathematical sense) that a program actually conforms to a given statement of intent. This thesis describes a software system which can verify (prove) some non-trivial programs automatically. The system described here is organized in a novel manner compared to most other theorem-proving systems. IL has a great deal of specific knowledge about integers and arrays of integers, yet it is not \"special-purpose\", since this knowledge is represented in procedures which are separate from the underlying structure of the system. It also incorporates some knowledge, gained by the author from both experiment and introspection, about how programs are often constructed, and uses this knowledge to guide the proof process. It uses its knowledge, plus contextual information from the program being verified, to simplify the theorems dramatically as they are being constructed, rather than relying on a super-powerful proof procedure. The system also provides for interactive editing of programs and assertions, and for detailed human control of the proof process when the system cannot produce a proof (or counterexample) on its own.", "references": ["e3aa3d449a62500dc04ce4a1f8074142e38963cf", "5b292d2f10da7585dd3487a197bf2852928f1c70", "efa59953df102e903e882f3589c23c9a505d7aa8", "bac85a0e8836b887c9b62156a8fd70df8264b7c2", "43aa639d8df2285bd805531889d1d5f49563c17d"], "page_rank": 0.0006403940886699507}, {"id": "7d06bf84338e89456f609896de4e41f61086d98e", "title": "An introduction to the construction and verification of Alphard programs", "authors": ["William A. Wulf", "Ralph L. London", "Mary Shaw"], "date": 1976, "abstract": "The programming language Alphard is designed to provide support for both the methodologies of \u201cwell-structured\u201d programming and the techniques of formal program verification. Language constructs allow a programmer to isolate an abstraction, specifying its behavior publicly while localizing knowledge about its implementation. The verification of such an abstraction consists of showing that its implementation behaves in accordance with its public specifications; the abstraction can then be used with confidence in constructing other programs, and the verification of that use employs only the public specifications.\n This paper introduces Alphard by developing and verifying a data structure definition and a program that uses it. It shows how each language construct contributes to the development of the abstraction and discusses the way the language design and the verification methodology were tailored to each other. It serves not only as an introduction to Alphard, but also as an example of the symbiosis between verification and methodology in language design. The strategy of program structuring, illustrated for Alphard, is also applicable to most of the \u201cdata abstraction\u201d mechanisms now appearing.", "references": ["a5dc91c06f08197e342c44d93d80c8e016469f9c"], "page_rank": 4.926108374384236e-05}, {"id": "b131faf6ff67a2e3000cff5deba6b47dca2c5bf6", "title": "Fran: A Simulation Model of Free Recall", "authors": ["John R. Anderson"], "date": 1972, "abstract": "Publisher Summary This chapter discusses fran\u2014a simulation model of free recall. The basic free recall experiment with which one will be concerned involves the presentation to a subject (S) of a list of words, one word at a time. After seeing all the words in the list, the S is asked to recall them in any order one chooses. The experimental paradigm derives its name from the fact that the S is not constrained to recall items in a particular order. The free recall paradigm has recently attracted much research interest because of evidence indicating the strong influence of various types of conceptual organization upon the S's recall. This chapter outlines how several different strategies could be implemented in terms of FRAN's mental mechanisms. The fact that these various strategies can be formulated in terms of FRAN's machinery supports the assertion that FRAN models the structures and processes underlying human memory in general.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "fad9814dee74c786e70b45a951874f6a4f26deaf", "title": "Changes in the memory operating characteristic during recognition learning", "authors": ["Walter Kintsch", "Warren J. Carlson"], "date": 1967, "abstract": "Four S s learned to recognize 30 lists of paired associates. Confidence judgments were obtained with all responses. The learning data were in agreement with a Markov model which requires a constant error probability in the initial state. The The confidence judgments were used to construct memory-operating characteristics. The MOC based upon all scores of the first test trial is smooth and symmetric and indicates good discrimination. MOCs were also constructed from scores of the first test trial which were followed by an error on some later trial, and from all scores before the last error. These two MOCs overlapped and were in between the MOC based upon all first-trial scores and the chance line. It is concluded that on trials before the last error S s possess some information about the learning material but that the amount of information does not increase during those trials.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "43adee17f2dbb49a5bac51b9a52ec4d7ad928593", "title": "Detection of a nonlinguistic stimulus is poorest at the end of a clause", "authors": ["Thomas G. Bever", "Richard R. Hurtig"], "date": 1975, "abstract": "Subjects detected a brief near-threshold tone while encoding two-clause sentences for later report. The objective tone locations were at the end of the first clause, at the beginning of the second clause, or in the clause boundary. The effects of intensity variations of the speech signal were assessed by having subjects detect the tones in the same speech stimuli played backward. Tones at the end of a clause are relatively harder to detect than in other positions, comparing forward and backward speech. This supports the view that listeners are preoccupied with internal processes at the end of a clause.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "334699230f81a209d76ef3e9b4639132c6d659d1", "title": "Decision processes during sentence comprehension: Effects of surf ace structure on decision times", "authors": ["Donald J. Foss", "Richard H. Lynch"], "date": 1969, "abstract": "Sentence comprehension is considered to be a set of decisions concerning the identification of entities at the various linguistic \u201clevels.\u201d Such decisions utilize overlapping fixed capacity psychological mechanisms. To the extent that one decision is difficult, others should take longer. This framework received support in two studies in which Ss\u2019 reaction times (RT) to the presence of a phoneme in a sentence were measured. When surface structure syntax was difficult, as in self-embedded sentences, RT was longer (given that S comprehended the sentence) than when surface structure syntax was relatively easy. Additionally, the presence in surface structure of a putative cue for underlying structure did not affect RT, though comprehension was significantly inferior.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "6ea79c63b58ee0b03b94aabb0d38500556896f7b", "title": "Learning as a function of word-frequency.", "authors": ["J F Hall"], "date": 1954, "abstract": "The experimental work of Howes and Solomon has indicated a high inverse relationship between the frequency with which a word appears in the language (as indicated by the Thorndike-Lorge word count1) and the duration of time required for S to perceive it.2 In an effort to provide a more exact statement of this relationship, Solomon and Postman controlled frequency more precisely by having Ss read and pronounce different nonsense words with frequencies ranging from 1 to 25' The results indicated the same inverse relationship between the recognition thresholds and the frequency of prior usage. If the assumption is made that there is a relationship between perceptual threshold and learning as measured by a recall score (that those words perceived most rapidly are easiest to learn), words which appear with a high frequency in the language should be learned more readily than those of low frequency. Aside from this theoretical consideration, the relationship found between learning and word-frequency should be of value in the calibration, construction, and utilization of word-lists which are so frequently used in rote-learning experiments. It should be noted that an earlier study by Peters did not seem to support the existence of such a relationship.4 In one phase of a series of experiments, Peters constructed five different word lists, the words in each list being selected from the 2500 most frequently used words found in the Teacher's Word Book of 10,000 Words.' List I contained words with frequencies of 1 to 500, List II contained words with frequencies of 501-1000, List III contained words with frequencies of 1000-1500, and so forth. The Ss were then presented with these lists in a counterbalanced order. Recall scores obtained after one presentation indicated no significant differences between lists.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "b5d6d96b0b93cc0e1a827c24ea0c0c1bcca3f72f", "title": "Understanding sentences with relative clauses", "authors": ["David T. Hakes", "John S. Evans", "Linda Brannon"], "date": 1976, "abstract": "Sentences containing self-embedded relative clauses are generally believed to be difficult to understand because such clauses interrupt the clauses in which they are embedded. However, the experiments that purport to have demonstrated this have confounded the self-embedded or fight-branching location of the relative clauses with their internal structure, comparing self-embedded object relatives with right-branching subject relatives. In order to break this confounding, Experiment I compared the comprehension difficulty of self-embedded and right-branching object relative clauses on two measures of comprehension difficulty. Experiment II made the same self-embedded vs. fight-branching comparison for subject relative clauses. The results of both experiments consistently failed to support the interruption hypothesis.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0117dc91d01091257e599ba407992e5ae8ea5357", "title": "Semantic facilitation and lexical access during sentence processing", "authors": ["Michelle A. Blank", "Donald J. Foss"], "date": 1978, "abstract": "An experiment was conducted testing predictions derived from context-dependent and context-independent models of lexical access. Four types of unambiguous test sentences were constructed. The direct object of each test sentence was preceded by a verb that was either semantically related or unrelated to it, and by an adjective that was semantically related or unrelated. Context-dependent models predict that the speed with which the object noun is retrieved from the mental lexicon will be faster when the verb and/or the adjective is semantically related; context-independent models predict no such facilitation. Forty-four subjects each heard 32 test sentences and were asked to monitor within the sentence for a word-initial target phoneme. The target phoneme occurred on the word following the object noun. Reaction times to detect the targets were obtained. According to context-dependent models, these times should be shorter when related words precede the object noun, and that is what was found. It was also observed that the facilitation effects due to the related verbs and adjectives were additive. Implications of these results were discussed.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "bd9878a15afd3f3f9f6d80c8d843b1114a267720", "title": "Sentence comprehension and relative pronouns", "authors": ["David T. Hakes", "Helen Smith Cairns"], "date": 1970, "abstract": "Doubly self-embedded sentences, differing only in whether the relative pronouns were present or deleted, were presented to two groups of Ss who were required to respond to a word beginning with a particular letter in each sentence and to paraphrase the sentence. The results of both the phoneme monitor task and the paraphrase task indicated that comprehension was better when the relative pronouns were present than when they were deleted. These results are consistent with earlier results for the paraphrase task but are not consistent with earlier results for the phoneme monitor task. Possible reasons for the inconsistency were considered. It was concluded that the phoneme monitor task does reflect comprehension difficulty in a manner consistent with the paraphrase task and that deleting the relative pronouns does make comprehending self-embedded sentences more difficult.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "1f87f79b5911f6abcc2b21220b4834ae2da5a99d", "title": "Does verb structure affect sentence comprehension?", "authors": ["David T. Hakes"], "date": 1971, "abstract": "Fodor, Garrett, and Bever have reported two experiments suggesting that sentences containing complex verbs are more difficult to comprehend than ones containing simple verbs. However, the tasks they used, paraphrasing and anagram solving, reflect comprehension difficulty only very indirectly. Both of the present experiments compared the difficulty of sentences containing simple and complex verbs using the paraphrasing task and also a task that assesses on-line comprehension difficulty, phoneme monitoring. The paraphrasing task yielded the predicted effect in Experiment 1 but not in Experiment 2. The phoneme monitoring task failed to yield the effect in either experiment. Two alternative accounts were suggested for the present data as well as those of Fodor et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "c5d082224239b26141d0de372bf011de15ec5b06", "title": "Tails of Words: Monitoring Ambiguity", "authors": ["Jacques Mehler", "Juan Segui", "Peter Carey"], "date": 1978, "abstract": "In order to measure reaction times for the detection of a phoneme target appropriately, the length and frequency of the word preceding the target phoneme should be controlled, but this control has been lacking in earlier studies. The addition of these controls, particularly for length, made possible the demonstration that the ambiguity of the word preceding a target phoneme did not cause an increase in the time required to detect the phoneme. This result tends to disconfirm the hypothesis that the presence of an ambiguous word necessarily complicates the real-time processing of a sentence. In fact, French sentences with a long ambiguous word just before a target phoneme led to faster reaction times than did sentences with a short unambiguous word just before the target phoneme.", "references": ["a6b42f24563b360b5537eef9ecd0cedaf92bfce4", "408e974bfb027cc4b1a1f42da29feb3cce4d7ff3", "d0d6ba169089cb18b525f2f1cb4f3004364f43f5", "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "89c44be803674adf94f2cc1453e1c33d2dd5fc53", "989b4df6e6606a0ad880e3ddc13ad0ba327c0ece", "6a866273326edc73d1b71b3fc86d06db8bd6f8db", "199cda6baea15aaf1df993cbb950331ba5f729d1", "1aee45faf20c9d16041cc7b7eb93791065bdd420", "12279cfe64cdcaa7292a3608734ccde70fde63d1"], "page_rank": 6.157635467980295e-05}, {"id": "3cf2f3c6b27b552f3c969137c90d91a9ea3efea2", "title": "Retention as a function of the method of measurement.", "authors": ["Leo Postman", "Leo Rau"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"Retention as a function of the method of measurement.\" by Leo Postman et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d0d6ba169089cb18b525f2f1cb4f3004364f43f5", "title": "Some effects of context on the comprehension of ambiguous sentences", "authors": ["Donald J. Foss", "Charles M. Jenkins"], "date": 1973, "abstract": "Ambiguous and unambiguous sentences were presented to 80 subjects in Experiment I. Half of the ambiguous sentences had contexts which biased the interpretation of the ambiguity and half had neutral contexts. The subjects monitored the sentences for a specified target phoneme and reaction times (RTs) for the monitoring responses were collected. In both neutral and biased contexts the RTs were longer when the target phoneme occurred shortly after an ambiguous word than when it occurred after an unambiguous control word. The results were the same both for those subjects who noticed that the sentences were ambiguous and for those who did not. Experiment II replicated Experiment I. Mechanisms of sentence processing were discussed and a model patterned after common sense was discarded.", "references": [], "page_rank": 0.0001778872468527641}, {"id": "3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "title": "IMPLICATIONS OF SHORT-TERM MEMORY FOR A GENERAL THEORY OF MEMORY,", "authors": ["Arthur W. Melton"], "date": 1963, "abstract": "Abstract : A dichotomy of human memory into immediate memory and long-term memory (associative memory, habit) has been widely accepted for many years and has been formally stated by some theorists. This assumed dichotomy of the phenomena of short-term memory and long-term memory is examined and rejected in this paper. First, a number of current issues in learning theory are restated as issues about the formation, storage, and retrieval of memory traces, and the major issue is identified as the question whether short-term memory and long- term memory are points on a continuum, or a dichotomy. Then this major issue is examined in the light of data from recent studies in which the recall of single to-be-remembered alphanumeric items followed a single or very few repetitions. Finally, the issue is examined in the light of new data that relate the slope of the short-term forgetting curve to the number of elements or recoded chunks in the to-be-remembered unit, and also new data that confirm and extend Hebb's finding that there is a specific accumulative strengthening effect of repetitions in the immediate memory situation involving to-be-remembered units beyond the span of immediate memory of human subjects. The principal consequence of the conclusion that a continuum, rather than a dichotomy, is involved in short-term and longterm memory is the rejection of the postulate of autonomous decay of traces in the case of shortterm memory and acceptance of the postulate of permanence of traces, once formed, throughout all varieties of memory. (Author)", "references": ["e940dce76e423390383fab059db2a77c3649e2ba", "62edcc7d7f76dc9b12c9ed6f6864ba1045577452", "64a86e32d2edb67bd284c4bd43950935a5597064", "c5b4337a952f602f90edb7b68995441259cd8674", "c0069916fe269ff59f601dfdf2a091387f883471", "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "38f60bccc379144fd3e039d8b00470731f1517ef", "fed1acee3ef1d8703c9ac30ddfe164ec5ab473a9", "d0c96f3fe415103229c34ae4cbf4498db3f04eae", "277ace70085295ca32d4514ce3ab99ef218d0c4b"], "page_rank": 0.0001231527093596059}, {"id": "6ab6440287db447321ca027a7e87ec4af415bbfe", "title": "Psychological statistics, 3rd ed.", "authors": ["Q. Mcnemar"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"Psychological statistics, 3rd ed.\" by Q. Mcnemar", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "04d3cd118c395e52208327b3ab633063442d43a6", "title": "Sample size, likelihood, and confidence in a decision", "authors": ["Gordon F. Pitz"], "date": 1967, "abstract": "Ss made decisions in a two-choice task following 3, 9, and 19 items of information, and rated their confidence in their decisions. When the likelihood of the informative events was constant, confidence was inversely related to sample size. The results were the reverse of findings in studies of the sequential revision of confidence.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e789b973cfc0d6a519d45109fa8a97a21a2bf07a", "title": "A theoretical analysis of insight into a reasoning task", "authors": ["Philip N. Johnson-Laird", "Peter Cathcart Wason"], "date": 1970, "abstract": "An information-processing analysis of insight into a singularly deceptive and difficult deductive problem is presented. Two models are described. The first represents an economical explanation of the Ss initial responses but is difficult to reconcile with their subsequent responses induced by certain remedial procedures. The second model does take account of such responses and shows how insight into the correct solution is correlated with the awareness that tests for falsification are more appropriate than tests for verification. The relevance of the experimental results and the explanatory model are discussed in relation to wider issues.", "references": ["c86b22c7e2b84a06eea51917aa3d4bf45775f624", "790cf503e315b4196f451333ba20842591b08e15", "8f71a9729ae782fa0d3b077d8cba97fcc636e70e", "5bdd2b788e572dee2029198ac11b0e9e895ff6ac", "4dc02d4ad2b0b9a2b9f050fb05418917f9687b25", "2a71481851b5e9a3f60285bb9d1d990ec471f9ef", "001d983fbb462ca617942d51ff59592a78e30008", "0060344c7de6d7294c6c4bc8ed56a0f5da47596e"], "page_rank": 4.926108374384236e-05}, {"id": "31462baa59dfa02c5801a549fca3b2af90510369", "title": "Cue-consistency and cue-utilization in judgment.", "authors": ["Paul Slovic"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"Cue-consistency and cue-utilization in judgment.\" by Paul Slovic", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b250f8439864b9ba517566c83e674dd7c887f24f", "title": "On the syntax of preverbs", "authors": ["Charles J. Fillmore"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"On the syntax of preverbs\" by Charles J. Fillmore", "references": [], "page_rank": 0.0004926108374384236}, {"id": "f81f80cafae77889ef1f42beb0077f604bd362a8", "title": "Psychometric methods, 2nd ed.", "authors": ["Joy Paul Guilford"], "date": 1954, "abstract": "Semantic Scholar extracted view of \"Psychometric methods, 2nd ed.\" by Joy Paul Guilford", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "78db21f07fb0065a97cdfc1a9e935877e36e3b4e", "title": "Regression effect in psychophysical judgment", "authors": ["S. S. Stevens", "Hilda Brody Greenbaum"], "date": 1966, "abstract": "Psychophysical judgment, like all other kinds of judgment, involves a matching or equating of two different domains. When the judgment involves the mate hing of values on two perceptual continua, the observer tends, on the average, to constrict the range of his adjustments on whichever variable is placed under his control. When the observer adjusts each variable in turn, two different regression lines are produced. This regression effect presumably occurs whenever the results of the matching judgments yield less than a perfect correlation. Illustrative examples are given for the continua, loudness, vibration, brightness, and duration.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "414f521d2073bd257ef4853967c9fc4a3a7b2aa7", "title": "PSYCHIATRY AND EXPERIMENTAL PSYCHOLOGY", "authors": ["Robert Sessions Woodworth}"], "date": 1940, "abstract": "Psychology and psychiatry have grown up in relative isolation from each other. Medicine, \u201cthe mother of the sciences,\u201d was less intimately connected with the birth and early life of psychology than with the origin of most of the other sciences. The study of the mind was first undertaken in systematic form by the philosophers, and this association remained for many centuries a controlling one in the history of psychology. Even at the present day, the question whether psychology should more properly affiliate with philosophy or with the natural sciences is regarded as a fit subject for discussion, and opposing views on it are expressed by eminent psychologists. Students of other sciences are sometimes inclined to deny the right of psychology to call itself a natural science, and for two reasons. On the side of method, there is still much that is current in psychological books and discussions that appears to the student of empirical science quite strange and foreign in tone. And on the side of results, doubt is expressed whether psychology really has anything to teach, which common sense and common observation have not sufficiently acquainted us with. Psychology seems sometimes to be engaged in an \u201celaboration of the obvious,\u201d in stating familiar facts in obscure phraseology, or at the best in putting together familiar facts into systematic shape, without adding to the store of facts. Whatever may be the proper abstract definition of a science, in the concrete we demand that a science which we are to study shall do more than classify and label facts that we already know; we require it to teach us something new; and on the practical side we wish it to guide our action where common sense is inadequate to meet the situation. These requirements are abundantly met by the physical, the natural, and the medical sciences; in comparison with these, psychology, the", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "be2b068a1a95fcac76aa818e3e6713bf51669cca", "title": "Predicting with intuitive correlations", "authors": ["Clifton Daggett Gray"], "date": 1968, "abstract": "An experiment in the context of man as intuitive statistician tested ability to produce an intuitive equivalent of simple linear-regression predictions as a function of the correlation between two scaled variables. Matching, rather than optimizing, characterized the resulting prediction behavior. Overmatching was most prominent when cue validity was low.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "405666eddc9d6db81880a195b26d5a66a153cc36", "title": "In Mathematical Aspects of Computer Science", "authors": ["Robert W. Floyd"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"In Mathematical Aspects of Computer Science\" by Robert W. Floyd", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "title": "An axiomatic definition of the programming language PASCAL", "authors": ["C. A. R. Hoare"], "date": 1972, "abstract": "Semantic Scholar extracted view of \"An axiomatic definition of the programming language PASCAL\" by C. A. R. Hoare", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "title": "Program proving: Jumps and functions", "authors": ["Maurice Clint", "C. A. R. Hoare"], "date": 2004, "abstract": "SummaryProof methods adequate for a wide range of computer programs have been expounded in [1] and [2]. This paper develops a method suitable for programs containing functions, and a certain kind Of jump. The method is illustrated by the proof of a useful and efficient program for table lookup by logarithmic search.", "references": ["5d8056e326d4199d157a17fbeee97a7349d2824c", "9983c1923bd2cb5aa26c5f51dae9b99b31fe523d"], "page_rank": 6.157635467980295e-05}, {"id": "ad933a59832e17bc350f06b42827b9a05fe3cea8", "title": "Inverses of phrase structure generators", "authors": ["Sheila A. Greibach"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Inverses of phrase structure generators\" by Sheila A. Greibach", "references": [], "page_rank": 0.0004926108374384236}, {"id": "a17e7dce1c39b30d22158e4a8c133d0a4b36f1ca", "title": "The current state of proving programs correct", "authors": ["Ralph L. London"], "date": 1972, "abstract": "Presented are successful efforts in proving that computer programs are correct. Included are (i) the methods used, (ii) the wide class of programs (including systems programs) that have been proved, and (iii) implemented computer systems for demonstrating correctness. There is also a partially annotated bibliography.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "c4b1cd7f68a1c9c9295a11c6a31f3e227bf0e783", "title": "Subjective randomness and the capacity to generate information", "authors": ["Willem A. Wagenaar"], "date": 1970, "abstract": "Literature concerning randomization tasks contains contradictory results since both increases and decreases of nonrandomness have been reported as a function of the response rate. The present paper proposes that the contradictory results are attributable to differences in experimental procedure and the different mathematical definitions of \u2018objective\u2019 randomness. In an experiment Ss randomized 2, 3, 4, 6 or 8 alternatives with speeds of 4, 2, 1 and 0.5 sec per response. The results showed that there was no significant effect of response rate. There appeared to be substantial individual differences for first- and second-order nonrandomness.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5a19db7d827f623143e92bca8c082de5c014d59c", "title": "Proof of a program: FIND", "authors": ["C. A. R. Hoare"], "date": 1971, "abstract": "A proof is given of the correctness of the algorithm \u201cFind.\u201d First, an informal description is given of the purpose of the program and the method used. A systematic technique is described for constructing the program proof during the process of coding it, in such a way as to prevent the intrusion of logical errors. The proof of termination is treated as a separate exercise. Finally, some conclusions relating to general programming methodology are drawn.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "a5dc91c06f08197e342c44d93d80c8e016469f9c", "title": "Proving Properties of Complex Data Structures", "authors": ["Ben Wegbreit", "Jay M. Spitzen"], "date": 1976, "abstract": "This paper is concerned with proving properties of programs which use data structures. The goal is to be able to prove that all instances of a class (e.g. as defined in Simula) satisfy some property. A method of proof which achieves this goal, generator induction, is studied and compared to other proof rules and methods: inductive assertions, recursion induction, computation induction, and, in some detail, structural induction. The paper concludes by using generator induction to prove a characteristic property of an implementation of hashtables.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "5d8056e326d4199d157a17fbeee97a7349d2824c", "title": "An Axiomatic Basis for Computer Programming", "authors": ["C. A. R. Hoare"], "date": 1968, "abstract": "In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.", "references": [], "page_rank": 0.00030788177339901473}, {"id": "2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "title": "Procedures and parameters: An axiomatic approach", "authors": ["C. A. R. Hoare"], "date": 1971, "abstract": "It has been suggested, Hoare (1969), that an axiomatic approach to formal language definition might simultaneously contribute to the clarity and reliability of programs expressed in the language, and to the efficiency of their translation and execution on an electronic digital computer. This paper gives an exa~Lple of the application of the axiomatic method to the definition of procedure and parameter passing features of a high-level progr~mning language. It reveals that ease of demonstrating program correctness and high efficiency of implementation may be achieved simultaneously, provided that the programmer is willing to observe a certain familiar and natural discipline in his use of parameters.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "efa59953df102e903e882f3589c23c9a505d7aa8", "title": "On Classes of Program Schemata", "authors": ["Robert L. Constable", "David Gries"], "date": 1971, "abstract": "We define the following classes of program schemata: P = class of schemes using a finite number of simple variables PA = class of schemes using simple and subscripted variables (arrays) PR = class of schemes allowing recursive functions PL = class of schemes allowing labels as values Pm = class of schemes allowing a finite number of special markers as values Ppds = class of schemes using pushdown stores With these, we can also discuss for example PAm, the class of schemes allowing arrays, and special markers as values; and PAL the class of schemes allowing arrays, and labels as values. We argue that PA, PR, and PL faithfully represent mechanisms of subscripting, recursion, and labels as values, that are present in many \"real\" programming languages. We show that P \u2282 PR \u2282 PA \u2261 PAm \u2261 PAL \u2261 Ppdsm. Each of the inclusions and equivalences is effective, except for the equivalences concerning PA. For example, given a scheme in PAm an equivalent scheme in PA exists, but we also prove that you cannot (in general) construct it. We conjecture that PAL, PAm, and Ppdsm are indeed \"universal\".", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "65ba8fd8ef9c2a70cee99d2e5cab9302d0307a1e", "title": "An Introduction To Probability Theory And Its Applications", "authors": ["Feller William"], "date": 1950, "abstract": "Office hours: MWF, immediately after class or early afternoon (time TBA). We will cover the mathematical foundations of probability theory. The basic terminology and concepts of probability theory include: random experiments, sample or outcome spaces (discrete and continuous case), events and their algebra, probability measures, conditional probability A First Course in Probability (8th ed.) by S. Ross. This is a lively text that covers the basic ideas of probability theory including those needed in statistics. Theoretical concepts are introduced via interesting concrete examples. In 394 I will begin my lectures with the basics of probability theory in Chapter 2. However, your first assignment is to review Chapter 1, which treats elementary counting methods. They are used in applications in Chapter 2. I expect to cover Chapters 2-5 plus portions of 6 and 7. You are encouraged to read ahead. In lectures I will not be able to cover every topic and example in Ross, and conversely, I may cover some topics/examples in lectures that are not treated in Ross. You will be responsible for all material in my lectures, assigned reading, and homework, including supplementary handouts if any.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5b292d2f10da7585dd3487a197bf2852928f1c70", "title": "A contribution to the development of ALGOL", "authors": ["Niklaus Wirth", "C. A. R. Hoare"], "date": 1966, "abstract": "A programming language similar in many respects to ALGOL 60, but incorporating a large number of improvements based on six years' experience with that language, is described in detail. Part I consists of an introduction to the new language and a summary of the changes made to ALGOL 60, together with a discussion of the motives behind the revisions. Part II is a rigorous definition of the proposed language. Part III describes a set of proposed standard procedures to be used with the language, including facilities for input/output. A preliminary version of this report was originally drafted by the first author on an invitation made by IFIP Working Group 2.1 at its meeting in May, 1965 at Prince-ton. It incorporated a number of opinions and suggestions made at that meeting and in its subcommittees, and it was distributed to members of the Working Group as \"Proposal for a Report on a Successor of ALGOL 60\" it was felt that the report did not represent a sufficient advance on A~GOL 60, either in its manner of language definition or in the content of the hmguage itself. The draft therefore no longer had the status of an official Working Document of the Group and by kind permission of the Chairman it was released for wider publication. At that time the authors agreed to collaborate on revising and supplementing the draft. The main changes were: (1) verbal improvements and clarifications, many of which were kindly suggested by recipients of the original draft; (2) additional or altered language features, in particular the replacement of tree structures by records as proposed by the second author; (3) changes which appeared desirable in the course of designing a simple trod efficient implementation of the 1 anguage; (4) addition of introductory and explanatory material , and further suggestions for standard procedures, in particular on input/output; (5) use of a convenient notational facility to abbreviate the description of syntax, as suggested by van Wijn-gaarden in \"Orthogonal Design and Description of a Fornial Language\" (MR76, Mathemat.ical Centre, Am-sterdam, Oct. 1965). The incorporation of the revisions is not intended to reinstate the report as a candidate for consideration as a successor to ALGOL 60. However, it is believed that its publication will serve three purposes: (1) To present to a wider public a view of the general direction in which the development of ALGOL is proceeding; (2) To provide an opportunity \u2026", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "bac85a0e8836b887c9b62156a8fd70df8264b7c2", "title": "PRIME: a modular architecture for terminal-oriented systems", "authors": ["Herbert B. Baskin", "Barry R. Borgerson", "Roger Roberts"], "date": 1971, "abstract": "The architecture of most interactive systems is based on the general strategy that suitable terminal service can be provided by a central processor that is timemultiplexed among all the active terminals. In order to achieve adequate response time in an interactive environment, the CPU is usually time sliced. Other major system facilities such as I/O channels and secondary storage units are also shared among the users, and multiprogramming techniques are employed to keep all the major system resources as fully utilized as possible. An operating system is usually developed which performs these functions as well as supervising the terminal communications, implementing a system-wide filing subsystem, handling user commands, etc. The result of combining these and other functions into a time-sharing operating system is a highly complex software system which transforms what is basically a batch processing computer structure into a multi-terminal system with significant limitations that are an outgrowth of this strategy. While a failure can occur in any section of the hardware or software, we know that hardware failures are more likely to occur in the electromechanical and core memory sectors than in solid state logic, and that software failures tend to be concentrated in the more complex areas of code. Failures of hardware components may require modification of the operating system in order to regain operational status since the allocation strategies may need more than parametric modification when system resources are affected.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "43aa639d8df2285bd805531889d1d5f49563c17d", "title": "Concurrent control with \u201creaders\u201d and \u201cwriters\u201d", "authors": ["Pierre-Jacques Courtois", "F. Heymans", "David Lorge Parnas"], "date": 1971, "abstract": "The problem of the mutual exclusion of several independent processes from simultaneous access to a \u201ccritical section\u201d is discussed for the case where there are two distinct classes of processes known as \u201creaders\u201d and \u201cwriters.\u201d The \u201creaders\u201d may share the section with each other, but the \u201cwriters\u201d must have exclusive access. Two solutions are presented: one for the case where we wish minimum delay for the readers; the other for the case where we wish writing to take place as early as possible.", "references": ["3fea018ca5e6fecf60a90c2612391f9805c86c15"], "page_rank": 9.852216748768472e-05}, {"id": "e3aa3d449a62500dc04ce4a1f8074142e38963cf", "title": "Automated programmering: the programmer's assistant", "authors": ["Warren Teitelman"], "date": 1972, "abstract": "This paper describes a research effort and programming system designed to facilitate the production of programs. Unlike automated programming, which focuses on developing systems that write programs, automated programmering involves developing systems which automate (or at least greatly facilitate) those tasks that a programmer performs other than writing programs: e.g., repairing syntactical errors to get programs to run in the first place, generating test cases, making tentative changes, retesting, undoing changes, reconfiguring, massive edits, et al., plus repairing and recovering from mistakes made during the above. When the system in which the programmer is operating is cooperative and helpful with respect to these activities, the programmer can devote more time and energy to the task of programming itself, i.e., to conceptualizing, designing and implementing. Consequently, he can be more ambitious, and more productive.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "a52c440632745862ea07987a15da575f4b442263", "title": "Sample proportions and subjective probability revisions", "authors": ["Lee Roy Beach", "James A. Wise", "Scott Barclay"], "date": 1970, "abstract": "The hypothesis was that the proportions of poker chips in the displayed samples influence subjective probability revisions that are obtained in \u201cbook bags-and-poker chips\u201d experiments. Subjects revised for simultaneous and sequential samples from two 80%\u201320% symmetrical binomial populations and two 70%\u201330% symmetrical binomial populations. Sample proportions account in large part for the revision responses for both kinds of populations for simultaneous samples. For sequential samples, however, proportions appeared to have less influence on revision responses even though 62% of the subjects claimed to use them. The implications are discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ba5b197521b5368f42c9f1d3e08220a379bd1643", "title": "Accuracy of perceptual recall: An analysis of organization", "authors": ["Murray Glanzer", "William H. Clark"], "date": 1963, "abstract": "Summary A series of investigations was carried out to analyze the determinants of the difficulty of perceptual recall of a systematically varied set of stimuli. The stimuli were arrays of eight shapes that were each either black or white. In Part I, the accuracy with which S s could reproduce these arrays under .5-sec. exposure was determined. In Part II, another method involving discrimination between arrays yielded a similar ranking of accuracy scores. This indicated that the scores obtained in Part I were not a function of the particular method used. The accuracy scores of Part I were then subjected to various types of analysis. An analysis based on information measure showed some success in accounting for the difficulty of individual arrays. For several reasons this analysis was found to be unsatisfactory. Another type of analysis was constructed, based on the verbal-loop hypothesis\u2014the hypothesis that the S s' perceptual processing includes a covert verbalization and that the length of the verbalization determines the difficulty of the stimulus for perceptual tasks. Empirically derived measures based on this hypothesis were shown to account for a major part of the variance in array difficulty. The relevance of the assumption and the findings to the general problem of perceptual organization and encoding is discussed. The verbal-loop hypothesis is presented as an alternative to gestalt and information theory analyses of organization.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "894fc603f9b16e775f95045fb805b5d7e6935944", "title": "Belief in the law of small numbers.", "authors": ["Amos Tversky", "Daniel Kahneman"], "date": 1971, "abstract": "\u201cSuppose you have run an experiment on 20 subjects, and have obtained a significant result which confirms your theory ( z = 2.23, p If you feel that the probability is somewhere around .85, you may be pleased to know that you belong to a majority group. Indeed, that was the median answer of two small groups who were kind enough to respond to a questionnaire distributed at meetings of the Mathematical Psychology Group and of the American Psychological Association. On the other hand, if you feel that the probability is around .48, you belong to a minority. Only 9 of our 84 respondents gave answers between .40 and .60. However, .48 happens to be a much more reasonable estimate than .85. Apparently, most psychologists have an exaggerated belief in the likelihood of successfully replicating an obtained finding. The sources of such beliefs, and their consequences for the conduct of scientific inquiry, are what this paper is about. Our thesis is that people have strong intuitions about random sampling; that these intuitions are wrong in fundamental respects; that these intuitions are shared by naive subjects and by trained scientists; and that they are applied with unfortunate consequences in the course of scientific inquiry.", "references": ["b30ce3350bcdce81b629473f14fa06b80298f10b", "63f8cd11439c2c2f15b96ad9507ed2643cc83bdf"], "page_rank": 0.00010399562123700055}, {"id": "2e55dd55b8fdedd809a90acb322696417d06de66", "title": "COMPARISON OF BAYESIAN AND REGRESSION APPROACHES TO THE STUDY OF INFORMATION PROCESSING IN JUDGMENT", "authors": ["Paul Slovic", "Sarah Lichtenstein"], "date": 1971, "abstract": "Abstract In recent years there have been several hundred studies within the rather narrowly-defined topic of information utilization in judgment and decision making. Much of this work has been accomplished within two basic schools of research, which we have labeled the \u201cregression\u201d and the \u201cBayesian\u201d approaches. Each has its characteristic tasks and characteristic information that must be processed to accomplish these tasks. For the most part, researchers have tended to work strictly within a single approach and there has been minimal communication between the resultant subgroups of workers. Our objective here is to present a review and comparative analysis of these two approaches. Within each, we examine (a) the models that have been developed for describing and prescribing the use of information in decision making; (b) the major experimental paradigms, including the types of judgment, prediction, and decision tasks and the kinds of information that have been available to the decision maker in these tasks; (c) the key independent variables that have been manipulated in experimental studies; and (d) the major empirical results and conclusions. In comparing these approaches, we seek the answers to two basic questions. First, do the specific models and methods characteristic of different paradigms direct the researcher's attention to certain problems and cause him to neglect others that may be equally important? Second, can a researcher studying a particular substantive problem increase his understanding by employing diverse models and diverse experimental methods?", "references": ["418b44c21429cb772a0091382a340baaf4f74ac0", "9a82d5fe91d442baaf0134796bb36df39fe3f0f9", "5f4f1a7f568fead8f32b863e925106fe1f12f648", "a6a9fc49be5dd67587dcfa8b3a0775fcdb5ceb55", "eaaee22c7d38b9799e0cb2c88d5f2eccfbe55e7a", "171d2fa8fb393f762b3245adb1548465c1ca33fe", "0a0fe1fe42f595e829ec06ad998365332009cbff", "da3b59db6a1c939796aab7800abd535a01f7ba7e", "ae9f2562374a52e5e18f3d2ea97790bdfe71a758", "d14879721351091992bc693015962edcb6b5f186"], "page_rank": 0.00010399562123700055}, {"id": "2dd45e770ddbb18e9509212fd5a24440740870c2", "title": "Man as an intuitive statistician.", "authors": ["Cameron R. Peterson", "Lee Roy Beach"], "date": 1967, "abstract": "This review considers experimental research that has used probability theory and statistics as a framework within which to study human statistical inference. The experiments have investigated estimates of proportions, means, variances, and correlations, both of samples and of populations. In some experiments, parameters of populations were stationary; in others, the parameters changed over time. The experiments also investigated the determination of sample size and trial-by-trial predictions of events to be sampled from a population. In general, the results indicate that probability theory and statistics can be used as the basis for psychological models that integrate and account for human performance in a wide range of inferential tasks.", "references": ["bb789689eaa2f3e5a8565dd4b0ada7e5cd5456f7", "6235a56028de520f22e97b68729fdcffd94279e0", "f758cde8eb244825203192052f34d32ee8aa4d2e", "59ba972373c39a65e8d4affb4862a615467bf0c9", "4f1adf8baa87ad2ce9b81677d34f761004dcee88", "1c62ebe01d9ad6796cd65fad5aa04a8e1419f0c3", "a14fe54972b00061187814008a76d38d3f97a843", "dfa707fa6d7affeed754fa67e5d3718454d7bf56", "46f3fee4a13588c03b12e3ba242adb9e125d04ca", "5f8c3ddf5f229dcc3cf3069439e48680fb489b06"], "page_rank": 4.926108374384236e-05}, {"id": "12279cfe64cdcaa7292a3608734ccde70fde63d1", "title": "Stimulus encoding and memory.", "authors": ["Robert E. Warren"], "date": 1972, "abstract": "An improved method and improved apparatus for harvesting baled crops wherein a trailing bale stacker is provided which is adapted to be pulled over the ground by a baling machine, the bale stacker including pallet support means and platform means, and enabling the manual stacking of baled crops onto pallets for subsequent haulage to and storage in a storage area.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "1aee45faf20c9d16041cc7b7eb93791065bdd420", "title": "Lexical information processing during sentence comprehension", "authors": ["Helen Smith Cairns", "Joan Kamerman"], "date": 1975, "abstract": "This article reports two experiments, a phoneme monitoring experiment and a sentence completion experiment, conducted with matched materials on subjects drawn from the same population. There are three major conclusions: ( a ) That in the case of ambiguous lexical items, all meanings are retrieved, but following a decision stage only one is transferred to working memory; ( b ) that the two experimental tasks investigated do not tap the same psycholinguistic processes; and ( c ) that there is a lexical retrieval stage operable in both comprehension and production which affects response latencies.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "da3b59db6a1c939796aab7800abd535a01f7ba7e", "title": "Subjective sampling distributions and conservatism", "authors": ["Gloria E. Wheeler", "Lee Roy Beach"], "date": 1968, "abstract": "When people revise subjective probabilities in light of data, revisions are less than the amount prescribed by the normative model, Bayes's theorem. Previous research suggests that this results from the subjects' lack of understanding of the implications of the data; i.e., from inaccurate subjective sampling distributions. This experiment examined the effects on conservatire revisions of training subjects about, the implications of data. The subjects estimated sampling distributions for two binomial populations, were shown samples from the populations in order to teach them veridical distributions, and again estimated sampling distributions. Estimated sampling distributions were good predictors of revisions and, as a result of training, both the sampling distributions and the revisions became more veridical. A number of recent experiments have shown that when people revise their subjective probabilities about hypotheses in light of data, the revisions tend to be less than the amount prescribed by the appropriate normative model (e.g., Phillips and Edwards, 1966; Phillips, Hays, and Edwards, 1966; Peterson and Miller, 1965; Schum, Goldstein, and Southard, 1966). This phenomenon is called conservatism (Edwards, Lindman, and Phillips, 1965) and two explanations have been advanced to account for it. The first is that subjects have difficulty in performing the mechanics of integrating the meaning of the data into their subjective", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "277ace70085295ca32d4514ce3ab99ef218d0c4b", "title": "Perseverative neural processes and consolidation of the memory trace.", "authors": ["Stephen E. Glickman"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"Perseverative neural processes and consolidation of the memory trace.\" by Stephen E. Glickman", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "2a71481851b5e9a3f60285bb9d1d990ec471f9ef", "title": "Discussions on Child Development.", "authors": ["Henry Work"], "date": 1959, "abstract": "Physical and Behavioral Growth. Report of the Twenty-Sixth Ross Pediatric Research Conference. Price, not given. Pp. 101, with 15 illustrations. Ross Laboratories, Columbus 16, Ohio, 1958. A previous review in this Journal described volumes 1 and 2 of the discussions on Child Development. The publication of the third volume adds to the knowledge and satisfaction to be gained from following these erudite discourses. Those interested in this method of presentation of knowledge about children and their growth will also be pleased with the publication of the Ross Conference on Growth. Of special interest in the WHO volume are the studies of Margaret Mead on sex differentiation and those of Erik Erikson on the adolescent's problem in finding his identity. Dr. Mead's contribution reflects the common concern over differentials in the manner in which the two sexes are reared. Her evidence concerning such environmental differences is most full and suggests important", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "0060344c7de6d7294c6c4bc8ed56a0f5da47596e", "title": "Elements of a theory of human problem solving.", "authors": ["Allen Newell", "J. C. Shaw", "Herbert A. Simon"], "date": 1958, "abstract": "Semantic Scholar extracted view of \"Elements of a theory of human problem solving.\" by Allen Newell et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d0c96f3fe415103229c34ae4cbf4498db3f04eae", "title": "Fate of first-list associations in transfer theory.", "authors": ["Janine M. Barnes", "Benton J. Underwood"], "date": 1959, "abstract": "Semantic Scholar extracted view of \"Fate of first-list associations in transfer theory.\" by Janine M. Barnes et al.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "001d983fbb462ca617942d51ff59592a78e30008", "title": "Cognition and Thought", "authors": ["Gregory R. Lockhead"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"Cognition and Thought\" by Gregory R. Lockhead", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "4dc02d4ad2b0b9a2b9f050fb05418917f9687b25", "title": "How Implication Is Understood", "authors": ["Philip N. Johnson-Laird", "Joanna Tagart"], "date": 1969, "abstract": "An electronic backgammon game is disclosed. A memory stores m unique sets of dice rolls, each of the m sets of dice rolls includes n sequential rolls and defines a unique game to be played. A game select input is provided for enabling a player to select a desired one of the m sets of dice rolls and thereby to select a unique game to be played. A human actuatable dice roll switch controls the operation of the game. Each time the dice roll switch is reactivated, the next sequential roll in the selected set of dice rolls is displayed on a display board.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "5bdd2b788e572dee2029198ac11b0e9e895ff6ac", "title": "Proving a Disjunctive Rule", "authors": ["Peter Cathcart Wason", "Philip N. Johnson-Laird"], "date": 1969, "abstract": "This experiment was designed to determine whether individuals reason correctly about disjunctive rules. The task consisted in the selection of appropriate instances either to prove, or to disprove, a given disjunctive rule. When the first component of the rule was negated, i.e. when the rule was logically equivalent to implication (p v q), the selection of appropriate instances was significantly more difficult than when the first component was not negated. The majority of subjects, however, revealed patterns of reasoning which were unstable and labile. The results are discussed in relation to those of previous experiments in which subjects had to reason about conditional sentences.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "8f71a9729ae782fa0d3b077d8cba97fcc636e70e", "title": "Reasoning about a Rule", "authors": ["Peter Cathcart Wason"], "date": 1968, "abstract": "Two experiments were carried out to investigate the difficulty of making the contra-positive inference from conditional sentences of the form, \u201cif P then Q.\u201d This inference, that not-P follows from not-Q, requires the transformation of the information presented in the conditional sentence. It is suggested that the difficulty is due to a mental set for expecting a relation of truth, correspondence, or match to hold between sentences and states of affairs. The elicitation of the inference was not facilitated by attempting to induce two kinds of therapy designed to break this set. It is argued that the subjects did not give evidence of having acquired the characteristics of Piaget's \u201cformal operational thought.\u201d", "references": ["ad88c13e5f51e169c832401e12cc51513a644a35", "0589beeac9ffdac4e8a06ef42f3d17beec879361", "5fe2437b4015cbea1013709471b7d384108a32e5"], "page_rank": 0.00016009852216748767}, {"id": "9983c1923bd2cb5aa26c5f51dae9b99b31fe523d", "title": "Notes on Avoiding \"go to\" Statements", "authors": ["Donald E. Knuth", "Robert W. Floyd"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Notes on Avoiding \"go to\" Statements\" by Donald E. Knuth et al.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "38f60bccc379144fd3e039d8b00470731f1517ef", "title": "The material basis of memory", "authors": ["Ralph Waldo Gerard"], "date": 1963, "abstract": "Any system--and I shall not define system more closely than as a group of somehow integrated subordinate units that forms an entity of its own--mus( have a certain architecture, structure, or morphology, which is reasonably constant in time. The way the system is put together, I like to call its \"being.\" This system interacts with its environment, responding to stimuli--whether at the more complex level of organisms giving behavioral responses or simply a rubber band yielding to a weight--and most of these responses are in effect ephemeral, reversible changes in time. The system yields and restores itself, and this I like to call its \"behaving.\" But under certain conditions the interaction of the system and its environment leads to irreversible changes; the system has altered as a result of its experience. I t fixes its experience and so becomes something different, and this I like to call \"becoming.\" So this addiction to alliteration gives us architecture or being, functioning or behaving, and development or becoming. Becoming subsumes, of course, development of the individual, evolution of the species, history of the particular society or social group of any kind, and learning in the individual. And learning may include, if you accept a broad definition, changes as varied as: the hypertrophy of a muscle with exercise; the horny hands of a laborer; and the many other material changes that record the p a s t as in that lovely couplet on weatherbeaten trees: Is it as plainly in our living shown, By slant and twist, which way the wind hath blown?", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "c86b22c7e2b84a06eea51917aa3d4bf45775f624", "title": "Insight into a logical relation", "authors": ["Philip N. Johnson-Laird", "Peter Cathcart Wason"], "date": 1970, "abstract": "Two experiments are reported which aimed to investigate factors affecting the gain of insight into the logical relation of implication. In the first experiment, subjects had to make a series of inferences about either a conditional sentence or a quantified sentence, both of which had the same underlying logical form. Under one condition the sentences had to be proved true, and under another condition, false. Proving a sentence false facilitated gain of insight, but the linguistic form of the sentence exerted no significant effect on the main dependent variable. In the second experiment, implication was not expressed as a sentence but was inherent in the structure of the task. The experimental material differed in complexity and allowed the cognitive load imposed on the subject to be varied. Results suggested that insight was not all-or-none. It was spontaneously gained when the material was simple, but temporarily lost when it was complex.", "references": ["11fa4c9340a37256089ccff8a7e43ec4c4b92d34", "5bdd2b788e572dee2029198ac11b0e9e895ff6ac", "5fe2437b4015cbea1013709471b7d384108a32e5", "8f71a9729ae782fa0d3b077d8cba97fcc636e70e", "4dc02d4ad2b0b9a2b9f050fb05418917f9687b25"], "page_rank": 6.157635467980295e-05}, {"id": "c0069916fe269ff59f601dfdf2a091387f883471", "title": "Flow of information within the organism", "authors": ["Donald E. Broadbent"], "date": 1963, "abstract": "Summary This paper outlines the general approach of drawing inferences about the flow of information inside the organism, and summarizes the evidence for distinguishing short-term from long-term memory. A minor experiment is described, in which short-term memory is compared for items which have been followed by a fixed number of interfering items at fast and at slow speeds. The latter produced inferior performance, which is consistent with a decay theory. The greatest emphasis is placed, however, upon the different role of similarity in short- and in long-term memory.", "references": [], "page_rank": 0.00017241379310344826}, {"id": "c5b4337a952f602f90edb7b68995441259cd8674", "title": "Interference and forgetting.", "authors": ["Benton J. Underwood"], "date": 1957, "abstract": "I know of no one who seriously maintains that interference among tasks is of no consequence in the production of forgetting. Whether forgetting is conceptualized at a strict psychological level or at a neural level (e.g., neural memory trace), some provision is made for interference to account for at least some of the measured forgetting. The many studies on retroactive inhibition are probably responsible for this general agreement that interference among tasks must produce a sizable proportion of forgetting. By introducing an interpolated interfering task very marked decrements in recall can be produced in a few minutes in the laboratory. But there is a second generalization which has resulted from these studies, namely, that most forgetting must be a function of the learning of tasks which interfere with that which has already been learned (19). Thus, if a single task is learned in the laboratory and retention measured after a week, the loss has been attributed to the interference from activities learned outside the laboratory during the week. It is this generalization with which I am concerned in the initial portions of this paper. Now, I cannot deny the data which show large amounts of forgetting produced by an interpolated list in a few minutes in the laboratory. Nor do I deny that this loss may be attributed to interference. But I will try to show 1 Address of the president, Midwestern Psychological Association, St. Louis, Missouri,", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "fed1acee3ef1d8703c9ac30ddfe164ec5ab473a9", "title": "Perception and communication", "authors": ["Donald E. Broadbent"], "date": 1958, "abstract": "First published in 1958, this book has become recognized as a classic in its field. It marked a transition between behaviourist learning theory and the modern 'information processing' or 'cognitive' approach to perception and communication skills. It continues to provide a principal starting point for theoretical and experimental work on selective attention. As Professor Posner writes in his Foreword to the reissue: 'it remains of great interest to view the work in its original form and to ponder those creative moments when the mind first grasps a new insight and then struggles to work out its consequences.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "64a86e32d2edb67bd284c4bd43950935a5597064", "title": "Proactive inhibition in short-term retention of single items", "authors": ["Geoffrey Keppel", "Benton J. Underwood"], "date": 1962, "abstract": "Summary Three experiments were performed to determine the relationship between certain variables influencing proactive inhibition in long-term retention of lists of verbal items and the influence of these variables on short-term retention of single items. More particularly, retention of single items over 18 sec. should, if the laws of long-term retention are applied, decrease with number of previous items to which S has been exposed. In addition, amount of forgetting should be a direct joint function of number of previous items and length of the retention interval. In Exp. 1 each S was presented consonant syllables singly, with retention being measured after 3, 9, and 18 sec. Forgetting of the first item presented (T-1) was less than for the second (T-2) or third (T-3) item, but forgetting of the latter (T-2 vs. T-3) did not differ. On all three tests forgetting was directly related to length of retention interval, but no interaction was evident between number of previous items and length of retention interval. In Exp. 2 a higher degree of initial learning of the items was achieved. Forgetting increased directly as a function of number of previous items presented. The predicted interaction was indeterminate since retention was essentially 100% on T-1 for all retention intervals. Experiment 3 tested retention of six successive items over 3- and 18-sec. intervals. Retention after 3 sec. showed an initial drop and then a rise over the six tests, the rise suggesting a practice effect. Forgetting over 18 sec. increased directly from T-1 to T-6 and there was no indication that a constant amount of proactive interference had been reached. The interaction between length of retention interval and number of potential proactively interfering items was very evident. The results were interpreted to mean that proactive inhibition in short-term memory of single items follows the same laws as proactive inhibition in long-term memory of lists of items.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "e940dce76e423390383fab059db2a77c3649e2ba", "title": "Short-Term Memory and Incidental Learning", "authors": ["Leo Postman"], "date": 1964, "abstract": "Publisher Summary This chapter reviews experimental methods and findings in the study of short-term retention and incidental learning. There are important continuities between the theoretical and methodological problems in these two areas. Both types of studies are concerned with basic capacities and dispositions that the learner brings to the experimental situation and that determine the initial reception and immediate storage of information. Such experiments are not primarily designed to investigate the laws governing the integration of responses and the growth of associative strength. Rather, they are concerned with a detailed analysis of some of the conditions that limit and bias the subject's (S's) responses in a learning situation. The extended practice takes its departure from these initial dispositions of the learner. In the conventional rote-learning experiment, exposures and tests follow each other in a fixed order within successive trials. When a single presentation of the learning materials is followed without delay by a test of performance, the measures of retention define the amount of immediate memory.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "790cf503e315b4196f451333ba20842591b08e15", "title": "A CONFLICT BETWEEN SELECTING AND EVALUATING INFORMATION IN AN INFERENTIAL TASK", "authors": ["Peter Cathcart Wason", "Philip N. Johnson-Laird"], "date": 1970, "abstract": "An inferential task was investigated in which the subjects had to select which of four cards they needed to inspect in order to determine whether a rule was true or false. In one condition crucial information was concealed on the other side of the cards, and in another condition it was on the same side of the cards, but covered by a mask. A previous experiment suggested that subjects sometimes confused the notion of \u2018the other side of the card\u2019. But no difference was found between these two conditions. Only two out of the 36 subjects initially made the correct selection. \n \n \n \nAn attempt was made subsequently to enable the subjects to correct their errors by asking them to evaluate the cards in relation to the rule. When a conflict occurred between the selection of the cards and their evaluation, some insight was gained. In other cases these two processes passed one another by, in spite of the fact that this involved self-contradiction.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "199cda6baea15aaf1df993cbb950331ba5f729d1", "title": "Judging the veracity of ambiguous sentences", "authors": ["Peter Wright Carey", "Jacques Mehler", "Thomas G. Bever"], "date": 1970, "abstract": "Eighty S s saw a picture, heard a sentence, and judged the sentence true or false with respect to the picture. After five unambiguous sentences of a single syntactic structure, and ambiguous sentence was presented. Results for the ambiguous sentence revealed ( a ) that the ambiguity was most often perceived when both interpretations of the sentence were true with respect to the picture, ( b ) that response latencies were shortest when both interpretations were false, ( c ) that S s who claimed to have seen the ambiguity before responding had longer latencies than those who claimed not to have seen the ambiguity, and ( d ) that these differences in processing were clear only in S s' responses to their first ambiguous sentence. A model for the pragmatic and syntactic processes is considered.", "references": ["126eeb7ed23c564673c093b9c1a9d1730bb48b2e", "c9cfc85a98014a8c638b1bc6608f5405b4b8c726", "ddc03dad33c9825a192847a82cd062fc8107935f", "6e661e50a75d2fc9199d2e6c6dbbd021698f1e89", "a62f66001092db361059fcd679fab6410d9ad054", "c6a34306fefed62adb0fde40b46863b848762ce7", "fa6e4b65800bdaff76b2098d0e1b19c0c7803e06", "3c677176ad1368247f9737a84358db9d9203744f", "c5000de14f9b80bb0ea12e4c70262a84e0cc2843"], "page_rank": 6.157635467980295e-05}, {"id": "6a866273326edc73d1b71b3fc86d06db8bd6f8db", "title": "Resolving ambiguity: Effects of biasing context in the unattended ear\u2606", "authors": ["James R. Lackner", "Merrill F. Garrett"], "date": 1972, "abstract": "Abstract Ambigous sentences and a disambiguating context sentence were dichotically presented to subjects who were instructed to attend to the channel over which the ambiguous sentences were presented. Subjects were required to paraphrase the sentence in the attended channel immediately upon its presentation. The disambiguating material (in the unattended channel) was presented at a level 5 to 10 db less intense than the attended channel; in post-test reports subjects were unable to produce any information about the content of the unattended channel. Nonetheless, for four types of ambiguity tested, the bias contexts significantly influenced the interpretation of the ambiguous sentences. This result is taken to indicate both that there is structural analysis of the material in the unattended channel and that, during their input, multiple readings are computed for ambiguous sentences.", "references": [], "page_rank": 0.00011631089217296114}, {"id": "05e30356a92a2fb2d370da3a2ea76f3cf2149169", "title": "INTRATRIAL AND INTERTRIAL RETENTION: NOTES TOWARDS A THEORY OF FREE RECALL VERBAL LEARNING.", "authors": ["Endel Tulving"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"INTRATRIAL AND INTERTRIAL RETENTION: NOTES TOWARDS A THEORY OF FREE RECALL VERBAL LEARNING.\" by Endel Tulving", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "989b4df6e6606a0ad880e3ddc13ad0ba327c0ece", "title": "Context effects in sentence comprehension: A study of the subjective lexicon", "authors": ["Carol K. Conrad"], "date": 1974, "abstract": "These studies explore the role of context in determining what information about the meanings of words is activated in memory at the time a word is encountered in a sentence. Using a color-naming paradigm, it was shown that both meanings of a word that has two distinct meanings are activated in memory at the time the word is heard in a sentence. This activation occurs even when there is sufficient contextual information to indicate which meaning was intended by the speaker. These results support the hypothesis that there exists in memory an isolable subjective lexicon. They suggest that context which is effective in disambiguating lexical ambiguities in the language has its effect only at a relatively late stage in the cognitive processing involved in language comprehension.", "references": ["d0d6ba169089cb18b525f2f1cb4f3004364f43f5", "6a866273326edc73d1b71b3fc86d06db8bd6f8db", "b226ef480082296af8b20082f4954f931ac29328", "c33136bbd76f56bb9dddc56eec981f55e1c64c47", "ddc03dad33c9825a192847a82cd062fc8107935f", "24e37aed8ee8f9de4d7bb73559dd36c40b6f4860", "5250aec907afa5651029a864f55acc2032cc0d3e", "7adb3c40ef03a458d35a3851fa66046936211cc3", "12279cfe64cdcaa7292a3608734ccde70fde63d1"], "page_rank": 6.157635467980295e-05}, {"id": "408e974bfb027cc4b1a1f42da29feb3cce4d7ff3", "title": "Phoneme-monitoring reaction time as a function of preceding intonation contour", "authors": ["Anne Cutler"], "date": 1976, "abstract": "An acoustically invariant one-word segment occurred in two versions of one syntactic context. In one version, the preceding intonation contour indicated that a stress would fall at the point where this word occurred. In the other version, the preceding contour predicted reduced stress at that point. Reaction time to the initial phoneme of the word was faster in the former case, despite the fact that no acoustic correlates of stress were present. It is concluded that a part of the sentence comprehension process is the prediction of upcoming sentence accents.", "references": ["d764e7a422596bb93938aa48cdd3a13f1b73d70d", "1eb1e18346db11cc2da9909673d964b87585afa9", "0d4fece503daa35a755c0ab2b49ccb73493a3bdb", "ccbbcd85073c188b741fbc93590a513bbd4c81a2", "b226ef480082296af8b20082f4954f931ac29328", "9052dc84c6fe4bb3c6e5c7aaa9ed1c6f9735d653", "d9d5f7bfa50432d14c92aa6c24bd6f2e967b4068", "a60896998331f1aad772bee0c0ea15c4d09a0794", "1d98f14032ce93812155feb2866b721d75ea6013", "d4c12e482ad2ab86d7092314f328ea8d468781c8"], "page_rank": 6.157635467980295e-05}, {"id": "429e26d55e4fbedaea78d1f910eca164f12f23a7", "title": "CULTURAL NORMS FOR VERBAL ITEMS IN 43 CATEGORIES", "authors": ["Burton H. Cohen", "Weston A. Bousfield", "G. A. Whitmarsh"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"CULTURAL NORMS FOR VERBAL ITEMS IN 43 CATEGORIES\" by Burton H. Cohen et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ab92bbf18b4862639c551043eb0286202e923085", "title": "Some-or-none characteristics of coding behavior", "authors": ["Burton H. Cohen"], "date": 1966, "abstract": "Evidence derived from a variety of experiments dealing with the free recall of categorized word-lists is presented in support of a some-or-none rationale. As discussed here, some-or-none represents the ability of S s to either recall a high proportion of the words of a given category or fail to recall any of the words of the category. When S s do recall the words of a category, the mean word-recall per category appears to be invariant with respect to list length, rate of presentation, serial position effects, sex of S , and size of category.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "62edcc7d7f76dc9b12c9ed6f6864ba1045577452", "title": "Some Tests of the Decay Theory of Immediate Memory", "authors": ["John Brown"], "date": 1958, "abstract": "The hypothesis of decay of the memory trace as a cause of forgetting has been unpopular. The reasons for this unpopularity are criticized and a theory of the memory span, based on this hypothesis, is put forward. Three experiments which test the hypothesis are described. In each, two kinds of stimuli are presented to the subject, viz., \u201crequired\u201d stimuli, which he attempts to remember, and \u201cadditional\u201d stimuli, to which he merely makes responses. The first experiment will show that even when the number of required stimuli is well below the memory span, forgetting occurs if the presentation of additional stimuli delays recall for several seconds. The second shows that the effect of the additional stimuli depends only slightly on their similarity to the required stimuli: it also shows that their effect is negligible when they precede, instead of follow, the required stimuli. The third shows that the effect of additional stimuli interpolated before recall remains considerable even when there is an interval of several seconds between presentation of required and additional stimuli.", "references": [], "page_rank": 0.00011083743842364531}, {"id": "caf677b63e29ee338db2f7db6b0449a82d38540e", "title": "Applications of multiprocess models for memory to continuous recognition tasks", "authors": ["Robert Freund", "Geoffrey R. Loftus", "Richard C. Atkinson"], "date": 1969, "abstract": "Abstract Several multiprocess models for memory and learning are applied to the results of an experiment comparing performance on four types of recognition tests. The task involved a continuous sequence of trials, each trial consisting of a test on one of the stimuli followed by a study on that same stimulus paired with a new response. One of four types of tests was presented on each trial, the choice of test being made randomly. The four types of tests employed were a 2, 4, and 26 alternative forced-choice test, and a yes-no test. During the study period the subject had no way of knowing which mode of test would be given, and thus could not engage in differential storage processes. The basic dependent variable was the probability of a correct response as a function of the number of trials intervening between study and test on a given stimulus-response pair (called the \u201clag\u201d). The lag curves for the forced-choice tests and the hit curve for the yes-no test increased monotonically as the lag increased, while the false alarm curve of the yes-no test increased as the lag increased. A model which postulates a distinction between short-term and long-term memory stores was applied successfully to these data. The model assumes that information in short-term store is perfectly retrievable and utilizes an analysis derived from Signal Detectability Theory to describe long-term processes.", "references": ["0ba1f9fd488b58df71dc823c99c5c6702291cd0e", "19b36be12d3e56f20b50e7679de22867f20538d4", "bc2243926c1bf47776e850f1f68dd9566d68398b", "8403f0d7d567b31a73cf5c71c61e939991bb18d0", "56c16d9e2a5270ba6b1d83271e2c10916591968d", "72c92770a7939b25942a8c372dcf957cc5327811", "eb8921d3edd3be7c699ed338af939825cd686d97", "70b28a0d4f275fdd3f6f128f72cb3d9928d60d84", "bea7bf6a505a397856fc7b8d30ce505b833088c3"], "page_rank": 4.926108374384236e-05}, {"id": "a7e6efb049b364c579945d4286d331b9503fe2a3", "title": "Structuration and destructuration of responses in free-recall learning", "authors": ["St{\\'e}phane Ehrlich"], "date": 1970, "abstract": "In a free-recall learning experiment, S s first learned a list of 20 familiar words. They were then tested with 10-word and 15-word subsets of the originally learned list, under conditions of single-trial free-recall conditions. The subsets partially overlapped each other on successive trials. In the final phase of the experiment, S s recalled the original 20-word list. The results showed that S s cannot reproduce correctly parts of the learned whole list. This result is explained in terms of the destructuration process that results from the omission of some of the elements of the associated chain developed by S s during original learning. The results support the general hypothesis that structuration is a necessary condition of free-recall learning.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0012e189a719cc29c3fb1ac597286e9052c0ba82", "title": "The simulation of verbal learning behavior", "authors": ["Edward A. Feigenbaum"], "date": 1961, "abstract": "An information processing model of elementary human symbolic learning is given a precise statement as a computer program, called Elementary Perceiver and Memorizer (EPAM). The program simulates the behavior of subjects in experiments involving the rote learning of nonsense syllables. A discrimination net which grows is the basis of EPAM's associative memory. Fundamental information processes include processes for discrimination, discrimination learning, memorization, association using cues, and response retrieval with cues. Many well-known phenomena of rote learning are to be found in EPAM's experimental behavior, including some rather complex forgetting phenomena. EPAM is programmed in Information Processing Language V.\n H. A. Simon has described some current research in the simulation of human higher mental processes and has discussed some of the techniques and problems which have emerged from this research. The purpose of this paper is to place these general issues in the context of a particular problem by describing in detail a simulation of elementary human symbolic learning processes.\n The information processing model of mental functions employed is realized by a computer program called Elementary Perceiver and Memorizer (EPAM). The EPAM program is the precise statement of an information processing theory of verbal learning that provides an alternative to other verbal learning theories which have been proposed. It is the result of an attempt to state quite precisely a parsimonious and plausible mechanism sufficient to account for the rote learning of nonsense syllables. The critical evaluation of EPAM must ultimately depend not upon the interest which it may have as a learning machine, but upon its ability to explain and predict the phenomena of verbal learning.\n I should like to preface my discussion of the simulation of verbal learning with some brief remarks about the class of information processing models of which EPAM is a member.\n a. These are models of mental processes, not brain hardware. They are psychological models of mental function. No physiological or neurological assumptions are made, nor is any attempt made to explain information processes in terms of more elementary neural processes.\n b. These models conceive of the brain as an information processor with sense organs as input channels, effector organs as output devices, and with internal programs for testing, comparing, analyzing, rearranging, and storing information.\n c. The central processing mechanism is assumed to be serial; i.e., capable of doing only one (or a very few) things at a time.\n d. These models use as a basic unit the information symbol; i.e., a pattern of bits which is assumed to be the brain's internal representation of environmental data.\n e. These models are essentially deterministic, not probabilistic. Random variables play no fundamental role in them.", "references": [], "page_rank": 0.00022577996715927748}, {"id": "3fea018ca5e6fecf60a90c2612391f9805c86c15", "title": "Solution of a problem in concurrent programming control", "authors": ["Edsger W. Dijkstra"], "date": 1965, "abstract": "A number of mainly independent sequential-cyclic processes with restricted means of communication with each other can be made in such a way that at any moment one and only one of them is engaged in the \"critical section\" of its cycle.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "63f8cd11439c2c2f15b96ad9507ed2643cc83bdf", "title": "Categories of Human Learning", "authors": ["Arthur W. Melton"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Categories of Human Learning\" by Arthur W. Melton", "references": [], "page_rank": 0.0002463054187192118}, {"id": "42f8f7a256c0668b6f6739190781f377ca20ec29", "title": "Forgetting: Trace Erosion or Retrieval Failure?", "authors": ["Richard M. Shiffrin"], "date": 1970, "abstract": "A series of lists of random words was presented. Following each list, the subject attempted to recall the words of the list prior to the list just presented. Recall probability for a given word depended on the length of the list in which it was embedded, not on the length of the list intervening between presentation and test. These results indicate that forgetting is a failure in the memory search during retrieval rather than a degradation of the memory trace occurring between presentation and test.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5e2d37ea860e1c99385717d08be226071f4325ba", "title": "A Multicomponent Theory of The Memory Trace", "authors": ["Gordon H. Bower"], "date": 1967, "abstract": "Publisher Summary This chapter presents a hypothesis concerning the formal structure of a memory trace. It seems reasonable to tie the memory trace of an event to the variables operating in the perception of that event. If it is supposed that the person does not store the literal input stimulus, but rather some encoded representation of it, then the representation stored is either the primary code by which the event is recognized or a secondary code that labels the primary code. In either event, the representation stored is sufficient to the degree that when it is fed into a motor\u2013output system, salient features of the original input event can be reconstructed and output. It is supposed that what is stored in memory is the primary code, the secondary code, or both. That is, a memory trace is represented as an ordered list of attributes with their corresponding values. There are two ways to represent information redundancy in an individual memory trace. One representation of redundancy supposes that more information components are encoded and stored in the memory trace than are minimally required to select the initiating event from its appropriate ensemble. This is called the excess components idea. The other idea is to represent redundancy in terms of intercorrelations among the component values of a trace. This is called the intercorrelation idea.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "46f3fee4a13588c03b12e3ba242adb9e125d04ca", "title": "A FURTHER STUDY OF ESTIMATING AVERAGES", "authors": ["J. Spencer"], "date": 1963, "abstract": "An investigation is described of the types of averaging response made to several volues of a variable when these nre presented symbolically or graphically. Symbolic information was presented to subjects on small white cards on which were typed either 10 or 20 two-digit numbers. Graphical information was presented as 10 or 20 points on inch ruled tenths graph paper. Sets of data differed according to whether they were normally or skew distributed about the arithmetic mean value, and in the extent of scatter about the mean. The results confirmed previous work in showing that error of judgment increased as scatter increased, and to a greater extent with symbolic than \"with grnphical material. Error was also greater with skew than with normal information. Comparison between the results obtained with the two types of distribution showed that the previously reported finding that error also increased with increasing amount of information was incorrect and was duo to inadequate experimental control of scatter for...", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5f8c3ddf5f229dcc3cf3069439e48680fb489b06", "title": "Optimal strategies for seeking information: Models for statistics, choice reaction times, and human information processing.", "authors": ["Ward Edwards"], "date": 1965, "abstract": "Abstract Models for optional stopping in statistics are also normative models for tasks in which subjects may purchase risk-reducing information before making a decision. A Bayesian model for optional stopping for the two-hypothesis continuous case is developed; it takes explicit account of cost of information, values of the possible outcomes of the final decision, and prior probabilities of the hypotheses. A nonparametric model for choice reaction times is derived. It makes strong predictions about times and errors; only one quantity in it is not directly observable. A second example uses the model to design and predict results of a binomial information-purchase experiment.", "references": ["1282effaf4f8d9f8f972adef4505c4a1d3c644b1", "7b9d8e9aa6d9ba36f0a4fbefef462fc303373fe8"], "page_rank": 4.926108374384236e-05}, {"id": "dfa707fa6d7affeed754fa67e5d3718454d7bf56", "title": "CONTINUOUS ESTIMATION OF A TIME-VARYING PROBABILITY", "authors": ["Gordon H. Robinson"], "date": 1964, "abstract": "This experiment examines the human ability to give a direct magnitude estimate of a time-varying probability. The subject positioned a \u2018 tracking \u2019 lever at his estimate of the current mean of a sequentially displayed binary distribution. The distribution samples were presented at a fixed rate by two lights. The distribution mean changed in step increments of varying size and spacing. The experimental variables included flash rate and a constraint on the randomness of the flash series Detailed measures were made of both the transient and static responses to each stop change. The transient response was more rapid and consistent than had been anticipated. The average static response showed no systematic bias as a function of probability A descriptive model is derived which satisfies some basic properties of the task behaviour", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1c62ebe01d9ad6796cd65fad5aa04a8e1419f0c3", "title": "MICROSTRUCTURE OF GUESS PROCESSES: PART C", "authors": ["Masanao Toda"], "date": 1963, "abstract": "Abstract : Trial-to-trial changes in the proportion of human subjects predicting the occurrence of one of two events in a complex sequence of binary events (probability learning) are analyzed in terms of several simple models. The direction of change predicted by linear-operator reinforcement models (Estes, Bush and Mosteller) is wrong on about 75% of the trials. A no-learning model, a time-dependent decay model, and a cycle-dependent decay model are used to provide some insight into the nature of probability learning. Some suboptimal procedures for estimating parameters of stochastic processes are compared. The method of minimum absolute error is recommended as being very useful.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a14fe54972b00061187814008a76d38d3f97a843", "title": "Multiple probability-learning with shifting weights of cues.", "authors": ["Charles R. Peterson", "Kenneth R. Hammond", "D A Summers"], "date": 1965, "abstract": "Man's adjustment to an uncertain environment has been thoroughly studied in the classical experiment on probability-learning, in which the stimulus-situation is characterized by some degree of randomness but is stationary through trials. Uncertainty is associated with the event that occurs on any specific trial, but the probability of which event will occur remains constant over trials.1 Many uncertain situations in the real world are not only partially random, but are also nonstationary. Characteristics of such situations are not stable but change over time. A deviation at any time may be either a random fluctuation or the reflection of a genuine change in the situation. Adjustment to a nonstationary, uncertain environment requires that S detect whether a deviant event is a random fluctuation or the signal of a real change. Responses should ignore random fluctuations, but should react to genuine changes. In the classical experiment on probability-learning, the probability of response stabilizes at, or somewhat above, the stationary probability of the corresponding event. When the probability of an event shifts as a function of trials, a corresponding shift occurs in the probability of the response.2 The present experiment is in the context of multiple probability-learning. Whereas previous experiments have compared the degree to which cue-weights (dependence upon cues) for responses correspond with", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "4f1adf8baa87ad2ce9b81677d34f761004dcee88", "title": "Conservatism in a simple probability inference task.", "authors": ["Lawrence D. Phillips", "William E. Edwards"], "date": 1966, "abstract": "3 experiments investigated the effects on posterior probability estimates of: (1) prior probabilities, amount of data, and diagnostic impact of the data; (2) payoffs; and (3) response modes. Ss usually behaved conservatively, i.e., the difference between their prior and posterior probability estimates was less than that prescribed by Bayes' theorem. Conservatism was unaffected by prior probabilities, remained constant as the amount of data increased, and decreased as the diagnostic value of each datum decreased. More learning occurred under payoff than under nonpayoff conditions and between-S variance was less under payoff conditions. Estimates were most nearly Bayesian under the (formally inappropriate) linear payoff, but considerable overestimation resulted; the log payoff condition yielded less conservatism than the quadratic payoff. Estimates were most nearly Bayesian when Ss estimated odds on a logarithmic scale.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "59ba972373c39a65e8d4affb4862a615467bf0c9", "title": "Psychoacoustics and Detection Theory", "authors": ["D. M. Green"], "date": 1960, "abstract": "This paper presents a fairly complete review of detection theory as it is applied to certain psychoacoustic data. Detection theory is treated as a combination of two theoretical structures: decision theory and the concept of ideal observer. The paper discusses how statistical decision theory has been used to analyze the auditory threshold process. By treating the threshold process as an instance of hypothesis testing, two determinants of the process are recognized: (1) the detectability of the signal and (2) the criterion level of the observer. The theory provides a technic of analysis which allows one to obtain a quantitative estimate of both factors. The measure of signal detectability appears to be independent of the psychophysical procedure when the physical parameters of signal and noise are held constant. The concept of ideal observer is reviewed with special emphasis on the assumptions of the derivation. The usefulness of this concept is illustrated by considering the shape of the psychophysical fu...", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0589beeac9ffdac4e8a06ef42f3d17beec879361", "title": "NEGATIVES: DENOTATION AND CONNOTATION.", "authors": ["Peter Cathcart Wason", "St Jones"], "date": 1963, "abstract": "The hypothesis was investigated that the connotations of the word \u2018not\u2019 affect the times taken to verify negative statements more than the function of this word as a logical constant. The task was to verify statements which asserted, or denied, that a given number was \u2018even\u2019. Two groups were used: an explicit group in which the statements were in English, and an implicit group in which neutral signs stood for assertion and denial, their functions being learned by discovering the conditions which made the statements true and false. The results showed that the difference between the response times to explicit affirmative and negative statements was significantly greater (P < 0\u00b7001) after practice than the corresponding difference for implicit statements. The introspections suggested, however, that the subjects' approach to the implicit statements was an important variable. Tentative evidence for two factors affecting the response times to negative statements was adduced.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "ad88c13e5f51e169c832401e12cc51513a644a35", "title": "The effect of self-contradiction on fallacious reasoning", "authors": ["Peter Cathcart Wason"], "date": 1964, "abstract": "Abstract A serial task was used in which the possibility of making related valid and fallacious inferences was alternated over a series of trials. The hypothesis was investigated that if valid inferences are forced to contradict previously made fallacious inferences, then subsequent fallacious inferences will be withheld. The results showed that self-contradiction tended to extinguish fallacious inferences in an all-or-none manner without affecting valid inferences.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "f758cde8eb244825203192052f34d32ee8aa4d2e", "title": "Characteristics of the human inference process in complex choice behavior situations", "authors": ["Linda Weathers Dudycha", "James C. Naylor"], "date": 1966, "abstract": "Abstract Ten subjects were assigned to each of six experimental two-cue inference conditions created by varying the validity of the first cue across levels of .40 and .80, and by varying the validity of the second cue across levels of .20, .40, and .60. In each case the two cues were orthogonal. All performance indices closely approximated the dictates of a probability matching strategy. Subject consistency did not deviate greatly from the predictability available in the stimulus system, and subjects exhibited a high degree of ability in matching their equations to those defining the environmental complex. The value of a second ecological cue was a function of both the validity of the cue itself and the validity of the cue it was paired with. Pairing an additional cue to one of low validity was always facilitating, while adding an additional cue to one of high validity was always detrimental.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "6235a56028de520f22e97b68729fdcffd94279e0", "title": "Sequential decision-making in a computer-controlled task", "authors": ["Amnon Rapoport"], "date": 1964, "abstract": "Twenty-four undergraduate college students participated individually as subjects in a computer-controlled task, intended as an investigation of decision-making under uncertainty in a nonstationary environment. Each subject was required on every trial to predict the value of an unknown P, 0 \u2264 P \u2264 1.00, and then to estimate its value after having received partial information about it from a computer. Each P value, for 120 trials, was generated by one of three different beta distributions. A model derived from Bayesian decision theory is presented, tested, and partially confirmed. A learning model was constructed and tried in a computer simulation of the subjects. Implications concerning information seeking and use of computers in experiments are discussed briefly.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "d14879721351091992bc693015962edcb6b5f186", "title": "The influence of cue redundancy upon the human inference process for tasks of varying degrees of predictability", "authors": ["James C. Naylor", "E. Allen Schenck"], "date": 1968, "abstract": "Abstract Nine groups of ten subjects each performed in a 200 trial typical multiple-cue learning situation. The groups were defined in terms of three levels of cue redundancy between the two cues ( r ij = .00, .40, .80) and three levels of total system predictability ( R e = .50, .70, .90). While both absolute and relative achievement was found to be significantly and directly related to r ij and R e , the effects of r ij were much greater at high R e levels. This interaction was attributed primarily to subject's increasing inability to match the total ecology at low R e levels as cue redundancy increased. Subject consistency also varied directly with r ij and R e , while subject's ability to develop proportional strategies varied only as a function of r ij . The obtained importance of cue redundancy as a variable in multiple cue inference tasks thus strongly supports Brunswik's argument for \u201crepresentativeness\u201d in probabilistic inference research.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "bb789689eaa2f3e5a8565dd4b0ada7e5cd5456f7", "title": "THE CONCEPT OF CORRELATION IN ADULTS", "authors": ["Jan Smedslund"], "date": 1963, "abstract": "In the first of two experiments on the concept of correlation in adult subjects, the subjects' frequency estimates and inferences of relationship were studied relative to five different 2 \u00d7 2 distributions, each presented in a fixed sequence. In experiment II, the subjects' spontaneous strategies in subdividing and analyzing one 2 \u00d7 2 distribution were studied in a free situation. It is concluded that adult subjects with no statistical training apparently have no adequate concept of correlation (based on the ratio of the two pairs of diagonal frequencies), and that, in so far as they reason statistically at all, they tend to depend exclusively on the frequency of ++ cases in judging relationship. The need for studies involving ordinal scale and fully quantified variates is stressed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "11fa4c9340a37256089ccff8a7e43ec4c4b92d34", "title": "On Understanding Logically Complex Sentences", "authors": ["Philip N. Johnson-Laird"], "date": 1969, "abstract": "An experimental investigation was made into the meaning of eight types of doubly-quantified sentence, e.g. \u201cEvery medicine cures some disease,\u201d \u201cSome disease is cured by every medicine.\u201d All the sentences were ambiguous, depending upon the interpretation of the quantifiers. Subjects classified diagrams representing different specific situations as truthfully or falsely described by the sentences. The classifications revealed that the order of occurrence of the two quantifiers had a crucial effect, causing active and correlative passive to receive different interpretations. This suggested that in the process of understanding an ambiguous sentence a bias towards one intepretation may be created by word order.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "ae9f2562374a52e5e18f3d2ea97790bdfe71a758", "title": "THE MODIFIABILITY OF DECISIONS MADE IN A CHANGING ENVIRONMENT. ESD-TR-64-657.", "authors": ["Richard S. Gibson", "Elizabeth H. Nicol"], "date": 1964, "abstract": "Abstract : This research is concerned with the question of whether a decision maker's sensitivity to incoming information undergoes any change after he has reached a decision. A dynamic decision task, embedded in a limited war context, was presented to the subject who was required to assess simulated reconnaissance reports and to estimate the location of the enemy's main force. After a certain number of trials in which the information samples pointed to one of the alternative locations, the subject was then presented with a growing body of contraindicative information. An experimental study of the factors influencing decision modification was designed to test the effect of three main variables: (1) nature of the original condition, (2) length of time over which expectations were built up, and (3) amount of change represented in the shift from the initial condition to the post-decision situation. Twelve subjects were tested individually in a series of six periods. The facility with which subjects modified their decisions was inversely related to the amount of situational change and the amount of experience in the situation before a change was introduced. The results indicated also that more information is required to change a decision than is originally needed to make the decision. Comparison of the subjects' performance scores with those of an 'ideal Bayesian observer' showed highly significant correlations.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "0a0fe1fe42f595e829ec06ad998365332009cbff", "title": "Cue-Response Correlations in the Attainment of a Scalar Concept", "authors": ["Hiroshi Azuma", "Lee J. Cronbach"], "date": 1966, "abstract": "The usual study of concept attainment involves a classificatory concept. E divides a collection of stimuli into two classes on the basis of the presence or absence of certain characteristics, and S is asked to discover the \"concept\" or basis for classification. A smaller number of studies have employed a scalar concept. E assigns a scale value to each stimulus in the collection by applying some rule to the numerical values of selected attributes; S is asked to learn to rate stimuli on this scale. To succeed on the classificatory problem, S has to determine which of the attributes are relevant and what conjunctive or disjunctive combination of attribute-values is common to the positive instances of the concept. To succeed on the scalar problem, S has to identify relevant attributes and find a way of combining the information into an estimate of the scale-value assigned by E. One might regard the classificatory problem as requiring a response on a two-valued scale; each response is either right or wrong. From this point of view, the scalar concept is a more general case. The present study examines how Ss use multiple cues in solving a scaledconcept problem. The desired response to each stimulus corresponds to a weighted sum of certain cue-variables. S is required on each trial to estimate the scale-value assigned to the stimulus by the (unknown) rule; he is then told the correct value.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "eaaee22c7d38b9799e0cb2c88d5f2eccfbe55e7a", "title": "The influence of prior probabilities on information seeking and decision-making", "authors": ["Gordon F. Pitz"], "date": 1969, "abstract": "Abstract Three experiments examined the effect of biased prior probabilities in a deferred decision making task. Priors were manipulated between-subjects in Experiment 1, and had little or no effect. In Experiment 2, all subjects received three different sets of priors; some effect on behavior was observed, subjects taking less information to arrive at the favored decision. In Experiment 3, subjects were not informed of the exact prior probabilities, but were given an extended opportunity to learn them. The predicted change in behavior did occur over blocks of trials, but only under conditions of an extreme bias in prior probabilities.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "171d2fa8fb393f762b3245adb1548465c1ca33fe", "title": "THE DISCRIMINANT-FUNCTION AS A MODEL FOR PERCEPTION.", "authors": ["Albert S. Rodwan", "Harold W. Hake"], "date": 1964, "abstract": "A broad area of perceptual theorizing is exemplified by Brunswik,' Bruner,'2 Bartlett,3 and others,4 who maintain that the perceptual system is, basically, a judgment-making system. These theorists concentrate on the functional relationship between the distal variable (the physical object) and the perceptual response. Points of similarity among their views are: (a) the O does not utilize all aspects of the information in the distal variable; (b) the perceptual response is mediated by some hypothetical process;5 and (c) perception is probabilistic or statistical in that the functional relationship between the distal variable and the mediating process, Brunswik's 'ecological validity' of cues, is neither straightforward nor perfect. A judgmental theory implies that perception is a judgment-making process; that is, the perceptual system operates on the input-information in making a judgment or inference. This inference is one of two types; either it is an estimation of some aspect of the physical object, or it is the classification of the object into a perceptual category. In either case, the major task of the perceptual system is to classify inputs, since estimation presumably cannot take place until the object is classified. If this summary analysis is correct, most of these theories can be fitted", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "b30ce3350bcdce81b629473f14fa06b80298f10b", "title": "The statistical power of abnormal-social psychological research: a review.", "authors": ["Jerry D Cohen"], "date": 1962, "abstract": "A coin storing and transfer device comprising a base open at its top and forming a box-like receiving container. An endless conveyor comprising a frame and a continuous belt is mounted having a portion within the receiving container and a portion extending upwardly above the level of the base to a remote height. That portion of the conveyor above the base is pivoted about a fulcrum extending transversely along the upper edge of the base adjacent one end. In this manner the upper portion of the conveyor is movable between its extending position and a folded position horizontally over the top of the base.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "c5000de14f9b80bb0ea12e4c70262a84e0cc2843", "title": "Functional Asymmetry of the Brain in Dichotic Listening", "authors": ["Doreen Kimura"], "date": 1967, "abstract": "Summary This paper reviews the evidence relating lateral asymmetry in auditory perception to the asymmetrical functioning of the two hemispheres of the brain. Because each ear has greater neural representation in the opposite cerebral hemisphere, the predominance of the left hemispere for speech is reflected in superior recognition for words arriving at the right ear, while the predominance of the right hemisphere in melodic-pattern perception is reflected in superior identification of melodies arriving at the left ear. Some applications of the dichotic listening technique to questions concerned with the development of cerebral dominance, and with the further specification of function of the left and right hemispheres, are also described.", "references": ["3e1b99ffd71eb301f4892f4688ed5944e9388be3", "07de6e8c43b171849c938b18739d18ad71f4209c", "abdc6bf0d94ede0e40a38ef4a86f4ce9bc7b271d", "3a53c1de94818ff167cb82f4431236c0b6aa1e39", "745f96f70c7fdd9ebe8e8568613712806f1ff46c", "82923da371772d541cd2a572609f47ba7808eb32", "9d798007565774e2991a2e0bcce71cea7b0cdb44", "039a672804e8ec84ce8f4a57ce293c377fbd1e8e", "af78be0bdd320d3b12ff69378a6238414031049f"], "page_rank": 0.00030103995621237}, {"id": "3c677176ad1368247f9737a84358db9d9203744f", "title": "Some syntactic determinants of sentential complexity, II : Verb structure", "authors": ["Jerry A. Fodor", "Merrill F. Garrett", "Thomas G. Bever"], "date": 1968, "abstract": "The effect of the lexical complexity of verbs on the processing of sentences was evaluated in two experiments. Verb complexity was indexed by the number of types of grammatical structure a verb permits (e.g., a verb may be transitive or intransitive and may permit various types of complement structures). Ss\u2019 performances in paraphrasing sentences and in solving anagrams containing complex verbs were significantly poorer than their performances with the same sentences and anagrams containing less complex verbs.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "fa6e4b65800bdaff76b2098d0e1b19c0c7803e06", "title": "An autonomic reflection of syntactic structure", "authors": ["Thomas G. Bever", "R. Kirk", "James R. Lackner"], "date": 1969, "abstract": "Abstract Subjects heard sentences in one ear during which a brief shock was administered before, in or after the division between two clauses. The galvanic skin response (GSR) to shocks objectively at the end of a clause was larger than the response to shocks at the beginning of a clause. This effect of syntax on GSR was larger for subjects who heard the speech in the right ear. An independent effect was that the GSR to shocks at the end of a clause decreased as a function of clause length; responses to shocks at the beginning of a clause were relatively unaffected by the length of the preceding clause in our stimulus materials.", "references": ["49a3da48d0e6def2245985145cc2a28eb3ed49d5", "1c6c4dc793065e02ced67bdeda4dc42312f52f7c"], "page_rank": 5.473453749315818e-05}, {"id": "c6a34306fefed62adb0fde40b46863b848762ce7", "title": "The role of syntactic structure in the recall of English nominalizations.", "authors": ["Nicholas L. Rohrman"], "date": 1968, "abstract": "Current linguistic theory holds that there are two levels of sentence structure, derived constituent or surface structure and underlying or deep structure. Some investigators have argued that sentences are remembered in terms of their deep structures, but recently other investigators have countered that sentence recall is better predicted by surface structure. Five experiments were conducted to determine which of these views more adequately characterizes the memory representation of a sentence. Experiments I-IV demonstrate that with materials with identical surface structures, but differing in deep-structure complexity, differences in recall are predictable from the latter. Thus, the underlying structures seem to be the level which is represented in memory. Experiment V attempted to determine whether transformational history or deep-structure complexity is the more important determinant of recall. Deep-structure complexity, indicated by number of nodes, seems to be the more important.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d4c12e482ad2ab86d7092314f328ea8d468781c8", "title": "Rhythmic (hierarchical) versus serial structure in speech and other behavior.", "authors": ["James G. Martin"], "date": 1972, "abstract": "A device for creating a slurry of pulverized material in a tank including a rotating cylindrical body having an adjustable nozzle therein for jetting the high-pressure water against the material in the tank. A sump is provided adjacent the bottom of the tank around the rotary cylindrical body and means is provided for injecting water into the sump to control the fluidity of the pulverized material falling into the sump. Detection rods are provided to detect the total amount of material in the tank and to detect the amount of pulverized material which is compacted against the sides of the tank. Controls are provided to regulate the flow and orientation of flow of water through the two nozzles and the orientation of the rotating nozzle to break down the caking of the pulverized material and to effect efficient discharge of the slurry of material from the tank.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "a60896998331f1aad772bee0c0ea15c4d09a0794", "title": "Contrastive Accent and Contrastive Stress", "authors": ["Dwight L. Bolinger"], "date": 1961, "abstract": "'This whiskey,' said O'Reilly, sampling spirits that claimed to be from his homeland, 'was not exported from Ireland; it was deported.' This is the familiar phenomenon of contrast, by which two or more items are counterbalanced and a preference indicated for some member or members of the group. It is the most conspicuous of all the occurrences of phonetic highlighting by reason of its frequency and the extra oomph that we put into it, and because our attention is focused in a way that makes us aware of our speech and not just of our meaning. The name we generally give is contrastive stress, but I propose contrastive accent because of the major contribution that the fundamental pitch of the voice makes to it.l I will keep the old term contrastive stress as well, but restrict its meaning in a way to be explained later. The primary role of pitch in contrastive accent has been known for a long time. H. O. Coleman, in his oft-quoted article of 1914,2 made it the basis of his distinction between 'prominence' and 'intensity.' As an example of prominence he gives", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5250aec907afa5651029a864f55acc2032cc0d3e", "title": "Statistical Principles in Experimental Design", "authors": ["B. J. Winer"], "date": 1962, "abstract": "CHAPTER 1: Introduction to Design CHAPTER 2: Principles of Estimation and Inference: Means and Variance CHAPTER 3: Design and Analysis of Single-Factor Experiments: Completely Randomized Design CHAPTER 4: Single-Factor Experiments Having Repeated Measures on the Same Element CHAPTER 5: Design and Analysis of Factorial Experiments: Completely-Randomized Design CHAPTER 6: Factorial Experiments: Computational Procedures and Numerical Example CHAPTER 7: Multifactor Experiments Having Repeated Measures on the Same Element CHAPTER 8: Factorial Experiments in which Some of the Interactions are Confounded CHAPTER 9: Latin Squares and Related Designs CHAPTER 10: Analysis of Covariance", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "bea7bf6a505a397856fc7b8d30ce505b833088c3", "title": "Decision processes in memory.", "authors": ["Harley A. Bernbach"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Decision processes in memory.\" by Harley A. Bernbach", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "eb8921d3edd3be7c699ed338af939825cd686d97", "title": "An Introduction to Mathematical Learning Theory", "authors": ["George E. Ferris"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"An Introduction to Mathematical Learning Theory\" by George E. Ferris", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d9d5f7bfa50432d14c92aa6c24bd6f2e967b4068", "title": "The \u201ctip of the tongue\u201d phenomenon", "authors": ["Roger Brown", "David McNeill"], "date": 1966, "abstract": "The \"tip of the tongue\" (TOT) phenomenon is a state in which one cannot quite recall a familiar word but can recall words of similar form and meaning. Several hundred such states were precipitated by reading to Ss the definitions of English words of low frequency and asking them to try to recall the words. It was demonstrated that while in the TOT state, and before recall occurred, Ss had knowledge of some of the letters in the missing word, the number of syllables in it, and the location of the primary stress. The nearer S was to successful recall the more accurate the information he possessed. The recall of parts of words and attributes of Words is termed \"generic recall.\" The interpretation offered for generic recall involves the assumption that users of a language possess the mental equivalent of a dictionary. The features that figure in generic recall may be entered in the dictionary sooner than other features and so, perhaps, are wired into a more elaborate associative network. These more easily retrieved features of lowfrequency words may be the features to which we chiefly attend in word-perception. \"lnae features favored by attention, especially the beginnings and endings of words, appear to carry more information than the features that are not favored, in particular the middles of words. William James wrote, in 1893: \"Suppose", "references": ["0012e189a719cc29c3fb1ac597286e9052c0ba82", "7adb3c40ef03a458d35a3851fa66046936211cc3", "b3f4b10ec062beef21a876e910476c8560122f3a", "c12bfb7ce1590055460d1ce4a5cda2756a8272ba", "d503c7cd71651a6b48651b1895f2a6401c6dd65d", "bf054b53bd765161bc394dc63f8505f6aa9e6164"], "page_rank": 4.926108374384236e-05}, {"id": "70b28a0d4f275fdd3f6f128f72cb3d9928d60d84", "title": "Criterion change in continuous recognition memory.", "authors": ["Walter Donaldson", "Bennet B. Murdock"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"Criterion change in continuous recognition memory.\" by Walter Donaldson et al.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "72c92770a7939b25942a8c372dcf957cc5327811", "title": "Multi-process Models for Memory with Applications to a Continuous Presentation Task", "authors": ["R. C.", "J. W.", "Meye r"], "date": 1966, "abstract": "Semantic Scholar extracted view of \"Multi-process Models for Memory with Applications to a Continuous Presentation Task\" by C. R. et al.", "references": ["6996541ec19390b224b870cadcc99612a1de4f6b", "085ff74e2c21bcd15536b1a790eaa98b5b6410c3", "87b00d2a3e84f6b66bc03b63374b281f0e277f29", "6e22d253b24b405b7331762b6d75dd619f2de887", "44099bc1bbd7264146e9ba4391161af178520e2e", "9537d2938a7c4e3b4ca781f95f256d6990fabd25", "c0069916fe269ff59f601dfdf2a091387f883471", "ade476bc01dcb23b46b545360b8414b207668542", "ce3149dfbb672fd0364c22e56645c0c11aad7df7"], "page_rank": 0.0001532567049808429}, {"id": "8403f0d7d567b31a73cf5c71c61e939991bb18d0", "title": "Recall of paired-associates as a function of overt and covert rehearsal procedures", "authors": ["John W. Brelsford", "Richard C. Atkinson"], "date": 1968, "abstract": "The effect on memory of the mode of studying paired associates was investigated with a continuous-presentation technique. Overt rehearsal was found to be superior to covert study for all S s. Furthermore, the form of the forgetting curve was qualitatively different for the two study procedures. The overt-rehearsal curve dropped slowly at first and then very rapidly defining an S-shaped function, whereas the curve for the covert-study condition decayed exponentially. A mathematical model employing a short-term rehearsal buffer and a long-term memory state accurately predicted the data obtained under the two study conditions.", "references": ["72434253ade951fede487502079aa2423c83bdb3", "56c16d9e2a5270ba6b1d83271e2c10916591968d", "a7dc8dd1e42873d39b233a19f98d69dad647b03a", "ce3149dfbb672fd0364c22e56645c0c11aad7df7", "72c92770a7939b25942a8c372dcf957cc5327811"], "page_rank": 5.473453749315818e-05}, {"id": "bc2243926c1bf47776e850f1f68dd9566d68398b", "title": "Strength models and serial position in short-term recognition memory \u2606", "authors": ["Wayne A. Wickelgren", "Donald A. Norman"], "date": 1966, "abstract": "Abstract A number of continuous strength models for memory are developed for and tested by an experimental study of recognition memory for three-digit numbers at all serial positions in lists of length two through seven. Empirical estimates of trace strength in different conditions, independent of response bias, are obtained by means of the operating characteristic. The principal theoretical findings are: (a) strength in short-term memory (STM) appears to decay exponentially with the number of subsequent items; (b) subjects report that they recognize an item if and only if strength in memory exceeds a criterion; (c) the first item of a list is remembered better than subsequent items because it receives a greater increment in strength in STM upon presentation, not because it decays more slowly in STM or because it acquires some strength in a long-term memory.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "1d98f14032ce93812155feb2866b721d75ea6013", "title": "The language-as-fixed-effect fallacy: A critique of language statistics in psychological research", "authors": ["Herbert H. Clark"], "date": 1973, "abstract": "Current investigators of words, sentences, and other language materials almost never provide statistical evidence that their findings generalize beyond the specific sample of language materials they have chosen. Nevertheless, these same investigators do not hesitate to conclude that their findings are true for language in general. In so doing, it is argued, they are committing the language-as-fixed-effect fallacy, which can lead to serious error. The problem is illustrated for one well-known series of studies in semantic memory. With the appropriate statistics these studies are shown to provide no reliable evidence for most of the main conolusions drawn from them. A review of other experiments in semantic memory shows that many of them are likewise suspect. It is demonstrated how this fallacy can be avoided by doing the right statistics, selecting the appropriate design, and sampling by systematic procedures, or, alternatively, by proceeding according to the so-called method of single cases.", "references": ["e027ffaa9a27d1e5d8768307337f9a7d6fcb2a60", "9b3ea2ad8a2d0f1cfc327578f2481d422176aca2", "b0feb9a46fd1c72cac5435c2d303a66dfb92ee11", "d60458cd8aa7fe9e3a944f9ad336829640ae137e", "bfedd01ee140abb617efeb23555c1bbd43544c5b", "23802bfe925899d0bbe55bc7252cf59fa3e36553", "4d6d4c8c63295ae07dd8cb2ad3e7704cbcd22b5a", "f66fc75f19594a0eb62bc05c4c709058823cdfe3", "092392a1b0ee3d28c85546505eb06355165fad74", "ea78b2ce88b7dca417aa7c96b02690a0428c1d98"], "page_rank": 0.00010399562123700055}, {"id": "19b36be12d3e56f20b50e7679de22867f20538d4", "title": "MULTIPLE REINFORCEMENT EFFECTS IN SHORT\u2010TERM MEMORY", "authors": ["John W. Brelsford", "Richard M. Shiffrin", "Richard C. Atkinson"], "date": 1968, "abstract": "A continuous memorizing situation was studied in which test and study trials alternated throughout an experimental session. The items studied were paired-associates. The interval between study and test for a particular item was randomly determined; and an item was given one, two, three or four reinforcements. A quantitative model is proposed which has two memory stores: a short-term store in which the subject generates a carefully controlled rehearsal scheme of fixed length, and a long-term store in which information is accumulated and lost. A large number of theoretical predictions of the model were verified quantitatively by the data, which confirm results of previous experiments, and support the hypothesis that highly structured rehearsal schemes play a major role in many short-term memory and learning situations.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "0d4fece503daa35a755c0ab2b49ccb73493a3bdb", "title": "Reaction time to temporally-displaced phoneme targets in continuous speech.", "authors": ["Robin Hirtz Meltzer", "James G. Martin", "Carol Bergfeld Mills", "David L. Imhoff", "Dov Zohar"], "date": 1976, "abstract": "In seven experiments, reaction time (RT) was recorded to phoneme targets in sentences. By means of tape splicing or other experimental interventions preceding the target, speech information was or was not discarded, and targets were either temporally displaced \"early\" or \"late\" or remained on time.\" RT to displaced targets was slower than to on-time targets, except when the tape-splicing manipulation in effect presented coarticulatory target information in advance, in which case RT was faster than RT to on-time targets. The latter manipulation also produced faster RT than RT to the same targets in the original sentence, that is, that containing no experimental intervention at all. In three of the experiments, half the targets were contained in stressed syllables, half in unstressed syllables. Stressed and unstressed targets were affected differently, depending upon practice and other factors. Results were interpreted in terms of listener expectancies based on timing redundancy in continuous speech. They indicate an interaction between the effects of segmental and suprasegmental cues during ongoing perception.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "1282effaf4f8d9f8f972adef4505c4a1d3c644b1", "title": "Models for choice-reaction time", "authors": ["Mervyn Stone"], "date": 1960, "abstract": "In the two-choice situation, the Wald sequential probability ratio decision procedure is applied to relate the mean and variance of the decision times, for each alternative separately, to the error rates and the ratio of the frequencies of presentation of the alternatives. For situations involving more than two choices, a fixed sample decision procedure (selection of the alternative with highest likelihood) is examined, and the relation is found between the decision time (or size of sample), the error rate, and the number of alternatives.", "references": [], "page_rank": 0.0002463054187192118}, {"id": "9052dc84c6fe4bb3c6e5c7aaa9ed1c6f9735d653", "title": "Some Effects of Semantic and Grammatical Context on the Production and Perception of Speech", "authors": ["Philip Lieberman"], "date": 1963, "abstract": "Three speakers read aloud meaningful grammatical English sentences at a fast rate. Some of these sentences contained common maxims and stereotyped phrases. Other sentences which were less familiar contained, in similar phonetic environments, certain test words that also occurred in the stereotyped sentences. The test words were \u201cexcised\u201d, i.e., gated out of all the sentences, and listening tests were performed by 43 listeners. Quite apart from these listening tests two operational measures of redundancy were derived. One measure was based on a Markovian model of perception and readers were asked to guess what word n would be after they had read all the words through word n\u20131 in a sentence. The second measure was based on the hypothesis that people reserve their final decision on the recognition of each word until they perceive an entire sentence. Readers read an entire sentence in the midst of which a word was represented by a dash. They were then asked to guess what the missing word was. Two groups of th...", "references": [], "page_rank": 0.00010399562123700055}, {"id": "7b9d8e9aa6d9ba36f0a4fbefef462fc303373fe8", "title": "Decision processes in perception.", "authors": ["John A. Swets", "Wilson P. Tanner", "Theodore G. Birdsall"], "date": 1961, "abstract": "About 5 years ago, the theory of statistical decision was translated into a theory of signal detection. Although the translation was motivated by problems in radar, the detection theory that resulted is a general theory for, like the decision theory, it specifies an ideal process. The generality of the theory suggested to us that it might also be relevant to the detection of signals by human observers. Beyond this, we were struck by several analogies between this description of ideal behavior and various aspects of the perceptual process. The detection theory seemed to provide a framework for a realistic description of the behavior of the human observer in a variety of perceptual tasks. 1 This paper is based upon Technical Report No. 40, issued by the Electronic Defense Group of the University of Michigan in 1955. The research was conducted in the Vision Research Laboratory of the University of Michigan with support from the United States Army Signal Corps and the Naval Bureau of Ships. Our thanks are due H. R. Blackwell and W. M. Kincaid for their assistance in the research, and D. H. Howes for suggestions concerning the presentation of this material. This paper was prepared in the Research Laboratory of Electronics, Massachusetts Institute of Technology, with support from the Signal Corps, Air Force (Operational Applications Laboratory and Office of Scientific Research), and Office of Naval Research. This is Technical Report No. ESD-TR-61-20. 2 For a formal treatment of statistical decision theory, see Wald (1950) ; for a brief and highly readable survey of the essentials, see Bross (1953). Parallel accounts of the detection theory may be found in Peterson, Birdsall, and Fox (1954) and in Van Meter and Middleton (1954). The particular feature of the theory that was of greatest interest to us was the promise that it held of solving an old problem in the field of psychophysics. This is the problem of controlling or specifying the criterion that the observer uses in making a perceptual judgment. The classical methods of psychophysics make effective provision for only a single free parameter, one that is associated with the sensitivity of the observer. They contain no analytical procedure for specifying independently the observer's criterion. These two aspects of performance are confounded, for example, in an experiment in which the dependent variable is the intensity of the stimulus that is required for a threshold response. The present theory provides a quantitative measure of the criterion. There is left, as a result, a relatively pure measure of sensitivity. The theory, therefore, promised to be of value to the student of personal and social processes in perception as well as to the student of sensory functions. A second feature of the theory that attracted us is that it is a normative theory. We believed that having a standard with which to compare the behavior of the human observer would aid in the description and in the interpretation of experimental results, and would be fruitful in suggesting new experiments. This paper begins with a brief review of the theory of statistical decision and then presents a description of the elements of the theory of signal detection appropriate to human observers.", "references": ["a65b444660374ba6c1a7386bc84a1c1ca1beeae0", "ba7d4181ccec06af4bc778952be3c38e6e197cd5", "7a226f6a2458326317eb2860d963be032d3db483", "5556f9df34d6aab78d597c3635574672707e7e77", "9a6dbf1fe7459de909a4883f1b6987b6d91e7ef4", "29f22749925fb3fcf46241cf35c269b17df041d0", "ee5ba31e83dd3b092150c6b64d60f473e7ec97cd", "b9cb6e013ccc90d282684b2d51ce74146b257b79", "386126154386600fe21bbd2edd0fff7615c11de6", "b1cba8eb7c858a61cdb6228b93654cdd50f2e80b"], "page_rank": 0.0002463054187192118}, {"id": "1eb1e18346db11cc2da9909673d964b87585afa9", "title": "Voice onset time, frication, and aspiration in word-initial consonant clusters.", "authors": ["Dennis H. Klatt"], "date": 1975, "abstract": "The voice onset time (VOT) and the duration of the burst of frication noise at the release of a plosive consonant were measured from spectrograms of word-initial consonant clusters. Mean data from three speakers reading English words in a sentence frame indicated that the VOT changed as a function of the place of articulation of the plosive and as a function of the identity of the following vowel or sonorant consonant. Burst durations varied in a similar way such that the remaining interval of aspiration in /p, t, k/ was nearly the same duration in comparable phonetic environments. The VOT was longer before sonorants and high vowels than before mid- and low vowels. Aspiration was also seen in an /s/-sonorant cluster. To explain these regularities, production strategies and perceptual cues to a voicing decision for English plosives are considered. Variations in VOT are explained in terms of articulatory mechanisms, perceptual constraints, and phonological rules. Some VOT data obtained from a connected discourse were also analyzed and organized into a set of rules for predicting voice onset time in any sentence context.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ccbbcd85073c188b741fbc93590a513bbd4c81a2", "title": "Decision processes during sentence comprehension: Effects of lexical item difficulty and position upon decision times", "authors": ["Donald J. Foss"], "date": 1969, "abstract": "In this study 40 college S s listened to 60 English sentences and attempted to, ( a ) press a button whenever a word in a sentence began with a /b/, and ( b ) comprehend the sentences. Reaction times (RT) of the button-push responses were recorded. The words starting with /b/ (target words) occurred either early or late in the sentences and, in addition, they were immediately preceded by words of either high or low frequency in the language. Average RT was significantly longer when the target word occurred after low frequency or hard words than when the target word occurred after easy words. In addition, RT was significantly longer when target words occurred early as opposed to late in the sentences. The results were discussed in terms of a decision-making conception of sentence comprehension.", "references": [], "page_rank": 0.00010399562123700055}, {"id": "24e37aed8ee8f9de4d7bb73559dd36c40b6f4860", "title": "COORDINATION OF INTERNAL CODES", "authors": ["Michael I. Posner"], "date": 1973, "abstract": "Publisher Summary Experimental psychologists working with adult subjects have been studying the different forms of representation of information in perceptual and learning tasks. This chapter presents four conclusions resulting from these studies: (1) the existence of objective techniques to demonstrate the reality of visual, verbal, and motor codes; (2) these forms of representation are enduring and not merely rapidly decaying residues of stimulation; (3) individuals differ in their propensity for forming and using different representations; and (4) different representations of the same stimulus may serve as isolable subsystems in the sense that it is possible to manipulate experimentally the availability of one code without affecting other codes of the same stimulus. The controversy between single and multiple code theories has also been present in studies of much simpler conceptual and memory tasks. The chapter highlights three instances of controversy between single and multiple coding views.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "0ba1f9fd488b58df71dc823c99c5c6702291cd0e", "title": "Multiprocess models for memory with applications to a continuous presentation task", "authors": ["Richard C. Atkinson", "John W. Brelsford", "Richard M. Shiffrin"], "date": 1967, "abstract": "Abstract A multiprocess model for memory and learning is applied to the results of two complementary experiments. In Experiment I the subject was required to keep track of the randomly changing responses associated with a fixed set of stimuli. The task involved a lengthy and continuous sequence of trials, each trial consisting of a test on one of the stimuli followed by study on that same stimulus paired with a new response. The size of the stimulus set, s , took on the values 4, 6, and 8. Experiment II differed from Experiment I in that a large number of stimuli were used even though in any experimental condition the subject was required to remember only 4, 6, or 8 stimuli at one time. In both experiments the basic dependent variable was the probability of a correct response as a function of the number of intervening trials between study and test on a given stimulus-response pair (called the \u201clag\u201d). The lag curves were all near 1.0 at lag 0 and monotonically decreased as the lag increased; the lag curves for the three conditions ( s = 4, 6, and 8) decreased at different rates in Experiment I, whereas in Experiment II these curves were identical. Using four estimated parameters the model generated accurate predictions for the various response measures collected.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d764e7a422596bb93938aa48cdd3a13f1b73d70d", "title": "On the role of sentence stress in sentence processing.", "authors": ["Ann Cutler", "Donald J. Foss"], "date": 1977, "abstract": "Words bearing high stress appear to be easier to process during sentence comprehension. Since sentence stress typically falls on content words this suggests that comprehension is organized according to a form class bias: process stressed items as content words. The present study measured reaction-time (RT) to word-initial phoneme targets on content and function words in sentence contexts. Half of the words of each type were stressed, half were not. In addition, a variable of \"normality\" of stress pattern was manipulated. It was found that RTs were shorter for stressed items independent of their syntactic function. No effect for content v. function words or normal v. non-normal stress pattern was observed. Results were interpreted within the framework of a predictive model utilizing the concept of semantic focus.", "references": ["ccbbcd85073c188b741fbc93590a513bbd4c81a2", "9052dc84c6fe4bb3c6e5c7aaa9ed1c6f9735d653", "b226ef480082296af8b20082f4954f931ac29328", "7132bd0224917003f222f7edf0acf09916ba32ef", "9b87d25efab3317955fc5cb1469ba0942b44ce26", "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "1d98f14032ce93812155feb2866b721d75ea6013", "9101b86d5b2aeb0a8dc10a26b1d63979b76d6c65", "4c3df055ba3861e3285a7354b88b181f9174d0ce", "d4c12e482ad2ab86d7092314f328ea8d468781c8"], "page_rank": 4.926108374384236e-05}, {"id": "ddc03dad33c9825a192847a82cd062fc8107935f", "title": "To end ambiguous sentences", "authors": ["Donald G. MacKay"], "date": 1966, "abstract": "A study of the time required to complete ambiguous sentences suggested that: even though Ss are unaware of the ambiguity while completing sentences, they take more time to complete ambiguous sentences than unambiguous ones: the degree of difficulty in completing ambiguous sentences is related to the linguistic level at which the ambiguity occurs: sentences containing two ambiguities are more difficult to complete than those containing only one, and when these two ambiguities occur at different linguistic levels, these sentences are harder to complete than when both occur within the same linguistic level: ambiguity may affect the grammaticality and relevance of completions; and may cause stuttering and laughter, even without awareness of the ambiguity. An attempt to fit these results to several theories of the processing of ambiguous sentences led us to the conclusion that ambiguity interferes with our understanding of a single meaning of a sentence, and that the degree of interference varies with the linguistic level at which the ambiguity occurs.", "references": [], "page_rank": 0.00010946907498631636}, {"id": "b226ef480082296af8b20082f4954f931ac29328", "title": "Some effects of ambiguity upon sentence comprehension", "authors": ["Donald J. Foss"], "date": 1970, "abstract": "Three groups of 20 S s were auditorily presented with 24 ambiguous and 24 unambiguous sentences. One group was asked to push a button whenever a particular phoneme occurred. Reaction time (RT) for this response was significantly longer in ambiguous sentences. A second group had to classify each sentence as ambiguous or not after it was presented. Lexical ambiguities were discovered somewhat faster than underlying structure ambiguities. A third group performed both of these tasks. In this group, RT to monitor for the phoneme was longer for ambiguous than for unambiguous sentences only when S himself classified the sentence as ambiguous. The S s in the third group were significantly slower on both tasks.", "references": [], "page_rank": 0.00015873015873015873}, {"id": "a6a9fc49be5dd67587dcfa8b3a0775fcdb5ceb55", "title": "Failure of predictions from subjectively expected utility theory in a Bayesian decision task", "authors": ["Thomas S. Wallsten"], "date": 1968, "abstract": "Abstract An experiment is presented which tests the adequacy of a subjectively expected utility (SEU) model for Bayesian decision making. The predictions were independent of individual measures of subjective probability or utility. They depended upon the findings that humans underestimate high Bayesian probabilities, with this tendency being greater when the Bayesian probabilities quickly approach one and zero than otherwise. The predictions were first stated with the assumption that utility is linearly related to money within the range of values used. The predictions were not sustained. However, part of the data restricted the possible utility functions to power functions. It was shown that no value of the exponent could account for the rest of the data, making the test independent of utility considerations. Implications for a theory of Bayesian decision making were discussed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5f4f1a7f568fead8f32b863e925106fe1f12f648", "title": "Confidence and decision speed in the revision of opinion", "authors": ["E. Scott Geller", "Gordon F. Pitz"], "date": 1968, "abstract": "Abstract The present experiment examined decision speed, measured without the subject's knowledge, and sequential confidence revision in a two-choice decision task. Subjects were presented with ten sequences of 20 events from one of two data-generating devices. Before each event was presented, the subjects predicted the event outcome, and after each event they decided which of the two data-generating devices was being used and gave a judgment of confidence in this decision. Following events that disconfirmed their favored hypothesis, subjects demonstrated a marked resistance to decreasing their confidence level (Inertia Effect). Two possible explanations of the Inertia Effect were postulated and their predictions were compared with the data. A commitment hypothesis assumed that subjects were unwilling to reduce their stated confidence following commitment to a decision, while a pattern-effect hypothesis was based upon a subject's hypothetical expectancies, reflected in his predictions. Both hypotheses were partially supported; the Inertia Effect was a function of subjects' predictions and was accompanied by a decrease in decision speed.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "a62f66001092db361059fcd679fab6410d9ad054", "title": "Some syntactic determinants of sentential complexity", "authors": ["Jerry A. Fodor", "Merrill F. Garrett"], "date": 1967, "abstract": "The perceptual complexity of three lists of self-embedded sentences was evaluated in terms of the accuracy and time required for their paraphrase. The lists differed by the presence of relative pronouns in one list, their absence in a second and by the addition of adjectives to the third. It was predicted that the presence of the relative pronouns would effect the only significant change in performance. In both auditory and visual presentations of the sentence lists, the presence of the relative pronouns proved to be facilitating, while the presence of the adjectives produced no significant changes.", "references": [], "page_rank": 0.00010946907498631636}, {"id": "9a82d5fe91d442baaf0134796bb36df39fe3f0f9", "title": "Analyzing the expert judge: A descriptive study of a stockbroker's decision process.", "authors": ["Paul Slovic"], "date": 1969, "abstract": "This study illustrates an analysis-of-variance technique for describ ing the use of information by persons making complex judgments. Ss were two stockbrokers who rated the growth potential of stocks on the basis of 11 factors taken from Standard S Poor reports. The technique proved capable of providing aprecise quantitative description of configural and nonconfigural information utilization. Each broker exhibited a substantial amount of configural processing. The technique appears to have promise for providing expercs with insight into their own processes and for teaching and evaluating \"student\" judges. ANALYZING THE EXPERT JUDGE: A DESCRIPTIVE STUDY OF A STOCKBROKER'S DECISION PROCESSES1 Paul Slovic Oregon Research Institute, Eugene The task of the expert judge, be he military officer, detective, businessman, physician, clinical psychologist, financial analyst, etc., requires him to combine items of information from a number of different sources into a decision or judgment. The key to the expert's success resides in his ability to interpret and integrate information appro priately. This means he must weight items of information differentially, according to their relevance, and must be able to qualify his inter pretations of a given fact when other considerations make such qualification necessary. There is no need to dwell upon the tremendous importance of being able to understand and describe how the expert uses information. However, such understanding does not come easily. All too often expert judgment is regarded as a mysterious, intuitive phenomenon\u2014 incapable of being described precisely. For example, Lusted (1960) relates a story about a radiologist famed for his diagnostic ability. Once, when he was questioned as to why he thought a particular shadow on an X-ray was a metastatic lesion, the physician replied, \"Because it looks like it!\" At the other extreme, we're all familiar with the expert who instructs others in the art of emulating his judgments by", "references": ["e519aba2a6cfa0bb1497453a18f90e1e76a0267c", "03adc941a986877551101c0ab78ed4abccdb8b3e", "5fea928a857160739a718c434dff16a2621cac89"], "page_rank": 4.926108374384236e-05}, {"id": "fa25ea935d8d3bdfa273950ba00594e0a66beb67", "title": "RECALL AND RECOGNITION IN INTENTIONAL AND INCIDENTAL LEARNING.", "authors": ["Morris N. Eagle", "Erich Leiter"], "date": 1964, "abstract": "Recall and recognition of a list of 36 words were studied in 3 groups: an intentional group (Group 1) instructed to remember the words; an incidental group (Group 2) instructed to respond to each stimulus word by indicating whether it was a noun, verb, or adjective; a 3rd group (Group 3) instructed to both remember the words and perform the orienting task. Group 1 showed superior recall, but Groups 2 and 3 recognized significantly more words than Group 1. The superior recall of Group 1 was attributable mainly to performance of those Ss reporting use of a memorizing strategy (e.g., grouping, rehearsing). These results support the view that intention to learn is crucial for learning only to the extent that it generates adequate learning operations. Furthermore, different learning responses (e.g., recall and recognition) require somewhat different operations.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "609e3f429cf8024d2d47fe26dccdbf44adb3e4e4", "title": "Effects of organization and semantic similarity on recall and recognition", "authors": ["George Mandler", "Zena Pearlstone", "Henry S. Koopmans"], "date": 1969, "abstract": "Three experiments were performed to extend the previous finding that number of cate-gories (NC) in organized, categorized lists determines the number of words recalled. The NC also influences recognition both in immediate tests and in a delay of two weeks. False alarm rates in recognition are generally unaffected by the use of synonyms as fillers, suggesting that perceptual features of words are used at least in addition to semantic features. To accommodate the novel finding that organization affects recognition, a model for the case of subject-organized lists was presented which introduces the notion of a postrecognition retrieval check. Previous findings on the relation between NC and recall were replicated.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "d22000e27a064baa027fa685353abb216b0d06da", "title": "Plan Recognition in a Programmer's Apprentice", "authors": ["Charles Rich"], "date": 1977, "abstract": "Brief Statement of the Problem: Stated most generally, the proposed research is concerned with understanding and representing the teleological structure of engineered devices. More specifically, I propose to study the teleological structure of computer programs written in LISP which perform a wide range of non-numerical computations. The major theoretical goal of the research is to further develop a formal representation for teleological structure, called plans, which will facilitate both the abstract description of particular programs, and the compilation of a library of programming expertise in the domain of non-numerical computation. Adequacy of the theory will be demonstrated by implementing a system (to eventually become part of a LISP Programmer's Apprentice) which will be able to recognize various plans in LISP programs written by human programmers and thereby generate cogent explanations of how the programs work, including the detection of some programming errors. Working Papers are informal papers intended for internal use.", "references": ["df12f998fa2160c2b04fce5aea143052a6b1f837", "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "02256c909265b66403b6cb08102174aa0b6ede1d", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "e64db9777a991f55029bd13c9d03741fa2b17046", "f5e796dc458978f1a01d1b1698636df9c5181934", "cf213e9ac5f8445a1f1c8a37fd03cf6bb68c42de", "816e9b69c6adb73205e841be8e69b3651d3bfbbf"], "page_rank": 0.00014778325123152708}, {"id": "639c596e253589d97c94c45619684e0fbdf1e6be", "title": "Memory and the theory of signal detection.", "authors": ["Robert S. Lockhart", "Bennet B. Murdock"], "date": 1970, "abstract": "A recurrent problem in the study of recognition memory has been that of combining hits and false alarms (or correct and incorrect responses) into a single index of performance. That the proportion of correct responses in a yes/no or multiple-choice recognition test may in some way be contaminated by factors other than the state of the memory system has never been seriously disputed, but only recently has the question received detailed and systematic attention. Traditionally the problem has been viewed in terms of \"correcting for chance success\"; in current terminology, and stated more generally, it is a problem of providing an adequate theory of the decision system which maps a given state of the memory system into an overt response. The reasons for this increased attention to decision processes in memory are not difficult to trace. In the first place, the development of quantitative theories of recognition memory has necessitated a precise and explicit account of how the memory and decision systems interact to produce a given response. If a theory of memory is to be tested against data from a recognition-me mory experiment, it is necessary to obtain performance measures that are independent of those parameters", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "418b44c21429cb772a0091382a340baaf4f74ac0", "title": "Probabilistic Information Processing Systems: Design and Evaluation", "authors": ["Ward Edwards", "Lawrence D. Phillips", "William L. Hays", "Barbara C. Goodman"], "date": 1968, "abstract": "A Probabilistic Information Processing System (PIP) uses men and machines in a novel way to perform diagnostic information processing. Men estimate likelihood ratios for each datum and each pair of hypotheses under consideration or a sufficient subset of these pairs. A computer aggregates these estimates by means of Bayes' theorem of probability theory into a posterior distribution that reflects the impact of all available data on all hypotheses being considered. Such a system circumvents human conservatism in information processing, the inability of men to aggregate information in such a way as to modify their opinions as much as the available data justify. It also fragments the job of evaluating diagnostic information into small separable tasks. The posterior distributions that are a PIP's output may be used as a guide to human decision making or may be combined with a payoff matrix to make decisions by means of the principle of maximizing expected value. A large simulation-type experiment compared a PIP with three other information processing systems in a simulated strategic war setting of the 1970's. The difference between PIP and its competitors was that in PIP the information was aggregated by computer, while in the other three systems, the operators aggregated the information in their heads. PIP processed the information dramatically more efficiently than did any competitor. Data that would lead PIP to give 99:1 odds in favor of a hypothesis led the next best system to give 4?: 1 odds.", "references": ["257fddda960373ee8683fff1402f73e5882af707", "128b31e6882545d9b36b81c5f262630765a607ce", "13a28912668b2c3e3a35cd6511896ce565c22c33", "70be6108576637e8672bcd4e233c21382ed8bf82", "b8023e4e9ee611a4b726128cf784ac5aaa9943df", "ea82b4e373aef2697a99ced7c75e96af95b6559a", "b01cf15a09678e17e5e4b84778deaf000f974d1a"], "page_rank": 4.926108374384236e-05}, {"id": "1c6c4dc793065e02ced67bdeda4dc42312f52f7c", "title": "The active use of grammar in speech perception", "authors": ["Merrill F. Garrett", "Thomas G. Bever", "Jerry A. Fodor"], "date": 1966, "abstract": "Judgments of the location of short bursts of noise in sentences were used to reveal perceptual segmentation of sentences. It was assumed that segmentation would correspond to major constituent boundaries. In order to control for correlated variables of pitch and intonation, identical acoustic material was provided with alternate constituent structures. It was found that differences in response to identical strings were predicted by the points of variation in constituent structure.", "references": ["1d8706c22e31da07e89550f3189f14eeef6f1592"], "page_rank": 0.0002463054187192118}, {"id": "039a672804e8ec84ce8f4a57ce293c377fbd1e8e", "title": "The role of auditory localization in attention and memory span.", "authors": ["Donald E. Broadbent"], "date": 1954, "abstract": "A compact, self-supporting solid article of manufacture consisting essentially high temperature and wear-resistant antifriction material of low thermal expansion in the form of a heterogeneous sintered article of a matrix of glass or glass ceramic of low thermal expansion and distributed within the matrix small particles of at least one temperature and wear-resistant antifriction oxide selected from the group consisting of nickel oxide, cobalt oxide, ferric oxide, bismuth oxide and chromic oxide.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9d798007565774e2991a2e0bcce71cea7b0cdb44", "title": "Identification of Consonants and Vowels Presented to Left and Right Ears*", "authors": ["Donald P. Shankweiler", "Michael Studdert-Kennedy"], "date": 1967, "abstract": "The results of earlier studies by several authors suggest that speech and nonspeech auditory patterns are processed primarily in different places in the brain and perhaps by different modes. The question arises in studies of speech perception whether all phonetic elements or all features of phonetic elements are processed in the same way. The technique of dichotic presentation was used to examine this question. The present study compared identifications of dichotically presented pairs of synthetic CV syllables and pairs of steady-state vowels. The results show a significant right-ear advantage for CV syllables but not for steady-state vowels. Evidence for analysis by feature in the perception of consonants is discussed.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "3a53c1de94818ff167cb82f4431236c0b6aa1e39", "title": "Dichotic Listening and Cerebral Dominance", "authors": ["James Inglis"], "date": 1965, "abstract": "It has recently been suggested that errors of report in the sequential recall of simultaneously presented auditory stimuli may be due to the inferior perception of those stimuli, under conditions of auditory competition, which are delivered to the ear ipsilateral to the cerebral hemisphere dominant for speech. It can, however, be argued that this is not necessarily the most adequate hypothesis to account for these and analagous data. Evidence may be adduced which suggests that the order of recall may be of more importance than laterality of recall. The errors of reproduction observed may therefore be due to a decay of input in a short-term store rather than due to the failure of part of the input to enter the system. Since such short-term storage may be a crucial and vulnerable link in the whole chain of learning it might be anticipated that defects in this process would be found in cases of learning disorder. Studies are cited which show that this appears, in fact, to be the case.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "82923da371772d541cd2a572609f47ba7808eb32", "title": "Accuracy of recognition for speech presented to the right and left ears", "authors": ["Donald E. Broadbent", "Margaret Gregory"], "date": 1964, "abstract": "Abstract Earlier reports by other authors had shown that recall of dichotically presented speech was better for words arriving at the right ear, while recognition of passages of music was better for music presented to the left ear. It seemed desirable to test recognition for speech, in order to show that the difference between materials rather than the difference in testing techniques was responsible for this disparity between ears. Accordingly 18 men were presented with dichotic lists of digits, and then tested for recognition of the digits. There was no indication of an advantage for the left ear, but rather of the advantage for the right already established for recall of speech materials.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "af78be0bdd320d3b12ff69378a6238414031049f", "title": "A study on the pathway from the medial geniculate body to the acoustic cortex in the dog.", "authors": ["Archie R. Tunturi"], "date": 1946, "abstract": "Semantic Scholar extracted view of \"A study on the pathway from the medial geniculate body to the acoustic cortex in the dog.\" by Archie R. Tunturi", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "745f96f70c7fdd9ebe8e8568613712806f1ff46c", "title": "Tachistoscopic recognition, handedness, and cerebral dominance\u2606", "authors": ["M. Philip Bryden"], "date": 1965, "abstract": "Abstract Twenty left-handers and twenty right-handers were given a dichotic listening test and a tachistoscopic recognition test. On both tests, right-handers were significantly more accurate in identifying material presented to the right side, while left-handers failed to show any consistent left-right differences. Left-handers showed a significantly greater variance in laterality scores on the dichotic listening task than did right-handers. A group of four familial left-handers were more left-dominant on both tasks than were the other left-handers. There was, however, no significant correlation between laterality scores on the two tasks. The results suggest that performance on both tasks is related to cerebral dominance, but that cerebral dominance must be viewed as having several components, rather than being a unitary process.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "bf054b53bd765161bc394dc63f8505f6aa9e6164", "title": "The American College Dictionary", "authors": ["Harry Morgan Ayres"], "date": 1948, "abstract": "Semantic Scholar extracted view of \"The American College Dictionary\" by Harry Morgan Ayres", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "abdc6bf0d94ede0e40a38ef4a86f4ce9bc7b271d", "title": "Dual functional asymmetry of the brain in visual perception", "authors": ["Doreen Kimura"], "date": 1966, "abstract": "Abstract Verbal and nonverbal stimuli were presented to normal subjects by means of a tachistoscope. The method employed was that of successive random presentation to either the left or the right visual half-field. Letters were more accurately identified in the right visual field, as previously established, but the enumeration of certain nonalphabetical stimuli was more accurate when they appeared in the left field. It was concluded that the left posterior part of the brain plays an important role in the identification of verbal-conceptual forms, while the corresponding area on the right has other functions in the registration of nonverbal stimuli.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "a7dc8dd1e42873d39b233a19f98d69dad647b03a", "title": "Institute for Mathematical Studies in the Social Sciences", "authors": ["Max E. Jerman", "Patrick Suppes"], "date": 1969, "abstract": "This report describes the third year (196546) of a longitudinal study of the accelerated program in elementary-school mathematics conducted by the Institute for Mathematical Studies in the Social Sciences. A description of the first year of the study, including details of the procedures by which the students were selected, was  reported  in  Suppes  and  Hansen (1965). The second year was reported  in Suppes (1966). The present report was written to be as homogeneous as possible with the earlier ones. In the second section, we describe the curriculum content of the third year. The third and fourth sections contain a brief description of the class composition and class procedure. The fifth section, on results, reporb the systematic behavioral data collected. The tables and figures are similar to those presented in the two previous reports, but there are some changes in presentation which reflect changes in curriculum and  teaching procedure.  For  example,,  no meaningful comparisons c m he made bet,ween this year and previous years in rate of problem acquisition in the Sets und Numbers curriculum because of the varying amount of class time allotted to Sels and Numbers work. This year less class time was devoted to these texts, and a greater amount of time was spent on special enrichment materials. The 32 children who participated in t'he 1965-GG program were bright third graders in the third year of an accelerated program in elementnry-school mathematics. These facts should be considered when reading the descriptions of the curriculum. ,", "references": ["d2b40f97365550c79e7fb670d99190e0a81a697b", "8c61f75e7e49a3b7a5fa85f7a92b329b581b2329", "354a7c13bfce5257b4b26debf38188c1300b0a54", "b1958feb50eb13773d2f455980ddc8ab49e5417f", "944419aade31268db5b183f3468a1d9ef21ffa5c", "29932930f93460eda82f07a71af641513ef5b606"], "page_rank": 9.852216748768472e-05}, {"id": "ce3149dfbb672fd0364c22e56645c0c11aad7df7", "title": "Mathematical models for memory and learning", "authors": ["Richard C. Atkinson", "Richard M. Shiffrin"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Mathematical models for memory and learning\" by Richard C. Atkinson et al.", "references": ["bfcea67c38315211bf3b260f43c2bd99b89906ef", "6f44009207a4316cb33f34c26050adbad4688d8c", "8d3c02f984634b77e4554cc7c439292c058f236f", "eb47b84bcf3839b8656a2d06023d4e4f8b791873", "87b00d2a3e84f6b66bc03b63374b281f0e277f29", "44099bc1bbd7264146e9ba4391161af178520e2e", "c0069916fe269ff59f601dfdf2a091387f883471", "3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "e79511511dd3475b60318645a6c1843ee1d53404"], "page_rank": 0.00016009852216748767}, {"id": "092392a1b0ee3d28c85546505eb06355165fad74", "title": "Retrieval of artificial facts from long-term memory", "authors": ["Edward E. Smith", "Susan E. Haviland", "Paul B. Buckley", "Michael J. Sack"], "date": 1972, "abstract": "This paper deals with how properties of a referential noun are organized and retrieved from long-term memory. In two experiments S judged the truth or falsity of previously learned artificial facts, each fact consisting of a noun from a natural language hierarchy paired with an arbitrary digit, e.g., Bird-2. Experiment I used a two-level hierarchy and the results were consistent with Collins and Quillian's (1969) model which assumes that those properties of a noun which are also properties of its superordinate are stored with and retrieved from the superordinate. Experiment II used a three-level hierarchy and the results were inconsistent with Collins and Quillian's model, but rather supported a model in which all properties of a noun are stored with and retrieved from that noun. A resolution of the present results with those of Collins and Quillian (1969) was discussed.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "4d6d4c8c63295ae07dd8cb2ad3e7704cbcd22b5a", "title": "Homographic entries in the internal lexicon: Effects of systematicity and relative frequency of meanings", "authors": ["Herbert Rubenstein", "Spafford S. Lewis", "Mollie A. Rubenstein"], "date": 1971, "abstract": "The task was to distinguish between English and nonsense words, which were displayed singly. The display persisted until S pressed the yes -key if he thought the stimulus was English or the no -key if he thought it was nonsense. In an earlier study ( Rubenstein, Garfield, & Millikan, Journal of Verbal Learning and Verbal Behavior, 1970 ) it was found that the response time is shorter when the English word is a homograph, a word with more than one meaning, than when it is a nonhomograph. The data of the present study showed that this facilitating effect of homography is observable when the meanings of the homograph ( a ) are not systematically related and ( b ) tend to equiprobability.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "f66fc75f19594a0eb62bc05c4c709058823cdfe3", "title": "Activation of Semantic Memory", "authors": ["Elizabeth F. Loftus"], "date": 1973, "abstract": "that the retrieval process consisted of at least two major steps: (a) entering the appropriate category and (b) finding the appropriate member of that category. Suppose now that we ask a subject to name a member of a category, and some time later ask him to name a different member of that category. Will the speed with which he retrieves the second instance depend at all on his having retrieved the first instance? Several recent lines of investigation suggest that producing an instance of a category will facilitate later production of another instance of that category. In one such investigation, Collins and Quillian (1970) presented sentences such as 'A canary is a bird' and required subjects to decide whether the sentences were true or false. Prior exposure to one sentence reduced reaction time to a second sentence, sometimes by as much as 600 msec, whenever the same subject noun was used. For example, prior exposure to 'A canary is a bird' reduced reaction time to other sentences about canaries.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b0feb9a46fd1c72cac5435c2d303a66dfb92ee11", "title": "Generalizing to a Language Population", "authors": ["Edmund B. Coleman"], "date": 1964, "abstract": "Many studies of verbal behavior have little scientific point if their conclusions have to be restricted to the specific language materials that were used in the experiment. It has not been customary, however, to perform significance tests that permit generalization beyond these specific materials, and thus there is little statistical evidence that such studies could be successfully replicated if a different sample of language materials were used. Three tests are described that will allow generalization to a population of language materials.", "references": ["af4b3d01c44d7a404f64aa6b8da49cdbc8cb0759", "3861c1e54a7be5f102f08aff6a49e09785157be0", "77c681cff8f2fd537d67fe807107c8957c4e8482", "dc7746457936702a66ce4aff1dae0f2700420028", "e9db51a3abf5cbbc3dee1f69ec25f90a912d9ef5", "1befa2c5d6977b8792190e13116760286159acb8"], "page_rank": 5.473453749315818e-05}, {"id": "b1cba8eb7c858a61cdb6228b93654cdd50f2e80b", "title": "Confidence Ratings and Message Reception for Filtered Speech", "authors": ["Louis R. Decker", "Irwin Pollack"], "date": 1958, "abstract": "The statistical decision model, which has achieved outstanding success in describing the detection of signals in noise was applied to the reception of filtered speech. A confidence rating was added to the articulation test procedure in order to obtain additional information about the listener's criterion of message acceptance and message rejection of filtered speech. The relation between correct confirmations and false alarms\u2014the Receiver Operating Characteristic\u2014obtained with filtered speech corresponds with that typically obtained with noise interference. It is suggested that the \u201cnoise\u201d of the decision model may be extended to a wide range of operations which perturb the signal.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "23802bfe925899d0bbe55bc7252cf59fa3e36553", "title": "ON THE INTERNAL STRUCTURE OF PERCEPTUAL AND SEMANTIC CATEGORIES1", "authors": ["Eleanor Rosch"], "date": 1973, "abstract": "Publisher Summary \nThis chapter focuses on the internal structure of perceptual and semantic categories. The semantic categories of natural languages are made to appear quite similar to such artificial concepts when they are treated as bundles of discrete features that clearly differentiate the category from all others and that determine the selection restrictions of category labels used in sentences. The concept of internal structure has implications for several areas of research, among them child development. Studies of the development of word meaning have tended to focus on the child's understanding of criterial attributes and hierarchies of super ordination; such studies have found consistent evidence that children do not categorize or define words by the same principles of abstraction used by adults. Internal structure also has implications for cross-cultural research. It has been argued that psychological categories have internal structure, that is, instances of categories differ in the degree to which they are like the focal examples of the category; that the nature of the structure of the perceptual categories of color and form is determined by perceptually salient natural prototypes; and that non-perceptual semantic categories also have internal structure that affects the way they are processed.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "d60458cd8aa7fe9e3a944f9ad336829640ae137e", "title": "Facilitating retrieval from semantic memory: The effect of repeating part of an inference\u2606", "authors": ["Allan Collins", "M. Ross Quillian"], "date": 1970, "abstract": "Abstract In Collins and Quillian (1969) we found evidence that people decide whether simple sentences are true or false by using inferences. For instance, a sentence like \u2018A canary can fly\u2019 apparently was confirmed by inference from the two facts that a canary is a bird and that birds can fly. If so, then this has a possible implication for reaction time (RT) to such sentences presented in succession. Prior exposure to one sentence should reduce RT to a second sentence whenever the same fact is involved in confirming both sentences. For example, prior exposure to \u2018A canary is a bird\u2019 should reduce RT to \u2018A canary can fly\u2019 more than to \u2018A canary can sing\u2019, since we assume that no inference is used to confirm the latter sentence. In total eight RT difference predictions were made for various kinds of sentence pairs, and all eight of these predictions held. Two possible models could explain these results.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "386126154386600fe21bbd2edd0fff7615c11de6", "title": "Monitoring Task in Speech Communication", "authors": ["James P. Egan"], "date": 1957, "abstract": "Some communication situations involve several noisy channels, and only certain ones of these carry relevant information to a given communication operator. The operator must receive and identify a restricted number of different messages, and he must ignore others. The performance of the listener in this situation will depend both on the discriminability of the messages and on the listener's criterion for accepting his response as correct or rejecting it as incorrect. The present paper gives a quantitative description of the monitor's behavior in terms of the operating characteristic and the articulation\u2010criterion function. The results of two experiments are reported. In one of these, the confusion matrices for the various sets of messages were also determined.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "4c3df055ba3861e3285a7354b88b181f9174d0ce", "title": "Sentence stress and sentence comprehension", "authors": ["A. Cutler"], "date": 1975, "abstract": "Ph.D. Dissertation, University of Texas, 1975 - Dissertation Abstracts International, 36(10-B), 5300)", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "b9cb6e013ccc90d282684b2d51ce74146b257b79", "title": "Theory of Recognition", "authors": ["Wilson P. Tanner"], "date": 1956, "abstract": "The theory of statistical decision has previously been applied to the problem of sensory detection of signals. In this paper, the theory is expanded to treat a simple recognition problem. While the data supporting the expansion have been collected in auditory experiments, the theory applies generally to all human sensory behavior.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "bfedd01ee140abb617efeb23555c1bbd43544c5b", "title": "On the process of comparing sentences against pictures.", "authors": ["Herbert H. Clark", "William G. Chase"], "date": 1972, "abstract": "Abstract The present study outlines a theory of how people compare sentences against pictures. This theory was tested in four experiments in which S s were timed as they judged whether a sentence (e.g., Star isn't above plus ) was true or false of a picture (e.g., + \u2217 ). The latencies in these tasks were consistent with the thesis that: (1) sentences are represented in terms of elementary propositions; (2) pictures are encoded in the same interpretive format; (3) these two codes are compared in an algorithmic series of mental operations, each of which contributes additively to the response latency; and (4) sentence encoding, picture encoding, comparing, and responding are four serially ordered stages, and their component latencies are additive. From these results, it was also possible to rule out certain explanations based on visual imagery, conversion (e.g., converting isn't above into is below ), reading time, normative word frequencies, and other factors. Finally, it was shown that this theory is consistent with previous studies on sentence comprehension, sentence verification, concept verification, and other related phenomena.", "references": ["9b3ea2ad8a2d0f1cfc327578f2481d422176aca2", "e027ffaa9a27d1e5d8768307337f9a7d6fcb2a60", "3b64f177e4807a8886e291e9473d9e654da7c7f5", "f1cb4346fd84d15169c57828fd62de64ea2fa0fb", "ded89043fab5c464b3ba4e2fceae29252fd7f3d3", "a62f66001092db361059fcd679fab6410d9ad054", "af48b84edd1e3fe747c7ac37fb1db72308848649", "20844744c26a05aea47ec6d8481d78cf701807f5", "407cc606df934bf8efa935131fb81fada347d4f8"], "page_rank": 5.473453749315818e-05}, {"id": "7132bd0224917003f222f7edf0acf09916ba32ef", "title": "A Re-Examination of 'Normal Stress'", "authors": ["Susan F. Schmerling"], "date": 1974, "abstract": "'Normal stress'-a notion frequently encountered in phonology and, especially, syntax-has never been adequately defined. Linguists have apparently made a tacit assumption that the stress in citations elicited from an informant is the same as the stress used by a speaker making a minimum of special assumptions; but this is shown to be false. It is argued that 'normal stress', a notion inherited from structuralist linguistics, was required by assumptions inconsistent with those of the generative framework; and that this notion, even if it can be defined so as to be consistent with generative assumptions, is not a particularly useful one.*", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9101b86d5b2aeb0a8dc10a26b1d63979b76d6c65", "title": "Notes on transitivity and theme in English: Part 2", "authors": ["Michael Halliday"], "date": 1967, "abstract": "Transitivity, mood and theme . Part I of this paper (sections 1\u20133) was an attempt to sketch some of the principal syntactic options, having the clause as point of origin, that are available to the speaker of English for the representation of processes and relations, and of objects, persons &c. as participants in them. The term \u2018transitivity\u2019 was used as a general label for this area of grammatical selection. Part II (sections 4\u20137) is concerned with another range of grammatical options, also associated with the clause, for which \u2018theme\u2019 is being used as the cover term.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "9a6dbf1fe7459de909a4883f1b6987b6d91e7ef4", "title": "The Measurement of Human Channel Transmission Characteristics", "authors": ["W. A. Munson", "J. E. Karlin"], "date": 1954, "abstract": "This paper uses concepts in information theory and psychophysics to provide a basis for the quantitative measurement of certain communication properties of the human being. The responses of an observer in a threshold of hearing test were found to be similar to the transmission of a noise and a noise plus a signal over a physical circuit having a bistable element with a variable operating point. With this analogy, the results of threshold tests are used to compute the noise level of human circuits and the operating points of the hypothetical bistable mechanisms in human channels. The noise level is the practical lower limit for the transmission of information. The operating point usually changes systematically with signal level. For many observers it decreases as the signal level is lowered but again increases when the signal approaches the noise level.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ee5ba31e83dd3b092150c6b64d60f473e7ec97cd", "title": "Source and Receiver Behavior in the Use of a Criterion", "authors": ["James P. Egan", "Frank R. Clarke"], "date": 1956, "abstract": "A listener in an articulation test is confident of some of his responses and dubious about others. On the basis of this fact, it is reasonable to require that a receiver decide whether or not his response is correct. If he adopts various criteria from test to test, a receiver operating characteristic may be obtained which will be one way of describing quantitatively this type of behavior. The curve showing the confirming and rejecting behavior of the receiver is compared to that of the source using the same test materials and the same speech\u2010to\u2010noise ratio.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "7a226f6a2458326317eb2860d963be032d3db483", "title": "Operating Characteristics Determined by Binary Decisions and by Ratings", "authors": ["James P. Egan", "Arthur I. Schulman", "Gordon Z. Greenberg"], "date": 1959, "abstract": "With the theory of signal detectability as a framework, two psychophysical experiments were conducted in which each observation interval was well defined for the listener. Each interval contained noise, and it either did or did not (p=0.5) contain a signal (1000 cps, 0.5 sec in duration). In separate sessions of the first experiment, either the listener gave a yes\u2010no decision or he responded with a rating (1\u20134) after each observation interval. Operating characteristics were obtained with E/N0 equal to 15.8. It is clear from the data that the trained listener can perform as well when he adopts the multiple criteria necessary for the rating method as when he adopts the single criterion required by the binary\u2010decision procedure. In the second experiment, only the rating method was used to determine the relation between d\u2032 and E/N0. The resulting function, for d\u2032 \u2a7d 3.0, approximates straight line which passes through the origin and which has nearly the same slope as that obtained in other laboratories.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5556f9df34d6aab78d597c3635574672707e7e77", "title": "Multiple Observations of Signals in Noise", "authors": ["John A. Swets", "Elizabeth F. Shipley", "Molly J. McKey", "D. M. Green"], "date": 1959, "abstract": "The use of repeated presentations of a given signal event as an experimental technique in psychoacoustic studies provides information about several general properties of the hearing process. From the relationship between the gain in detectability that results from additional observations and the type of signal and noise employed, inferences can be made about: (1) the observer's ability to integrate over time, (2) the amount of noise generated by the auditory system, (3) the nature of the process of frequency analysis, and (4) the observer's mode of dealing with uncertainty as to signal frequency. The first set of experiments permitted five observations of each signal where the signal consisted of a pulsed tone, of known frequency, in noise. Both variable noise, i.e., noise that is statistically independent from one presentation to another, and constant noise, i.e., noise that is exactly the same on each of the five presentations, were used. With variable noise, the detectability index d\u2032 improves, as pred...", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "5fea928a857160739a718c434dff16a2621cac89", "title": "An analysis-of-variance model for the assessment of configural cue utilization in clinical judgment.", "authors": ["Paul J. Hoffman", "Paul Slovic", "Leonard G. Rorer"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"An analysis-of-variance model for the assessment of configural cue utilization in clinical judgment.\" by Paul J. Hoffman et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "e519aba2a6cfa0bb1497453a18f90e1e76a0267c", "title": "Differential feedback in two multiple-cue probability learning tasks.", "authors": ["Frederick J. Todd", "Kenneth R. Hammond"], "date": 1965, "abstract": "In a learning situation, does the principle \u201cthe more information, the better the performance\u201d always apply? Questioned in this article is the traditional use of outcome feedback (knowledge of results) as a means of providing more positive effects on learning. These authors suggest that it is the kind of information that counts in multiple-cue probability tasks.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "03adc941a986877551101c0ab78ed4abccdb8b3e", "title": "ANALYZING THE COMPONENTS OF CLINICAL INFERENCE.", "authors": ["Kenneth R. Hammond", "Carolyn J. Hursch", "Frederick J. Todd"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"ANALYZING THE COMPONENTS OF CLINICAL INFERENCE.\" by Kenneth R. Hammond et al.", "references": [], "page_rank": 0.00016420361247947453}, {"id": "29f22749925fb3fcf46241cf35c269b17df041d0", "title": "Definitions of d\u2032 and \u03b7 as Psychophysical Measures", "authors": ["Wilson P. Tanner", "Theodore G. Birdsall"], "date": 1958, "abstract": "Because studies employing d\u2032 and \u03b7 are based on the theory of signal detectability, the theory is reviewed in sufficient detail for the purposes of definition. The efficiency, \u03b7, is defined as the ratio of the energy required by an ideal receiver to the energy required by a receiver under study when the performance of the two is the same. The measure d\u2032 is that value of (2E/N0)12 necessary for the ideal receiver to match the performance of the receiver under study, where E is the energy of the signal, and N0 is the noise power per unit band width. The measure is extended to include the recognizability of two signals. Every set of signals is described by a Euclidean space in which distances are the square roots of the energy of the difference signal, (E\u0394)12. The unit of measure is the square root of one\u2010half of the noise power per unit band width (N0/2)12.", "references": ["abd419845a23bce62484bda6f671e77e2d46a383"], "page_rank": 4.926108374384236e-05}, {"id": "b01cf15a09678e17e5e4b84778deaf000f974d1a", "title": "Man as tranducer for probabilities in Bayesian command and control systems", "authors": ["Ward Edwards", "Lawrence D. Phillips"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"Man as tranducer for probabilities in Bayesian command and control systems\" by Ward Edwards et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "b8023e4e9ee611a4b726128cf784ac5aaa9943df", "title": "Feedback in a complex multiman-machine system.", "authors": ["Irwin L. Goldstein", "Jack F. Southard", "David A. Schum"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"Feedback in a complex multiman-machine system.\" by Irwin L. Goldstein et al.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "ea82b4e373aef2697a99ced7c75e96af95b6559a", "title": "Information Theory as the Basis for Thermostatics and Thermodynamics", "authors": ["Myron Tribus"], "date": 1961, "abstract": "Semantic Scholar extracted view of \"Information Theory as the Basis for Thermostatics and Thermodynamics\" by Myron Tribus", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "13a28912668b2c3e3a35cd6511896ce565c22c33", "title": "Operator Decision Performance Using Probabilistic Displays of Object Location", "authors": ["L. M. Herman", "George N. Ornstein", "Harry P. Bahrick"], "date": 1964, "abstract": "This study compares the relative effectiveness of a conventional display of object location with the effectiveness of various probabilistic displays of object location. Several display possibilities for presenting probabilistic information to a system operator are discussed, and an experiment is performed to test the potential usefulness of selected probabilistic display types in improving operator performance. Performance evaluation metrics include the degree of accuracy achieved by the operator in estimating probabilities of events and the expected value of his decisions within a simulated search-attack mission. The effectiveness of operators using a nonprobabilistic (conventional) display also is tested by these same metrics. In general, probabilistic information processing appears to offer an improved alternative to nonprobabilistic information processing. With premission training and definitively presented probability information, probabilistic displays can be interpreted accurately by the operator and employed effectively by him within a system.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "128b31e6882545d9b36b81c5f262630765a607ce", "title": "Subjective probability revisions under several cost-payoff arrangements", "authors": ["David A. Schum", "Irwin L. Goldstein", "William C. Howell", "Jack F. Southard"], "date": 1967, "abstract": "Abstract Human performance at a complex probabilistic inference task was evaluated in a simulated military threat-diagnosis context. Subjects' estimates of posterior probabilities were compared with theoretically optimal revisions calculated from a modification of Bayes' theorem. Cost-payoff arrangement was one variable. Subjects performing under a logarithmic cost-payoff arrangement came closest to being optimal in their estimates of posterior probabilities and were generally the least variable in their performance. Subjects who received a fixed amount for choosing correct hypotheses and nothing for choosing incorrect hypotheses (an all-or-nothing payoff) were the most cautious in their estimates of posterior probabilities. Subjects performing under a linear cost-payoff arrangement were the least optimal in their estimates of posterior probability and were extremely variable in their performance. In all three groups, costs and payoffs affected the size of confidence judgments (posterior probabilities) but not the ability to place highest posterior probabilities under true hypotheses. By this criterion, performance was identical in all groups. Amount of evidence to be evaluated was another variable. The experiment provided further evidence that subjects extract a smaller proportion of total diagnosticity from evidence as the amount of evidence increases. There was no evidence of interaction between payoff arrangement and amount of evidence being evaluated. Degree of prior uncertainty, a third variable, had little effect upon the degree of optimality of subjects' posterior probability estimates.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "1d8706c22e31da07e89550f3189f14eeef6f1592", "title": "Perception of Sequence in Auditory Events", "authors": ["Peter Ladefoged", "Donald E. Broadbent"], "date": 1960, "abstract": "A series of tape-recorded sentences were presented to various groups of listeners, totalling 164. During each sentence an extraneous sound was present on the recording, and the listener had to indicate the exact point in the sentence at which this sound occurred. It was found that errors were made which were large compared with the duration of a single speech sound; which suggests that the listener does not deal with each sound separately but rather with a group of sounds. Errors were reduced if the sentence consisted of a series of digits rather than an ordinary text, or if the listeners were trained in phonetics. Prior knowledge of the content of the sentence did not affect accuracy. The direction of error was usually to refer the extra sound to an early point, but it is affected by the relative position of the extra sound in the sentence. These results can be regarded as an extension, to the case where all stimuli are presented to the same sense, of classic results on prior entry.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "70be6108576637e8672bcd4e233c21382ed8bf82", "title": "Prior uncertainty and amount of diagnostic evidence as variables in a probabilistic inference task", "authors": ["David A. Schum"], "date": 1966, "abstract": "Abstract Subjects' probabilistic inference capabilities were evaluated in a simulated threat-diagnosis task. Subjects revised probabilities on the basis of equivocal, contradictory, and unreliable evidence. Revisions of subjective probability were compared with theoretical revisions calculated using a modification of Bayes' theorem. Subjects' revisions and the theoretical revisions showed a significantly increasing disparity as the amount of evidence to be processed was increased. The overall disparity between subjects' and theoretical revisions obtained when a uniform prior probability distribution was assumed did not differ significantly from the disparity obtained under an assumed nonuniform prior probability distribution. A general paradigm for complex inference task situations is discussed.", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "257fddda960373ee8683fff1402f73e5882af707", "title": "AN APPLICATION OF BAYES THEOREM AS A HYPOTHESIS-SELECTION AID IN A COMPLEX INFORMATION-PROCESSING SYSTEM.", "authors": ["Jack F. Southard", "Davis A. Schum", "George E. Briggs"], "date": 1964, "abstract": "Abstract : The first of a series of experiments investigating the value of automated hypothesis-evaluation aids in multimanmachine systems devoted to assessing or diagnosing threat is described. In the experiment, an eight-man team evaluated threats posed by a hypothetical aggressor. The team made these evaluations on the basis of intelligence information gathered during simulated reconnaissance overflights of aggressor's territory. The primary output of the threat-evaluation team was the commanding officer's posterior probabilities estimates as to aggressor's most likely hostile strategies. During half of the experimental trials, the commander had access to computer-produced posterior probabilities based upon a modification of the Bayes Theorem. The major experimental issue was whether or not these would aid the commander in his hypothesis evaluation. Also investigated was the effect of data-processing load upon system operation. Although some improvement in the posterior probabilities estimates resulted from the commander's having access to the hypothesis-evaluation aid and this improvement became more pronounced as system load increased, the main-order effect of access to the aid was not found to be statistically significant. (Author)", "references": [], "page_rank": 7.037297677691766e-05}, {"id": "cf213e9ac5f8445a1f1c8a37fd03cf6bb68c42de", "title": "Analysis of algorithm implementations", "authors": ["Gregory R. Ruth"], "date": 1974, "abstract": "Semantic Scholar extracted view of \"Analysis of algorithm implementations\" by Gregory R. Ruth", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "f5e796dc458978f1a01d1b1698636df9c5181934", "title": "Spatial Disposition of Axes in a Generalized Cylinder Representation of Objects That Do Not Encompass the Viewer", "authors": ["David C. Marr", "H. K. Hishihara"], "date": 1975, "abstract": "Abstract : It is proposed that the 3-D representation of an object is represented primarily by a stick-figure configuration, where each stick represents one or more axes in the object's generalized cylinder representation. The loosely hierarchical description of a stick figure is interpreted by a special-purpose processor, able to maintain two vector and the gravitational vertical relative to a Cartesian space-frame. It delivers information about the appearance of these vectors.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "29932930f93460eda82f07a71af641513ef5b606", "title": "Paramodulation and TP in first order theories with equality", "authors": ["George Augustus Robinson", "Larry Wos"], "date": 1969, "abstract": "Semantic Scholar extracted view of \"Paramodulation and TP in first order theories with equality\" by George Augustus Robinson et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "e79511511dd3475b60318645a6c1843ee1d53404", "title": "Immediate Memory: Data and Theory.", "authors": ["Lloyd R. Peterson"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Immediate Memory: Data and Theory.\" by Lloyd R. Peterson", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "1befa2c5d6977b8792190e13116760286159acb8", "title": "The association hierarchy as an indicator of extraexperimental interference", "authors": ["Edmund B. Coleman"], "date": 1963, "abstract": "Summary Continuous adjective associations with their latencies were collected to 80 nouns for 26 S s. Each S 's associations to a particular noun were considered to be his individual hierarchy of associations to that noun. From his own association hierarchies, two matched lists of noun-adjective paired associates were prepared for each S : one in which the adjectives were preceded by many stronger associates (high extraexperimental interference), and one in which the adjectives were preceded by fewer stronger associates (low extraexperimental interference). Paired associates subject to low extraexperimental interference were easier to learn and retain. Most of the words given as intrusions could be traced back to pre-experimental association hierarchies.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "e9db51a3abf5cbbc3dee1f69ec25f90a912d9ef5", "title": "Testing a linear relation among variances.", "authors": ["William Gemmell Cochran"], "date": 1951, "abstract": "Semantic Scholar extracted view of \"Testing a linear relation among variances.\" by William Gemmell Cochran", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "8d3c02f984634b77e4554cc7c439292c058f236f", "title": "A mechanical model for human attention and immediate memory.", "authors": ["Donald E. Broadbent"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"A mechanical model for human attention and immediate memory.\" by Donald E. Broadbent", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "bfcea67c38315211bf3b260f43c2bd99b89906ef", "title": "A Detection Method and Probabilistic Models for Assessing Information Processing from Brief Visual Displays.", "authors": ["William K. Estes"], "date": 1964, "abstract": "Semantic Scholar extracted view of \"A Detection Method and Probabilistic Models for Assessing Information Processing from Brief Visual Displays.\" by William K. Estes", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "6f44009207a4316cb33f34c26050adbad4688d8c", "title": "A comparison of paired-associate learning models having different acquisition and retention axioms \u2606", "authors": ["Richard C. Atkinson", "Edward J. Crothers"], "date": 1964, "abstract": "Abstract Several alternative interpretations of all-or-none processes for paired-associate learning and concept formation are examined. These models, along with three linear models, are applied to data from eight paired-associate learning experiments. The principal analyses involve goodness-of-fit tests for observed response sequences and conditional probabilities. The results favor a three-process model that postulates a distinction between long-term and short-term retention and allows for forgetting between successive presentations of the same stimulus item.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "dc7746457936702a66ce4aff1dae0f2700420028", "title": "Improving comprehensibility by shortening sentences.", "authors": ["Edmund B. Coleman"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"Improving comprehensibility by shortening sentences.\" by Edmund B. Coleman", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "02256c909265b66403b6cb08102174aa0b6ede1d", "title": "GPS, a program that simulates human thought", "authors": ["Allen Newell", "Herbert A. Simon"], "date": 1988, "abstract": "Abstract : Effort was directed toward showing that the techniques that have emerged for constructing sophisticated problem-solving programs also provide us with new, strong tools for constructing theories of human thinking. They allow us to merge the rigor and objectivity associated with behaviorism with the wealth of data and complex behavior associated with the gestalt movement. To this end their key feature is not that they provide a general framework for understanding problem-solving behavior (although they do that too), but that they finally reveal with great clarity that the free behavior of a reasonably intelligent human can be understood as the product of a complex but finite and determinate set of laws. Although we know this only for small fragments of behavior, the depth of the explanation is striking. (Author)", "references": ["e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "2edc8083073837564306943aab77d6dcc19d0cdc", "0012e189a719cc29c3fb1ac597286e9052c0ba82", "224f5b57675009dcf3b82452fba16dd922181038", "e50129abcbd13b2fe4196681590026f7ce7a6bb6", "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69", "97876c2195ad9c7a4be010d5cb4ba6af3547421c", "8b2d4d160571bc54dedfade5244d37e082618a53"], "page_rank": 6.157635467980295e-05}, {"id": "77c681cff8f2fd537d67fe807107c8957c4e8482", "title": "Complex analyses of variance: General problems", "authors": ["Bert F. Green", "J. W. Tukey"], "date": 1960, "abstract": "Problems in applying the analysis of variance are discussed. Emphasis is placed on using the technique to understand the data. The scale of the dependent variable is important for the analysis. Crossed and nested categories must be recognized. The error terms in the analysis depend on whether the classes of each independent variable are (1) all out of a few or (2) a few out of many. To simplify the analysis, mean squares should be aggregated with their error term when they are less than twice its size. An illustrative example is discussed in detail.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "3861c1e54a7be5f102f08aff6a49e09785157be0", "title": "Approximations to English: some comments on the method.", "authors": ["Edmund B. Coleman"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Approximations to English: some comments on the method.\" by Edmund B. Coleman", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "af4b3d01c44d7a404f64aa6b8da49cdbc8cb0759", "title": "An approximate distribution of estimates of variance components.", "authors": ["Franklin E. Satterthwaite"], "date": 1946, "abstract": "Semantic Scholar extracted view of \"An approximate distribution of estimates of variance components.\" by Franklin E. Satterthwaite", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "407cc606df934bf8efa935131fb81fada347d4f8", "title": "Recall for answers to \"conducive\" questions.", "authors": ["Samuel Fillenbaum"], "date": 1968, "abstract": "The phrasing of a question may reveal presumptions as to its answer. In the case of negative Yes-No questions a grammatical analysis suggests that neg + some questions are biased to a \" Yes \" reply, while neg + any questions are biased to a \" No\" reply. There are data indicating that, indeed, listeners predominantly believe that the questioner expects a positive answer in the first case, and a negative answer in the second case. The present study was carried out to discover whether such\" conducive \" presumptions in questions may systematically bias recall of their answers, and to determine the locus or loci of such an effect. It was found that recall was systematically biased by question format, the results indicating a significant effect of question format at time of recall test, and also suggesting an independent effect as a fiinction of question format at time of original presentation of the information. Some comments are made on the implications of these findings.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "944419aade31268db5b183f3468a1d9ef21ffa5c", "title": "Locking: a restriction of resolution", "authors": ["Robert S. Boyer"], "date": 1971, "abstract": "Semantic Scholar extracted view of \"Locking: a restriction of resolution\" by Robert S. Boyer", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "20844744c26a05aea47ec6d8481d78cf701807f5", "title": "DECODING A DECEPTIVE INSTRUCTION", "authors": ["Sheila J. Jones"], "date": 1966, "abstract": "Equivalent forms of instruction for a task requiring response to one of two equal classes of items were investigated. The class of items requiring response was defined positively (by inclusion) in one form of instruction and negatively (by exclusion) in the other. It was hypothesized that preference for positive rather than negatively defined classes would lead subjects to transform (decode) the exclusion form of instruction into the equivalent inclusion form. Performance of subjects first given the \u2018exclusion\u2019 instruction and afterwards transferred to the \u2018inclusion\u2019 form was compared with a control group, using the latter form throughout. Less than half the subjects decoded the \u2018exclusion\u2019 instruction, and the remainder performed the task at a significantly slower speed both before and after transfer. The error pattern of these subjects indicated that an instruction of the form \u2018Respond to all items except x and y\u2019 generates a spurious set to make a response to the items x and y. The implicit nature of the negative in the qualifier \u2018except\u2019 appeared to be a contributing factor to the deceptive effect of this type of instruction.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "3b64f177e4807a8886e291e9473d9e654da7c7f5", "title": "Semantics in the perception of verticality.", "authors": ["William G. Chase", "Homer H. Clark"], "date": 1971, "abstract": "Seymour has shown recently that people take less time to judge that the word above correctly describes the spatial position of a small circle drawn above a large reference square than they do for the word below and the circle below the square. Seymour has attributed this asymmetry to the tendency for people to invariably scan a picture from top to bottom. In the present study, the first experiment confirms Seymour's results, but the next three demonstrate that the asymmetry Seymour found cannot be accounted for by an attentional-scanning process. Instead, it is proposed that people interpret the words above and below as abstract symbols at a first stage of processing, interpret the pictures above and below as abstract symbols at an independent second stage, and compare these two sets of symbols at an independent third stage. In support of this model, the results show, for example, that above is interpreted about 80 msec. faster than below at the first stage quite independently of what happens at the second and third stages. The asymmetry Seymour found is therefore attributable to the difference in the interpretation times of above and below at the first stage of processing.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "af48b84edd1e3fe747c7ac37fb1db72308848649", "title": "SPACE, TIME, SEMANTICS, AND THE CHILD", "authors": ["Herbert H. Clark"], "date": 1973, "abstract": "Publisher Summary This chapter presents the thesis that the child acquires English spatial expressions by learning how to apply them to the child's prior knowledge about space and that the child acquires English temporal expressions in turn by extending the spatial terms in a metaphor about time. The main evidence for this thesis is the strong correspondence between the properties of the spatial terms and the properties of man's innate perceptual apparatus, and between English spatial and temporal expressions. The correspondence is so strong that it simply could not be coincidental, and it, therefore, needs explanation. Time, for example, is not just expressed with an occasional spatial simile, but rather it is based on a thoroughly systematic spatial metaphor, suggesting a complete cognitive system that space and time expressions have in common. The chapter outlines the thesis, its evidence, and what it could mean for the acquisition of English. This theory argues that grammatical relations are fundamentally locative in nature, and they are, therefore, derived ultimately from notions of location.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "ded89043fab5c464b3ba4e2fceae29252fd7f3d3", "title": "SPATIAL AND VERBAL COMPONENTS OF THE ACT OF RECALL *", "authors": ["LEE R. BROOKSf"], "date": 2005, "abstract": "Research presented in this paper shows that while a person is recalling a line diagram he can more readily signal information about that diagram by speaking than by spatially monitored output (e.g., pointing to correct items in a column of symbols). When recalling a sentence, he can more readily signal information about that sentence by spatially monitored output than by speaking. These results suggest that spatial and verbal information is recalled and processed in a modality-specific manner. Recall of verbal information is most readily disrupted by concurrent vocal activity; recall of spatial information is most readily disrupted by concurrent spatially monitored activity. This differential conflict occurs even though the concurrent activity is a recoding of the information that is being recalled. A PEBSON is asked to describe from memory a diagram such as a map or floor plan, he is likely to say that he generated a mental representation of the diagram and then derived his description from that. Even in the absence of vivid mental imagery, there is a clear impression that some underlying visual or spatial process is involved in this type of performance. In contrast, the process involved in recalling a specific sentence seems to have more to do with speech than with vision or spatial movements. If there is a visualized component in sentence recall, it appears to be less crucial than in the recall of spatial relationships. This paper will present performance data to support the subjectively plausible notion that verbal and spatial information are handled in distinct, modality-specific manners. These data are obtained from experiments which induce conflict between overt responding and the act of recall. Subjects are asked to recall memorized material (sentences or line diagrams) and to simultaneously signal information about that material. If making signals in one modality (for example, speaking) uniquely disrupts recall of one of these types of material, then it will be assumed that the recall of that material is accomplished in a modalityspecific manner. If a different modality of response (for example, pointing to a sequence of symbols) provides the strongest conflict when the \u2022This research was supported by N.R.C. grant A.P.A. 210. I would like to thank Mrs. Wilma Clarke and Miss Rosemary Squire for their assistance in running subjects and tabulating and analyzing the data of these experiments. {Present address: Department of Psychology, McMaster University, Hamilton, Ontario.", "references": ["f7ac83c788b6c4562db4d89813a007c58d8541da", "769e02fe5aacd5876dc16f384e631f89a885c361", "9ddc16ec7124c915ca0cd72704fd415b5629590f", "e2c1a780fe4a9e4d9258ba8cf8b07067b90bdf1d", "c5000de14f9b80bb0ea12e4c70262a84e0cc2843"], "page_rank": 5.473453749315818e-05}, {"id": "354a7c13bfce5257b4b26debf38188c1300b0a54", "title": "Elementary Logic", "authors": ["Benson Mates"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Elementary Logic\" by Benson Mates", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b1958feb50eb13773d2f455980ddc8ab49e5417f", "title": "Introduction To Logic", "authors": ["Patrick Suppes"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"Introduction To Logic\" by Patrick Suppes", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "title": "Some Rules For The Automatic Synthesis Of Programs", "authors": ["C. Cordell Green", "David R. Barstow"], "date": 1975, "abstract": "A set of rules (or facts) about program synthesis is presented. The rules are about the process of programming, and are sutficient for the synthesis of an insertion sort program. The use of the rules to write a short LISP program is described. Taken together, the rules are an embodiment of a detailed theory which explains one small part of the programming process. The size of the set of rules suggests the complexity of the process of writing programs and indicates that much work will be required to codify significant amounts of programming knowledge as a step toward the development of program-understanding systems.", "references": [], "page_rank": 0.00016009852216748767}, {"id": "df12f998fa2160c2b04fce5aea143052a6b1f837", "title": "Overview of a Linguistic Theory of Design.", "authors": ["Mark L. Miller", "Ira P. Goldstein"], "date": 1977, "abstract": "Abstract : SPADE is a theory of the design of computer programs in terms of complementary planning and debugging processes. An overview of the author's recent research on this theory is provided. SPADE borrows tools from computational linguistics -- grammars, augmented transition networks (ATN's), chart-based parsers -- to formalize planning and debugging. The theory has been applied to parsing protocols of programming episodes, constructing a grammar-based editor in which programs are written in a structured fashion. (Author)", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "d2b40f97365550c79e7fb670d99190e0a81a697b", "title": "Just noticeable differences for segment duration in natural speech.", "authors": ["Arlen W. Huggins"], "date": 1969, "abstract": "A threshold\u2010tracking method was used to measure both the incremental and the decremental just noticeable differences for segment duration in naturally spoken sentences. The measurements were made for a /p/ in five different contexts, including two that were word initial, two that were word medial, and one that was word final; for /\u222b,m/, and /l/ all in initial prestress position; and for a stressed vowel /\u0254/. The results show that subjects are much more sensitive to changes in vowel duration than to changes in consonant duration, and that changes in segment duration may have several different perceptual effects, including changes in perceived stress and perceived rhythm. When subjects based their judgments on changes in perceived stress or rhythm, they were usually able to detect smaller changes in duration than when they attended to other aspects of the stimuli.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "8c61f75e7e49a3b7a5fa85f7a92b329b581b2329", "title": "Computer-Assisted Instruction: Stanford's 1965-66 Arithmetic Program.", "authors": ["Patrick Suppes"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"Computer-Assisted Instruction: Stanford's 1965-66 Arithmetic Program.\" by Patrick Suppes", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "abd419845a23bce62484bda6f671e77e2d46a383", "title": "The human use of information-II: Signal detection for the case of an unknown signal parameter", "authors": ["Wilson P. Tanner", "R. Z. Norman"], "date": 1954, "abstract": "Two specific cases of signal detection involving uncertainty in the frequency of a sound signal are compared with the case of the signal-known-exactly. In the first case the signal is either of two known frequencies; in the second case the signal is any frequency within a given range. It is suggested that detection behavior that is optimal for the three cases requires a deal mechanism: a combination of a wide-open receiver end a panoramic receiver. Evidence is presented that supports the existence of such a mechanism. Estimates of the bandwidth end seen-rate of the receiver are included.", "references": [], "page_rank": 0.0004926108374384236}, {"id": "a65b444660374ba6c1a7386bc84a1c1ca1beeae0", "title": "The human use of information-III: Decision-making in signal detection and recognition situations involving multiple alternatives", "authors": ["John A. Swets", "Theodore G. Birdsall"], "date": 1956, "abstract": "A general theory of signal detectability, constructed after the model provided by decision theory, is applied to the performance of the human observer faced with the problem of choosing among multiple signal alternatives on the basis of a fixed, finite observation interval. An extension of the theory, previously reported, of deciding among two alternatives, is developed in detail, permitting the treatment of the two simple cases involving multiple alternatives that are studied experimentally. In both cases, a priori probabilities are assigned to the occurrence of the relevant signal alternatives, and values are assigned to the possible decision outcomes. This assignment permits definition of the expected value of a decision, and specifies as the optimum decision criteria those that maximize expected value. The experiments are primarily concerned with determining the ability of the human observer to successively establish optimum decision criteria in accordance with changes in the a priori probabilities and risk functions. The experimental results are portrayed in the form of comparisons of the payoff obtained by the observer and the theoretically maximum payoff attainable, and comparisons of the response-frequency tables generated by the theory and by the observer. The results indicate that a highly simplified theory is adequate for prediction of the obtained payoff and response-frequency tables to within a few percent. They also indicate the fairly large extent to which intelligence may influence a sensory process usually assumed to involve fixed parameters.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "ba7d4181ccec06af4bc778952be3c38e6e197cd5", "title": "Modern statistical approaches to reception in communication theory", "authors": ["David van Meter", "David Middleton"], "date": 1954, "abstract": "When reception in the theory of communication is recognized as a problem in statistical inference, system design and system analysis appear as the counterparts of designing and evaluating statistical tests. This paper discusses the optimum properties of designs based on statistical decision theory from the risk point of view, and from that of information theory. Connections between risk and information loss are established, which result in a unified theory of system design. This includes Minimax methods capable in principle of handling all degrees of a priori knowledge of signal and noise statistics, new methods for comparing actual and ideal systems for the same purpose, and new interpretations of previously used formulations as special cases of the more general theory. Both detection and extraction of signals in noise are considered, the former as a problem of testing statistical hypotheses and the latter as one of estimating parameters. Formulation of the general reception problem as a decision operation is followed by a summary of statistical decision theory from the risk point of view, with some examples of Bayes and Minimax tests and optimum classes of decision rules. Applications to detection show the optimum nature of likelihood ratio receivers as a class, and indicate methods for defining the minimum detectable signal and for comparing system performance. As an illustration, curves of Bayes and Minimax risk are given for detection of a pulsed carrier in noise. Applications to extraction show the nature of optimum extraction and the roles of the mean square error and maximum likelihood criteria from the more general point of view of risk theory. Conditions under which information loss is an extremum in detection and extraction are established, and information loss itself as a criterion of performance is compared with that of the risk measure.", "references": [], "page_rank": 4.926108374384236e-05}, {"id": "72434253ade951fede487502079aa2423c83bdb3", "title": "Short-term recall of paired-associates as a function of the number of interpolated pairs", "authors": ["John W. Brelsford", "Jr. L. Keller", "Richard M. Shiffrin", "Richard C. Atkinson"], "date": 1966, "abstract": "A continuous technique for studying short-term memory of paired-associates was used. For 4, 6, or 8 stimuli, recall was found to be a decreasing function of the number of pairs of items interpolated between study and test on a given item. Reliable differences between the functions for 4, 6, and 8 stimuli were found.", "references": [], "page_rank": 9.852216748768472e-05}, {"id": "9537d2938a7c4e3b4ca781f95f256d6990fabd25", "title": "Effects of list length on short-term memory", "authors": ["James L. Phillips", "Richard M. Shiffrin", "Richard C. Atkinson"], "date": 1967, "abstract": "A memory experiment has been performed with the following procedure. On each trial of the experiment a display of items was presented in a serial order. At the conclusion of each display S was tested for recall on one of the items. The length of the displays varied from 3 to 14 items. Plotted as serial position curves, the results showed an S-shaped recency effect and a smaller primacy effect. A specific version of a memory model formulated by Atkinson and Shiffrin (1965) was presented and applied to the data. The model assumes two memory states: a temporary storage state, called the buffer, from which retrieval is perfect; and a long-term storage state called LTS, from which retrieval is imperfect. Both response data and confidence ratings were accurately fit by the model.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "ade476bc01dcb23b46b545360b8414b207668542", "title": "Keeping track of variables that have few or many states.", "authors": ["Douwe B. Yntema", "G E Mueser"], "date": 1962, "abstract": "Semantic Scholar extracted view of \"Keeping track of variables that have few or many states.\" by Douwe B. Yntema et al.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "44099bc1bbd7264146e9ba4391161af178520e2e", "title": "An introduction to mathematical learning theory", "authors": ["Richard C. Atkinson", "Gordon H. Bower", "Edward J. Crothers"], "date": 1967, "abstract": "Semantic Scholar extracted view of \"An introduction to mathematical learning theory\" by Richard C. Atkinson et al.", "references": [], "page_rank": 0.0006157635467980295}, {"id": "6996541ec19390b224b870cadcc99612a1de4f6b", "title": "A technique for the study of steady-state short term memory", "authors": ["Leonard R. Katz"], "date": 1966, "abstract": "Steady-state STM was studied by a method in which S was required to keep track of the randomly changing response member of each of 5 stimulus words. On each of 220 consecutive presentations, S had to recall the response last paired with a given stimulus and then had to learn a (possily) new response to the same stimulus. A measure of S\u2019s STM was his proportion of correct recalls as a function of the number of items intervening between successive appearances of a given item. Results suggest that the method gives a stable measure of STM. Specifically, proactive effects appeared to be constant throughout the sequence of presentations.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "6e22d253b24b405b7331762b6d75dd619f2de887", "title": "Acoustic similarity and retroactive interference in short-term memory", "authors": ["Wayne A. Wickelgren"], "date": 1965, "abstract": "Summary Short-term memory for a list of four letters, followed by a list of eight letters that the S s copied as they were presented, followed by immediate recall of the original four-letter list, was shown to be a function of the acoustic similarity of the intervening list to the original list. An interfering list whose letters have similar pronunciation to the letters in the original list produces greater RI than an interfering list whose letters have a very different pronunciation from the letters in the original list. An interfering list composed of items identical to items in the original list, but in a different order, tends to produce less RI in the recall of items and more RI in the recall of the correct position of these items than an interfering list composed of similar items. These findings for STM are completely consistent with analogous studies of RI as a function of similarity in LTM.", "references": [], "page_rank": 6.157635467980295e-05}, {"id": "085ff74e2c21bcd15536b1a790eaa98b5b6410c3", "title": "A variation on the minimum chi-square test \u2606", "authors": ["Paul W. Holland"], "date": 1967, "abstract": "The \u201csum of chi-squares\u201d technique suggested by several authors attempting to assess the fit of certain learning models to data (Atkinson, Bower and Crothers, 1965, Calfee and Atkinson, 1965) is examined in a general framework and its asymptotic distribution theory derived. The fit statistic used is seen not to have an asymptotic chi-square distribution except under very special circumstances. Its asymptotic mean and variance are found both when no parameters are estimated and when s parameters are estimated by minimizing the fit statistic. Bounds are found for the asymptotic mean and variance of this statistic. Some partial results are given regarding the direction and size of the error made in assuming the fit statistic has an approximate chi-square distribution with the \u201cright\u201d degrees of freedom for calculating significance levels. The estimate of the parameter found by minimizing the fit statistic is examined and compared to some of its possible competitors. Finally these theoretical results are applied to some data given by Calfee and Atkinson (1965).", "references": ["6fa20a427d0a373869b9afa7b5dd29dbc59875d0", "44099bc1bbd7264146e9ba4391161af178520e2e", "4099bebbb01fa4b26b735230c2cf73d7ee318a56"], "page_rank": 6.157635467980295e-05}, {"id": "d503c7cd71651a6b48651b1895f2a6401c6dd65d", "title": "Stimulus Selection in Verbal Learning.", "authors": ["Benton J. Underwood"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Stimulus Selection in Verbal Learning.\" by Benton J. Underwood", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "b3f4b10ec062beef21a876e910476c8560122f3a", "title": "A Course in Modern Linguistics", "authors": ["Charles Francis Hockett"], "date": 1958, "abstract": "Semantic Scholar extracted view of \"A Course in Modern Linguistics\" by Charles Francis Hockett", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "07de6e8c43b171849c938b18739d18ad71f4209c", "title": "Order of Report, Ear Asymmetry and Handedness in Dichotic Listening", "authors": ["Paul Satz", "Karl E. Achenbach", "Evan Pattishall", "Eileen B. Fennell"], "date": 1965, "abstract": "When different numbers were presented simultaneously to each ear, Ss tended to report all the numbers from one channel before reporting any from the other (Broadbent effect). The results contradicted Kimura's findings on the low incidence of the ear order effect, but showed that when verbal input is increased, the frequency of the effect and over-all accuracy decrease. Kimura's ear asymmetry hypothesis, however, was shown to be true for varying amounts of verbal input. The fact that the ear asymmetry occurred for recall on the delayed \u201cstorage\u201d half-spans suggested a rapproachement between both Broadbent and Kimura. The striking relationship between handedness and ear asymmetry was discussed in relation to speech-brainedness.", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "c12bfb7ce1590055460d1ce4a5cda2756a8272ba", "title": "Spelling errors and the serial-position effect.", "authors": ["Beverly Y. Kooi", "Richard E. Schutz", "Robert Louis Baker"], "date": 1965, "abstract": "Semantic Scholar extracted view of \"Spelling errors and the serial-position effect.\" by Beverly Y. Kooi et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "006ae42f9e23ae43afff97082a484de2443c616f", "title": "Plan Verification in a Programmer's Apprentice", "authors": ["Howard E. Shrobe"], "date": 1978, "abstract": "In this proposal we describe an interactive programming environment to be used as a tool by the expert programmer in the process of program design and maintenance. This environment (called the. Programmer's Apprentice) will be capable of u.nderstandin., explaining and reaso-ing about the behavior. of real-world :programs, with particular. emphasis on LISP programs involving side effects on complex data-structures. In achieving such a system, our pivotal theoretical committment will be to a. view of programs as engineered devices whose analysis can be carried-out at many level of abstraction . The analysis of a program will lead to a set of logical dependencies between modules which explains how and why modules interact to achieve an overall intention. Such a network of dependencies is a teleological structure which we call a plan; the process of elucidating such a plan stucture and showing that it is coherent and\" that. it achieves its overall intended. behavior we call plan verification. Our research will be concerned with the design of a plan. verification programn called REASON. Given a descriiption of the data flow between modules (specified at any level of abstraction), REASON will determine whether or not these modules cooperate to. achieve their intended net behavior. In so doing, REASON elucidates the plan. structure which, in turn, is used as the pivotal data structure in the pertubation analysis necessary for programr evolution, explanation and debugging. ,This approach to program verification will be sharply contrasted with the traditional Floyd.-Hoare systems. which overly restrict themselves to surface features. of the programming language. More similar in philosophy is. the evolving methodology of languages like CLU or ALPH'ARD which, stress conceptual layering in program structure and therefore have a verification methodology more like our own. Finally, a methodology of .program design will be explored in which man and machine interact in the .formation of -a verified plan at an appropriate level of abstraction. It is hoped that examination of this process will shed light on the theory of \u2022automatic design, thereby aiding in the future development of rich automatic programming systems. The~sis Proposal", "references": ["bf15ce3d1575d124527496cb249dc1249eee0acb", "d22000e27a064baa027fa685353abb216b0d06da", "ebbec3500ddff64f04a46b0a9a3d48b6e92ba2b5", "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "5a19db7d827f623143e92bca8c082de5c014d59c", "7c5144af9595b8ff33347906eb28a7ed9f7f55e3", "23533725173200046d037c055779e190febeb4cd", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "16e813d1aa3684fa6b4b5b25cf2441f6e13dd3f1", "91a3d5a53961587bb6a5c1fc9fbe8b078013ba44"], "page_rank": 4.926108374384236e-05}, {"id": "3e1b99ffd71eb301f4892f4688ed5944e9388be3", "title": "\"Cortical\" hearing test and cerebral dominance.", "authors": ["Carlo Calearo", "Antonino Roberto Antonelli"], "date": 1963, "abstract": "Adoption of sensibilized speech tests for diagnosis of lesions affecting the central levels of the acoustic paths reopens discussion of the classical views on hemispheric dominance considered as a regulator, both of the impressive and expressive phases of speech.The results of the investigations performed by the authors definitely show that sensorial auditory perception lacks lateralization of motor expression, as cortical integration of the auditory message is a symmetric and equivalent function on both sides. Actually, the intelligibility curves for distorted and interrupted speech in normal subjects show that no significant difference exists between the two sides. When the same tests are repeated in subjects with pathology of the temporal cortex, the contralateral discrimination deficit is of the same degree both for localizations in the right and in the left temporal cortex.Thus, on the basis of bilateral equivalence of axonic connections (as suggested by Penfield & Roberts) and on the impossibility o...", "references": [], "page_rank": 5.473453749315818e-05}, {"id": "fbd7345a8f96da9fc6cd7b1a7bf301ed620e262b", "title": "Concerning imagery.", "authors": ["Donald Olding Hebb"], "date": 1968, "abstract": "Semantic Scholar extracted view of \"Concerning imagery.\" by Donald Olding Hebb", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "8b2d4d160571bc54dedfade5244d37e082618a53", "title": "Empirical explorations with the logic theory machine", "authors": ["Allen Newell", "J. C. Shaw", "Hans-Arno. Auteur ou responsable intellectuel Simon"], "date": 1963, "abstract": "Semantic Scholar extracted view of \"Empirical explorations with the logic theory machine\" by Allen Newell et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "cd7385c514f137948c3075f2c740006f056615ef", "title": "A significance test for one parameter isosensitivity functions", "authors": ["Vivian Gourevitch", "Eugene Galanter"], "date": 1967, "abstract": "A large-sample test for the significance of the difference between two detection data points is developed based upon the assumptions of a one-parameter signal detectability model. In essence, the null hypothesis tested is that two observed data points belong to the samed' function.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "49d84484014b2b1d50d99e96ff3fb450fe5ccf9e", "title": "An Experimental Study of Imagination", "authors": ["Cheves West Perky}"], "date": 1940, "abstract": "Semantic Scholar extracted view of \"An Experimental Study of Imagination\" by Cheves West Perky", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "224f5b57675009dcf3b82452fba16dd922181038", "title": "A Study of Thinking", "authors": ["Fred W. Householder", "Jerome S. Bruner", "Jacqueline J. Goodnow", "George A. Austin"], "date": 1957, "abstract": "Semantic Scholar extracted view of \"A Study of Thinking\" by Fred W. Householder et al.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "df8da64b592a223c02a05286eb0795621eda511d", "title": "Perception and Communication", "authors": ["J. A. Deutsch"], "date": 1958, "abstract": "Perception and CommunicationBy D. E. Broadbent. Pp. v + 338. (London and New York: Pergamon Press, 1958.) 55s. net.", "references": [], "page_rank": 8.210180623973726e-05}, {"id": "f7ac83c788b6c4562db4d89813a007c58d8541da", "title": "The Suppression of Visualization by Reading", "authors": ["L R Brooks"], "date": 1967, "abstract": "Four experiments are described which demonstrate a conflict between reading verbal messages and imagining the spatial relations described by those messages. Listening to the same messages did not produce comparable interference with visualization. The conflict between reading and visualizing was obtained even when the subject previously had seen the referent of the message. In contrast, when the subject was induced to treat the messages as rote strings of words instead of visualizing their referents, reading was a more effective means of presentation than was listening. Two interpretations of these results were proposed. (a) Visualization and reading compete of the use of neural pathways specialized for visual perception. (b) The process of reading hinders the conversion of input material into any non-verbal form; that is, reading forces the subject to deal with information in a more exclusively verbal form than does listening. It was suggested that regardless of interpretation this method provides a means of investigating the internal processes underlying concrete verbal reference.", "references": [], "page_rank": 0.00032840722495894905}, {"id": "faf2933887410cb110cd7c68487cb0dd81d363ee", "title": "Signal detection theory and psychophysics", "authors": ["D. M. Green", "John A. Swets"], "date": 1966, "abstract": "418,865. Time-pieces. HARWOOD, J., 120, Pinner Road, Harrow, Middlesex. Oct. 23, 1933, No. 29295. [Class 139.] Winding-apparatus.-In a time-piece, which is self-wound by an oscillating weight, the weight oscillates in or parallel to a vertical plane through the axis of the indicating hands and is combined with a toothless winding mechanism. Rigid with the winding arbor A of the mainspring A is a disc B, the edge of which coacts with two studs G, G and a lever I on a trefoil plate E. Lever I is connected by link O to an arm 0 rigid with the weight W which oscillates about pivot R against spring X. When the link O is raised, the nose of lever I binds on the edge of disc B and the latter is moved counter-clockwise, Fig. 2, with plate E to wind the mainspring. A similar trefoil plate D with pins F, F and lever H prevents retrograde movement of disc B in a clockwise direction. V, V are buffer springs. To prevent overwinding the link O may be made in two relatively sliding parts which are joined together by an S-spring slightly weaker than the fully wound mainspring. A mark on the weight may be visible through a hole in the dial to indicate whether the weight is oscillating freely. The movement of the timepiece, or the timepiece as a whole, may be mounted for oscillation so as to function as the winding weight.", "references": [], "page_rank": 8.210180623973726e-05}]